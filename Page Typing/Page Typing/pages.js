const dataA = {
  "page1": "When you begin a journey, it's a good idea to have a mental map of the terrain you'll be passing through. The same is true for an intellectual journey, such as learning to write computer programs.\u00aeIn this case, you'll need to know the basics of what computers are and how they work. You'll want to have some idea of what a computer program is and how one is created. Since you will be writing programs in the Java programming language,\u00aeyou'll want to know something about that language in particular and about the modern, networked computing environment for which Java is designed. As you read this chapter, don't worry if you can't understand everything in detail. (In fact, it would be impossible for you to learn all the details from the brief expositions in this chapter.)\u00aeConcentrate on learning enough about the big ideas to orient yourself, in preparation for the rest of the book. Most of what is covered in this chapter will be covered in much greater detail later in the book. A computer is a complex system consisting of many different components. But at the heart or the brain, if you want of the computer is a single component that does the actual computing.\u00aeThis is the Central Processing Unit, or CPU. In a modern desktop computer, the CPU is a single \"chip\" on the order of one square inch in size. The job of the CPU is to execute programs. A program is simply a list of unambiguous instructions meant to be followed mechanically by a computer.",
  "page2": "Data on the disk has to be copied into memory before it can be accessed. Unfortunately, on the scale of speed at which the CPU operates, the disk drive is extremely slow. When the CPU needs data\u00aefrom the disk, it sends a signal to the disk drive telling it to locate the data and get it ready. (This signal is sent synchronously, under the control of a regular program.) Then, instead of just waiting the long and unpredictable amount\u00aeof time that the disk drive will take to do this, the CPU goes on with some other task. When the disk drive has the data ready, it sends an interrupt signal to the CPU. The interrupt handler can then read the requested data. Now, you might have noticed that all this only makes sense if the CPU actually has several tasks to perform. If it\u00aehas nothing better to do, it might as well spend its time polling for input or waiting for disk drive operations to complete. All modern computers use multitasking to perform several tasks at once. Some computers can be used by several people at once. Since the CPU is so fast, it can quickly switch its attention from one user to another, devoting a fraction of a second to each user in turn.\u00aeThis application of multitasking is called timesharing. But a modern personal computer with just a single user also uses multitasking. For example, the user might be typing a paper while a clock is continuously displaying the time and a file is being downloaded over the network.\u00aeEach of the individual tasks that the CPU is working on is called a thread. (Or a process; there are technical differences between threads and processes, but they are not important here.) At any given time, only one thread can actually be executed by a CPU.",
  "page3": "Again, you should understand that this is a purely mechanical process: A device signals an interrupt simply by turning on a wire. The CPU is built so that when that wire is turned on, the CPU save\u00aes enough information about what it is currently doing so that it can return to the same state later. This information consists of the contents of important internal registers such as the program counter. Then the CPU jumps to some predetermined\u00aememory location and begins executing the instructions stored there. Those instructions make up an interrupt handler that does the processing necessary to respond to the interrupt. (This interrupt handler is part of the device driver software for the device that signaled the interrupt.) At the end of the interrupt handler is an instruction\u00aethat tells the CPU to jump back to what it was doing; it does that by restoring its previously saved state. Interrupts allow the CPU to deal with asynchronous events. In the regular fetch-and execute cycle, things happen in a predetermined order; everything that happens is \"synchronized\" with everything else. Interrupts make it possible for the CPU to deal efficiently with events\u00aethat happen \"asynchronously,\" that is, at unpredictable times. As another example of how interrupts are used, consider what happens when the CPU need to access data that is stored on the hard disk. The CPU can access data directly only if it is in main memory. Data\u00aeon the disk has to be copied into memory before it can be accessed. Unfortunately, on the scale of speed at which the CPU operates, the disk drive is extremely slow. When the CPU needs data from the disk, it sends a signal to the disk drive telling it to locate the data and get it ready. (This signal is sent synchronously, under the control of a regular program.)\u00aeThen, instead of just waiting the long and unpredictable amount of time that the disk drive will take to do this, the CPU goes on with some other task. When the disk drive has the data ready, it sends an interrupt signal to the CPU. The interrupt handler can then read the requested data. Now, you might have noticed that all this only makes sense if the CPU actually has several tasks to perform. If it has nothing better to do, it might as well spend its time polling for input or waiting for disk drive operations to complete. All modern computers use multitasking to perform several tasks at once.",
  "page4": "The Java Virtual Machine Machine language consists of very simple instructions that can be executed directly by the CPU of a computer. Almost all programs, though, are written in high-level\u00aeprogramming languages such as Java, Pascal, or C++. A program written in a high-level language cannot be run directly on any computer. First, it has to be translated into machine language. This translation can be done by a program called a compi\u00aeler. A compiler takes a high-level-language program and translates it into an executable machine-language program. Once the translation is done, the machine-language program can be run any number of times, but of course it can only be run on one type of computer (since each type of computer has its own individual machine language). If the\u00aeprogram is to run on another type of computer it has to be re-translated, using a different compiler, into the appropriate machine language. There is an alternative to compiling a high-level language program. Instead of using a compiler, which translates the program all at once, you can use an interpreter, which translates it instruction-by-instruction, as necessary. An interpreter is a program\u00aethat acts much like a CPU, with a kind of fetch-and-execute cycle. In order to execute a program, the interpreter runs in a loop in which it repeatedly reads one instruction from the program, decides what is necessary to carry out that instruction, and then performs the\u00aeappropriate machine-language commands to do so. one use of interpreters is to execute high-level language programs. For example, the programming language Lisp is usually executed by an interpreter rather than a compiler. However, interpreters have another purpose: they can let you use a machine-language program meant for one type of computer on a completely different type of computer. For example, there is a program called \"Virtual PC\" that runs on Macintosh computers. Virtual PC is an interpreter that executes machine-language programs written for IBM-PC-clone computers. ",
  "page5": "Programs must be designed. No one can just sit down at the computer and compose a program of any complexity. The discipline called software engineering is concerned with the construction of correct\u00aeworking, well-written programs. The software engineer tends to use accepted and proven methods for analyzing the problem to be solved and for designing a program to solve that problem. During the 1970s and into the 80s, the primary soft\u00aeware engineering methodology was structured programming. The structured programming approach to program design was based on the following advice: To solve a large problem, break the problem into several pieces and work on each piece separately; to solve each piece, treat it as a new problem which can itself be broken down into smaller problems\u00aeeventually, you will work your way down to problems that can be solved directly, without further decomposition. This approach is called top-down programming. There is nothing wrong with top-down programming. It is a valuable and often-used approach to problem-solving. However, it is incomplete. For one thing, it deals almost entirely with producing the instructions necessary to solve a\u00aeproblem. But as time went on, people realized that the design of the data structures for a program was as least as important as the design of subroutines and control structures. Top-down programming doesn't give adequate consideration to the data that the program manipulates.\u00aeAnother problem with strict top-down programming is that it makes it difficult to reuse work done for other projects. By starting with a particular problem and subdividing it into convenient pieces, top-down programming tends to produce a design that is unique to that problem. It is unlikely that you will be able to take a large chunk of programming from another program and fit it into your project, at least not without extensive modification.",
  "page6": "For example, consider a drawing program that lets the user draw lines, rectangles, ovals, polygons, and curves on the screen. In the program, each visible object on the screen could be represented\u00aeby a software object in the program. There would be five classes of objects in the program, one for each type of visible object that can be drawn. All the lines would belong to one class, all the rectangles to another class, and so on. The\u00aese classes are obviously related; all of them represent \"drawable objects.\" They would, for example, all presumably be able to respond to a \"draw yourself\" message. Another level of grouping, based on the data to represent each type of object, is less obvious, but would be very useful in a program: We can group polygons\u00aeand curves together as \"multipoint objects,\" while lines, rectangles, and ovals are \"two-point objects.\" (A line is determined by its endpoints, a rectangle by two of its corners, and an oval by two corners of the rectangle that contains it.) We could diagram these relationships as follows DrawableObject, MultipointObject, and TwoPointObject would be classes in the progr\u00aeam. MultipointObject and TwoPointObject would be subclasses of DrawableObject. The class Line would be a subclass of TwoPointObject and (indirectly) of DrawableObject. A subclass of a class is said to inherit the properties of that class. The subclass can add to its inheritance\u00aeand it can even \"override\" part of that inheritance (by defining a different response to some method). Nevertheless, lines, rectangles, and so on are drawable objects, and the class DrawableObject expresses this relationship. Inheritance is a powerful means for organizing a program. It is also related to the problem of reusing software components.",
  "page7": "The use of the term \"message\" here is deliberate. Messages, as you saw in the previous section, are sent to objects. In fact, Java GUI components are implemented as objects. Java includes\u00aemany predefined classes that represent various types of GUI components. Some of these classes are subclasses of others. Here is a diagram showing some of Swing's GUI classes and their relationships. Don't worry about the details\u00aefor now, but try to get some feel about how object-oriented programming and inheritance are used here. Note that all the GUI classes are subclasses, directly or indirectly, of a class called JComponent, which represents general properties that are shared by all Swing components. Two of the direct subclasses of JComponent themselves have\u00aesubclasses. The classes JTextArea and JTextField, which have certain behaviors in common, are grouped together as subclasses of JTextComponent. Similarly JButton and JToggleButton are subclasses of JAbstractButton, which represents properties common to both buttons and checkboxes. (JComboBox, by the way, is the Swing class that represents pop-up menus.) Just from this brief discussion, perhaps\u00aeyou can see how GUI programming can make effective use of object-oriented design. In fact, GUI's, with their \"visible objects,\" are probably a major factor contributing to the popularity of OOP. The Internet and the World-Wide Web - Computers can be connected\u00aetogether on networks. A computer on a network can communicate with other computers on the same network by exchanging data and files or by sending and receiving messages. Computers on a network can even work together on a large computation. Today, millions of computers throughout the world are connected to a single huge network called the Internet. New computers are being connected to the Internet every day. Computers can join the Internet by using a modem to establish a connection through telephone lines. Broadband connections to the Internet, such as DSL and cable modems, are increasingly common. They allow faster data transmission than is possible through telephone modems. There are elaborate protocols for communication over the Internet.",
  "page8": "On a basic level (the level of machine language), a computer can perform only very simple operations. A computer performs complex tasks by stringing together large numbers of such operations. Such\u00aetasks must be \"scripted\" in complete and perfect detail by programs. Creating complex programs will never be really easy, but the difficulty can be handled to some extent by giving the program a clear overall structure. The desig\u00aen of the overall structure of a program is what I call \"programming in the large.\" Programming in the small, which is sometimes called coding, would then refer to filling in the details of that design. The details are the explicit, step-by-step instructions for performing fairly small-scale tasks. When you do coding, you are working\u00aefairly \"close to the machine,\" with some of the same concepts that you might use in machine language: memory locations, arithmetic operations, loops and branches. In a high-level language such as Java, you get to work with these concepts on a level several steps above machine language. However, you still have to worry about getting all the details exactly right. This chapter and\u00aethe next examine the facilities for programming in the small in the Java programming language. Don't be misled by the term \"programming in the small\" into thinking that this material is easy or unimportant. This material is an essential foundation for all types\u00aeof programming. If you don't understand it, you can't write programs, no matter how good you get at designing their large-scale structure. The Basic Java Application - A program is a sequence of instructions that a computer can execute to perform some task. A simple enough idea, but for the computer to make any use of the instructions, they must be written in a form that the computer can use. This means that programs have to be written in programming languages. ",
  "page9": "Here is a Java program to display the message \"Hello World!\". Don't expect to understand what's going on here just yet some of it you won't really understand until a few\u00aechapters from now: // A program to display the message // \\\"Hello World!\\\" on standard output public class HelloWorld { public static void main(String[] args) { System.out.println(\\\"Hello World!\\\"); } } // end of class HelloWorld The\u00aecommand that actually displays the message is: System.out.println(\\\"Hello World!\\\"); This command is an example of a subroutine call statement. It uses a \"built-in subroutine\" named System.out.println to do the actual work. Recall that a subroutine consists of the instructions for performing some task, chunked together and g\u00aeiven a name. That name can be used to \"call\" the subroutine whenever that task needs to be performed. A built-in subroutine is one that is already defined as part of the language and therefore automatically available for use in any program. When you run this program, the message \"Hello World!\" (without the quotes) will be displayed on standard output. Unfortunately, I can\\u00aeu2019t say exactly what that means! Java is meant to run on many different platforms, and standard output will mean different things on different platforms. However, you can expect the message to show up in some convenient place. (If you use a command-line interface, like that\u00aein Sun Microsystem's Java Development Kit, you type in a command to tell the computer to run the program. The computer will type the output from the program, Hello World!, on the next line.) You must be curious about all the other stuff in the above program.",
  "page10": "Variables and the Primitive Types - Names are fundamental to programming. In programs, names are used to refer to many different sorts of things. In order to use those things, a programmer must\u00aeunderstand the rules for giving names to things and the rules for using the names to work with those things. That is, the programmer must understand the syntax and the semantics of names. According to the syntax rules of Java, a name is a se\u00aequence of one or more characters. It must begin with a letter or underscore and must consist entirely of letters, digits, and underscores. (\"Underscore\" refers to the character ' '.) For example, here are some legal names: No spaces are allowed in identifiers; HelloWorld is a legal identifier, but \"Hello World\u00aeis not. Upper case and lower case letters are considered to be different, so that HelloWorld, helloworld, HELLOWORLD, and hElloWorLD are all distinct names. Certain names are reserved for special uses in Java, and cannot be used by the programmer for other purposes. These reserved words include: class, public, static, if, else, while, and several dozen other words. Java is actually pretty libera\u00ael about what counts as a letter or a digit. Java uses the Unicode character set, which includes thousands of characters from many different languages and different alphabets, and many of these characters count as letters or digits. However, I will be sticking to what can be typ\u00aeed on a regular English keyboard. The pragmatics of naming includes style guidelines about how to choose names for things. For example, it is customary for names of classes to begin with upper case letters, while names of variables and of subroutines begin with lower case letters; you can avoid a lot of confusion by following the same convention in your own programs. Most Java programmers do not use underscores in names, although some do use them at the beginning of the names of certain kinds of variables.",
  "page11": "Types and Literals- A variable in Java is designed to hold only one particular type of data; it can legally hold that type of data and no other. The compiler will consider it to be a syntax error\u00aeif you try to violate this rule. We say that Java is a strongly typed language because it enforces this rule. There are eight so-called primitive types built into Java. The primitive types are named byte, short, int, long, float, double, c\u00aehar, and boolean. The first four types hold integers (whole numbers such as 17, -38477, and 0). The four integer types are distinguished by the ranges of integers they can hold. The float and double types hold real numbers (such as 3.6 and -145.99). Again, the two real types are distinguished by their range and accuracy. A variable of type\u00aechar holds a single character from the Unicode character set. And a variable of type boolean holds one of the two logical values true or false. Any data value stored in the computer's memory must be represented as a binary number, that is as a string of zeros and ones. A single zero or one is called a bit. A string of eight bits is called a byte. Memory is usually measured in terms of b\u00aeytes. Not surprisingly, the byte data type refers to a single byte of memory. A variable of type byte holds a string of eight bits, which can represent any of the integers between -128 and 127, inclusive. (There are 256 integers in that range; eight bits can represent 256 two r\u00aeaised to the power eight different values.) As for the other integer types, short corresponds to two bytes (16 bits). Variables of type short have values in the range -32768 to 32767. int corresponds to four bytes (32 bits). Variables of type int have values in the range -2147483648 to 2147483647. long corresponds to eight bytes (64 bits). Variables of type long have values in the range -9223372036854775808 to 9223372036854775807. You don't have to remember these numbers, but they do give you some idea of the size of integers that you can work with. Usually, you should just stick to the int data type, which is good enough for most purposes.",
  "page12": "Variables in Programs- A variable can be used in a program only if it has first been declared. A variable declaration statement is used to declare one or more variables and to give them names. When\u00aethe computer executes a variable declaration, it sets aside memory for the variable and associates the variable's name with that memory. A simple variable declaration takes the form. (type-name) (variable-name-or-names); The hvariab\u00aele-name-or-namesi can be a single variable name or a list of variable names separated by commas. (We'll see later that variable declaration statements can actually be somewhat more complicated than this.) Good programming style is to declare only one variable in a declaration statement, unless the variables are closely related in some\u00aeway. For example: int numberOfStudents; String name; double x, y; boolean isFinished; char firstInitial, middleInitial, lastInitial; It is also good style to include a comment with each variable declaration to explain its purpose in the program, or to give other information that might be useful to a human reader. in this chapter, we will only use variables declared inside the main() subrouti\u00aene of a program. Variables declared inside a subroutine are called local variables for that subroutine. They exist only inside the subroutine, while it is running, and are completely inaccessible from outside. Variable declarations can occur anywhere inside the subroutine, as l\u00aeong as each variable is declared before it is used in any expression. Some people like to declare all the variables at the beginning of the subroutine. Others like to wait to declare a variable until it is needed. My preference: Declare important variables at the beginning of the subroutine, and use a comment to explain the purpose of each variable.",
  "page13": "To begin to get a handle on all of this complexity, let's look at the subroutine System.out.print as an example. As you have seen earlier in this chapter, this subroutine is used to display\u00aeinformation to the user. For example, System.out.print(\\\"Hello World\\\") displays the message, Hello World. System is one of Java's standard classes. One of the static member variables in this class is named out. Since this variable\u00aeis contained in the class System, its full name which you have to use to refer to it in your programs is System.out. The variable System.out refers to an object, and that object in turn contains a subroutine named print. The compound identifier System.out.print refers to the subroutine print in the object out in the class System. (As an\u00aeaside, I will note that the object referred to by System.out is an object of the class PrintStream. PrintStream is another class that is a standard part of Java. Any object of type PrintStream is a destination to which information can be printed; any object of type PrintStream has a print subroutine that can be used to send information to that destination. The object System.out is just one pos\u00aesible destination, and System.out.print is the subroutine that sends information to that particular destination. Other objects of type PrintStream might send information to other destinations such as files or across a network to other computers. This is object-oriented programm\u00aeing: Many different things which have something in common they can all be used as destinations for information can all be used in the same way through a print subroutine.",
  "page14": "The function call Math.sqrt(x) represents a value of type double, and it can be used anyplace where a numeric literal of type double could be used. The Math class contains many static member\u00aefunctions. Here is a list of some of the more important of them: Math.abs(x), which computes the absolute value of x. The usual trigonometric functions, Math.sin(x), Math.cos(x), and Math.tan(x). (For all the trigonometric functions, angles are\u00aemeasured in radians, not degrees.) The inverse trigonometric functions arcsin, arccos, and arctan, which are written as: Math.asin(x), Math.acos(x), and Math.atan(x). The return value is expressed in radians, not degrees. The exponential function Math.exp(x) for computing the number e raised to the power x, and the natural logarithm funct\u00aeion Math.log(x) for computing the logarithm of x in the base e. Math.pow(x,y) for computing x raised to the power y. Math.floor(x), which rounds x down to the nearest integer value that is less than or equal to x. Even though the return value is mathematically an integer, it is returned as a value of type double, rather than of type int as you might expect. For example, Math.floor(3.76) is 3.\u00ae0. The function Math.round(x) returns the integer that is closest to x. Math.random(), which returns a randomly chosen double in the range 0.0 <= Math.random() < 1.0. (The computer actually calculates so-called \"pseudorandom\" numbers, which are not truly random but ar\u00aee random enough for most purposes.) For these functions, the type of the parameter the x or y inside the parentheses can be any value of any numeric type. For most of the functions, the value returned by the function is of type double no matter what the type of the parameter. However, for Math.abs(x), the value returned will be the same type as x; if x is of type int, then so is Math.abs(x). So, for example, while Math.sqrt(9) is the double value 3.0, Math.abs(9) is the int value 9.",
  "page15": "s1.substring(N,M), where N and M are integers, returns a value of type String. The returned value consists of the characters in s1 in positions N, N+1,. . . , M-1. Note that the character in\u00aeposition M is not included. The returned value is called a substring of s1. s1.indexOf(s2) returns an integer. If s2 occurs as a substring of s1, then the returned value is the starting position of that substring. Otherwise, the returned\u00aevalue is -1. You can also use s1.indexOf(ch) to search for a particular character, ch, in s1. To find the first occurrence of x at or after position N, you can use s1.indexOf(x,N). s1.compareTo(s2) is an integer-valued function that compares the two strings. If the strings are equal, the value returned is zero. If s1 is less than s2\u00aethe value returned is a number less than zero, and if s1 is greater than s2, the value returned is some number greater than zero. (If both of the strings consist entirely of lower case letters, then \"less than\" and \"greater than\" refer to alphabetical order. Otherwise, the ordering is more complicated.) s1.toUpperCase() is a String-valued function that returns a\u00aenew string that is equal to s1, except that any lower case letters in s1 have been converted to upper case. For example, \\\"Cat\\\".toUpperCase() is the string \\\"CAT\\\". There is also a function s1.toLowerCase(). s1.trim() is a String-valued function that returns a n\u00aeew string that is equal to s1 except that any non-printing characters such as spaces and tabs have been trimmed from the beginning and from the end of the string.",
  "page16": "enum Season { SPRING, SUMMER, FALL, WINTER } By convention, enum values are given names that are made up of upper case letters, but that is a style guideline and not a syntax rule. Enum values\u00aeare not variables. Each value is a constant that always has the same value. In fact, the possible values of an enum type are usually referred to as enum constants. Note that the enum constants of type Season are considered to be \"contain\u00aeed in\" Season, which means following the convention that compound identifiers are used for things that are contained in other things the names that you actually use in your program to refer to them are Season.SPRING, Season.SUMMER, Season.FALL, and Season.WINTER. Once an enum type has been created, it can be used to declare variables\u00aein exactly the same ways that other types are used. For example, you can declare a variable named vacation of type Season with the statement: Season vacation; After declaring the variable, you can assign a value to it using an assignment statement. The value on the right-hand side of the assignment can be one of the enum constants of type Season. Remember to use the full name of the constant,\u00aeincluding \"Season\"! For example: vacation = Season.SUMMER; You can print out an enum value with an output statement such as System.out.print(vacation). The output value will be the name of the enum constant (without the \"Season.\"). In this case, For some un\u00aefathomable reason, Java has never made it easy to read data typed in by the user of a program. You've already seen that output can be displayed to the user using the subroutine System.out.print.",
  "page17": "Types and Literals- A variable in Java is designed to hold only one particular type of data; it can legally hold that type of data and no other. The compiler will consider it to be a syntax error\u00aeif you try to violate this rule. We say that Java is a strongly typed language because it enforces this rule. There are eight so-called primitive types built into Java. The primitive types are named byte, short, int, long, float, double, c\u00aehar, and boolean. The first four types hold integers (whole numbers such as 17, -38477, and 0). The four integer types are distinguished by the ranges of integers they can hold. The float and double types hold real numbers (such as 3.6 and -145.99). Again, the two real types are distinguished by their range and accuracy. A variable of type\u00aechar holds a single character from the Unicode character set. And a variable of type boolean holds one of the two logical values true or false. Any data value stored in the computer's memory must be represented as a binary number, that is as a string of zeros and ones. A single zero or one is called a bit. A string of eight bits is called a byte. Memory is usually measured in terms of b\u00aeytes. Not surprisingly, the byte data type refers to a single byte of memory. A variable of type byte holds a string of eight bits, which can represent any of the integers between -128 and 127, inclusive. (There are 256 integers in that range; eight bits can represent 256 two r\u00aeaised to the power eight different values.) As for the other integer types, short corresponds to two bytes (16 bits). Variables of type short have values in the range -32768 to 32767.",
  "page18": "are to be output. Here is a statement that will print a number in the proper format for a dollar amount, where amount is a variable of type double: System.out.printf( \\\"%1.2f\\\", amount ); Tex\u00aetIO can also do formatted output. The function TextIO.putf has the same functionality as System.out.printf. Using TextIO, the above example would be: TextIO.printf(\\\"%1.2\\\",amount); and you could say TextIO.putln(\\\"%1.2f\\\",principal\u00ae); instead of TextIO.putln(principal); in the Interest2 program to get the output in the right format. The output format of a value is specified by a format specifier. The format string (in the simple cases that I cover here) contains one format specifier for each of the values that is to be output. Some typical format specifiers are %d, %\u00ae12d, %10s, %1.2f, %15.8e and %1.8g. Every format specifier begins with a percent sign (%) and ends with a letter, possibly with some extra formatting information in between. The letter specifies the type of output that is to be produced. For example, in %d and %12d, the \"d\" specifies that an integer is to be written. The \"12\" in %12d specifies the minimum number of spaces\u00aethat should be used for the output. If the integer that is being output takes up fewer than 12 spaces, extra blank spaces are added in front of the integer to bring the total up to 12. We say that the output is \"right-justified in a field of length 12.\" The value is n\u00aeot forced into 12 spaces; if the value has more than 12 digits, all the digits will be printed, with no extra spaces. The specifier %d means the same as %1d; that is an integer will be printed using just as many spaces as necessary.",
  "page19": "Introduction to File I/O System.out sends its output to the output destination known as \"standard output.\" But standard output is just one possible output destination. For example, data\u00aecan be written to a file that is stored on the user's hard drive. The advantage to this, of course, is that the data is saved in the file even after the program ends, and the user can print the file, email it to someone else, edit it\u00aewith another program, and so on. TextIO has the ability to write data to files and to read data from files. When you write output using the put, putln, or putf method in TextIO, the output is sent to the current output destination. By default, the current output destination is standard output. However, TextIO has some subroutines that can\u00aebe used to change the current output destination. To write to a file named \"result.txt\", for example, you would use the statement: TextIO.writeFile(\\\"result.txt\\\"); After this statement is executed, any output from TextIO output statements will be sent to the file named \"result.txt\" instead of to standard output. The file should be created in the same directory that co\u00aentains the program. Note that if a file with the same name already exists, its previous contents will be erased! In many cases, you want to let the user select the file that will be used for output. The statement TextIO.writeUserSelectedFile(); will open a typical graphical-use\u00aer-interface file selection dialog where the user can specify the output file. If you want to go back to sending output to standard output, you can say TextIO.writeStandardOutput();",
  "page20": "Arithmetic Operators Arithmetic operators include addition, subtraction, multiplication, and division. They are indicated by +, -, *, and /. These operations can be used on values of any numeric\u00aetype: byte, short, int, long, float, or double. When the computer actually calculates one of these operations, the two values that it combines must be of the same type. If your program tells the computer to combine two values of different t\u00aeypes, the computer will convert one of the values from one type to another. For example, to compute 37.4 + 10, the computer will convert the integer 10 to a real number 10.0 and will then compute 37.4 + 10.0. This is called a type conversion. Ordinarily, you don't have to worry about type conversion in expressions, because the computer\u00aedoes it automatically. When two numerical values are combined (after doing type conversion on one of them, if necessary), the answer will be of the same type. If you multiply two ints, you get an int; if you multiply two doubles, you get a double. This is what you would expect, but you have to be very careful when you use the division operator /. When you divide two integers, the answer wil\u00ael always be an integer; if the quotient has a fractional part, it is discarded. For example, the value of 7/2 is 3, not 3.5. If N is an integer variable, then N/100 is an integer, and 1/N is equal to zero for any N greater than one! This fact is a common source of programming e\u00aerrors. You can force the computer to compute a real number as the answer by making one of the operands real: For example, when the computer evaluates 1.0/N, it first converts N to a real number in order to match the type of 1.0, so you get a real number as the answer.",
  "page21": "Relational Operators Java has boolean variables and boolean-valued expressions that can be used to express conditions that can be either true or false. One way to form a boolean-valued expression\u00aeto compare two values using a relational operator. Relational operators are used to test whether two values are equal, whether one value is greater than another, and so forth. The relational operators in Java are: ==, !=, <, >, <=, and >=.\u00aeThe meanings of these operators are: A == B Is A \\\"equal to\\\" B? A != B Is A \\\"not equal to\\\" B? A < B Is A \\\"less than\\\" B? A > B Is A \\\"greater than\\\" B? A <= B Is A \\\"less than or equal to\\\" B? A >= B Is A \\\"greater than or equal to\\\" B? These operators can be used to compare values of any of the numeric types.\u00aeThey can also be used to compare values of type char. For characters, < and > are defined according the numeric Unicode values of the characters. (This might not always be what you want. It is not the same as alphabetical order because all the upper case letters come before all the lower case letters.) When using boolean expressions, you should remember that as far as the computer is concerne\u00aed, there is nothing special about boolean values. In the next chapter, you will see how to use them in loop and branch statements. But you can also assign boolean-valued expressions to boolean variables, just as you can assign numeric values to numeric variables. By the way, th\u00aee operators == and != can be used to compare boolean values. This is occasionally useful. For example, can you figure out what this does: boolean sameSign; sameSign = ((x > 0) == (y > 0)); One thing that you cannot do with the relational operators <, >, <=, and <= is to use them to compare values of type String.",
  "page22": "Conditional Operator Any good programming language has some nifty little features that aren't really necessary but that let you feel cool when you use them. Java has the conditional operator\u00ae. It's a ternary operator that is, it has three operands and it comes in two pieces, ? and :, that have to be used together. It takes the form boolean-expression ? expression1 : expression2 The computer tests the value of hboolean-expr\u00aeessioni. If the value is true, it evaluates hexpression1i; otherwise, it evaluates hexpression2i. For example: next = (N % 2 == 0) ? (N/2) : (3*N+1); will assign the value N/2 to next if N is even (that is, if N % 2 == 0 is true), and it will assign the value (3*N+1) to next if N is odd. (The parentheses in this example are not required,\u00aebut they do make the expression easier to read.) Assignment Operators and Type-Casts You are already familiar with the assignment statement, which uses the symbol \"=\" to assign the value of an expression to a variable. In fact, = is really an operator in the sense that an assignment can itself be used as an expression or as part of a more complex expression. The value of an assignmen\u00aet such as A=B is the same as the value that is assigned to A. So, if you want to assign the value of B to A and test at the same time whether that value is zero, you could say: if ( (A=B) == 0 )... Usually, I would say, don't do things like that! In general, the type of th\u00aee expression on the right-hand side of an assignment statement must be the same as the type of the variable on the left-hand side. However, in some cases, the computer will automatically convert the value computed by the expression to match the type of the variable.",
  "page23": "Type Conversion of Strings In addition to automatic type conversions and explicit type casts, there are some other cases where you might want to convert a value of one type into a value of a diff\u00aeerent type. One common example is the conversion of a String value into some other type, such as converting the string \\\"10\\\" into the int value 10 or the string \\\"17.42e-2\\\" into the double value 0.1742. In Java, these conversions\u00aeare handled by built-in functions. There is a standard class named Integer that contains several subroutines and variables related to the int data type. (Recall that since int is not a class, int itself can't contain any subroutines or variables.) In particular, if str is any expression of type String, then Integer.parseInt(str) is a\u00aefunction call that attempts to convert the value of str into a value of type int. For example, the value of Integer.parseInt(\\\"10\\\") is the int value 10. If the parameter to Integer.parseInt does not represent a legal int value, then an error occurs. Similarly, the standard class named Double includes a function Double.parseDouble that tries to convert a parameter of type String into a va\u00aelue of type double. For example, the value of the function call Double.parseDouble(\\\"3.14\\\") is the double value 3.14. (Of course, in practice, the parameter used in Double.parseDouble or Integer.parseInt would be a variable or expression rather than a constant string.) Typ\u00aee conversion functions also exist for converting strings into enumerated type values. (Enumerated types, or enums, were introduced in Subsection 2.3.3.) For any enum type, a predefined function named valueOf is automatically defined for that type.",
  "page24": "Java Development Kit- The basic development system for Java programming is usually referred to as the JDK (Java Development Kit). It is a part of J2SE, the Java 2 Platform Standard Edition. This\u00aebook requires J2SE version 5.0 (or higher). Confusingly, the JDK that is part of J2SE version 5.0 is sometimes referred to as JDK 1.5 instead of 5.0. Note that J2SE comes in two versions, a Development Kit version and a Runtime version. The\u00aeRuntime can be used to run Java programs and to view Java applets in Web pages, but it does not allow you to compile your own Java programs. The Development Kit includes the Runtime and adds to it the JDK which lets you compile programs. You need a JDK for use with this textbook. Java was developed by Sun Microsystems, Inc., which makes\u00aeits JDK for Windows and Linux available for free download at its Java Web site, java.sun.com. If you have a Windows computer, it might have come with a Java Runtime, but you might still need to download the JDK. Some versions of Linux come with the JDK either installed by default or on the installation media. If you need to download and install the JDK, be sure to get JDK 5.0 (or higher). As o\u00aef June, 2006, the download page for JDK 5.0 can be found at http://java.sun.com/j2se/1.5.0/download.jsp. Mac OS comes with Java. The version included with Mac OS 10.4 is 1.4.2, the version previous to Java 5.0. However, JDK Version 5.0 is available for Mac OS 10.4 on Apple\u00ae9s Web site and can also be installed through the standard Mac OS Software Update application. If a JDK is installed on your computer, you can use the command line environment to compile and run Java programs. Some IDEs depend on the JDK, so even if you plan to use an IDE for programming, you might still need a JDK.",
  "page25": "The most basic commands for using Java on the command line are javac and java; javac is used to compile Java source code, and java is used to run Java stand-alone applications. If a JDK is correc\u00aetly installed on your computer, it should recognize these commands when you type them in on the command line. Try typing the commands java -version and javac -version which should tell you which version of Java is installed. If you get a me\u00aessage such as \"Command not found,\" then Java is not correctly installed. If the \"java\" command works, but \"javac\" does not, it means that a Java Runtime is installed rather than a Development Kit. To create your own programs, you will need a text editor. A text editor is a computer program that allows you to\u00aecreate and save documents that contain plain text. It is important that the documents be saved as plain text, that is without any special encoding or formatting information. Word processor documents are not appropriate, unless you can get your word processor to save as plain text. A good text editor can make programming a lot more pleasant. Linux comes with several text editors. On Windows, yo\u00aeu can use notepad in a pinch, but you will probably want something better. For Mac OS, you might download the free TextWrangler application. One possibility that will work on any platform is to use jedit, a good programmer's text editor that is itself written in Java and t\u00aehat can be downloaded for free from www.jedit.org. To create your own programs, you should open a command line window and cd into the working directory where you will store your source code files. Start up your text editor program, such as by double-clicking its icon or selecting it from a Start menu. Type your code into the editor window, or open an existing source code file that you want to modify.",
  "page26": "The Problem of Packages- Every class in Java is contained in something called a package. Classes that are not explicitly put into a different package are in the \"default\" package. Almos\u00aet all the examples in this textbook are in the default package, and I will not even discuss packages in any depth until Section 4.5. However, some IDEs might force you to pay attention to packages. When you create a class in Eclipse, you mi\u00aeght notice a message that says that \"The use of the default package is discouraged.\" Although this is true, I have chosen to use it anyway, since it seems easier for beginning programmers to avoid the whole issue of packages, at least at first. Some IDEs might be even less willing than Eclipse to use the default package. If you\u00aecreate a class in a package, the source code starts with a line that specifies which package the class is in. For example, if the class is in a package named testpkg, then the first line of the source code will be package testpkg; In an IDE, this will not cause any problem unless the program you are writing depends on TextIO. You will not be able to use TextIO in a program unless TextIO is pla\u00aeced into the same package as the program. This means that you have to modify the source code file TextIO.java to specify the package; just add a package statement using the same package name as the program. Then add the modified TextIO.java to the same folder that contains the\u00aeprogram source code. Once you've done this, the example should run in the same way as if it were in the default package. By the way, if you use packages in a command-line environment, other complications arise. For example, if a class is in a package named testpkg, then the source code file",
  "page27": "The Basic While Loop The block statement by itself really doesn't affect the flow of control in a program. The five remaining control structures do. They can be divided into two classes: loo\u00aep statements and branching statements. You really just need one control structure from each category in order to have a completely general-purpose programming language. More than that is just convenience. In this section, I'll introduc\u00aee the while loop and the if statement. I'll give the full details of these statements and of the other three control structures in later sections. A while loop is used to repeat a given statement over and over. Of course, it's not likely that you would want to keep repeating it forever. That would be an infinite loop, which is\u00aegenerally a bad thing. (There is an old story about computer pioneer Grace Murray Hopper, who read instructions on a bottle of shampoo telling her to \"lather, rinse, repeat.\" As the story goes, she claims that she tried to follow the directions, but she ran out of shampoo. (In case you don't get it, this is a joke about the way that computers mindlessly follow instructions.)) To\u00aebe more specific, a while loop will repeat a statement over and over, but only so long as a specified condition remains true. A while loop has the form: while (boolean-expression) statement Since the statement can be, and usually is, a block, many while loops have the form: whi\u00aele (boolean-expression) { (statements) } The semantics of this statement go like this: When the computer comes to a while statement, it evaluates the boolean-expression, which yields either true or false as the value. If the value is false, the computer skips over the rest of the while loop and proceeds to the next command in the program.",
  "page28": "The Basic If Statement An if statement tells the computer to take one of two alternative courses of action, depending on whether the value of a given boolean-valued expression is true or false. I\u00aet is an example of a \"branching\" or \"decision\" statement. An if statement has the form: if ( boolean-expression ) statement1 else statement2 When the computer executes an if statement, it evaluates the boolean expression\u00ae. If the value is true, the computer executes the first statement and skips the statement that follows the \"else\". If the value of the expression is false, then the computer skips the first statement and executes the second one. Note that in any case, one and only one of the two statements inside the if statement is executed. The\u00aetwo statements represent alternative courses of action; the computer decides between these courses of action based on the value of the boolean expression. In many cases, you want the computer to choose between doing something and not doing it. You can do this with an if statement that omits the else part. Algorithm Development- Programming is difficult (like many activities that are useful\u00aeand worth while and like most of those activities, it can also be rewarding and a lot of fun). When you write a program, you have to tell the computer every small detail of what to do. And you have to get everything exactly right, since the computer will blindly follow your pro\u00aegram exactly as written. How, then, do people write any but the most simple programs? It's not a big mystery, actually. It's a matter of learning to think in the right way. A program is an expression of an idea. A programmer starts with a general idea of a task for the computer to perform. Presumably, the programmer has some idea of how to perform the task by hand, at least in general outline.",
  "page29": "Coding, Testing, Debugging - It would be nice if, having developed an algorithm for your program, you could relax, press a button, and get a perfectly working program. Unfortunately, the process\u00aeof turning an algorithm into Java source code doesn't always go smoothly. And when you do get to the stage of a working program, it's often only working in the sense that it does something. Unfortunately not what you want it to do\u00ae. After program design comes coding: translating the design into a program written in Java or some other language. Usually, no matter how careful you are, a few syntax errors will creep in from somewhere, and the Java compiler will reject your program with some kind of error message. Unfortunately, while a compiler will always detect syntax\u00aeerrors, it's not very good about telling you exactly what's wrong. Sometimes, it's not even good about telling you where the real error is. A spelling error or missing \"{\" on line 45 might cause the compiler to choke on line 105. You can avoid lots of errors by making sure that you really understand the syntax rules of the language and by following some basic progra\u00aemming guidelines. For example, I never type a \"{\" without typing the matching \"}\". Then I go back and fill in the statements between the braces. A missing or extra brace can be one of the hardest errors to find in a large program. Always, always indent your\u00aeprogram nicely. If you change the program, change the indentation to match. It's worth the trouble. Use a consistent naming scheme, so you don't have to struggle to remember whether you called that variable interestrate or interestRate. In general, when the compiler gives multiple error messages, don't try to fix the second error message from the compiler until you've",
  "page30": "I will confess that I only rarely use debuggers myself. A more traditional approach to debugging is to insert debugging statements into your program. These are output statements that print out in\u00aeformation about the state of the program. Typically, a debugging statement would say something like System.out.println(\\\"At start of while loop, N = \\\"+ N); You need to be able to tell from the output where in your program the output is\u00aecoming from, and you want to know the value of important variables. Sometimes, you will find that the computer isn't even getting to a part of the program that you think it should be executing. Remember that the goal is to find the first point in the program where the state is not what you expect it to be. That's where the bug is\u00aeAnd finally, remember the golden rule of debugging: If you are absolutely sure that everything in your program is right, and if it still doesn't work, then one of the things that you are absolutely sure of is wrong. The while and do..while Statements Statements in Java can be either simple statements or compound statements. Simple statements, such as assignment statements and subrouti\u00aene call statements, are the basic building blocks of a program. Compound statements, such as while loops and if statements, are used to organize simple statements into complex structures, which are called control structures because they control the order in which the statements\u00aeare executed. The next five sections explore the details of control structures that are available in Java, starting with the while statement and the do..while statement in this section. At the same time, we'll look at examples of programming with each control structure and apply the techniques for designing algorithms that were introduced in the previous section. The while Statement The while statement was already introduced in Section 3.1.",
  "page31": "Notice that I've rearranged the body of the loop. Since an integer is read before the loop, the loop has to begin by processing that integer. At the end of the loop, the computer reads a new\u00aeinteger. The computer then jumps back to the beginning of the loop and tests the integer that it has just read. Note that when the computer finally reads the sentinel value, the loop ends before the sentinel value is processed. It is not a\u00aedded to the sum, and it is not counted. This is the way it's supposed to work. The sentinel is not part of the data. The original algorithm, even if it could have been made to work without priming, was incorrect since it would have summed and counted all the integers, including the sentinel. (Since the sentinel is zero, the sum would\u00aestill be correct, but the count would be off by one. Such so-called off-by-one errors are very common. Counting turns out to be harder than it looks!) We can easily turn the algorithm into a complete program. Note that the program cannot use the statement \"average = sum/count;\" to compute the average. Since sum and count are both variables of type int, the value of sum/count is an i\u00aenteger. The average should be a real number. We've seen this problem before: we have to convert one of the int values to a double to force the computer to compute the quotient as a real number. This can be done by type-casting one of the variables to type double. The type\u00aecast \"(double)sum\" converts the value of sum to a real number, so in the program the average is computed as \"average = ((double)sum) / count;\". Another solution in this case would have been to declare sum to be a variable of type double in the first place. One other issue is addressed by the program: If the user enters zero as the first input value, there are no data to process.",
  "page32": "By the way, a more-than-usually-pedantic programmer would sneer at the test \"while (wantsToContinue == true)\". This test is exactly equivalent to \"while (wantsToContinue)\". Te\u00aesting whether \"wantsToContinue == true\" is true amounts to the same thing as testing whether \"wantsToContinue\" is true. A little less offensive is an expression of the form \"flag == false\", where flag is a bool\u00aeean variable. The value of \"flag == false\" is exactly the same as the value of \"!flag\", where ! is the boolean negation operator. So you can write \"while (!flag)\" instead of \"while (flag == false)\", and you can write \"if (!flag)\" instead of \"if (flag == false)\". Although a dowhile\u00aestatement is sometimes more convenient than a while statement, having two kinds of loops does not make the language more powerful. Any problem that can be solved using dowhile loops can also be solved using only while statements, and vice versa. In fact, if doSomething represents any block of program code, then do { doSomething } while ( boolean-expression ); break and continue- The syntax o\u00aef the while and do..while loops allows you to test the continuation condition at either the beginning of a loop or at the end. Sometimes, it is more natural to have the test in the middle of the loop, or to have several tests at different places in the same loop. Java provides\u00aea general method for breaking out of the middle of any loop. It's called the break statement, which takes the form break; When the computer executes a break statement in a loop, it will immediately jump out of the loop. It then continues on to whatever follows the loop in the program. Consider for example: while (true) // looks like it will run forever! TextIO.put(\\\"Enter a positive number: \\\");",
  "page33": "The for Statement We turn in this section to another type of loop, the for statement. Any for loop is equivalent to some while loop, so the language doesn't get any additional power by havin\u00aeg for statements. But for a certain type of problem, a for loop can be easier to construct and easier to read than the corresponding while loop. It's quite possible that in real programs, for loops actually outnumber while loops. For L\u00aeoops The for statement makes a common type of while loop easier to write. Many while loops have the general form: initialization while ( continuation-condition ) { statements update } The initialization, continuation condition, and updating have all been combined in the first line of the for loop. This keeps everything involved in the\u00ae1ccontrol\" of the loop in one place, which helps makes the loop easier to read and understand. The for loop is executed in exactly the same way as the original code: The initialization part is executed once, before the loop begins. The continuation condition is executed before each execution of the loop, and the loop ends when this condition is false. The update part is executed at the end o\u00aef each execution of the loop, just before jumping back to check the condition. The formal syntax of the for statement is as follows: for ( initialization; continuation-condition; update) statement or, using a block statement: for ( initialization; continuation-condition; update\u00ae) { statements } The continuation-condition must be a boolean-valued expression. The initialization can be any expression, but is usually an assignment statement. The update can also be any expression, but is usually an increment, a decrement, or an assignment statement. Any of the three can be empty.",
  "page34": "The Problem of Packages- Every class in Java is contained in something called a package. Classes that are not explicitly put into a different package are in the \"default\" package. Almos\u00aet all the examples in this textbook are in the default package, and I will not even discuss packages in any depth until Section 4.5. However, some IDEs might force you to pay attention to packages. When you create a class in Eclipse, you mi\u00aeght notice a message that says that \"The use of the default package is discouraged.\" Although this is true, I have chosen to use it anyway, since it seems easier for beginning programmers to avoid the whole issue of packages, at least at first. Some IDEs might be even less willing than Eclipse to use the default package. If you\u00aecreate a class in a package, the source code starts with a line that specifies which package the class is in. For example, if the class is in a package named testpkg, then the first line of the source code will be package testpkg; In an IDE, this will not cause any problem unless the program you are writing depends on TextIO. You will not be able to use TextIO in a program unless TextIO is pla\u00aeced into the same package as the program. This means that you have to modify the source code file TextIO.java to specify the package; just add a package statement using the same package name as the program. Then add the modified TextIO.java to the same folder that contains the\u00aeprogram source code. Once you've done this, the example should run in the same way as if it were in the default package. By the way, if you use packages in a command-line environment, other complications arise. For example, if a class is in a package named testpkg, then the source code file",
  "page35": "Exceptions in TextIO- When TextIO reads a numeric value from the user, it makes sure that the user's response is legal, using a technique similar to the while loop and try..catch in the prev\u00aeious example. However, TextIO can read data from other sources besides the user. When it is reading from a file, there is no reasonable way for TextIO to recover from an illegal value in the input, so it responds by throwing an exception. T\u00aeo keep things simple, TextIO only throws exceptions of type IllegalArgumentException, no matter what type of error it encounters. For example, an exception will occur if an attempt is made to read from a file after all the data in the file has already been read. In TextIO, the exception is of type IllegalArgumentException. If you have a\u00aebetter response to file errors than to let the program crash, you can use a try..catch to catch exceptions of type IllegalArgumentException. For example, suppose that a file contains nothing but real numbers, and we want a program that will read the numbers and find their sum and their average. Since it is unknown how many numbers are in the file, there is the question of when to stop reading.\u00aeOne approach is simply to try to keep reading indefinitely. When the end of the file is reached, an exception occurs. This exception is not really an error it's just a way of detecting the end of the data, so we can catch the exception and finish up the program. We can rea\u00aed the data in a while (true) loop and break out of the loop when an exception occurs. This is an example of the somewhat unusual technique of using an exception as part of the expected flow of control in a program. To read from the file, we need to know the file's name.",
  "page36": "The first line of the class definition above says that the class \"extends Applet.\" Applet is a standard class that is defined in the java.applet package. It defines all the basic proper\u00aeties and behaviors of applet objects. By extending the Applet class, the new class we are defining inherits all those properties and behaviors. We only have to define the ways in which our class differs from the basic Applet class. In our c\u00aease, the only difference is that our applet will draw itself differently, so we only have to define the paint() routine that does the drawing. This is one of the main advantages of object-oriented programming. (Actually, in the future, our applets will be defined to extend JApplet rather than Applet. The JApplet class is itself an extension\u00aeof Applet. The Applet class has existed since the original version of Java, while JApplet is part of the newer \"Swing\" set of graphical user interface components. For the moment, the distinction is not important.) One more thing needs to be mentioned and this is a point where Java's syntax gets unfortunately confusing. Applets are objects, not classes. Instead of being static\u00aemembers of a class, the subroutines that define the applet's behavior are part of the applet object. We say that they are \"non-static\" subroutines. Of course, objects are related to classes because every object is described by a class. Now here is the part that\u00aecan get confusing: Even though a non-static subroutine is not actually part of a class (in the sense of being part of the behavior of the class), it is nevertheless defined in a class (in the sense that the Java code that defines the subroutine is part of the Java code that defines the class).",
  "page37": "When you write an applet, you get to build on the work of the people who wrote the Applet class. The Applet class provides a framework on which you can hang your own work. Any programmer can crea\u00aete additional frameworks that can be used by other programmers as a basis for writing specific types of applets or stand-alone programs. I've written a small framework that makes it possible to write applets that display simple animati\u00aeons. One example that we will consider is an animated version of the nested rectangles applet from earlier in this section. You can see the applet in action at the bottom of the on-line version of this page. A computer animation is really just a sequence of still images. The computer displays the images one after the other. Each image differs\u00aea bit from the preceding image in the sequence. If the differences are not too big and if the sequence is displayed quickly enough, the eye is tricked into perceiving continuous motion. In the example, rectangles shrink continually towards the center of the applet, while new rectangles appear at the edge. The perpetual motion is, of course, an illusion. If you think about it, you'll\u00aesee that the applet loops through the same set of images over and over. In each image, there is a gap between the borders of the applet and the outermost rectangle. This gap gets wider and wider until a new rectangle appears at the border. Only it's not a new rectangle. Wh\u00aeat has really happened is that the applet has started over again with the first image in the sequence.The \"import java.awt.*;\" is required to get access to graphics-related classes such as Graphics and Color. You get to fill in any name you want for the class, and you get to fill in the statements inside the subroutine. The drawFrame() subroutine will be called by the system each time a frame needs to be drawn.",
  "page38": "A subroutine is sometimes said to be a \"black box\" because you can't see what's \"inside\" it (or, to be more precise, you usually don't want to see inside it, be\u00aecause then you would have to deal with all the complexity that the subroutine is meant to hide). Of course, a black box that has no way of interacting with the rest of the world would be pretty useless. A black box needs some kind of interf\u00aeace with the rest of the world, which allows some interaction between what's inside the box and what's outside. A physical black box might have buttons on the outside that you can push, dials that you can set, and slots that can be used for passing information back and forth. Since we are trying to hide complexity, not create it,\u00aewe have the first rule of black boxes The interface of a black box should be fairly straight forward, well-defined, and easy to understand Are there any examples of black boxes in the real world? Yes; in fact, you are surrounded by them. Your television, your car, your VCR, your refrigerator. You can turn your television on and off, change channels, and set the volume by using elements of t\u00aehe television's interface dials, remote control, don't forget to plug in the power without understanding anything about how the thing actually works. The same goes for a VCR, although if the stories are true about how hard people find it to set the time on a VCR, then\u00aemaybe the VCR violates the simple interface rule. Now, a black box does have an inside the code in a subroutine that actually performs the task, all the electronics inside your television set. The inside of a black box is called its implementation. The second rule of black boxes is that To use a black box, you shouldn't need to know anything about its implementation; all you need to know is its interface.",
  "page39": "The contract of a subroutine says, essentially, \"Here is what you have to do to use me, and here is what I will do for you, guaranteed.\" When you write a subroutine, the comments that y\u00aeou write for the subroutine should make the contract very clear. (I should admit that in practice, subroutines' contracts are often inadequately specified, much to the regret and annoyance of the programmers who have to use them.) For\u00aethe rest of this chapter, I turn from general ideas about black boxes and subroutines in general to the specifics of writing and using subroutines in Java. But keep the general ideas and principles in mind. They are the reasons that subroutines exist in the first place, and they are your guidelines for using them. This should be especially\u00aeclear in Section 4.6, where I will discuss subroutines as a tool in program development. You should keep in mind that subroutines are not the only example of black boxes in programming. For example, a class is also a black box. We'll see that a class can have a \"public\" part, representing its interface, and a \"private\" part that is entirely inside its hidden implemen\u00aetation. All the principles of black boxes apply to classes as well as to subroutines. Static Subroutines and Static Variables- Every subroutine in Java must be defined inside some class. This makes Java rather unusual among programming languages, since most languages allow free\u00ae-floating, independent subroutines. One purpose of a class is to group together related subroutines and variables. Perhaps the designers of Java felt that everything must be related to something. As a less philosophical motivation, Java's designers wanted to place firm controls on the ways things are named, since a Java program potentially has access to a huge number of subroutines created by many different programmers.",
  "page40": "the modifiers are public and static, the return type is void, the subroutine name is main, and the parameter list is \"String[] args\". The only question might be about \"String[]\u00ae1d, which has to be a type if it is to match the syntax of a parameter list. In fact, String[] represents a so-called \"array type\", so the syntax is valid. (The parameter, args, represents information provided to the program when the\u00aemain() routine is called by the system. In case you know the term, the information consists of any \"command-line arguments\" specified in the command that the user typed to run the program.) You've already had some experience with filling in the implementation of a subroutine. In this chapter, you'll learn all about writing\u00aeyour own complete subroutine definitions, including the interface part. Calling Subroutines- When you define a subroutine, all you are doing is telling the computer that the subroutine exists and what it does. The subroutine doesn't actually get executed until it is called. (This is true even for the main() routine in a class even though you don't call it, it is called by the sy\u00aestem when the system runs your program.) For example, the playGame() method given as an example above could be called using the following subroutine call statement: playGame(); This statement could occur anywhere in the same class that includes the definition of playGame(), whe\u00aether in a main() method or in some other subroutine. Since playGame() is a public method, it can also be called from other classes, but in that case, you have to tell the computer which class it comes from. Since playGame() is a static method, its full name includes the name of the class in which it is defined. Let's say, for example, that playGame() is defined in a class named Poker. Then to call playGame() from outside the Poker class, you would have to say Poker.playGame();",
  "page41": "Let computersNumber be a random number between 1 and 100 Let guessCount = 0 while (true): Get the user's guess Count the guess by adding 1 to guess count if the user's guess equals comp\u00aeutersNumber: Tell the user he won break out of the loop if the number of guesses is 6: Tell the user he lost break out of the loop if the user's guess is less than computersNumber: Tell the user the guess was low else if the user'\u00aes guess is higher than computersNumber: Tell the user the guess was high With variable declarations added and translated into Java, this becomes the definition of the playGame() routine. A random integer between 1 and 100 can be computed as (int)(100 * Math.random()) + 1. I've cleaned up the interaction with the user to make it flow\u00aebetter. Member Variables- A class can include other things besides subroutines. In particular, it can also include variable declarations. Of course, you can declare variables inside subroutines. Those are called local variables. However, you can also have variables that are not part of any subroutine. To distinguish such variables from local variables, we call them member variables, since they\u00aeare members of a class. Just as with subroutines, member variables can be either static or non-static. In this chapter, we'll stick to static variables. A static member variable belongs to the class itself, and it exists as long as the class exists. Memory is allocated fo\u00aer the variable when the class is first loaded by the Java interpreter. Any assignment statement that assigns a value to the variable changes the content of that memory, no matter where that assignment statement is located in the program. Any time the variable is used in an expression, the value is fetched from that same memory, no matter where the expression is located in the program. This means that the value of a static member variable can be set in one subroutine and used in another subroutine.",
  "page42": "Using Parameters- As an example, let's go back to the \"3N+1\" problem that was discussed in Subsection 3.2.2. (Recall that a 3N+1 sequence is computed according to the rule, \"i\u00aef N is odd, multiply by 3 and add 1; if N is even, divide by 2; continue until N is equal to 1.\" For example, starting from N=3 we get the sequence: 3, 10, 5, 16, 8, 4, 2, 1.) Suppose that we want to write a subroutine to print out suc\u00aeh sequences. The subroutine will always perform the same task: Print out a 3N+1 sequence. But the exact sequence it prints out depends on the starting value of N. So, the starting value of N would be a parameter to the subroutine. The parameter list of this subroutine, \"(int startingValue)\", specifies that the subroutine has one\u00aeparameter, of type int. Within the body of the subroutine, the parameter name can be used in the same way as a variable name. However, the parameter gets its initial value from outside the subroutine. When the subroutine is called, a value must be provided for this parameter in the subroutine call statement. This value will be assigned to the parameter, startingValue, before the body of the s\u00aeubroutine is executed. For example, the subroutine could be called using the subroutine call statement \"print3NSequence(17);\". When the computer executes this statement, the computer assigns the value 17 to startingValue and then executes the statements in the subrout\u00aeine. This prints the 3N+1 sequence starting from 17. If K is a variable of type int, then when the computer executes the subroutine call statement \"print3NSequence(K);\", it will take the value of the variable K, assign that value to startingValue, and execute the body of the subroutine. The class that contains print3NSequence can contain a main() routine (or other subroutines) that call print3NSequence.",
  "page43": "(There are a few technical differences between this and \"doTask(17,Math.sqrt(z+1),z>=10);\" besides the amount of typing because of questions about scope of variables and what happens wh\u00aeen several variables or parameters have the same name.) Beginning programming students often find parameters to be surprisingly confusing. Calling a subroutine that already exists is not a problem the idea of providing information to the su\u00aebroutine in a parameter is clear enough. Writing the subroutine definition is another matter. A common mistake is to assign values to the formal parameters at the beginning of the subroutine, or to ask the user to input their values. This represents a fundamental misunderstanding. When the statements in the subroutine are executed, the for\u00aemal parameters will already have values. The values come from the subroutine call statement. Remember that a subroutine is not independent. It is called by some other routine, and it is the calling routine's responsibility to provide appropriate values for the parameters. Overloading In order to call a subroutine legally, you need to know its name, you need to know how many formal parame\u00aeters it has, and you need to know the type of each parameter. This information is called the subroutine's signature. The signature of the subroutine doTask, used as an example above, can be expressed as as: doTask(int,double,boolean). Note that the signature does not inclu\u00aede the names of the parameters; in fact, if you just want to use the subroutine, you don't even need to know what the formal parameter names are, so the names are not part of the interface. Java is somewhat unusual in that it allows two different subroutines in the same class to have the same name, provided that their signatures are different. (The language C++ on which Java is based also has this feature.)",
  "page44": "Throwing Exceptions- I have been talking about the \"contract\" of a subroutine. The contract says what the subroutine will do, provided that the caller of the subroutine provides accepta\u00aeble values for subroutine's parameters. The question arises, though, what should the subroutine do when the caller violates the contract by providing bad parameter values? We've already seen that some subroutines respond to bad pa\u00aerameter values by throwing exceptions. For example, the contract of the built-in subroutine Double.parseDouble says that the parameter should be a string representation of a number of type double; if this is true, then the subroutine will convert the string into the equivalent numeric value. If the caller violates the contract by passing a\u00aen invalid string as the actual parameter, the subroutine responds by throwing an exception of type NumberFormatException. Many subroutines throw IllegalArgumentExceptions in response to bad parameter values. You might want to take this response in your own subroutines. This can be done with a throw statement. An exception is an object, and in order to throw an exception, you must create an ex\u00aeception object. You won't officially learn how to do this until Chapter 5, but for now, you can use the following syntax for a throw statement that throws an IllegalArgumentException throw new IllegalArgumentException( error-message ); where error-message is a string that\u00aedescribes the error that has been detected. (The word \"new\" in this statement is what creates the object.) To use this statement in a subroutine, you would check whether the values of the parameters are legal. If not, you would throw the exception. For example, consider the print3NSequence subroutine from the beginning of this section. The parameter of print3NSequence is supposed to be a positive integer.",
  "page45": "Return Values A subroutine that returns a value is called a function. A given function can only return a value of a specified type, called the return type of the function. A function call general\u00aely occurs in a position where the computer is expecting to find a value, such as the right side of an assignment statement, as an actual parameter in a subroutine call, or in the middle of some larger expression. A boolean-valued function c\u00aean even be used as the test condition in an if, while, for or do..while statement. (It is also legal to use a function call as a stand-alone statement, just as if it were a regular subroutine. In this case, the computer ignores the value computed by the subroutine. Sometimes this makes sense. For example, the function TextIO.getln(), with\u00aea return type of String, reads and returns a line of input typed in by the user. Usually, the line that is returned is assigned to a variable to be used later in the program, as in the statement \"name = TextIO.getln();\". However, this function is also useful as a subroutine call statement \"TextIO.getln();\", which still reads all input up to and including the next carriage\u00aereturn. Since the return value is not assigned to a variable or used in an expression, it is simply discarded. So, the effect of the subroutine call is to read and discard some input. Sometimes, discarding unwanted input is exactly what you need to do.) The return statement You\u00ae've already seen how functions such as Math.sqrt() and TextIO.getInt() can be used. What you haven't seen is how to write functions of your own. A function takes the same form as a regular subroutine, except that you have to specify the value that is to be returned by the subroutine. This is done with a return statement, which has the following syntax: return expression ; Such a return statement can only occur inside the definition of a function, and the type of the",
  "page46": "APIs, Packages, and Javadoc- As computers and their user interfaces have become easier to use, they have also become more complex for programmers to deal with. You can write programs for a simple\u00aeconsole-style user interface using just a few subroutines that write output to the console and read the user's typed replies. A modern graphical user interface, with windows, buttons, scroll bars, menus, text-input boxes, and so on, m\u00aeight make things easier for the user, but it forces the programmer to cope with a hugely expanded array of possibilities. The programmer sees this increased complexity in the form of great numbers of subroutines that are provided for managing the user interface, as well as for other purposes. Toolboxes- Someone who wants to program for Mac\u00aeintosh computers and to produce programs that look and behave the way users expect them to must deal with the Macintosh Toolbox, a collection of well over a thousand different subroutines. There are routines for opening and closing windows, for drawing geometric figures and text to windows, for adding buttons to windows, and for responding to mouse clicks on the window. There are other routin\u00aees for creating menus and for reacting to user selections from menus. Aside from the user interface, there are routines for opening files and reading data from them, for communicating over a network, for sending output to a printer, for handling communication between programs,\u00aeand in general for doing all the standard things that a computer has to do. Microsoft Windows provides its own set of subroutines for programmers to use, and they are quite a bit different from the subroutines used on the Mac. Linux has several different GUI toolboxes for the programmer to choose from. The analogy of a \"toolbox\" is a good one to keep in mind. Every programming project involves a mixture of innovation and reuse of existing tools. ing track of bank accounts,he device. Scientists who write a set of routines for doing some kind of complex computation such as solving \"differential equations,\" say would provide an API to allow others to use those routines without understanding the details of the computations they perform.",
  "page47": "The Java programming language is supplemented by a large, standard API. You've seen part of this API already, in the form of mathematical subroutines such as Math.sqrt(), the String data typ\u00aee and its associated routines, and the System.out.print() routines. The standard Java API includes routines for working with graphical user interfaces, for network communication, for reading and writing files, and more. It's tempting t\u00aeo think of these routines as being built into the Java language, but they are technically subroutines that have been written and made available for use in Java programs. Java is platform-independent. That is, the same program can run on platforms as diverse as Macintosh, Windows, Linux, and others. The same Java API must work on all these\u00aeplatforms. But notice that it is the interface that is platform-independent; the implementation varies from one platform to another. A Java system on a particular computer includes implementations of all the standard API routines. A Java program includes only calls to those routines. When the Java interpreter executes a program and encounters a call to one of the standard routines, it will pu\u00aell up and execute the implementation of that routine which is appropriate for the particular platform on which it is running. This is a very powerful idea. It means that you only need to learn one API to program for a wide variety of platforms. Java's Standard Packages Lik\u00aee all subroutines in Java, the routines in the standard API are grouped into classes. To provide larger-scale organization, classes in Java can be grouped into packages, which were introduced briefly in Subsection 2.6.4. You can have even higher levels of grouping, since packages can also contain other packages. In fact, the entire standard Java API is implemented in several packages. One of these, which is named \"java\", contains several no as buttons and menus in thjavax.ograms.",
  "page48": "Using Classes from Packages Let's say that you want to use the class java.awt.Color in a program that you are writing. Like any class, java.awt.Color is a type, which means that you can use\u00aeit to declare variables and parameters and to specify the return type of a function. One way to do this is to use the full name of the class as the name of the type. For example, suppose that you want to declare a variable named rectColor o\u00aef type java.awt.Color. You could say: java.awt.Color rectColor; This is just an ordinary variable declaration of the form \"htype-namei hvariable-namei;\". Of course, using the full name of every class can get tiresome, so Java makes it possible to avoid using the full name of a class by importing the class. If you put import java.\u00aeawt.Color; at the beginning of a Java source code file, then, in the rest of the file, you can abbreviate the full name java.awt.Color to just the simple name of the class, Color. Note that the import line comes at the start of a file and is not inside any class. Although it is sometimes referred to as a statement, it is more properly called an import directive since it is not a statement in\u00aethe usual sense. Using this import directive would allow you to say Color rectColor; to declare the variable. Note that the only effect of the import directive is to allow you to use simple class names instead of full \"package.class\" names; you aren't really impo\u00aerting anything substantial. If you leave out the import directive, you can still access the class you just have to use its full name. There is a shortcut for importing all the classes from a given package. You can import all the classes from java.awt by saying import java.awt.*; The \"*\" is a wildcard that matches every class in the package. (However, it does not match sub-packages; you cannot import the entire contents of all the sub-packages of from another package named class ",
  "page49": "Javadoc- To use an API effectively, you need good documentation for it. The documentation for most Java APIs is prepared using a system called Javadoc. For example, this system is used to prepare\u00aethe documentation for Java's standard packages. And almost everyone who creates a toolbox in Java publishes Javadoc documentation for it. Javadoc documentation is prepared from special comments that are placed in the Java source code\u00aefile. Recall that one type of Java comment begins with and ends with. A Javadoc comment takes the same form, but it begins with /** rather than simply /*. You have already seen comments of this form in some of the examples in this book, such as this subroutine This subroutine prints a 3N+1 sequence to standard output, using startingValue a\u00aes the initial value of N. It also prints the number of terms in the sequence. The value of the parameter, startingValue, must be a positive integer. Note that the Javadoc comment is placed just before the subroutine that it is commenting on. This rule is always followed. You can have Javadoc comments for subroutines, for member variables, and for classes. The Javadoc comment always immediatel\u00aey precedes the thing it is commenting on. Like any comment, a Javadoc comment is ignored by the computer when the file is compiled. But there is a tool called javadoc that reads Java source code files, extracts any Javadoc comments that it finds, and creates a set of Web pages\u00aecontaining the comments in a nicely formatted, interlinked form. By default, javadoc will only collect information about public classes, subroutines, and member variables, but it allows the option of creating documentation for non-public things as well. If javadoc doesn't find any Javadoc comment for something, it will construct one, but the comment will contain only basic information such as the name and type of a member variable or the n text, the comment can content. Y",
  "page50": "More on Program Design- Understanding how programs work is one thing. Designing a program to perform some particular task is another thing altogether. In Section 3.2, I discussed how pseudocode a\u00aend stepwise refinement can be used to methodically develop an algorithm. We can now see how subroutines can fit into the process. Stepwise refinement is inherently a top-down process, but the process does have a \"bottom,\" that is,\u00aea point at which you stop refining the pseudocode algorithm and translate what you have directly into proper programming language. In the absence of subroutines, the process would not bottom out until you get down to the level of assignment statements and very primitive input/output operations. But if you have subroutines lying around to\u00aeperform certain useful tasks, you can stop refining as soon as you've managed to express your algorithm in terms of those tasks. This allows you to add a bottom-up element to the top-down approach of stepwise refinement. Given a problem, you might start by writing some subroutines that perform tasks relevant to the problem domain. The subroutines become a toolbox of ready-made tools that\u00aeyou can integrate into your algorithm as you develop it. (Alternatively, you might be able to buy or find a software toolbox written by someone else, containing subroutines that you can use in your project as black boxes.) Subroutines can also be helpful even in a strict top-d\u00aeown approach. As you refine your algorithm, you are free at any point to take any sub-task in the algorithm and make it into a subroutine. Developing that subroutine then becomes a separate problem, which you can work on separately. Your main algorithm will merely call the subroutine. This, of course, is just a way of breaking your problem down into separate, smaller problems. It is still a top-down approach because the top-dothing that must be true whed thatition and postcondition should be added to the Javadoc system for explicit labeling of preconditions and postconditions, but that has not yet been done.)",
  "page51": "The Truth About Declarations- Names are fundamental to programming, as I said a few chapters ago. There are a lot of details involved in declaring and using names. I have been avoiding some of th\u00aeose details. In this section, I'll reveal most of the truth (although still not the full truth) about declaring and using variables in Java. The material in the subsections \"Initialization in Declarations\" and \"Named Con\u00aestants\" is particularly important, since I will be using it regularly in future chapters. Named Constants- Sometimes, the value of a variable is not supposed to change after it is initialized. For example, in the above example where interestRate is initialized to the value 0.05, it's quite possible that that is meant to be the va\u00aelue throughout the entire program. In this case, the programmer is probably defining the variable, interestRate, to give a meaningful name to the otherwise meaningless number, 0.05. It's easier to understand what's going on when a program says \"principal += principal*interestRate;\" rather than \"principal += principal*0.05;\". In Java, the modifier \"final\u00aecan be applied to a variable declaration to ensure that the value stored in the variable cannot be changed after the variable has been initialized. For example, if the member variable interestRate is declared with final static double interestRate = 0.05; then it would be impossible\u00aefor the value of interestRate to change anywhere else in the program. Any assignment statement that tries to assign a value to interestRate will be rejected by the computer as a syntax error when the program is compiled. It is legal to apply the final modifier to local variables and even to formal parameters, but it is most useful for member variables. I will often refer to a static member variable that is declared to be final as a named const Math class contains a variignmen",
  "page52": "Whereas a subroutine represents a single task, an object can encapsulate both data (in the form of instance variables) and a number of different tasks or \"behaviors\" related to that dat\u00aea (in the form of instance methods). Therefore objects provide another, more sophisticated type of structure that can be used to help manage the complexity of large programs. This chapter covers the creation and use of objects in Java. Sect\u00aeion 5.5 covers the central ideas of object-oriented programming: inheritance and polymorphism. However, in this textbook, we will generally use these ideas in a limited form, by creating independent classes and building on existing classes rather than by designing entire hierarchies of classes from scratch. Object-oriented programming (OOP\u00ae) represents an attempt to make programs more closely model the way people think about and deal with the world. In the older styles of programming, a programmer who is faced with some problem must identify a computing task that needs to be performed in order to solve the problem. Programming then consists of finding a sequence of instructions that will accomplish that task. But at the heart o\u00aeobject oriented programming,instead of tasks we find objects entities that have behaviors, that hold information, and that can interact with one another. Programming consists of designing a set of objects that somehow model the problem at hand. Software objects in the program c\u00aean represent real or abstract entities in the problem domain. This is supposed to make the design of the program more natural and hence easier to get right and easier to understand. To some extent, OOP is just a change in point of view. We can think of an object in standard programming terms as nothing more than a set of variables together with some subroutines for manipulating those variables. In fact, it is possible to use object-oriented techniques infrom classes? And why does longs\" to a class.) From the point of view of programming, it is more exact to say that classes are used to create objects. A class is a kind of factory for constructing objects. The non-static parts of the class specify, or describe, what variables and subroutines the objects will contain. This is part of the explanation of how objects differ from classes: Objects are created and destroyed as the program runs, and",
  "page53": "Consider a simple class whose job is to group together a few static member variables. For example, the following class could be used to store information about the person who is using the program\u00ae: class UserData { static String name; static int age; } In a program that uses this class, there is only one copy of each of the variables UserData.name and UserData.age. There can only be one \"user,\" since we only have memory sp\u00aeace to store data about one user. The class, UserData, and the variables it contains exist as long as the program runs. Now, consider a similar class that includes non-static variables: class PlayerData { String name; int age; } In this case, there is no such variable as PlayerData.name or PlayerData.age, since name and age are not static\u00aemembers of PlayerData. So, there is nothing much in the class at all except the potential to create objects. But, it's a lot of potential, since it can be used to create any number of objects! Each object will have its own variables called name and age. There can be many \"players\" because we can make new objects to represent new players on demand. A program might use this class\u00aeto store information about multiple players in a game. Each player has a name and an age. When a player joins the game, a new PlayerData object can be created to represent that player. If a player leaves the game, the PlayerData object that represents that player can be destro\u00aeyed. A system of objects in the program is being used to dynamically model what is happening in the game. You can't do this with \"static\" variables! In Section 3.8, we worked with applets, which are objects. The reason they didn't seem to be any different from classes is because we were only working with one applet in each class that we looked at. But one class can be used to make many applets. Think of an applet that hat class. The variables th",
  "page54": "An applet that scrolls a message across a Web page might include a subroutine named scroll(). Since the applet is an object, this subroutine is an instance method of the applet. The source code f\u00aeor the method is in the class that is used to create the applet. Still, it's better to think of the instance method as belonging to the object, not to the class. The non-static subroutines in the class merely specify the instance metho\u00aeds that every object created from the class will contain. The scroll() methods in two different applets do the same thing in the sense that they both scroll messages across the screen. But there is a real difference between the two scroll() methods. The messages that they scroll can be different. You might say that the method definition in\u00aethe class specifies what type of behavior the objects will have, but the specific behavior can vary from object to object, depending on the values of their instance variables. As you can see, the static and the non-static portions of a class are very different things and serve very different purposes. Many classes contain only static members, or only non-static. However, it is possible to mi\u00aex static and non-static members in a single class, and we'll see a few examples later in this chapter where it is reasonable to do so. You should distiguish between the source code for the class, and the class itself. The source code determines both the class and the objec\u00aets that are created from that class. The \"static\" definitions in the source code specify the things that are part of the class itself, whereas the non-static definitions in the source code specify things that will become part of every instance object that is created from the class. By the way, static member variables and static member subroutines in a class are sometimes called class variables and class methods, sinc test3) / 3; } } // end of int and boolean. So, a class name can be used to specify the type of a variable in a declaration statement, the type of a formal parameter, or the return type of a function. For example, a program could define a variable named std of type Student with the statement Student std; However, declaring a variable does not create an object! This is an important point, which is related to this Very Important Fact In Java, no variable can ever hold an object. A variable can only hold a reference to an object.",
  "page55": "You should think of objects as floating around independently in the computer's memory. In fact, there is a special portion of memory called the heap where objects live. Instead of holding an\u00aeobject itself, a variable holds the information necessary to find the object in memory. This information is called a reference or pointer to the object. In effect, a reference to an object is the address of the memory location where the ob\u00aeject is stored. When you use a variable of class type, the computer uses the reference in the variable to find the actual object. In a program, objects are created using an operator called new, which creates an object and returns a reference to that object. For example, assuming that std is a variable of type Student, declared as above, th\u00aee assignment statement std = new Student(); would create a new object which is an instance of the class Student, and it would store a reference to that object in the variable std. The value of the variable is a reference to the object, not the object itself. It is not quite true, then, to say that the object is the \"value of the variable std\" (though sometimes it is hard to avoid us\u00aeing this terminology). It is certainly not at all true to say that the object is \"stored in the variable std.\" The proper terminology is that \"the variable std refers to the object,\" and I will try to stick to that terminology as much as possible. So, suppos\u00aee that the variable std refers to an object belonging to the class Student. That object has instance variables name, test1, test2, and test3. These instance variables can be referred to as std.name, std.test1, std.test2, and std.test3. This follows the usual naming convention that when B is part of A, then the full name of B is A.B. For example, a program might include the lines System.out.println(\\\"Hello, \\\" + std.name + \\\". Your te number of characters in the student's name. It is possible for a variable like std, whose type is given by a class, to refer to no object at all. We say in this case that std holds a null reference. The null reference is written in Java as \"null\". You can store a null reference in the variable std by saying std = null; and you could test whether the value of std is null by testing if (std == null)",
  "page56": "If the value of a variable is null, then it is, of course, illegal to refer to instance variables or instance methods through that variable since there is no object, and hence no instance variabl\u00aees to refer to. For example, if the value of the variable std is null, then it would be illegal to refer to std.test1. If your program attempts to use a null reference illegally like this, the result is an error called a null pointer except\u00aeion. When one object variable is assigned to another, only a reference is copied. The object referred to is not copied. When the assignment \"std2 = std1;\" was executed, no new object was created. Instead, std2 was set to refer to the very same object that std1 refers to. This has some consequences that might be surprising. For ex\u00aeample, std1.name and std2.name are two different names for the same variable, namely the instance variable in the object that both std1 and std2 refer to. After the string \\\"Mary Jones\\\" is assigned to the variable std1.name, it is also true that the value of std2.name is \\\"Mary Jones\\\". There is a potential for a lot of confusion here, but you can help protect yourself from it if you\u00aekeep telling yourself, \"The object is not in the variable. The variable just holds a pointer to the object.\" You can test objects for equality and inequality using the operators == and !=, but here again, the semantics are different from what you are used to. When yo\u00aeu make a test \"if (std1 == std2)\", you are testing whether the values stored in std1 and std2 are the same. But the values are references to objects, not objects. So, you are testing whether std1 and std2 refer to the same object, that is, whether they point to the same location in memory. This is fine, if its what you want to do. But sometimes, what you want to check is whether the instance variables in the objects have the same values.",
  "page57": "When writing new classes, it's a good idea to pay attention to the issue of access control. Recall that making a member of a class public makes it accessible from anywhere, including from ot\u00aeher classes. On the other hand, a private member can only be used in the class where it is defined. In the opinion of many programmers, almost all member variables should be declared private. This gives you complete control over what can be\u00aedone with the variable. Even if the variable itself is private, you can allow other classes to find out what its value is by providing a public accessor method that returns the value of the variable. For example, if your class contains a private member variable, title, of type String, you can provide a method public String getTitle() { re\u00aeturn title; } that returns the value of title. By convention, the name of an accessor method for a variable is obtained by capitalizing the name of variable and adding \"get\" in front of the name. So, for the variable title, we get an accessor method named \"get\" + \"Title\", or getTitle(). Because of this naming convention, accessor methods are more often referred t\u00aeo as getter methods. A getter method provides \"read access\" to a variable. You might also want to allow \"write access\" to a private variable. That is, you might want to make it possible for other classes to specify a new value for the variable. This is done\u00aewith a setter method. (If you don't like simple, Anglo-Saxon words, you can use the fancier term mutator method.) The name of a setter method should consist of \"set\" followed by a capitalized copy of the variable's name, and it should have a parameter with the same type as the variable. A setter method for the variable title could be written public void setTitle( String newTitle ) { title = newTitle; } It is actually very co assigned to the variable is legal",
  "page58": "Even if you can't think of any extra chores to do in a getter or setter method, you might change your mind in the future when you redesign and improve your class. If you've used a gette\u00aer and setter from the beginning, you can make the modification to your class without affecting any of the classes that use your class. The private member variable is not part of the public interface of your class; only the public getter and\u00aesetter methods are. If you haven't used get and set from the beginning, you'll have to contact everyone who uses your class and tell them, \"Sorry guys, you'll have to track down every use that you've made of this variable and change your code to use my new get and set methods instead.\" A couple of final notes\u00ae: Some advanced aspects of Java rely on the naming convention for getter and setter methods, so it's a good idea to follow the convention rigorously. And though I've been talking about using getter and setter methods for a variable, you can define get and set methods even if there is no variable. A getter and/or setter method defines a property of the class, that might or might not\u00aecorrespond to a variable. For example, if a class includes a public void instance method with signature setValue(double), then the class has a \"property\" named value of type double, and it has this property whether or not the class has a member variable named value. C\u00aeonstructors and Object Initialization- Object types in Java are very different from the primitive types. Simply declaring a variable whose type is given as a class does not automatically create an object of that class. Objects must be explicitly constructed. For the computer, the process of constructing an object means, first, finding some unused memory in the heap that can be used to hold the object and, second, filling in the object's inll design of a program is created. Here, the idea is to identify things in the problem domain that can be modeled as objects. On another level, object-oriented programming encourages programmers to produce generalized software components that can be used in a wide variety of programming projects.",
  "page59": "Some Built-in Classes- Although the focus of object-oriented programming is generally on the design and implementation of new classes, it's important not to forget that the designers of Java\u00aehave already provided a large number of reusable classes. Some of these classes are meant to be extended to produce new classes, while others can be used directly to create useful objects. A true mastery of Java requires familiarity with a\u00aelarge number of built-in classes something that takes a lot of time and experience to develop. In the next chapter, we will begin the study of Java's GUI classes, and you will encounter other built-in classes throughout the remainder of this book. But let's take a moment to look at a few built-in classes that you might find usef\u00aeul. A string can be built up from smaller pieces using the + operator, but this is not very efficient. If str is a String and ch is a character, then executing the command \"str = str + ch;\" involves creating a whole new string that is a copy of str, with the value of ch appended onto the end. Copying the string takes some time. Building up a long string letter by letter would requir\u00aee a surprising amount of processing. The class StringBuffer makes it possible to be efficient about building up a long string from a number of smaller pieces. To do this, you must make an object belonging to the StringBuffer class. For example: StringBuffer buffer = new StringB\u00aeuffer(); (This statement both declares the variable buffer and initializes it to refer to a newly created StringBuffer object. Combining declaration with initialization was covered in Subsection 4.7.1 and works for objects just as it does for primitive types.) Like a String, a StringBuffer contains a sequence of characters. However, it is possible to add new characters onto the end of a StringBuffer without making a copy of the data that it already it.",
  "page60": "A number of useful classes are collected in the package java.util. For example, this package contains classes for working with collections of objects. We will study these collection classes in Ch\u00aeapter 10. Another class in this package, java.util.Date, is used to represent times. When a Date object is constructed without parameters, the result represents the current date and time, so an easy way to display this information is: Syste\u00aem.out.println( new Date() ); Of course, to use the Date class in this way, you must make it available by importing it with one of the statements \"import java.util.Date;\" or \"import java.util.*;\" at the beginning of your program. (See Subsection 4.5.3 for a discussion of packages and import.) I will also mention the clas\u00aes java.util.Random. An object belonging to this class is a source of random numbers (or, more precisely pseudorandom numbers). The standard function Math.random() uses one of these objects behind the scenes to generate its random numbers. An object of type Random can generate random integers, as well as random real numbers. If randGen is created with the command: Random randGen = new Random()\u00ae; and if N is a positive integer, then randGen.nextInt(N) generates a random integer in the range from 0 to N-1. For example, this makes it a little easier to roll a pair of dice. Instead of saying \"die1 = (int)(6*Math.random())+1;\", one can say \"die1 = randGen.n\u00aeextInt(6)+1;\". (Since you also have to import the class java.util.Random and create the Random object, you might not agree that it is actually easier.) An object of type Random can also be used to generate so-called Gaussian distributed random real numbers. The main point here, again, is that many problems have already been solved, and the solutions are available in Java's standard classes. If you are faced with a taypes, Long, Short, Byte, Float, and Boolean. These classes are called wrapper classes. Although they contain useful static members, they have another use as well: They are used for creating objects that represent primitive type values.",
  "page61": "The class \"Object\"- We have already seen that one of the major features of object-oriented programming is the ability to create subclasses of a class. The subclass inherits all the prop\u00aeerties or behaviors of the class, but can modify and add to what it inherits. In Section 5.5, you'll learn how to create subclasses. What you don't know yet is that every class in Java (with just one exception) is a subclass of so\u00aeme other class. If you create a class and don't explicitly make it a subclass of some other class, then it automatically becomes a subclass of the special class named Object. (Object is the one class that is not a subclass of any other class.) Class Object defines several instance methods that are inherited by every other class. These\u00aemethods can be used with any object whatsoever. I will mention just one of them here. You will encounter more of them later in the book. The instance method toString() in class Object returns a value of type String that is supposed to be a string representation of the object. You've already used this method implicitly, any time you've printed out an object or concatenated an object\u00aeonto a string. When you use an object in a context that requires a string, the object is automatically converted to type String by calling its toString() method. The version of toString that is defined in Object just returns the name of the class that the object belongs to, co\u00aencatenated with a code number called the hash code of the object; this is not very useful. When you create a class, you can write a new toString() method for it, which will replace the inherited version. For example, we might add the following method to any of the PairOfDice classes from the previous section.public String toString() { // Return a String representation of a pair of dice, where die1 // and die2 are instance variables containing the num",
  "page62": "Object-oriented Analysis and Design- Every programmer builds up a stock of techniques and expertise expressed as snippets of code that can be reused in new programs using the tried-and-true metho\u00aed of cut-and-paste: The old code is physically copied into the new program and then edited to customize it as necessary. The problem is that the editing is error-prone and time-consuming, and the whole enterprise is dependent on the program\u00aemer's ability to pull out that particular piece of code from last year's project that looks like it might be made to fit. (On the level of a corporation that wants to save money by not reinventing the wheel for each new project, just keeping track of all the old wheels becomes a major task.) Well-designed classes are software com\u00aeponents that can be reused without editing. A welldesigned class is not carefully crafted to do a particular job in a particular program. Instead, it is crafted to model some particular type of object or a single coherent concept. Since objects and concepts can recur in many problems, a well-designed class is likely to be reusable without modification in a variety of projects. Furthermore, in\u00aean object-oriented programming language, it is possible to make subclasses of an existing class. This makes classes even more reusable. If a class needs to be customized, a subclass can be created, and additions or modifications can be made in the subclass without making any c\u00aehanges to the original class. This can be done even if the programmer doesn't have access to the source code of the class and doesn't know any details of its internal, hidden implementation. The PairOfDice class in the previous section is already an example of a generalized software component, although one that could certainly be improved. The class represents a single, coherent concept, \"a pair of dice.\" Ts are often used). Admittedly, it's much more difficult to develop a general-purpose student class than a general-purpose pair-of-dice class. But this particular Student class is good mostly as an example in a programming textbook.",
  "page63": "A large programming project goes through a number of stages, starting with specification of the problem to be solved, followed by analysis of the problem and design of a program to solve it. Then\u00aecomes coding, in which the program's design is expressed in some actual programming language. This is followed by testing and debugging of the program. After that comes a long period of maintenance, which means fixing any new problems\u00aethat are found in the program and modifying it to adapt it to changing requirements. Together, these stages form what is called the software life cycle. (In the real world, the ideal of consecutive stages is seldom if ever achieved. During the analysis stage, it might turn out that the specifications are incomplete or inconsistent. A prob\u00aelem found during testing requires at least a brief return to the coding stage. If the problem is serious enough, it might even require a new design. Maintenance usually involves redoing some of the work from previous stages) Large, complex programming projects are only likely to succeed if a careful, systematic approach is adopted during all stages of the software life cycle. The systematic a\u00aepproach to programming, using accepted principles of good design, is called software engineering. The software engineer tries to efficiently construct programs that verifiably meet their specifications and that are easy to modify if necessary. There is a wide range of \"met\u00aehodologies\" that can be applied to help in the systematic design of programs. (Most of these methodologies seem to involve drawing little boxes to represent program components, with labeled arrows to represent relationships among the boxes.) We have been discussing object orientation in programming languages, which is relevant to the coding stage of program development. But there are also object-oriented methodologies forantage of similarities among classes. This is perhaps a bit simple-minded, but the idea is clear and the general approach can be effective: Analyze the problem to discover the concepts that are involved, and create classes to represent those concepts. The design should arise from the problem itself, and you should end up with a program whose structure reflects the structure of the problem in a natural way.",
  "page64": "Programming Example: Card, Hand, Deck- In this section, we look at some specific examples of object-oriented design in a domain that is simple enough that we have a chance of coming up with somet\u00aehing reasonably reusable. Consider card games that are played with a standard deck of playing cards (a so-called \"poker\" deck, since it is used in the game of poker). Designing the classes In a typical card game, each player gets\u00aea hand of cards. The deck is shuffled and cards are dealt one at a time from the deck and added to the players' hands. In some games, cards can be removed from a hand, and new cards can be added. The game is won or lost depending on the value (ace, 2, king) and suit (spades, diamonds, clubs, hearts) of the cards that a player receives\u00ae. If we look for nouns in this description, there are several candidates for objects: game, player, hand, card, deck, value, and suit. Of these, the value and the suit of a card are simple values, and they will just be represented as instance variables in a Card object. In a complete program, the other five nouns might be represented by classes. But let's work on the ones that are most o\u00aebviously reusable: card, hand, and deck. If we look for verbs in the description of a card game, we see that we can shuffle a deck and deal a card from a deck. This gives use us two candidates for instance methods in a Deck class: shuffle() and dealCard(). Cards can be added to\u00aeand removed from hands. This gives two candidates for instance methods in a Hand class: addCard() and removeCard(). Cards are relatively passive things, but we need to be able to determine their suits and values. We will discover more instance methods as we go along.",
  "page65": "The Basic GUI Application- There are two basic types of GUI program in Java: stand-alone applications and applets. An applet is a program that runs in a rectangular area on a Web page. Applets ar\u00aee generally small programs, meant to do fairly simple things, although there is nothing to stop them from being very complex. Applets were responsible for a lot of the initial excitement about Java when it was introduced, since they could d\u00aeo things that could not otherwise be done on Web pages. However, there are now easier ways to do many of the more basic things that can be done with applets, and they are no longer the main focus of interest in Java. Nevertheless, there are still some things that can be done best with applets, and they are still fairly common on the Web. W\u00aee will look at applets in the next section. A stand-alone application is a program that runs on its own, without depending on a Web browser. You've been writing stand-alone applications all along. Any class that has a main() routine defines a stand-alone application; running the program just means executing this main() routine. However, the programs that you've seen up till now have\u00aebeen \"commandline\" programs, where the user and computer interact by typing things back and forth to each other. A GUI program offers a much richer type of user interface, where the user uses a mouse and keyboard to interact with GUI components such as windows, menus\u00ae, buttons, check boxes, text input boxes, scroll bars, and so on. The main routine of a GUI program creates one or more such components and displays them on the computer screen. Very often, that's all it does. Once a GUI component has been created, it follows its own programming programming that tells it how to draw itself on the screen and how to respond to events such as being clicked on by the user. A GUI program doesn't have to be immensely complex. We can, for example, write a very simple GUI \"Hello World\" program that says \"Hello\" to the user, but does it by opening a window where the the greeting is displayed: import javax.swing.JOptionPane; public class HelloWorldGUI1 { public static void main(String[] args) { JOptionPane.showMessageDialog( null, \\\"Hello World!\\\" ); } }",
  "page66": "When this program is run, a window appears on the screen that contains the message \"Hello World!\". The window also contains an \"OK\" button for the user to click after reading\u00aethe message. When the user clicks this button, the window closes and the program ends. By the way, this program can be placed in a file named HelloWorldGUI1.java, compiled, and run just like any other Java program. Now, this program is alre\u00aeady doing some pretty fancy stuff. It creates a window, it draws the contents of that window, and it handles the event that is generated when the user clicks the button. The reason the program was so easy to write is that all the work is done by showMessageDialog(), a static method in the built-in class JOptionPane. (Note that the source c\u00aeode \"imports\" the class javax.swing.JOptionPane to make it possible to refer to the JOptionPane class using its simple name. See Subsection 4.5.3 for information about importing classes from Java's standard packages.) If you want to display a message to the user in a GUI program, this is a good way to do it: Just use a standard class that already knows how to do the work! And i\u00aen fact, JOptionPane is regularly used for just this purpose (but as part of a larger program, usually). Of course, if you want to do anything serious in a GUI program, there is a lot more to learn. To give you an idea of the types of things that are involved, we'll look at\u00aea short GUI program that does the same things as the previous program open a window containing a message and an OK button, and respond to a click on the button by ending the program but does it all by hand instead of by using the built-in JOptionPane class. Mind you, this is not a good way to write the program, but it will illustrate some important aspects of GUI programming in Java. Here is the source code for the program. You are not expected to understand it yet. I will explain how it works below, but it will take the rest of the chapter before you will really understand completely. In this section, you will just get a brief overview of GUI programming.",
  "page67": "JFrame and JPanel- In a Java GUI program, each GUI component in the interface is represented by an object in the program. One of the most fundamental types of component is the window. Windows hav\u00aee many behaviors. They can be opened and closed. They can be resized. They have \"titles\" that are displayed in the title bar above the window. And most important, they can contain other GUI components such as buttons and menus. Ja\u00aeva, of course, has a built-in class to represent windows. There are actually several different types of window, but the most common type is represented by the JFrame class (which is included in the package javax.swing). A JFrame is an independent window that can, for example, act as the main window of an application. One of the most import\u00aeant things to understand is that a JFrame object comes with many of the behaviors of windows already programmed in. In particular, it comes with the basic properties shared by all windows, such as a titlebar and the ability to be opened and closed. Since a JFrame comes with these behaviors, you don't have to program them yourself! This is, of course, one of the central ideas of objectori\u00aeented programming. What a JFrame doesn't come with, of course, is content, the stuff that is contained in the window. If you don't add any other content to a JFrame, it will just display a large blank area. You can add content either by creating a JFrame object and t\u00aehen adding the content to it or by creating a subclass of JFrame and adding the content in the constructor of that subclass. The main program above declares a variable, window, of type JFrame and sets it to refer to a new window object with the statement: JFrame window = new JFrame(\\\"GUI Test\\\");",
  "page68": "The content that is displayed in a JFrame is called its content pane. (In addition to its content pane, a JFrame can also have a menu bar, which is a separate thing that I will talk about later.)\u00aeA basic JFrame already has a blank content pane; you can either add things to that pane or you can replace the basic content pane entirely. In my sample program, the line window.setContentPane(content) replaces the original blank content p\u00aeane with a different component. (Remember that a \"component\" is just a visual element of a graphical user interface). In this case, the new content is a component of type JPanel. JPanel is another of the fundamental classes in Swing. The basic JPanel is, again, just a blank rectangle. There are two ways to make a useful JPanel: T\u00aehe first is to add other components to the panel; the second is to draw something in the panel. Both of these techniques are illustrated in the sample program. In fact, you will find two JPanels in the program: content, which is used to contain other components, and displayPanel, which is used as a drawing surface. Let's look more closely at displayPanel. This variable is of type HelloWo\u00aerldDisplay, which is a nested static class inside the HelloWorldGUI2 class. (Nested classes were introduced in This class defines just one instance method, paintComponent(), which overrides a method of the same name in the JPanel class. Components and Layout- Another way of usi\u00aeng a JPanel is as a container to hold other components. Java has many classes that define GUI components. Before these components can appear on the screen, they must be added to a container. In this program, the variable named content refers to a JPanel that is used as a container, and two other components are added to that container. This is done in the statements: content.add(displayPanel, BorderLayout.CENTER); content.add(okButton, BorderLayout.SOUTH); Here, content refers to an object of type JPanel; later in the program, this panel becomes the content pane of the window. The first component that is added to content is displayPanel which, as discussed above, displays the message, \"Hello World!\". The second is okButton which represents the button that the user clicks to close the window. The variable okButton is of type JButton, the Java class that represents push buttons.",
  "page69": "The \"BorderLayout\" stuff in these statements has to do with how the two components are arranged in the container. When components are added to a container, there has to be some way of d\u00aeeciding how those components are arranged inside the container. This is called \"laying out\" the components in the container, and the most common technique for laying out components is to use a layout manager. A layout manager is a\u00aen object that implements some policy for how to arrange the components in a container; different types of layout manager implement different policies. One type of layout manager is defined by the BorderLayout class. In the program, the statement. content.setLayout(new BorderLayout()); creates a new BorderLayout object and tells the content\u00aepanel to use the new object as its layout manager. Essentially, this line determines how components that are added to the content panel will be arranged inside the panel. We will cover layout managers in much more detail later, but for now all you need to know is that adding okButton in the BorderLayout.SOUTH position puts the button at the bottom of the panel, and putting displayPanel in th\u00aee BorderLayout.CENTER position makes it fill any space that is not taken up by the button. This example shows a general technique for setting up a GUI: Create a container and assign a layout manager to it, create components and add them to the container, and use the container a\u00aes the content pane of a window or applet. A container is itself a component, so it is possible that some of the components that are added to the top-level container are themselves containers, with their own layout managers and components. This makes it possible to build up complex user interfaces in a hierarchical fashion, with containers inside containers inside containers.",
  "page70": "Events and Listeners- The structure of containers and components sets up the physical appearance of a GUI, but it doesn't say anything about how the GUI behaves. That is, what can the user d\u00aeo to the GUI and how will it respond? GUIs are largely event-driven; that is, the program waits for events that are generated by the user's actions (or by some other cause). When an event occurs, the program responds by executing an ev\u00aeent-handling method. In order to program the behavior of a GUI, you have to write event-handling methods to respond to the events that you are interested in. The most common technique for handling events in Java is to use event listeners. A listener is an object that includes one or more event-handling methods. When an event is detected by\u00aeanother object, such as a button or menu, the listener object is notified and it responds by running the appropriate event-handling method. An event is detected or generated by an object. Another object, the listener, has the responsibility of responding to the event. The event itself is actually represented by a third object, which carries information about the type of event, when it occurr\u00aeed, and so on. This division of responsibilities makes it easier to organize large programs. As an example, consider the OK button in the sample program. When the user clicks the button, an event is generated. This event is represented by an object belonging to the class Action\u00aeEvent. The event that is generated is associated with the button; we say that the button is the source of the event. The listener object in this case is an object belonging to the class ButtonHandler, which is defined as a nested class inside HelloWorldGUI2. private static class ButtonHandler implements ActionListener { public void actionPerformed(ActionEvent e) { System.exit(0); } } This class implements the ActionListener interface a requirement for listener objects that handle events from buttons. The eventhandling method is named actionPerformed, as specified by the ActionListener interface. This method contains the code that is executed when the user clicks the button; in this case, the code is a call to System.exit(), which will terminate the program. There is one more ingredient that is necessary to get the event from the button to the listener object:",
  "page71": "This statement tells okButton that when the user clicks the button, the ActionEvent that is generated should be sent to listener. Without this statement, the button has no way of knowing that som\u00aee other object would like to listen for events from the button. This example shows a general technique for programming the behavior of a GUI: Write classes that include event-handling methods. Create objects that belong to these classes and\u00aeregister them as listeners with the objects that will actually detect or generate the events. When an event occurs, the listener is notified, and the code that you wrote in one of its event-handling methods is executed. At first, this might seem like a very roundabout and complicated way to get things done, but as you gain experience with\u00aeit, you will find that it is very flexible and that it goes together very well with object oriented programming. (We will return to events and listeners in much more detail in Section 6.3 and later sections, and I do not expect you to completely understand them at this time.) Applets and HTML- Although stand-alone applications are probably more important than applets at this point in the his\u00aetory of Java, applets are still widely used. They can do things on Web pages that can't easily be done with other technologies. It is easy to distribute applets to users: The user just has to open a Web page, and the applet is there, with no special installation required (\u00aealthough the user must have an appropriate version of Java installed on their computer). And of course, applets are fun; now that the Web has become such a common part of life, it's nice to be able to see your work running on a web page. The good news is that writing applets is not much different from writing stand-alone applications. The structure of an applet is essentially the same as the structure of the JFrames that were introduced in the previous section, and events are handled in the same way in both types of program. So, most of what you learn about applications applies to applets, and vice versa. Of course, one difference is that an applet is dependent on a Web page, so to use applets effectively, you have to learn at least a little about creating Web pages. Web pages are written using a language called HTML (HyperText Markup Language). In Subsection 6.2.3, below, you'll learn how to use HTML to create Web pages that display applets.",
  "page72": "JApplet- The JApplet class (in package javax.swing) can be used as a basis for writing applets in the same way that JFrame is used for writing stand-alone applications. The basic JApplet class re\u00aepresents a blank rectangular area. Since an applet is not a stand-alone application, this area must appear on a Web page, or in some other environment that knows how to display an applet. Like a JFrame, a JApplet contains a content pane (an\u00aed can contain a menu bar). You can add content to an applet either by adding content to its content pane or by replacing the content pane with another component. In my examples, I will generally create a JPanel and use it as a replacement for the applet's content pane. To create an applet, you will write a subclass of JApplet. The JAp\u00aeplet class defines several instance methods that are unique to applets. These methods are called by the applet's environment at certain points during the applet's \"life cycle.\" In the JApplet class itself, these methods do nothing; you can override these methods in a subclass. The most important of these special applet methods is public void init() An applet's init()\u00aemethod is called when the applet is created. You can use the init() method as a place where you can set up the physical structure of the applet and the event handling that will determine its behavior. (You can also do some initialization in the constructor for your class, but t\u00aehere are certain aspects of the applet's environment that are set up after its constructor is called but before the init() method is called, so there are a few operations that will work in the init() method but will not work in the constructor.) The other applet life-cycle methods are start(), stop(), and destroy(). I will not use these methods for the time being and will not discuss them here except to mention that destroy() is called at the end of the applet's lifetime and can be used as a place to do any necessary cleanup, such as closing any windows that were opened by the applet.",
  "page73": "Graphics and Painting- Everthing you see on a computer screen has to be drawn there, even the text. The Java API includes a range of classes and methods that are devoted to drawing. In this secti\u00aeon, I'll look at some of the most basic of these. The physical structure of a GUI is built of components. The term component refers to a visual element in a GUI, including buttons, menus, text-input boxes, scroll bars, check boxes, and\u00aeso on. In Java, GUI components are represented by objects belonging to subclasses of the class java.awt.Component. Most components in the Swing GUI although not top-level components like JApplet and JFramebelong to subclasses of the class javax.swing.JComponent, which is itself a subclass of java.awt.Component. Every component is responsi\u00aeble for drawing itself. If you want to use a standard component, you only have to add it to your applet or frame. You don't have to worry about painting it on the screen. That will happen automatically, since it already knows how to draw itself. Fonts- A font represents a particular size and style of text. The same character will appear different in different fonts. In Java, a font is ch\u00aearacterized by a font name, a style, and a size. The available font names are system dependent, but you can always use the following four strings as font names: \"Serif\", \"SansSerif\", \"Monospaced\", and \"Dialog\". (A \"serif\" is a l\u00aeittle decoration on a character, such as a short horizontal line at the bottom of the letter i. \"SansSerif\" means \"without serifs.\" \"Monospaced\" means that all the characters in the font have the same width. The \"Dialog\" font is the one that is typically used in dialog boxes.). Shapes The Graphics class includes a large number of instance methods for drawing various shapes, such as lines, rectangles, and ovals. The shapes are specified using the (x,y) coordinate system described above. They are drawn in the current drawing color of the graphics context. The current drawing color is set to the foreground color of the component when the graphics context is created, but it can be changed at any time using the setColor() method. Here is a list of some of the most important drawing methods. With all these commands, any drawing that is done outside the boundaries of the component is ignored.",
  "page74": "Mouse Events- Events are central to programming for a graphical user interface. A GUI program doesn't have a main() routine that outlines what will happen when the program is run, in a step-\u00aeby-step process from beginning to end. Instead, the program must be prepared to respond to various kinds of events that can happen at unpredictable times and in an order that the program doesn't control. The most basic kinds of events\u00aeare generated by the mouse and keyboard. The user can press any key on the keyboard, move the mouse, or press a button on the mouse. The user can do any of these things at any time, and the computer has to respond appropriately. In Java, events are represented by objects. When an event occurs, the system collects all the information releva\u00aent to the event and constructs an object to contain that information. Different types of events are represented by objects belonging to different classes. For example, when the user presses one of the buttons on a mouse, an object belonging to a class called MouseEvent is constructed. The object contains information such as the source of the event (that is, the component on which the user cli\u00aecked), the (x,y) coordinates of the point in the component where the click occurred, and which button on the mouse was pressed. When the user presses a key on the keyboard, a KeyEvent is created. After the event object is constructed, it is passed as a parameter to a designated\u00aesubroutine. By writing that subroutine, the programmer says what should happen when the event occurs. As a Java programmer, you get a fairly high-level view of events. There is a lot of processing that goes on between the time that the user presses a key or moves the mouse and the time that a subroutine in your program is called to respond to the event. Fortunately, you don't need to know much about that processing. But you should understand this much: Even though your GUI program doesn't have a main() routine, there is a sort of main routine running somewhere that executes a loop of the form while the program is still running: Wait for the next event to occur Call a subroutine to handle the event This loop is called an event loop. Every GUI program has an event loop. In Java, you don't have to write the loop. It's part of \"the system.\" If you write a GUI program in some other language.",
  "page75": "Event Handling- For an event to have any effect, a program must detect the event and react to it. In order to detect an event, the program must \"listen\" for it. Listening for events is\u00aesomething that is done by an object called an event listener. An event listener object must contain instance methods for handling the events for which it listens. For example, if an object is to serve as a listener for events of type MouseE\u00aevent, then it must contain the following method (among several others): public void mousePressed(MouseEvent evt) { } The body of the method defines how the object responds when it is notified that a mouse button has been pressed. The parameter, evt, contains information about the event. This information can be used by the listener object t\u00aeo determine its response. The methods that are required in a mouse event listener are specified in an interface named MouseListener. To be used as a listener for mouse events, an object must implement this MouseListener interface. (To review briefly: An interface in Java is just a list of instance methods. A class can \"implement\" an interface by doing two things. First, the class mu\u00aest be declared to implement the interface, as in \"class MyListener implements MouseListener\" or \"class MyApplet extends JApplet implements MouseListener\". Second, the class must include a definition for each instance method specified in the interface. An int\u00aeerface can be used as the type for a variable or formal parameter. We say that an object implements the MouseListener interface if it belongs to a class that implements the MouseListener interface. Note that it is not enough for the object to include the specified methods. It must also belong to a class that is specifically declared to implement the interface.) Many events in Java are associated with GUI components. For example, when the user presses a button on the mouse, the associated component is the one that the user clicked on. Before a listener object can \"hear\" events associated with a given component, the listener object must be registered with the component. If a MouseListener object, mListener, needs to hear mouse events associated with a Component object, comp, the listener must be registered with the component by calling \"comp.addMouseListener(mListener);\".",
  "page76": "1. Put the import specification \"import java.awt.event.*;\" (or individual imports) at the beginning of your source code; 2. Declare that some class implements the appropriate listener i\u00aenterface, such as MouseListener; 3. Provide definitions in that class for the subroutines from the interface; 4. Register the listener object with the component that will generate the events by calling a method such as addMouseListener() in\u00aethe component. Any object can act as an event listener, provided that it implements the appropriate interface. A component can listen for the events that it itself generates. A panel can listen for events from components that are contained in the panel. Keyboard Events- In Java, user actions become events in a program. These events are as\u00aesociated with GUI components. When the user presses a button on the mouse, the event that is generated is associated with the component that contains the mouse cursor. What about keyboard events? When the user presses a key, what component is associated with the key event that is generated? A GUI uses the idea of input focus to determine the component associated with keyboard events. At any g\u00aeiven time, exactly one interface element on the screen has the input focus, and that is where all keyboard events are directed. If the interface element happens to be a Java component, then the information about the keyboard event becomes a Java object of type KeyEvent, and it\u00aeis delivered to any listener objects that are listening for KeyEvents associated with that component. The necessity of managing input focus adds an extra twist to working with keyboard events. It's a good idea to give the user some visual feedback about which component has the input focus. For example, if the component is the typing area of a word-processor, the feedback is usually in the form of a blinking text cursor. Another common visual clue is to draw a brightly colored border around the edge of a component when it has the input focus, as I do in the examples given later in this section.",
  "page77": "A component that wants to have the input focus can call the method requestFocus(), which is defined in the Component class. Calling this method does not absolutely guarantee that the component wi\u00aell actually get the input focus. Several components might request the focus; only one will get it. This method should only be used in certain circumstances in any case, since it can be a rude surprise to the user to have the focus suddenly\u00aepulled away from a component that the user is working with. In a typical user interface, the user can choose to give the focus to a component by clicking on that component with the mouse. And pressing the tab key will often move the focus from one component to another. Some components do not automatically request the input focus when the u\u00aeser clicks on them. To solve this problem, a program has to register a mouse listener with the component to detect user clicks. In response to a user click, the mousePressed() method should call requestFocus() for the component. This is true, in particular, for the components that are used as drawing surfaces in the examples in this chapter. These components are defined as subclasses of JPane\u00ael, and JPanel objects do not receive the input focus automatically. If you want to be able to use the keyboard to interact with a JPanel named drawingSurface, you have to register a listener to listen for mouse events on the drawingSurface and call drawingSurface.requestFocus()\u00aein the mousePressed() method of the listener object. As our first example of processing key events, we look at a simple program in which the user moves a square up, down, left, and right by pressing arrow keys. When the user hits the 'R', 'G', 'B', or 'K' key, the color of the square is set to red, green, blue, or black, respectively. Of course, none of these key events are delivered to the program unless it has the input focus. The panel in the program changes its appearance when it has the input focus: When it does, a cyan-colored border is drawn around the panel; when it does not, a gray-colored border is drawn. Also, the panel displays a different message in each case. If the panel does not have the input focus, the user can give the input focus to the panel by clicking on it. The complete source code for this example can be found in the file KeyboardAndFocusDemo.java.",
  "page78": "State Machines- The information stored in an object's instance variables is said to represent the state of that object. When one of the object's methods is called, the action taken by t\u00aehe object can depend on its state. (Or, in the terminology we have been using, the definition of the method can look at the instance variables to decide what to do.) Furthermore, the state can change. (That is, the definition of the method\u00aecan assign new values to the instance variables.) In computer science, there is the idea of a state machine, which is just something that has a state and can change state in response to events or inputs. The response of a state machine to an event or input depends on what state it's in. An object is a kind of state machine. Sometimes,\u00aethis point of view can be very useful in designing classes. The state machine point of view can be especially useful in the type of event-oriented programming that is required by graphical user interfaces. When designing a GUI program, you can ask yourself: What information about state do I need to keep track of? What events can change the state of the program? How will my response to a give\u00aen event depend on the current state? Should the appearance of the GUI be changed to reflect a change in state? How should the paintComponent() method take the state into account? All this is an alternative to the top-down, step-wise-refinement style of program design, which doe\u00aes not apply to the overall design of an event-oriented program. Basic Components- In preceding sections, you've seen how to use a graphics context to draw on the screen and how to handle mouse events and keyboard events. In one sense, that's all there is to GUI programming. If you're willing to program all the drawing and handle all the mouse and keyboard events, you have nothing more to learn. However, you would either be doing a lot more work than you need to do, or you would be limiting yourself to very simple user interfaces. A typical user interface uses standard GUI components such as buttons, scroll bars, text-input boxes, and menus. These components have already been written for you, so you don't have to duplicate the work involved in developing them. They know how to draw themselves, and they can handle the details of processing the mouse and keyboard events that concern them.",
  "page79": "Consider one of the simplest user interface components, a push button. The button has a border, and it displays some text. This text can be changed. Sometimes the button is disabled, so that clic\u00aeking on it doesn't have any effect. When it is disabled, its appearance changes. When the user clicks on the push button, the button changes appearance while the mouse button is pressed and changes back when the mouse button is release\u00aed. In fact, it's more complicated than that. If the user moves the mouse outside the push button before releasing the mouse button, the button changes to its regular appearance. To implement this, it is necessary to respond to mouse exit or mouse drag events. Furthermore, on many platforms, a button can receive the input focus. The bu\u00aetton changes appearance when it has the focus. If the button has the focus and the user presses the space bar, the button is triggered. This means that the button must respond to keyboard and focus events as well. Fortunately, you don't have to program any of this, provided you use an object belonging to the standard class javax.swing.JButton. A JButton object draws itself and processes\u00aemouse, keyboard, and focus events on its own. You only hear from the Button when the user triggers it by clicking on it or pressing the space bar while the button has the input focus. When this happens, the JButton object creates an event object belonging to the class java.awt.\u00aeevent.ActionEvent. The event object is sent to any registered listeners to tell them that the button has been pushed. Your program gets only the information it needs the fact that a button was pushed. JTextField and JTextArea- The JTextField and JTextArea classes represent components that contain text that can be edited by the user. A JTextField holds a single line of text, while a JTextArea can hold multiple lines. It is also possible to set a JTextField or JTextArea to be read-only so that the user can read the text that it contains but cannot edit the text. Both classes are subclasses of an abstract class, JTextComponent, which defines their common properties. JTextField and JTextArea have many methods in common. The instance method setText(), which takes a parameter of type String, can be used to change the text that is displayed in an input component.",
  "page80": "Dialogs- One of the commands in the \"Color\" menu of the MosaicDraw program is \"Custom Color\". When the user selects this command, a new window appears where the user can selec\u00aet a color. This window is an example of a dialog or dialog box. A dialog is a type of window that is generally used for short, single purpose interactions with the user. For example, a dialog box can be used to display a message to the user\u00ae, to ask the user a question, to let the user select a file to be opened, or to let the user select a color. In Swing, a dialog box is represented by an object belonging to the class JDialog or to a subclass. The JDialog class is very similar to JFrame and is used in much the same way. Like a frame, a dialog box is a separate window. Unlik\u00aee a frame, however, a dialog is not completely independent. Every dialog is associated with a frame (or another dialog), which is called its parent window. The dialog box is dependent on its parent. For example, if the parent is closed, the dialog box will also be closed. It is possible to create a dialog box without specifying a parent, but in that case a an invisible frame is created by the\u00aesystem to serve as the parent. Dialog boxes can be either modal or modeless. When a modal dialog is created, its parent frame is blocked. That is, the user will not be able to interact with the parent until the dialog box is closed. Modeless dialog boxes do not block their par\u00aeents in the same way, so they seem a lot more like independent windows. In practice, modal dialog boxes are easier to use and are much more common than modeless dialogs.",
  "page81": "Creating Jar Files- As the final topic for this chapter, we look again at jar files. Recall that a jar file is a \"java archive\" that can contain a number of class files. When creating a\u00aeprogram that uses more than one class, it's usually a good idea to place all the classes that are required by the program into a jar file, since then a user will only need that one file to run the program. discusses how a jar file can\u00aebe used for an applet. Jar files can also be used for stand-alone applications. In fact, it is possible to make a so-called executable jar file. A user can run an executable jar file in much the same way as any other application, usually by double-clicking the icon of the jar file. (The user's computer must have a correct version of\u00aeJava installed, and the computer must be configured correctly for this to work. The configuration is usually done automatically when Java is installed, at least on Windows and Mac OS.) The question, then, is how to create a jar file. The answer depends on what programming environment you are using. The two basic types of programming environment command line and IDE were discussed in Section 2\u00ae.6. Any IDE (Integrated Programming Environment) for Java should have a command for creating jar files. In the Eclipse IDE, for example, it's done as follows: In the Package Explorer pane, select the programming project (or just all the individual source code files that yo\u00aeu need). Right-click on the selection, and choose \"Export\" from the menu that pops up. In the window that appears, select \"JAR file\" and click \"Next\". In the window that appears next, enter a name for the jar file in the box labeled \"JAR file\". (Click the \"Browse\" button next to this box to select the file name using a file dialog box.) The name of the file should end with \".jar\". If you are creating a regular jar file, not an executable one, you can hit \"Finish\" at this point, and the jar file will be created. You could do this, for example, if the jar file contains an applet but no main program. To create an executable file, hit the \"Next\" button twice to get to the \"Jar Manifest Specification\" screen. At the bottom of this screen is an input box labeled \"Main class\".",
  "page82": "Creating and Using Arrays- When a number of data items are chunked together into a unit, the result is a data structure. Data structures can be very complex, but in many applications, the appropr\u00aeiate data structure consists simply of a sequence of data items. Data structures of this simple variety can be either arrays or records. The term \"record\" is not used in Java. A record is essentially the same as a Java object that\u00aehas instance variables only, but no instance methods. Some other languages, which do not support objects in general, nevertheless do support records. The C programming language, for example, is not object-oriented, but it has records, which in C go by the name \"struct.\" The data items in a record in Java, an object's instan\u00aece variables are called the fields of the record. Each item is referred to using a field name. In Java, field names are just the names of the instance variables. The distinguishing characteristics of a record are that the data items in the record are referred to by name and that different fields in a record are allowed to be of different types. Arrays- Like a record, an array is a sequence of\u00aeitems. However, where items in a record are referred to by name, the items in an array are numbered, and individual items are referred to by their position number. Furthermore, all the items in an array must be of the same type. The definition of an array is: a numbered sequen\u00aece of items, which are all of the same type. The number of items in an array is called the length of the array. The position number of an item in an array is called the index of that item. The type of the individual items in an array is called the base type of the array. The base type of an array can be any Java type, that is, one of the primitive types, or a class name, or an interface name. If the base type of an array is int, it is referred to as an \"array of ints.\" An array with base type String is referred to as an \"array of Strings.\" However, an array is not, properly speaking, a list of integers or strings or other values. It is better thought of as a list of variables of type int, or of type String, or of some other type. As always, there is some potential for confusion between the two uses of a variable: as a name for a memory location and as a name for the value stored in that memory location.",
  "page83": "Java is a general-purpose, robust, secure, and object-oriented programming language. It is a high-level language, I.e., its syntax uses English like language. It was developed by Sun Microsystems\u00aein the year 1995. It is now maintained and distributed by Oracle. Java has its runtime environment and API; therefore, it is also called a platform. Java is used in a large number of applications over the years. However, it has various adv\u00aeantages and disadvantages given below. Java is a simple programming language since it is easy to learn and easy to understand. Its syntax is based on C++, and it uses automatic garbage collection; therefore, we don't need to remove the unreferenced objects from memory. Java has also removed the features like explicit pointers, operator ove\u00aerloading, etc., making it easy to read and write. Java uses an object-oriented paradigm, which makes it more practical. Everything in Java is an object which takes care of both data and behavior. Java uses object-oriented concepts like object, class, inheritance, encapsulation, polymorphism, and abstraction. Java is a secured programming language because it doesn't use Explicit pointers. Also\u00ae, Java programs run inside the virtual machine sandbox. JRE also provides a classloader, which is used to load the class into JVM dynamically. It separates the class packages of the local file system from the ones that are being imported from the network. Java is a robust progr\u00aeamming language since it uses strong memory management. We can also handle exceptions through the Java code. Also, we can use type checking to make our code more secure. It doesn't provide explicit pointers so that the programmer cannot access the memory directly from the code. Java code can run on multiple platforms directly, I.e., we need not compile it every time. It is right once, runs anywhere language (WORA) which can be converted into byte code at the compile time. The byte code is a platform-independent code that can run on multiple platforms. Java uses a multi-threaded environment in which a bigger task can be converted into various threads and run separately. The main advantage of multi-threading is that we need not provide memory to every running thread.",
  "page84": "The items in an array really, the individual variables that make up the array are more often referred to as the elements of the array. In Java, the elements in an array are always numbered starti\u00aeng from zero. That is, the index of the first element in the array is zero. If the length of the array is N, then the index of the last element in the array is N-1. Once an array has been created, its length cannot be changed. Java arrays a\u00aere objects. This has several consequences. Arrays are created using a form of the new operator. No variable can ever hold an array; a variable can only refer to an array. Any variable that can refer to an array can also hold the value null, meaning that it doesn't at the moment refer to anything. Like any object, an array belongs to a\u00aeclass, which like all classes is a subclass of the class Object. The elements of the array are, essentially, instance variables in the array object, except that they are referred to by number rather than by name. Nevertheless, even though arrays are objects, there are differences between arrays and other kinds of objects, and there are a number of special language features in Java for creati\u00aeng and using arrays. Random Access- So far, all my examples of array processing have used sequential access. That is, the elements of the array were processed one after the other in the sequence in which they occur in the array. But one of the big advantages of arrays is that t\u00aehey allow random access. That is, every element of the array is equally accessible at any given time. As an example, let's look at a well-known problem called the birthday problem: Suppose that there are N people in a room. What's the chance that there are two people in the room who have the same birthday? (That is, they were born on the same day in the same month, but not necessarily in the same year.) Most people severely underestimate the probability. We",
  "page85": "will actually look at a different version of the question: Suppose you choose people at random and check their birthdays. How many people will you check before you find one who has the same birth\u00aeday as someone you've already checked? Of course, the answer in a particular case depends on random factors, but we can simulate the experiment with a computer program and run the program several times to get an idea of how many people\u00aeneed to be checked on average. Dynamic Arrays and ArrayLists- The size of an array is fixed when it is created. In many cases, however, the number of data items that are actually stored in the array varies with time. Consider the following examples: An array that stores the lines of text in a word-processing program. An array that holds t\u00aehe list of computers that are currently downloading a page from a Web site. An array that contains the shapes that have been added to the screen by the user of a drawing program. Clearly, we need some way to deal with cases where the number of data items in an array is not fixed. Partially Full Arrays- Consider an application where the number of items that we want to store in an array changes\u00aeas the program runs. Since the size of the array can't actually be changed, a separate counter variable must be used to keep track of how many spaces in the array are in use. (Of course, every space in the array has to contain something; the question is, how many spaces c\u00aeontain useful or valid items?) Consider, for example, a program that reads positive integers entered by the user and stores them for later processing. The program stops reading when the user inputs a number that is less than or equal to zero. The input numbers can be kept in an array, numbers, of type int[ ]. Let's say that no more than 100 numbers will be input. Then the size of the array can be fixed at 100. But the program must keep track of how many numbers have actually been read and stored in the array. For this, it can use an integer variable, numCount. Each time a number is stored in the array, numCount must be incremented by one. As a rather silly example, let's write a program that will read the numbers input by the user and then print them in reverse order.",
  "page86": "Dynamic Arrays- In each of the above examples, an arbitrary limit was set on the number of items 100 ints, 10 Players, 100 Shapes. Since the size of an array is fixed, a given array can only hold\u00aea certain maximum number of items. In many cases, such an arbitrary limit is undesirable. Why should a program work for 100 data values, but not for 101? The obvious alternative of making an array that's so big that it will work in an\u00aey practical case is not usually a good solution to the problem. It means that in most cases, a lot of computer memory will be wasted on unused space in the array. That memory might be better used for something else. And what if someone is using a computer that could handle as many data values as the user actually wants to process, but does\u00aen't have enough memory to accommodate all the extra space that you've allocated for your huge array? Clearly, it would be nice if we could increase the size of an array at will. This is not possible, but what is possible is almost as good. Remember that an array variable does not actually hold an array. It just holds a reference to an array object. We can't make the array bigge\u00aer, but we can make a new, bigger array object and change the value of the array variable so that it refers to the bigger array. Of course, we also have to copy the contents of the old array into the new array. The array variable then refers to an array object that contains all\u00aethe data of the old array, with room for additional data. The old array will be garbage collected, since it is no longer in use. ArrrayLists- The DynamicArrayOfInt class could be used in any situation where an array of int with no preset limit on the size is needed. However, if we want to store Shapes instead of ints, we would have to define a new class to do it. That class, probably named \"DynamicArrayOfShape\", would look exactly the same as the DynamicArrayOfInt class except that everywhere the type \"int\" appears, it would be replaced by the type \"Shape\". Similarly, we could define a DynamicArrayOfDouble class, a DynamicArrayOfPlayer class, and so on. But there is something a little silly about this, since all these classes are close to being identical. It would be nice to be able to write some kind of source code, once and for all,",
  "page87": "In Java, every class is a subclass of the class named Object. This means that every object can be assigned to a variable of type Object. Any object can be put into an array of type Object[ ]. If\u00aewe defined a DynamicArrayOfObject class, then we could store objects of any type. This is not true generic programming, and it doesn't apply to the primitive types such as int and double. But it does come close. In fact, there is no ne\u00aeed for us to define a DynamicArrayOfObject class. Java already has a standard class named ArrayList that serves much the same purpose. The ArrayList class is in the package java.util, so if you want to use it in a program, you should put the directive \"import java.util.ArrayList;\" at the beginning of your source code file. The Ar\u00aerayList class differs from my DynamicArrayOfInt class in that an ArrayList object always has a definite size, and it is illegal to refer to a position in the ArrayList that lies outside its size. In this, an ArrayList is more like a regular array. However, the size of an ArrayList can be increased at will. The ArrayList class defines many instance methods. Parameterized Types The main differe\u00aence between true generic programming and the ArrayList examples in the previous subsection is the use of the type Object as the basic type for objects that are stored in a list. This has at least two unfortunate consequences: First, it makes it necessary to use type-casting in\u00aealmost every case when an element is retrieved from that list. Second, since any type of object can legally be added to the list, there is no way for the compiler to detect an attempt to add the wrong type of object to the list; the error will be detected only at run time when the object is retrieved from the list and the attempt to type-cast the object fails. Compare this to arrays. An array of type BaseType[ ] can only hold objects of type BaseType. An attempt to store an object of the wrong type in the array will be detected by the compiler, and there is no need to type-cast items that are retrieved from the array back to type BaseType. To address this problem, Java 5.0 introduced parameterized types. ArrayList is an example: Instead of using the plain \"ArrayList\" type, it is possible to use ArrayList<BaseType>, where BaseType is any object type, that is, the name of a class or of an interface.",
  "page88": "Searching and Sorting- Two array processing techniques that are particularly common are searching and sorting. Searching here refers to finding an item in the array that meets some specified crit\u00aeerion. Sorting refers to rearranging all the items in the array into increasing or decreasing order (where the meaning of increasing and decreasing can depend on the context). Sorting and searching are often discussed, in a theoretical sort\u00aeof way, using an array of numbers as an example. In practical situations, though, more interesting types of data are usually involved. For example, the array might be a mailing list, and each element of the array might be an object containing a name and address. Given the name of a person, you might want to look up that person's addr\u00aeess. This is an example of searching, since you want to find the object in the array that contains the given name. It would also be useful to be able to sort the array according to various criteria. One example of sorting would be ordering the elements of the array so that the names are in alphabetical order. Another example would be to order the elements of the array according to zip code be\u00aefore printing a set of mailing labels. (This kind of sorting can get you a cheaper postage rate on a large mailing.) This example can be generalized to a more abstract situation in which we have an array that contains objects, and we want to search or sort the array based on th\u00aee value of one of the instance variables in that array. We can use some terminology here that originated in work with \"databases,\" which are just large, organized collections of data. We refer to each of the objects in the array as a record. The instance variables in an object are then called fields of the record. In the mailing list example, each record would contain a name and address. The fields of the record might be the first name, last name, street address, state, city and zip code. For the purpose of searching or sorting, one of the fields is designated to be the key field. Searching then means finding a record in the array that has a specified value in its key field. Sorting means moving the records around in the array so that the key fields of the record are in increasing (or decreasing) order.",
  "page89": "Introduction to Correctness and Robustness- A program is correct if it accomplishes the task that it was designed to perform. It is robust if it can handle illegal inputs and other unexpected sit\u00aeuations in a reasonable way. For example, consider a program that is designed to read some numbers from the user and then print the same numbers in sorted order. The program is correct if it works for any set of input numbers. It is robust\u00aeif it can also deal with non-numeric input by, for example, printing an error message and ignoring the bad input. A non-robust program might crash or give nonsensical output in the same circumstance. Every program should be correct. (A sorting program that doesn't sort correctly is pretty useless.) It's not the case that every pr\u00aeogram needs to be completely robust. It depends on who will use it and how it will be used. For example, a small utility program that you write for your own use doesn't have to be particularly robust. The question of correctness is actually more subtle than it might appear. A programmer works from a specification of what the program is supposed to do. The programmer's work is correc\u00aet if the program meets its specification. But does that mean that the program itself is correct? What if the specification is incorrect or incomplete? A correct program should be a correct implementation of a complete and correct specification. The question is whether the speci\u00aefication correctly expresses the intention and desires of the people for whom the program is being written. This is a question that lies largely outside the domain of computer science. Horror Stories- Most computer users have personal experience with programs that don't work or that crash. In many cases, such problems are just annoyances, but even on a personal computer there can be more serious consequences, such as lost work or lost money. When computers are given more important tasks, the consequences of failure can be proportionately more serious. Just a few years ago, the failure of two multi-million space missions to Mars was prominent in the news. Both failures were probably due to software problems, but in both cases the problem was not with an incorrect program as such.",
  "page90": "A few months later, the Mars Polar Lander probably crashed because its software turned off its landing engines too soon. The program was supposed to detect the bump when the spacecraft landed and\u00aeturn off the engines then. It has been determined that deployment of the landing gear might have jarred the spacecraft enough to activate the program, causing it to turn off the engines when the spacecraft was still in the air. The unpower\u00aeed spacecraft would then have fallen to the Martian surface. A more robust system would have checked the altitude before turning off the engines! There are many equally dramatic stories of problems caused by incorrect or poorly written software. Let's look at a few incidents recounted in the book Computer Ethics by Tom Forester and Pe\u00aerry Morrison. (This book covers various ethical issues in computing. It, or something like it, is essential reading for any student of computer science.) In 1985 and 1986, one person was killed and several were injured by excess radiation, while undergoing radiation treatments by a mis-programmed computerized radiation machine. In another case, over a ten-year period ending in 1992, almost 1,\u00ae000 cancer patients received radiation dosages that were 30% less than prescribed because of a programming error. In 1985, a computer at the Bank of New York started destroying records of on-going security transactions because of an error in a program. It took less than 24 hour\u00aes to fix the program, but by that time, the bank was out $5,000,000 in overnight interest payments on funds that it had to borrow to cover the problem. The programming of the inertial guidance system of the F-16 fighter plane would have turned the plane upside-down when it crossed the equator, if the problem had not been discovered in simulation. The Mariner 18 space probe was lost because of an error in one line of a program. The Gemini V space capsule missed its scheduled landing target by a hundred miles, because a programmer forgot to take into account the rotation of the Earth. In 1990, AT&T's long-distance telephone service was disrupted throughout the United States when a newly loaded computer program proved to contain a bug. These are just a few examples. Software problems are all too common. As programmers, we need to understand why that is true and what can be done about it.",
  "page91": "Java to the Rescue- Part of the problem, according to the inventors of Java, can be traced to programming languages themselves. Java was designed to provide some protection against certain types\u00aeof errors. How can a language feature help prevent errors? Let's look at a few examples. Early programming languages did not require variables to be declared. In such languages, when a variable name is used in a program, the variable i\u00aes created automatically. You might consider this more convenient than having to declare every variable explicitly. But there is an unfortunate consequence: An inadvertent spelling error might introduce an extra variable that you had no intention of creating. This type of error was responsible, according to one famous story, for yet another\u00aelost spacecraft. In the FORTRAN programming language, the command \"DO 20 I = 1,5\" is the first statement of a counting loop. Now, spaces are insignificant in FORTRAN, so this is equivalent to \"DO20I=1,5\". On the other hand, the command \"DO20I=1.5\", with a period instead of a comma, is an assignment statement that assigns the value 1.5 to the variable DO20I. Supp\u00aeosedly, the inadvertent substitution of a period for a comma in a statement of this type caused a rocket to blow up on take-off. Because FORTRAN doesn't require variables to be declared, the compiler would be happy to accept the statement \"DO20I=1.5.\" It would ju\u00aest create a new variable named DO20I. If FORTRAN required variables to be declared, the compiler would have complained that the variable DO20I was undeclared. While most programming languages today do require variables to be declared, there are other features in common programming languages that can cause problems. Java has eliminated some of these features. Some people complain that this makes Java less efficient and less powerful. While there is some justice in this criticism, the increase in security and robustness is probably worth the cost in most circumstances. The best defense against some types of errors is to design a programming language in which the errors are impossible. In other cases, where the error can't be completely eliminated, the language can be designed so that when the error does occur, it will automatically be detected. This will at least prevent the error from causing further harm.",
  "page92": "An array is created with a certain number of locations, numbered from zero up to some specified maximum index. It is an error to try to use an array location that is outside of the specified rang\u00aee. In Java, any attempt to do so is detected automatically by the system. In some other languages, such as C and C++, it's up to the programmer to make sure that the index is within the legal range. Suppose that an array, A, has three\u00aelocations, A[0], A[1], and A[2]. Then A[3], A[4], and so on refer to memory locations beyond the end of the array. In Java, an attempt to store data in A[3] will be detected. The program will be terminated (unless the error is \"caught\", as discussed in Section 3.7). In C or C++, the computer will just go ahead and store the data\u00aein memory that is not part of the array. Since there is no telling what that memory location is being used for, the result will be unpredictable. The consequences could be much more serious than a terminated program. (See, for example, the discussion of buffer overflow errors later in this section.) Pointers are a notorious source of programming errors. In Java, a variable of object type hold\u00aes either a pointer to an object or the special value null. Any attempt to use a null value as if it were a pointer to an actual object will be detected by the system. In some other languages, again, it's up to the programmer to avoid such null pointer errors. In my old Mac\u00aeintosh computer, a null pointer was actually implemented as if it were a pointer to memory location zero. A program could use a null pointer to change values stored in memory near location zero. Unfortunately, the Macintosh stored important system data in those locations. Changing that data could cause the whole system to crash, a consequence more severe than a single failed program. Another type of pointer error occurs when a pointer value is pointing to an object of the wrong type or to a segment of memory that does not even hold a valid object at all. These types of errors are impossible in Java, which does not allow programmers to manipulate pointers directly. In other languages, it is possible to set a pointer to point, essentially, to any location in memory. If this is done incorrectly, then using the pointer can have unpredictable results.",
  "page93": "Another type of error that cannot occur in Java is a memory leak. In Java, once there are no longer any pointers that refer to an object, that object is \"garbage collected\" so that the\u00aememory that it occupied can be reused. In other languages, it is the programmer's responsibility to return unused memory to the system. If the programmer fails to do this, unused memory can build up, leaving less memory for programs an\u00aed data. There is a story that many common programs for older Windows computers had so many memory leaks that the computer would run out of memory after a few days of use and would have to be restarted. Many programs have been found to suffer from buffer overflow errors. Buffer overflow errors often make the news because they are responsibl\u00aee for many network security problems. When one computer receives data from another computer over a network, that data is stored in a buffer. The buffer is just a segment of memory that has been allocated by a program to hold data that it expects to receive. A buffer overflow occurs when more data is received than will fit in the buffer. The question is, what happens then? If the error is dete\u00aected by the program or by the networking software, then the only thing that has happened is a failed network data transmission. The real problem occurs when the software does not properly detect buffer overflows. In that case, the software continues to store data in memory even\u00aeafter the buffer is filled, and the extra data goes into some part of memory that was not allocated by the program as part of the buffer. That memory might be in use for some other purpose. It might contain important data. It might even contain part of the program itself. This is where the real security issues come in. Suppose that a buffer overflow causes part of a program to be replaced with extra data received over a network. When the computer goes to execute the part of the program that was replaced, it's actually executing data that was received from another computer. That data could be anything. It could be a program that crashes the computer or takes it over. A malicious programmer who finds a convenient buffer overflow error in networking software can try to exploit that error to trick other computers into executing his programs.",
  "page94": "For software written completely in Java, buffer overflow errors are impossible. The language simply does not provide any way to store data into memory that has not been properly allocated. To do\u00aethat, you would need a pointer that points to unallocated memory or you would have to refer to an array location that lies outside the range allocated for the array. As explained above, neither of these is possible in Java. (However, there\u00aecould conceivably still be errors in Java's standard classes, since some of the methods in these classes are actually written in the C programming language rather than in Java.) It's clear that language design can help prevent errors or detect them when they occur. Doing so involves restricting what a programmer is allowed to do.\u00aeOr it requires tests, such as checking whether a pointer is null, that take some extra processing time. Some programmers feel that the sacrifice of power and efficiency is too high a price to pay for the extra security. In some applications, this is true. However, there are many situations where safety and security are primary considerations. Java is designed for such situations. Robust Hand\u00aeling of Input- One place where correctness and robustness are important and especially difficult is in the processing of input data, whether that data is typed in by the user, read from a file, or received over a network. Files and networking will be covered in Chapter 11, whic\u00aeh will make essential use of material that will be covered in the next two sections of this chapter. For now, let's look at an example of processing user input. Examples in this textbook use my TextIO class for reading input from the user. This class has built-in error handling. For example, the function TextIO.getDouble() is guaranteed to return a legal value of type double. If the user types an illegal value, then TextIO will ask the user to re-enter their response; your program never sees the illegal value. However, this approach can be clumsy and unsatisfactory, especially when the user is entering complex data. In the following example, I'll do my own error-checking. Sometimes, it's useful to be able to look ahead at what's coming up in the input without actually reading it. For example, a program might need to know whether the next item in the input is a number or a word. For this purpose, the TextIO class includes the function TextIO.peek().",
  "page95": "Exceptions and Exception Classes- We have already seen that Java (like its cousin, C++) provides a neater, more structured alternative method for dealing with errors that can occur while a progra\u00aem is running. The method is referred to as exception handling. The word \"exception\" is meant to be more general than \"error.\" It includes any circumstance that arises as the program is executed which is meant to be treat\u00aeed as an exception to the normal flow of control of the program. An exception might be an error, or it might just be a special case that you would rather not have clutter up your elegant algorithm. When an exception occurs during the execution of a program, we say that the exception is thrown. When this happens, the normal flow of the prog\u00aeram is thrown off-track, and the program is in danger of crashing. However, the crash can be avoided if the exception is caught and handled in some way. An exception can be thrown in one part of a program and caught in a different part. An exception that is not caught will generally cause the program to crash. (More exactly, the thread that throws the exception will crash. In a multithreaded\u00aeprogram, it is possible for other threads to continue even after one crashes. We will cover threads in Section 8.5. In particular, GUI programs are multithreaded, and parts of the program might continue to function even while other parts are non-functional because of exceptions\u00ae.) By the way, since Java programs are executed by a Java interpreter, having a program crash simply means that it terminates abnormally and prematurely. It doesn't mean that the Java interpreter will crash. In effect, the interpreter catches any exceptions that are not caught by the program. The interpreter responds by terminating the program. In many other programming languages, a crashed program will sometimes crash the entire system and freeze the computer until it is restarted. With Java, such system crashes should be impossible which means that when they happen, you have the satisfaction of blaming the system rather than your own program. along with the trycatch statement, which is used to catch and handle exceptions. However, that section did not cover the complete syntax of trycatch or the full complexity of exceptions. In this section, we cover these topics in full detail.",
  "page96": "When an exception occurs, the thing that is actually \"thrown\" is an object. This object can carry information (in its instance variables) from the point where the exception occurs to th\u00aee point where it is caught and handled. This information always includes the subroutine call stack, which is a list of the subroutines that were being executed when the exception was thrown. (Since one subroutine can call another, several s\u00aeubroutines can be active at the same time.) Typically, an exception object also includes an error message describing what happened to cause the exception, and it can contain other data as well. All exception objects must belong to a subclass of the standard class java.lang.Throwable. In general, each different type of exception is represen\u00aeted by its own subclass of Throwable, and these subclasses are arranged in a fairly complex class hierarchy that shows the relationship among various types of exception. Throwable has two direct subclasses, Error and Exception. These two subclasses in turn have many other predefined subclasses. In addition, a programmer can create new exception classes to represent new types of exception. Mos\u00aet of the subclasses of the class Error represent serious errors within the Java virtual machine that should ordinarily cause program termination because there is no reasonable way to handle them. In general, you should not try to catch and handle such errors. An example is a Cl\u00aeassFormatError, which occurs when the Java virtual machine finds some kind of illegal data in a file that is supposed to contain a compiled Java class. If that class was being loaded as part of the program, then there is really no way for the program to proceed. On the other hand, subclasses of the class Exception represent exceptions that are meant to be caught. In many cases, these are exceptions that might naturally be called \"errors,\" but they are errors in the program or in input data that a programmer can anticipate and possibly respond to in some reasonable way. (However, you should avoid the temptation of saying, \"Well, I'll just put a thing here to catch all the errors that might occur, so my program won't crash.\" If you don't have a reasonable way to respond to the error, it's best just to let the program crash, because trying to go on will probably only lead to worse things down the road in the worst case,",
  "page97": "Throwing Exceptions- There are times when it makes sense for a program to deliberately throw an exception. This is the case when the program discovers some sort of exceptional or error condition,\u00aebut there is no reasonable way to handle the error at the point where the problem is discovered. The program can throw an exception in the hope that some other part of the program will catch and handle the exception. This can be done with\u00aea throw statement. You have already seen an example of this in Subsection 4.3.5. In this section, we cover the throw statement more fully. The syntax of the throw statement is: throw hexception-objecti ; The hexception-objecti must be an object belonging to one of the subclasses of Throwable. Usually, it will in fact belong to one of the s\u00aeubclasses of Exception. In most cases, it will be a newly constructed object created with the new operator. For example: throw new ArithmeticException(\\\"Division by zero\\\"); The parameter in the constructor becomes the error message in the exception object; if e refers to the object, the error message can be retrieved by calling e.getMessage(). (You might find this example a bit odd, beca\u00aeuse you might expect the system itself to throw an ArithmeticException when an attempt is made to divide by zero. So why should a programmer bother to throw the exception? Recall that if the numbers that are being divided are of type int, then division by zero will indeed throw\u00aean ArithmeticException. However, no arithmetic operations with floating-point numbers will ever produce an exception. Instead, the special value Double.NaN is used to represent the result of an illegal operation. In some situations, you might prefer to throw an ArithmeticException when a real number is divided by zero.) An exception can be thrown either by the system or by a throw statement. The exception is processed in exactly the same way in either case. Suppose that the exception is thrown inside a try statement. If that try statement has a catch clause that handles that type of exception, then the computer jumps to the catch clause and executes it. The exception has been handled. After handling the exception, the computer executes the finally clause of the try statement, if there is one. It then continues normally with the rest of the program, which follows the try statement. If the exception is not immediately caught and handled, the processing of the exception will continue.",
  "page98": "When an exception is thrown during the execution of a subroutine and the exception is not handled in the same subroutine, then that subroutine is terminated (after the execution of any pending fi\u00aenally clauses). Then the routine that called that subroutine gets a chance to handle the exception. That is, if the subroutine was called inside a try statement that has an appropriate catch clause, then that catch clause will be executed a\u00aend the program will continue on normally from there. Again, if the second routine does not handle the exception, then it also is terminated and the routine that called it (if any) gets the next shot at the exception. The exception will crash the program only if it passes up through the entire chain of subroutine calls without being handled\u00ae. (In fact, even this is not quite true: In a multithreaded program, only the thread in which the exception occurred is terminated.) Mandatory Exception Handling In the preceding example, declaring that the subroutine root() can throw an IllegalArgumentException is just a courtesy to potential readers of this routine. This is because handling of IllegalArgumentExceptions is not \"mandator\u00aey.\" A routine can throw an IllegalArgumentException without announcing the possibility. And a program that calls that routine is free either to catch or to ignore the exception, just as a programmer can choose either to catch or to ignore an exception of type NullPointerEx\u00aeception. For those exception classes that require mandatory handling, the situation is different. If a subroutine can throw such an exception, that fact must be announced in a throws clause in the routine definition. Failing to do so is a syntax error that will be reported by the compiler. On the other hand, suppose that some statement in the body of a subroutine can generate an exception of a type that requires mandatory handling. The statement could be a throw statement, which throws the exception directly, or it could be a call to a subroutine that can throw the exception. In either case, the exception must be handled. This can be done in one of two ways: The first way is to place the statement in a try statement that has a catch clause that handles the exception.",
  "page99": "in this case, the exception is handled within the subroutine, so that any caller of the subroutine will never see the exception. The second way is to declare that the subroutine can throw the exc\u00aeeption. This is done by adding a \"throws\" clause to the subroutine heading, which alerts any callers to the possibility that an exception might be generated when the subroutine is executed. The caller will, in turn, be forced eith\u00aeer to handle the exception in a try statement or to declare the exception in a throws clause in its own header. Exception-handling is mandatory for any exception class that is not a subclass of either Error or RuntimeException. Exceptions that require mandatory handling generally represent conditions that are outside the control of the pro\u00aegrammer. For example, they might represent bad input or an illegal action taken by the user. There is no way to avoid such errors, so a robust program has to be prepared to handle them. The design of Java makes it impossible for programmers to ignore the possibility of such errors. Among the exceptions that require mandatory handling are several that can occur when using Java's input/out\u00aeput routines. This means that you can't even use these routines unless you understand something about exception-handling. Chapter 11 deals with input/output and uses mandatory exception-handling extensively. Programming with Exceptions- Exceptions can be used to help write\u00aerobust programs. They provide an organized and structured approach to robustness. Without exceptions, a program can become cluttered with if statements that test for various possible error conditions. With exceptions, it becomes possible to write a clean implementation of an algorithm that will handle all the normal cases. The exceptional cases can be handled elsewhere, in a catch clause of a try statement. When a program encounters an exceptional condition and has no way of handling it immediately, the program can throw an exception. In some cases, it makes sense to throw an exception belonging to one of Java's predefined classes, such as IllegalArgumentException or IOException. However, if there is no standard class that adequately represents the exceptional condition, the programmer can define a new exception class. The new class must extend the standard class Throwable or one of its subclasses.",
  "page100": "Here, for example, is a class that extends Exception, and therefore requires mandatory exception handling when it is used: public class ParseError extends Exception { public ParseError(String me\u00aessage) { // Create a ParseError object containing // the given message as its error message. super(message); } } The class contains only a constructor that makes it possible to create a ParseError object containing a given error message. (T\u00aehe statement \"super(message)\" calls a constructor in the superclass, Exception. See Subsection 5.6.3.) Of course the class inherits the getMessage() and printStackTrace() routines from its superclass. If e refers to an object of type ParseError, then the function call e.getMessage() will retrieve the error message that was specif\u00aeied in the constructor. But the main point of the ParseError class is simply to exist. When an object of type ParseError is thrown, it indicates that a certain type of error has occurred. (Parsing, by the way, refers to figuring out the syntax of a string. A ParseError would indicate, presumably, that some string that is being processed by the program does not have the expected form.) A throw\u00aestatement can be used in a program to throw an error of type ParseError. The constructor for the ParseError object must specify an error message. The ability to throw exceptions is particularly useful in writing general-purpose subroutines and classes that are meant to be used\u00aein more than one program. In this case, the person writing the subroutine or class often has no reasonable way of handling the error, since that person has no way of knowing exactly how the subroutine or class will be used. In such circumstances, a novice programmer is often tempted to print an error message and forge ahead, but this is almost never satisfactory since it can lead to unpredictable results down the line. Printing an error message and terminating the program is almost as bad, since it gives the program no chance to handle the error. The program that calls the subroutine or uses the class needs to know that the error has occurred. In languages that do not support exceptions, the only alternative is to return some special value or to set the value of some variable to indicate that an error has occurred.",
  "page101": "Introduction to Threads- Like people, computers can multitask. That is, they can be working on several different tasks at the same time. A computer that has just a single central processing unit\u00aecan't literally do two things at the same time, any more than a person can, but it can still switch its attention back and forth among several tasks. Furthermore, it is increasingly common for computers to have more than one processin\u00aeg unit, and such computers can literally work on several tasks simultaneously. It is likely that from now on, most of the increase in computing power will come from adding additional processors to computers rather than from increasing the speed of individual processors. To use the full power of these multiprocessing computers, a programmer\u00aemust do parallel programming, which means writing a program as a set of several tasks that can be executed simultaneously. Even on a single-processor computer, parallel programming techniques can be useful, since some problems can be tackled most naturally by breaking the solution into a set of simultaneous tasks that cooperate to solve the problem. In Java, a single task is called a thread.\u00aeThe term \"thread\" refers to a \"thread of control\" or \"thread of execution,\" meaning a sequence of instructions that are executed one after another the thread extends through time, connecting each instruction to the next. In a multithreaded program\u00ae, there can be many threads of control, weaving through time in parallel and forming the complete fabric of the program. (Ok, enough with the metaphor, already!) Every Java program has at least one thread; when the Java virtual machine runs your program, it creates a thread that is responsible for executing the main routine of the program. This main thread can in turn create other threads that can continue even after the main thread has terminated. In a GUI program, there is at least one additional thread, which is responsible for handling events and drawing components on the screen. This GUI thread is created when the first window is opened. So in fact, you have already done parallel programming! When a main routine opens a window, both the main thread and the GUI thread can continue to run in parallel. Of course, parallel programming can be used in much more interesting ways.",
  "page102": "Unfortunately, parallel programming is even more difficult than ordinary, single-threaded programming. When several threads are working together on a problem, a whole new category of errors is p\u00aeossible. This just means that techniques for writing correct and robust programs are even more important for parallel programming than they are for normal programming. (That's one excuse for having this section in this chapter another\u00aeis that we will need threads at several points in future chapters, and I didn't have another place in the book where the topic fits more naturally.) Since threads are a difficult topic, you will probably not fully understand everything in this section the first time through the material. Your understanding should improve as you encoun\u00aeter more examples of threads in future sections. Creating and Running Threads- In Java, a thread is represented by an object belonging to the class java.lang.Thread (or to a subclass of this class). The purpose of a Thread object is to execute a single method. The method is executed in its own thread of control, which can run in parallel with other threads. When the execution of the method is\u00aefinished, either because the method terminates normally or because of an uncaught exception, the thread stops running. Once this happens, there is no way to restart the thread or to use the same Thread object to start another thread. Operations on Threads- The Thread class inc\u00aeludes several useful methods in addition to the start() method that was discussed above. I will mention just a few of them. If thrd is an object of type Thread, then the boolean-valued function thrd.isAlive() can be used to test whether or not the thread is alive. A thread is \"alive\" between the time it is started and the time when it terminates. After the thread has terminated it is said to be \"dead\". (The rather gruesome metaphor is also used when we refer to \"killing\" or \"aborting\" a thread.)",
  "page103": "The static method Thread.sleep(milliseconds) causes the thread that executes this method to \"sleep\" for the specified number of milliseconds. A sleeping thread is still alive, but it i\u00aes not running. While a thread is sleeping, the computer will work on any other runnable threads (or on other programs). Thread.sleep() can be used to insert a pause in the execution of a thread. The sleep method can throw an exception of ty\u00aepe InterruptedException, which is an exception class that requires mandatory exception handling. In practice, this means that the sleep method is usually used in a trycatch statement that catches the potential InterruptedException: try { Thread.sleep(lengthOfPause); } catch (InterruptedException e) { } One thread can interrupt another thre\u00aead to wake it up when it is sleeping or paused for some other reason. A Thread, thrd, can be interrupted by calling its method thrd.interrupt(), but you are not likely to do this until you start writing rather advanced applications, and you are not likely to need to do anything in response to an InterruptedException (except to catch it). It's unfortunate that you have to worry about it a\u00aet all, but that's the way that mandatory exception handling works. Mutual Exclusion with \"synchronized\" Programming several threads to carry out independent tasks is easy. The real difficulty arises when threads have to interact in some way. One way that threads\u00aeinteract is by sharing resources. When two threads need access to the same resource, such as a variable or a window on the screen, some care must be taken that they don't try to use the same resource at the same time. Otherwise, the situation could be something like this: Imagine several cooks sharing the use of just one measuring cup, and imagine that Cook A fills the measuring cup with milk, only to have Cook B grab the cup before Cook A has a chance to empty the milk into his bowl. There has to be some way for Cook A to claim exclusive rights to the cup while he performs the two operations: Add-Milk-To-Cup and Empty-Cup-Into-Bowl.",
  "page104": "Wait and Notify- Threads can interact with each other in other ways besides sharing resources. For example, one thread might produce some sort of result that is needed by another thread. This im\u00aeposes some restriction on the order in which the threads can do their computations. If the second thread gets to the point where it needs the result from the first thread, it might have to stop and wait for the result to be produced. Since\u00aethe second thread can't continue, it might as well go to sleep. But then there has to be some way to notify the second thread when the result is ready, so that it can wake up and continue its computation. Java, of course, has a way to do this kind of waiting and notification: It has wait() and notify() methods that are defined as inst\u00aeance methods in class Object and so can be used with any object. The reason why wait() and notify() should be associated with objects is not obvious, so don't worry about it at this point. It does, at least, make it possible to direct different notifications to a different recipients, depending on which object's notify() method is called. Volatile Variables- And a final note on comm\u00aeunication among threads: In general, threads communicate by sharing variables and accessing those variables in synchronized methods or synchronized statements. However, synchronization is fairly expensive computationally, and excessive use of it should be avoided. So in some ca\u00aeses, it can make sense for threads to refer to shared variables without synchronizing their access to those variables. However, a subtle problem arises when the value of a shared variable is set is one thread and used in another. Because of the way that threads are implemented in Java, the second thread might not see the changed value of the variable immediately. That is, it is possible that a thread will continue to see the old value of the shared variable for some time after the value of the variable has been changed by another thread. This is because threads are allowed to cache shared data. That is, each thread can keep its own local copy of the shared data. When one thread changes the value of a shared variable, the local copies in the caches of other threads are not immediately changed, so the other threads continue to see the old value.",
  "page105": "It is still possible to use a shared variable outside of synchronized code, but in that case, the variable must be declared to be volatile. The volatile keyword is a modifier that can be added t\u00aeo a variable declaration, as in private volatile int count; If a variable is declared to be volatile, no thread will keep a local copy of that variable in its cache. Instead, the thread will always use the official, main copy of the variabl\u00aee. This means that any change made to the variable will immediately be available to all threads. This makes it safe for threads to refer to volatile shared variables even outside of synchronized code. (Remember, though, that synchronization is still the only way to prevent race conditions.) When the volatile modifier is applied to an objec\u00aet variable, only the variable itself is declared to be volatile, not the contents of the object that the variable points to. For this reason, volatile is generally only used for variables of simple types such as primitive types and enumerated types.Analysis of Algorithms- This chapter has concentrated mostly on correctness of programs. In practice, another issue is also important: efficiency.\u00aeWhen analyzing a program in terms of efficiency, we want to look at questions such as, \"How long does it take for the program to run?\" and \"Is there another approach that will get the answer more quickly?\" Efficiency will always be less important than corre\u00aectness; if you don't care whether a program works correctly, you can make it run very quickly indeed, but no one will think it's much of an achievement! On the other hand, a program that gives a correct answer after ten thousand years isn't very useful either, so efficiency is often an important issue.The term \"efficiency\" can refer to efficient use of almost any resource, including time, computer memory, disk space, or network bandwidth. In this section, however, we will deal exclusively with time efficiency, and the major question that we want to ask about a program is, how long does it take to perform its task?",
  "page106": "it really makes little sense to classify an individual program as being \"efficient\" or \"inefficient.\" It makes more sense to compare two (correct) programs that perform the s\u00aeame task and ask which one of the two is \"more efficient,\" that is, which one performs the task more quickly. However, even here there are difficulties. The running time of a program is not well-defined. The run time can be differ\u00aeent depending on the number and speed of the processors in the computer on which it is run and, in the case of Java, on the design of the Java Virtual Machine which is used to interpret the program. It can depend on details of the compiler which is used to translate the program from high-level language to machine language. Furthermore, the\u00aerun time of a program depends on the size of the problem which the program has to solve. It takes a sorting program longer to sort 10000 items than it takes it to sort 100 items. When the run times of two programs are compared, it often happens that Program A solves small problems faster than Program B, while Program B solves large problems faster than Program A, so that it is simply not the\u00aecase that one program is faster than the other in all cases. the efficiency of programs. The field is known as Analysis of Algorithms. The focus is on algorithms, rather than on programs as such, to avoid having to deal with multiple implementations of the same algorithm writt\u00aeen in different languages, compiled with different compilers, and running on different computers. Analysis of Algorithms is a mathematical field that abstracts away from these down-and-dirty details. Still, even though it is a theoretical field, every working programmer should be aware of some of its techniques and results. This section is a very brief introduction to some of those techniques and results. Because this is not a mathematics book, the treatment will be rather informal.",
  "page107": "One of the main techniques of analysis of algorithms is asymptotic analysis. The term \"asymptotic\" here means basically \"the tendency in the long run.\" An asymptotic analysis\u00aeof an algorithm's run time looks at the question of how the run time depends on the size of the problem. The analysis is asymptotic because it only considers what happens to the run time as the size of the problem increases without li\u00aemit; it is not concerned with what happens for problems of small size or, in fact, for problems of any fixed finite size. Only what happens in the long run, as the problem size increases without limit, is important. Showing that Algorithm A is asymptotically faster than Algorithm B doesn't necessarily mean that Algorithm A will run fa\u00aester than Algorithm B for problems of size 10 or size 1000 or even size 1000000 it only means that if you keep increasing the problem size, you will eventually come to a point where Algorithm A is faster than Algorithm B. An asymptotic analysis is only a first approximation, but in practice it often gives important and useful information. Central to asymptotic analysis is Big-Oh notation. Usi\u00aeng this notation, we might say, for example, that an algorithm has a running time that is O(n2 ) or O(n) or O(log(n)). These notations are read \"Big-Oh of n squared,\" \"Big-Oh of n,\" and \"Big-Oh of log n\" (where log is a logarithm function). More ge\u00aenerally, we can refer to O(f(n)) (\"Big-Oh of f of n\"), where f(n) is some function that assigns a positive real number to every positive integer n. The \"n\" in this notation refers to the size of the problem. Before you can even begin an asymptotic analysis, you need some way to measure problem size. Usually, this is not a big issue. For example, if the problem is to sort a list of items, then the problem size can be taken to be the number of items in the list. When the input to an algorithm is an integer, as in the case of algorithm that checks whether a given positive integer is prime, the usual measure of the size of a problem is the number of bits in the input integer rather than the integer itself. More generally, the number of bits in the input to a problem is often a good measure of the size of the problem.",
  "page108": "To say that the running time of an algorithm is O(f(n)) means that for large values of the problem size, n, the running time of the algorithm is no bigger than some constant times f(n). (More ri\u00aegorously, there is a number C and a positive integer M such that whenever n is greater than M, the run time is less than or equal to C*f(n).) The constant takes into account details such as the speed of the computer on which the algorithm i\u00aes run; if you use a slower computer, you might have to use a bigger constant in the formula, but changing the constant won't change the basic fact that the run time is O(f(n)). The constant also makes it unnecessary to say whether we are measuring time in seconds, years, CPU cycles, or any other unit of measure; a change from one unit\u00aeof measure to another is just multiplication by a constant. Note also that O(f(n)) doesn't depend at all on what happens for small problem sizes, only on what happens in the long run as the problem size increases without limit. To look at a simple example, consider the problem of adding up all the numbers in an array. The problem size, n, is the length of the array. Using A as the name\u00aeof the array, the algorithm can be expressed in Java as: total = 0; for (int i = 0; i < n; i++) total = total + A[i]; This algorithm performs the same operation, total = total + A[i], n times. The total time spent on this operation is a*n, where a is the time it takes to perfor\u00aem the operation once. time spent on this operation is a*n, where a is the time it takes to perform the operation once. Now, this is not the only thing that is done in the algorithm. The value of i is incremented and is compared to n each time through the loop. This adds an additional time of b*n to the run time, for some constant b. Furthermore, i and total both have to be initialized to zero; this adds some constant amount c to the running time. The exact running time would then be (a+b)*n+c, where the constants a, b, and c depend on factors such as how the code is compiled and what computer it is run on.",
  "page109": "Using the fact that c is less than or equal to c*n for any positive integer n, we can say that the run time is less than or equal to (a+b+c)*n. That is, the run time is less than or equal to a c\u00aeonstant times n. By definition, this means that the run time for this algorithm is O(n). If this explanation is too mathematical for you, we can just note that for large values of n, the c in the formula (a+b)*n+c is insignificant compared\u00aeto the other term, (a+b)*n. We say that c is a \"lower order term.\" When doing asymptotic analysis, lower order terms can be discarded. A rough, but correct, asymptotic analysis of the algorithm would go something like this: Each iteration of the for loop takes a certain constant amount of time. There are n iterations of the loop,\u00aeso the total run time is a constant times n, plus lower order terms (to account for the initialization). Disregarding lower order terms, we see that the run time is O(n). Note that to say that an algorithm has run time O(f(n)) is to say that its run time is no bigger than some constant times n (for large values of n). O(f(n)) puts an upper limit on the run time. However, the run time could b\u00aee smaller, even much smaller. For example, if the run time is O(n), it would also be correct to say that the run time is O(n2 ) or even O(n10). If the run time is less than a constant times n, then it is certainly less than the same constant times n 2 or n10 Of course, sometime\u00aes it's useful to have a lower limit on the run time. That is, we want to be able to say that the run time is greater than or equal to some constant times f(n) (for large values of n). The notation for this is Ohm(f(n)), read \"Omega of f of n.\" \"Omega\" is the name of a letter in the Greek alphabet, and Ohm is the upper case version of that letter. (To be technical, saying that the run time of an algorithm is Ohm(f(n)) means that there is a positive number C and a positive integer M such that whenever n is greater than M, the run time is greater than or equal to C*f(n).) O(f(n)) tells you something about the maximum amount of time that you might have to wait for an algorithm to finish; Ohm(f(n)) tells you something about the minimum time.",
  "page110": "The algorithm for adding up the numbers in an array has a run time that is Ohm(n) as well as O(n). When an algorithm has a run time that is both Ohm(f(n)) and O(f(n)), its run time is said\u00aeto be Theta(f(n)), read \"Theta of f of n.\" (Theta is another letter from the Greek alphabet.) To say that the run time of an algorithm is Theta(f(n)) means that for large values of n, the run time is between a*f(n) and b*f(n),\u00aewhere a and b are constants (with b greater than a, and both greater than 0).So far, my analysis has ignored an important detail. We have looked at how run time depends on the problem size, but in fact the run time usually depends not just on the size of the problem but on the specific data that has to be processed. For example, the run ti\u00aeme of a sorting algorithm can depend on the initial order of the items that are to be sorted, and not just on the number of items. To account for this dependency, we can consider either the worst case run time analysis or the average case run time analysis of an algorithm. For a worst case run time analysis, we consider all possible problems of size n and look at the longest possible run time\u00aefor all such problems. For an average case analysis, we consider all possible problems of size n and look at the average of the run times for all such problems. Usually, the average case analysis assumes that all problems of size n are equally likely to be encountered, althoug\u00aeh this is not always realistic or even possible in the case where there is an infinite number of different problems of a given size. In many cases, the average and the worst case run times are the same to within a constant multiple. This means that as far as asymptotic analysis is concerned, they are the same. That is, if the average case run time is O(f(n)) or Theta(f(n)), then so is the worst case. However, later in the book, we will encounter a few cases where the average and worst case asymptotic analyses differ.",
  "page111": "So, what do you really have to know about analysis of algorithms to read the rest of this book? We will not do any rigorous mathematical analysis, but you should be able to follow informal discu\u00aession of simple cases such as the examples that we have looked at in this section. Most important, though, you should have a feeling for exactly what it means to say that the running time of an algorithm is O(f(n)) or Theta(f(n)) for some\u00aecommon functions f(n). The main point is that these notations do not tell you anything about the actual numerical value of the running time of the algorithm for any particular case. They do not tell you anything at all about the running time for small values of n. What they do tell you is something about the rate of growth of the running t\u00aeime as the size of the problem increases. Suppose you compare two algorithm that solve the same problem. The run time of one algorithm is Theta(n2), while the run time of the second algorithm is Theta(n3). What does this tell you? If you want to know which algorithm will be faster for some particular problem of size, say, 100, nothing is certain. As far as you can tell just from the asympto\u00aetic analysis, either algorithm could be faster for that particular case or in any particular case. But what you can say for sure is that if you look at larger and larger problems, you will come to a point where the Theta(n2) algorithm is faster than the Theta(n3) algorithm. F\u00aeurthermore, as you continue to increase the problem size, the relative advantage of the Theta(n2) algorithm will continue to grow. There will be values of n for which the Theta(n2) algorithm is a thousand times faster, a million times faster, a billion times faster, and so on. This is because for any positive constants a and b, the function a*n3 grows faster than the function b*n2 as n gets larger. (Mathematically, the limit of the ratio of a*n3 to b*n2 is infinite as n approaches infinity.) This means that for \"large\" problems, a Theta(n2) algorithm will definitely be faster than a Theta(n3) algorithm. You just don't know based on the asymptotic analysis alone exactly how large \"large\" has to be. In practice, in fact, it is likely that the Theta(n2) algorithm will be faster even for fairly small values of n, and absent other information you would generally prefer a Theta(n2) algorithm to a Theta(n3) algorithm.",
  "page112": "Recursion- At one time or another, you've probably been told that you can't define something in terms of itself. Nevertheless, if it's done right, defining something at least part\u00aeially in terms of itself can be a very powerful technique. A recursive definition is one that uses the concept or thing that is being defined as part of the definition. For example: An \"ancestor\" is either a parent or an ancestor\u00aeof a parent. A \"sentence\" can be, among other things, two sentences joined by a conjunction such as \"and.\" A \"directory\" is a part of a disk drive that can hold files and directories. In mathematics, a \"set\" is a collection of elements, which can themselves be sets. A \"statement\" in Java ca\u00aen be a while statement, which is made up of the word \"while\", a boolean-valued condition, and a statement. Recursive definitions can describe very complex situations with just a few words. A definition of the term \"ancestor\" without using recursion might go something like \"a parent, or \"and so on\" is not very rigorous. (I've often thought that recursion\u00aeis really just a rigorous way of saying \"and so on.\") You run into the same problem if you try to define a \"directory\" as \"a file that is a list of files, where some of the files can be lists of files, where some of those files can be lists of files, a\u00aend so on.\" Trying to describe what a Java statement can look like, without using recursion in the definition, would be difficult and probably pretty comical. Recursion can be used as a programming technique. A recursive subroutine is one that calls itself, either directly or indirectly. To say that a subroutine calls itself directly means that its definition contains a subroutine call statement that calls the subroutine that is being defined. To say that a subroutine calls itself indirectly means that it calls a second subroutine which in turn calls the first subroutine (either directly or indirectly). A recursive subroutine can define a complex task in just a few lines of code. In the rest of this section, we'll look at a variety of examples, and we'll see other examples in the rest of the book.",
  "page113": "Recursive Binary Search- Binary search is used to find a specified value in a sorted list of items (or, if it does not occur in the list, to determine that fact). The idea is to test the element\u00aein the middle of the list. If that element is equal to the specified value, you are done. If the specified value is less than the middle element of the list, then you should search for the value in the first half of the list. Otherwise, yo\u00aeu should search for the value in the second half of the list. The method used to search for the value in the first or second half of the list is binary search. That is, you look at the middle element in the half of the list that is still under consideration, and either you've found the value you are looking for, or you have to apply b\u00aeinary search to one half of the remaining elements. And so on! This is a recursive description, and we can write a recursive subroutine to implement it. Before we can do that, though, there are two considerations that we need to take into account. Each of these illustrates an important general fact about recursive subroutines. First of all, the binary search algorithm begins by looking at the\u00ae\"middle element of the list.\" But what if the list is empty? If there are no elements in the list, then it is impossible to look at the middle element. In the terminology of Subsection 8.2.1, having a non-empty list is a \"precondition\" for looking at the mi\u00aeddle element, and this is a clue that we have to modify the algorithm to take this precondition into account. What should we do if we find ourselves searching for a specified value in an empty list? The answer is easy: If the list is empty, we can be sure that the value does not occur in the list, so we can give the answer without any further work. An empty list is a base case for the binary search algorithm. A base case for a recursive algorithm is a case that is handled directly, rather than by applying the algorithm recursively. The binary search algorithm actually has another type of base case: If we find the element we are looking for in the middle of the list, we are done. There is no need for further recursion. The second consideration has to do with the parameters to the subroutine. The problem is phrased in terms of searching for a value in a list. In the original, non-recursive binary search subroutine, the list was given as an array.",
  "page114": "Linked Data Structures- Every useful object contains instance variables. When the type of an instance variable is given by a class or interface name, the variable can hold a reference to another\u00aeobject. Such a reference is also called a pointer, and we say that the variable points to the object. (Of course, any variable that can contain a reference to an object can also contain the special value null, which points to nowhere.) Whe\u00aen one object contains an instance variable that points to another object, we think of the objects as being \"linked\" by the pointer. Data structures of great complexity can be constructed by linking objects together. Recursive Linking- Something interesting happens when an object contains an instance variable that can refer to ano\u00aether object of the same type. In that case, the definition of the object's class is recursive. Such recursion arises naturally in many cases. For example, consider a class designed to represent employees at a company. Suppose that every employee except the boss has a supervisor, who is another employee of the company. As the while loop is executed, runner points in turn to the original e\u00aemployee, emp, then variable is incremented each time runner \"visits\" a new employee. The loop ends when runner.supervisor is null, which indicates that runner has reached the boss. At that point, count has counted the number of steps between emp and the boss. In this\u00aeexample, the supervisor variable is quite natural and useful. In fact, data structures that are built by linking objects together are so useful that they are a major topic of study in computer science. We'll be looking at a few typical examples. In this section and the next, we'll be looking at linked lists. A linked list consists of a chain of objects of the same type, linked together by pointers from one object to the next. This is much like the chain of supervisors between emp and the boss in the above example. It's also possible to have more complex situations, in which one object can contain links to several other objects.",
  "page115": "Linked Lists- For most of the examples in the rest of this section, linked lists will be constructed out of objects belonging to the class Node which is defined as follows: class Node { String i\u00aetem; Node next; } The term node is often used to refer to one of the objects in a linked data structure. Objects of type Node can be chained together as shown in the top part of the above picture. Each node holds a String and a pointer to t\u00aehe next node in the list (if any). The last node in such a list can always be identified by the fact that the instance variable next in the last node holds the value null instead of a pointer to another node. The purpose of the chain of nodes is to represent a list of strings. The first string in the list is stored in the first node, the s\u00aeecond string is stored in the second node, and so on. The pointers and the node objects are used to build the structure, but the data that we are interested in representing is the list of strings. Of course, we could just as easily represent a list of integers or a list of JButtons or a list of any other type of data by changing the type of the item that is stored in each node. Although the N\u00aeodes in this example are very simple, we can use them to illustrate the common operations on linked lists. Typical operations include deleting nodes from the list, inserting new nodes into the list, and searching for a specified String among the items in the list. We will look\u00aeat subroutines to perform all of these operations, among others. For a linked list to be used in a program, that program needs a variable that refers to the first node in the list. It only needs a pointer to the first node since all the other nodes in the list can be accessed by starting at the first node and following links along the list from one node to the next. In my examples, I will always use a variable named head, of type Node, that points to the first node in the linked list. When the list is empty, the value of head is null.",
  "page116": "Stacks, Queues, and ADTs- A linked list is a particular type of data structure, made up of objects linked together by pointers. In the previous section, we used a linked list to store an ordered\u00aelist of Strings, and we implemented insert, delete, and find operations on that list. However, we could easily have stored the list of Strings in an array or ArrayList, instead of in a linked list. We could still have implemented the same\u00aeoperations on the list. The implementations of these operations would have been different, but their interfaces and logical behavior would still be the same. The term abstract data type, or ADT, refers to a set of possible values and a set of operations on those values, without any specification of how the values are to be represented or h\u00aeow the operations are to be implemented. An \"ordered list of strings\" can be defined as an abstract data type. Any sequence of Strings that is arranged in increasing order is a possible value of this data type. The operations on the data type include inserting a new string, deleting a string, and finding a string in the list. There are often several different ways to implement the s\u00aeame abstract data type. For example, the \"ordered list of strings\" ADT can be implemented as a linked list or as an array. A program that only depends on the abstract definition of the ADT can use either implementation, interchangeably. In particular, the implementati\u00aeon of the ADT can be changed without affecting the program as a whole. This can make the program easier to debug and maintain, so ADTs are an important tool in software engineering. In this section, we'll look at two common abstract data types, stacks and queues. Both stacks and queues are often implemented as linked lists, but that is not the only possible implementation. You should think of the rest of this section partly as a discussion of stacks and queues and partly as a case study in ADTs.",
  "page117": "Stacks- A stack consists of a sequence of items, which should be thought of as piled one on top of the other like a physical stack of boxes or cafeteria trays. Only the top item on the stack is\u00aeaccessible at any given time. It can be removed from the stack with an operation called pop. An item lower down on the stack can only be removed after all the items on top of it have been popped off the stack. A new item can be added to the\u00aetop of the stack with an operation called push. We can make a stack of any type of items. If, for example, the items are values of type int, then the push and pop operations can be implemented as instance methods void push (int newItem) Add newItem to top of stack. int pop() Remove the top int from the stack and return it. It is an error\u00aeto try to pop an item from an empty stack, so it is important to be able to tell whether a stack is empty. We need another stack operation to do the test, implemented as an instance method boolean isEmpty() Returns true if the stack is empty. To get a better handle on the difference between stacks and queues, consider the sample program DepthBreadth.java. I suggest that you run the program or\u00aetry the applet version that can be found in the on-line version of this section. The program shows a grid of squares. Initially, all the squares are white. When you click on a white square, the program will gradually mark all the squares in the grid, starting from the one wher\u00aee you click. To understand how the program does this, think of yourself in the place of the program. When the user clicks a square, you are handed an index card. The location of the square its row and column is written on the card. You put the card in a pile, which then contains just that one card. Then, you repeat the following: If the pile is empty, you are done. Otherwise, take an index card from the pile. The index card specifies a square. Look at each horizontal and vertical neighbor of that square. If the neighbor has not already been encountered, write its location on a new index card and put the card in the pile.",
  "page118": "While a square is in the pile, waiting to be processed, it is colored red; that is, red squares have been encountered but not yet processed. When a square is taken from the pile and processed, i\u00aets color changes to gray. Once a square has been colored gray, its color won't change again. Eventually, all the squares have been processed, and the procedure ends. In the index card analogy, the pile of cards has been emptied. The pr\u00aeogram can use your choice of three methods: Stack, Queue, and Random. In each case, the same general procedure is used. The only difference is how the \"pile of index cards\" is managed. For a stack, cards are added and removed at the top of the pile. For a queue, cards are added to the bottom of the pile and removed from the top.\u00aeIn the random case, the card to be processed is picked at random from among all the cards in the pile. The order of processing is very different in these three cases. You should experiment with the program to see how it all works. Try to understand how stacks and queues are being used. Try starting from one of the corner squares. While the process is going on, you can click on other white squ\u00aeares, and they will be added to the pile. When you do this with a stack, you should notice that the square you click is processed immediately, and all the red squares that were already waiting for processing have to wait. On the other hand, if you do this with a queue, the squa\u00aere that you click will wait its turn until all the squares that were already in the pile have been processed. Queues seem very natural because they occur so often in real life, but there are times when stacks are appropriate and even essential. For example, consider what happens when a routine calls a subroutine. The first routine is suspended while the subroutine is executed, and it will continue only when the subroutine returns. Now, suppose that the subroutine calls a second subroutine, and the second subroutine calls a third, and so on. Each subroutine is suspended while the subsequent subroutines are executed. The computer has to keep track of all the subroutines that are suspended. It does this with a stack.",
  "page119": "When a subroutine is called, an activation record is created for that subroutine. The activation record contains information relevant to the execution of the subroutine, such as its local variab\u00aeles and parameters. The activation record for the subroutine is placed on a stack. It will be removed from the stack and destroyed when the subroutine returns. If the subroutine calls another subroutine, the activation record of the second\u00aesubroutine is pushed onto the stack, on top of the activation record of the first subroutine. The stack can continue to grow as more subroutines are called, and it shrinks as those subroutines return. Postfix Expressions As another example, stacks can be used to evaluate postfix expressions. An ordinary mathematical expression such as 2+(1\u00ae5-12)*17 is called an infix expression. In an infix expression, an operator comes in between its two operands, as in \"2 + 2\". In a postfix expression, an operator comes after its two operands, as in \"2 2 +\". The infix expression \"2+(15-12)*17\" would be written in postfix form as \"2 15 12 - 17 * +\". The \"-\" operator in this expression applies t\u00aeo the two operands that precede it, namely \"15\" and \"12\". The \"*\" operator applies to the two operands that precede it, namely \"15 12 -\" and \"17\". And the \"+\" operator applies to \"2\" and \"15 12 - 17\u00ae01d. These are the same computations that are done in the original infix expression. Now, suppose that we want to process the expression \"2 15 12 - 17 * +\", from left to right and find its value. The first item we encounter is the 2, but what can we do with it? At this point, we don't know what operator, if any, will be applied to the 2 or what the other operand might be. We have to remember the 2 for later processing. We do this by pushing it onto a stack. Moving on to the next item, we see a 15, which is pushed onto the stack on top of the 2. Then the 12 is added to the stack. Now, we come to the operator, \"-\".",
  "page120": "These numbers are multiplied, and the result, 51 is pushed onto the stack. The next item in the expression is a \"+\" operator, which is processed by popping 51 and 2 from the stack, add\u00aeing them, and pushing the result, 53, onto the stack. Finally, we've come to the end of the expression. The number on the stack is the value of the entire expression, so all we have to do is pop the answer from the stack, and we are do\u00aene! The value of the expression is 53. Tree Traversal- Consider any node in a binary tree. Look at that node together with all its descendents (that is, its children, the children of its children, and so on). This set of nodes forms a binary tree, which is called a subtree of the original tree. For example, in the picture, nodes 2, 4, and\u00ae5 form a subtree. This subtree is called the left subtree of the root. Similarly, nodes 3 and 6 make up the right subtree of the root. We can consider any non-empty binary tree to be made up of a root node, a left subtree, and a right subtree. Either or both of the subtrees can be empty. This is a recursive definition, matching the recursive definition of the TreeNode class. So it should not\u00aebe a surprise that recursive subroutines are often used to process trees. Consider the problem of counting the nodes in a binary tree. (As an exercise, you might try to come up with a non-recursive algorithm to do the counting, but you shouldn't expect to find one.) The he\u00aeart of problem is keeping track of which nodes remain to be counted. It's not so easy to do this, and in fact it's not even possible without an auxiliary data structure such as a stack or queue. With recursion, however, the algorithm is almost trivial. Either the tree is empty or it consists of a root and two subtrees. If the tree is empty, the number of nodes is zero. (This is the base case of the recursion.) Otherwise, use recursion to count the nodes in each subtree. Add the results from the subtrees together, and add one to count the root.",
  "page121": "Binary Sort Trees- One of the examples in Section 9.2 was a linked list of strings, in which the strings were kept in increasing order. While a linked list works well for a small number of strin\u00aegs, it becomes inefficient for a large number of items. When inserting an item into the list, searching for that item's position requires looking at, on average, half the items in the list. Finding an item in the list requires a simila\u00aer amount of time. If the strings are stored in a sorted array instead of in a linked list, then searching becomes more efficient because binary search can be used. However, inserting a new item into the array is still inefficient since it means moving, on average, half of the items in the array to make a space for the new item. A binary tr\u00aeee can be used to store an ordered list of strings, or other items, in a way that makes both searching and insertion efficient. A binary tree used in this way is called a binary sort tree. A binary sort tree is a binary tree with the following property: For every node in the tree, the item in that node is greater than every item in the left subtree of that node, and it is less than or equal t\u00aeo all the items in the right subtree of that node. Here for example is a binary sort tree containing items of type String. (In this picture, I haven't bothered to draw all the pointer variables. Non-null pointers are shown as arrows.)",
  "page122": "Binary sort trees have this useful property: An inorder traversal of the tree will process the items in increasing order. In fact, this is really just another way of expressing the definition. F\u00aeor example, if an inorder traversal is used to print the items in the tree shown above, then the items will be in alphabetical order. The definition of an inorder traversal guarantees that all the items in the left subtree of \"judy\u00ae1d are printed before \"judy\", and all the items in the right subtree of \"judy\" are printed after \"judy\". But the binary sort tree property guarantees that the items in the left subtree of \"judy\" are precisely those that precede \"judy\" in alphabetical order, and all the items in the right subtre\u00aee follow \"judy\" in alphabetical order. So, we know that \"judy\" is output in its proper alphabetical position. But the same argument applies to the subtrees. \"Bill\" will be output after \"alice\" and before \"fred\" and its descendents. \"Fred\" will be output after \"dave\" and before \"jane\" and \"joe\". And so on\u00ae. Suppose that we want to search for a given item in a binary search tree. Compare that item to the root item of the tree. If they are equal, we're done. If the item we are looking for is less than the root item, then we need to search the left subtree of the root the righ\u00aet subtree can be eliminated because it only contains items that are greater than or equal to the root. Similarly, if the item we are looking for is greater than the item in the root, then we only need to look in the right subtree. In either case, the same procedure can then be applied to search the subtree. Inserting a new item is similar: Start by searching the tree for the position where the new item belongs. When that position is found, create a new node and attach it to the tree at that position.",
  "page123": "Searching and inserting are efficient operations on a binary search tree, provided that the tree is close to being balanced. A binary tree is balanced if for each node, the left subtree of that\u00aenode contains approximately the same number of nodes as the right subtree. In a perfectly balanced tree, the two numbers differ by at most one. Not all binary trees are balanced, but if the tree is created by inserting items in a random ord\u00aeer, there is a high probability that the tree is approximately balanced. (If the order of insertion is not random, however, it's quite possible for the tree to be very unbalanced.) During a search of any binary sort tree, every comparison eliminates one of two subtrees from further consideration. If the tree is balanced, that means cu\u00aetting the number of items still under consideration in half. This is exactly the same as the binary search algorithm, and the result, is a similarly efficient algorithm. In terms of asymptotic analysis, searching, inserting, and deleting in a binary search tree have average case run time Theta(log(n)). The problem size, n, is the number of items in the tree, and the average is taken over all\u00aethe different orders in which the items could have been inserted into the tree. As long the actual insertion order is random, the actual run time can be expected to be close to the average. However, the worst case run time for binary search tree operations is Theta(n), which\u00aeis much worse than Theta(log(n)). The worst case occurs for certain particular insertion orders. For example, if the items are inserted into the tree in order of increasing size, then every item that is inserted moves always to the right as it moves down the tree. The result is a \"tree\" that looks more like a linked list, since it consists of a linear string of nodes strung together by their right child pointers. Operations on such a tree have the same performance as operations on a linked list. Now, there are data structures that are similar to simple binary sort trees, except that insertion and deletion of nodes are implemented in a way that will always keep the tree balanced, or almost balanced. For these data structures, searching, inserting, and deleting have both average case and worst case run times that are Theta(log(n)).",
  "page124": "Backus-Naur Form- Natural and artificial languages are similar in that they have a structure known as grammar or syntax. Syntax can be expressed by a set of rules that describe what it means to\u00aebe a legal sentence or program. For programming languages, syntax rules are often expressed in BNF (Backus-Naur Form), a system that was developed by computer scientists John Backus and Peter Naur in the late 1950s. Interestingly, an equiva\u00aelent system was developed independently at about the same time by linguist Noam Chomsky to describe the grammar of natural language. BNF cannot express all possible syntax rules. For example, it can't express the fact that a variable must be defined before it is used. Furthermore, it says nothing about the meaning or semantics of the\u00aelangauge. The problem of specifying the semantics of a language even of an artificial programming langauge is one that is still far from being completely solved. However, BNF does express the basic structure of the language, and it plays a central role in the design of translation programs. Files- The data and programs in a computer's main memory survive only as long as the power is on.\u00aeFor more permanent storage, computers use files, which are collections of data stored on a hard disk, on a USB memory stick, on a CD-ROM, or on some other type of storage device. Files are organized into directories (sometimes called folders). A directory can hold other directo\u00aeries, as well as files. Both directories and files have names that are used to identify them. Programs can read data from existing files. They can create new files and can write data to files. In Java, such input and output can be done using streams. Human-readable character data is read from a file using an object belonging to the class FileReader, which is a subclass of Reader. Similarly, data is written to a file in human-readable format through an object of type FileWriter, a subclass of Writer. For files that store data in machine format, the appropriate I/O classes are FileInputStream and FileOutputStream. In this section, I will only discuss character oriented file I/O using the FileReader and FileWriter classes. However, FileInputStream and FileOutputStream are used in an exactly parallel fashion. All these classes are defined in the java.io package.",
  "page125": "Word Counting- The final example in this section also deals with storing information about words. The problem here is to make a list of all the words that occur in a file, along with the number\u00aeof times that each word occurs. The file will be selected by the user. The output of the program will consist of two lists. Each list contains all the words from the file, along with the number of times that the word occurred. One list is s\u00aeorted alphabetically, and the other is sorted according to the number of occurrences, with the most common words at the top and the least common at the bottom. The problem here is a generalization, which asked you to make an alphabetical list of all the words in a file, without counting the number of occurrences. Symbol Tables- We begin wi\u00aeth a straightforward but important application of maps. When a compiler reads the source code of a program, it encounters definitions of variables, subroutines, and classes. The names of these things can be used later in the program. The compiler has to remember the name is encountered later in the program. This is a natural application for a Map. The name can be used as a key in the map. The\u00aevalue associated to the key is the definition of the name, encoded somehow as an object. A map that is used in this way is called a symbol table. In a compiler, the values in a symbol table can be quite complicated, since the compiler has to deal with names for various sorts o\u00aef things, and it needs a different type of information for each different type of name. We will keep things simple by looking at a symbol table in another context. Suppose that we want a program that can evaluate expressions entered by the user, and suppose that the expressions can contain variables, in addition to operators, numbers, and parentheses. For this to make sense, we need some way of assigning values to variables. When a variable is used in an expression, we need to retrieve the variable's value. A symbol table can be used to store the data that we need. The keys for the symbol table are variable names. The value associated with a key is the value of that variable, which is of type double.",
  "page126": "Almost all applications require persistent data. Persistence is one of the fundamental concepts in application development. If an information system didn't preserve data when it was powered\u00aeoff, the system would be of little practical use. Object persistence means individual objects can outlive the application process; they can be saved to a data store and be re-created at a later point in time. When we talk about persistence\u00aein Java, we're normally talking about mapping and storing object instances in a database using SQL. We start by taking a brief look at the technology and how it's used in Java. Armed with this information, we then continue our discussion of persistence and how it's implemented in object-oriented applications. You, like mos\u00aet other software engineers, have probably worked with SQL and relational databases; many of us handle such systems every day. Relational database management systems have SQL-based application programming interfaces; hence, we call today's relational database products SQL database management systems (DBMS) or, when we're talking about particular systems, SQL databases. Relational te\u00aechnology is a known quantity, and this alone is sufficient reason for many organizations to choose it. But to say only this is to pay less respect than is due. Relational databases are entrenched because they're an incredibly flexible and robust approach to data management\u00ae. Due to the well-researched theoretical foundation of the relational data model, relational databases can guarantee and protect the integrity of the stored data, among other desirable characteristics. You may be familiar with E.F. Codd's four-decades-old introduction of the relational model, A Relational Model of Data for Large Shared Data Banks (Codd, 1970). A more recent compendium worth reading, with a focus on SQL, is C. J. Date's SQL and Relational Theory (Date, 2009).  Relational DBMSs aren't specific to Java, nor is an SQL database specific to a particular application. This important principle is known as data independence. In other words, and we can't stress this important fact enough, data lives longer than any application does. Relational technology provides a way of sharing data among different applications, or among different parts of the same overall system (the data entry application and the reporting application, for example).",
  "page127": "Before we go into more detail about the practical aspects of SQL databases, we have to mention an important issue: although marketed as relational, a database system providing only an SQL data l\u00aeanguage interface isn't really relational and in many ways isn't even close to the original concept. Naturally, this has led to confusion. SQL practitioners blame the relational data model for shortcomings in the SQL language, and\u00aerelational data management experts blame the SQL standard for being a weak implementation of the relational model and ideals. Application engineers are stuck somewhere in the middle, with the burden of delivering something that works. We highlight some important and significant aspects of this issue throughout this book, but generally we\u00aefocus on the practical aspects. If you're interested in more background material, we highly recommend Practical Issues in Database Management: A Reference for the Thinking Practitioner by Fabian Pascal (Pascal, 2000) and An Introduction to Database Systems by Chris Date (Date, 2003) for the theory, concepts, and ideals of (relational) database systems. The latter book is an excellent ref\u00aeerence (it's big) for all questions you may possibly have about databases and data management. To use Hibernate effectively, you must start with a solid understanding of the relational model and SQL. You need to understand the relational model and topics such as normaliza\u00aetion to guarantee the integrity of your data, and you'll need to use your knowledge of SQL to tune the performance of your Hibernate application. Hibernate automates many repetitive coding tasks, but your knowledge of persistence technology must extend beyond Hibernate itself if you want to take advantage of the full power of modern SQL databases. To dig deeper, consult the bibliography at the end of this book.",
  "page128": "You've probably used SQL for many years and are familiar with the basic operations and statements written in this language. Still, we know from our own experience that SQL is sometimes hard\u00aeto remember, and some terms vary in usage.  Let's review some of the SQL terms used in this book. You use SQL as a data definition language (DDL) when creating, altering, and dropping artifacts such as tables and constraints in the c\u00aeatalog of the DBMS. When this schema is ready, you use SQL as a data manipulation language (DML) to perform operations on data, including insertions, updates, and deletions. You retrieve data by executing queries with restrictions, projections, and Cartesian products. For efficient reporting, you use SQL to join, aggregate, and group data\u00aeas necessary. You can even nest SQL statements inside each other a technique that uses subselects. When your business requirements change, you'll have to modify the database schema again with DDL statements after data has been stored; this is known as schema evolution.  If you're an SQL veteran and you want to know more about optimization and how SQL is executed, get a copy of the\u00aeexcellent book SQL Tuning, by Dan Tow (Tow, 2003). For a look at the practical side of SQL through the lens of how not to use SQL, SQL Antipatterns: Avoiding the Pitfalls of Database Programming (Karwin, 2010) is a good resource.  Although the SQL database is one part of ORM,\u00aethe other part, of course, consists of the data in your Java application that needs to be persisted to and loaded from the database.",
  "page129": "Using SQL in Java- When you work with an SQL database in a Java application, you issue SQL statements to the database via the Java Database Connectivity (JDBC) API. Whether the SQL was written b\u00aey hand and embedded in the Java code or generated on the fly by Java code, you use the JDBC API to bind arguments when preparing query parameters, executing the query, scrolling through the query result, retrieving values from the result se\u00aet, and so on. These are low-level data access tasks; as application engineers, we're more interested in the business problem that requires this data access. What we'd really like to write is code that saves and retrieves instances of our classes, relieving us of this lowlevel drudgery. Because these data access tasks are often s\u00aeo tedious, we have to ask, are the relational data model and (especially) SQL the right choices for persistence in objectoriented applications? We answer this question unequivocally: yes! There are many reasons why SQL databases dominate the computing industry relational database management systems are the only proven generic data management technology, and they're almost always a requir\u00aeement in Java projects.  Note that we aren't claiming that relational technology is always the best solution. There are many data management requirements that warrant a completely different approach. For example, internet-scale distributed systems (web search engines, con\u00aetent distribution networks, peer-to-peer sharing, instant messaging) have to deal with exceptional transaction volumes. Many of these systems don't require that after a data update completes, all processes see the same updated data (strong transactional consistency). Users might be happy with weak consistency; after an update, there might be a window of inconsistency before all processes see the updated data. Some scientific applications work with enormous but very specialized datasets. Such systems and their unique challenges typically require equally unique and often custom-made persistence solutions. Generic data management tools such as ACID-compliant transactional SQL databases, JDBC, and Hibernate would play only a minor role.",
  "page130": "ORM and JPA- In a nutshell, object/relational mapping is the automated (and transparent) persistence of objects in a Java application to the tables in an SQL database, using metadata that descri\u00aebes the mapping between the classes of the application and the schema of the SQL database. In essence, ORM works by transforming (reversibly) data from one representation to another. Before we move on, you need to understand what Hibernate\u00aecan't do for you. A supposed advantage of ORM is that it shields developers from messy SQL. This view holds that object-oriented developers can't be expected to understand SQL or relational databases well and that they find SQL somehow offensive. On the contrary, we believe that Java developers must have a sufficient level of fa\u00aemiliarity with and appreciation of relational modeling and SQL in order to work with Hibernate. ORM is an advanced technique used by developers who have already done it the hard way. To use Hibernate effectively, you must be able to view and interpret the SQL statements it issues and understand their performance implications Let's look at some of the benefits of Hibernate: Productivity H\u00aeibernate eliminates much of the grunt work (more than you'd expect) and lets you concentrate on the business problem. No matter which application-development strategy you prefer top-down, starting with a domain model, or bottom-up, starting with an existing database schema\u00aeHibernate, used together with the appropriate tools, will significantly reduce development time. Maintainability Automated ORM with Hibernate reduces lines of code (LOC), making the system more understandable and easier to refactor. Hibernate provides a buffer between the domain model and the SQL schema, insulating each model from minor changes to the other. Performance Although hand-coded persistence might be faster in the same sense that assembly code can be faster than Java code, automated solutions like Hibernate allow the use of many optimizations at all times. One example of this is efficient and easily tunable caching in the application tier. This means developers can spend more energy hand-optimizing the few remaining real bottlenecks instead of prematurely optimizing everything. Vendor independence Hibernate can help mitigate some of the risks associated with vendor lock-in. Even if you plan never to change your DBMS product.",
  "page131": "The Hibernate approach to persistence was well received by Java developers, and the standard Java Persistence API was designed along similar lines. JPA became a key part of the simplifications\u00aeintroduced in recent EJB and Java EE specifications. We should be clear up front that neither Java Persistence nor Hibernate are limited to the Java EE environment; they're general-purpose solutions to the persistence problem that any\u00aetype of Java (or Groovy, or Scala) application can use.  The JPA specification defines the following: A facility for specifying mapping metadata how persistent classes and their properties relate to the database schema. JPA relies heavily on Java annotations in domain model classes, but you can also write mappings in XML files. APIs for\u00aeperforming basic CRUD operations on instances of persistent classes, most prominently javax.persistence.EntityManager to store and load data. A language and APIs for specifying queries that refer to classes and properties of classes. This language is the Java Persistence Query Language (JPQL) and looks similar to SQL. The standardized API allows for programmatic creation of criteria queries w\u00aeithout string manipulation. How the persistence engine interacts with transactional instances to perform dirty checking, association fetching, and other optimization functions. The latest JPA specification covers some basic caching strategies. Hibernate implements JPA and suppo\u00aerts all the standardized mappings, queries, and programming interfaces.",
  "page132": "With object persistence, individual objects can outlive their application process, be saved to a data store, and be re-created later. The object/relational mismatch comes into play when the data\u00aestore is an SQL-based relational database management system. For instance, a network of objects can't be saved to a database table; it must be disassembled and persisted to columns of portable SQL data types. A good solution for this\u00aeproblem is object/relational mapping (ORM). ORM isn't a silver bullet for all persistence tasks; its job is to relieve the developer of 95% of object persistence work, such as writing complex SQL statements with many table joins and copying values from JDBC result sets to objects or graphs of objects. A full-featured ORM middleware\u00aesolution may provide database portability, certain optimization techniques like caching, and other viable functions that aren't easy to hand-code in a limited time with SQL and JDBC. Better solutions than ORM might exist someday. We (and many others) may have to rethink everything we know about data management systems and their languages, persistence API standards, and application integ\u00aeration. But the evolution of today's systems into true relational database systems with seamless object-oriented integration remains pure speculation. We can't wait, and there is no sign that any of these issues will improve soon (a multibillion-dollar industry is\u00aevery agile). ORM is the best solution currently available, and it's a timesaver for developers facing the object/relational mismatch every day.",
  "page133": "Hibernate is an ambitious project that aims to provide a complete solution to the problem of managing persistent data in Java. Today, Hibernate is not only an ORM service, but also a collection\u00aeof data management tools extending well beyond ORM. The Hibernate project suite includes the following: Hibernate ORM Hibernate ORM consists of a core, a base service for persistence with SQL databases, and a native proprietary API. Hibern\u00aeate ORM is the foundation for several of the other projects and is the oldest Hibernate project. You can use Hibernate ORM on its own, independent of any framework or any particular runtime environment with all JDKs. It works in every Java EE/J2EE application server, in Swing applications, in a simple servlet container, and so on. As long\u00aeas you can configure a data source for Hibernate, it works. Hibernate EntityManager This is Hibernate's implementation of the standard Java Persistence APIs, an optional module you can stack on top of Hibernate ORM. You can fall back to Hibernate when a plain Hibernate interface or even a JDBC Connection is needed. Hibernate's native features are a superset of the JPA persistence fe\u00aeatures in every respect. Hibernate Validator Hibernate provides the reference implementation of the Bean Validation (JSR 303) specification. Independent of other Hibernate projects, it provides declarative validation for your domain model (or any other) classes. Hibernat\u00aee Envers Envers is dedicated to audit logging and keeping multiple versions of data in your SQL database. This helps you add data history and audit trails to your application, similar to version control systems you might already be familiar with such as Subversion and Git. Hibernate Search Hibernate Search keeps an index of your domain model data up to date in an Apache Lucene database. It lets you query this database with a powerful and naturally integrated API. Many projects use Hibernate Search in addition to Hibernate ORM, adding full-text search capabilities. If you have a free text search form in your application's user interface, and you want happy users, work with Hibernate Search. Hibernate Search isn't covered in this book; you can find more information in Hibernate Search in Action by Emmanuel Bernard (Bernard, 2008).",
  "page134": "The \"Hello World\" example in the previous chapter introduced you to Hibernate; certainly, it isn't useful for understanding the requirements of real-world applications with comple\u00aex data models. For the rest of the book, we use a much more sophisticated example application CaveatEmptor, an online auction system to demonstrate Hibernate and Java Persistence. (Caveat emptor means \"Let the buyer beware\".) We\u00ae2019ll start our discussion of the application by introducing a layered application architecture. Then, you'll learn how to identify the business entities of a problem domain. You'll create a conceptual model of these entities and their attributes, called a domain model, and you'll implement it in Java by creating persistent c\u00aelasses. We'll spend some time exploring exactly what these Java classes should look like and where they fit within a typical layered application architecture. We'll also look at the persistence capabilities of the classes and how this aspect influences the design and implementation. We'll add Bean Validation, which helps to automatically verify the integrity of the domain model\u00aedata not only for persistent information but all business logic.  We'll then explore mapping metadata options the ways you tell Hibernate how your persistent classes and their properties relate to database tables and columns. This can be as simple as adding annotations d\u00aeirectly in the Java source code of the classes or writing XML documents that you eventually deploy along with the compiled Java classes that Hibernate accesses at runtime. After reading this chapter, you'll know how to design the persistent parts of your domain model in complex real-world projects, and what mapping metadata option you'll primarily prefer and use. Let's start with the example application.",
  "page135": "The example CaveatEmptor application- The CaveatEmptor example is an online auction application that demonstrates ORM techniques and Hibernate functionality. You can download the source code for\u00aethe application from www.jpwh.org. We won't pay much attention to the user interface in this book (it could be web based or a rich client); we'll concentrate instead on the data access code. When a design decision about data acce\u00aess code that has consequences for the user interface has to be made, we'll naturally consider both.  In order to understand the design issues involved in ORM, let's pretend the CaveatEmptor application doesn't yet exist and that you're building it from scratch. Let's start by looking at the architecture. A layered\u00aearchitecture- With any nontrivial application, it usually makes sense to organize classes by concern. Persistence is one concern; others include presentation, workflow, and business logic. A typical object-oriented architecture includes layers of code that represent the concerns. A layered architecture defines interfaces between code that implements the various concerns, allowing changes to\u00aebe made to the way one concern is implemented without significant disruption to code in the other layers. Layering determines the kinds of inter-layer dependencies that occur. The rules are as follows: Layers communicate from top to bottom. A layer is dependent only on the int\u00aeerface of the layer directly below it. Each layer is unaware of any other layers except for the layer just below it.",
  "page136": "The CaveatEmptor domain model The CaveatEmptor site auctions many different kinds of items, from electronic equipment to airline tickets. Auctions proceed according to the English auction strate\u00aegy: users continue to place bids on an item until the bid period for that item expires, and the highest bidder wins.  In any store, goods are categorized by type and grouped with similar goods into sections and onto shelves. The auction ca\u00aetalog requires some kind of hierarchy of item categories so that a buyer can browse these categories or arbitrarily search by category and item attributes. Lists of items appear in the category browser and search result screens. Selecting an item from a list takes the buyer to an item-detail view where an item may have images attached to i\u00aet.  An auction consists of a sequence of bids, and one is the winning bid. User details include name, address, and billing information.  The result of this analysis, the high-level overview of the domain model, is shown in figure 3.3. Let's briefly discuss some interesting features of this model.  Each item can be auctioned only once, so you don't need to make Item distinct from\u00aeany auction entities. Instead, you have a single auction item entity named Item. Thus, Bid is associated directly with Item. You model the Address information of a User as a separate class, a User may have three addresses, for home, billing, and shipping. You do allow the user\u00aeto have many BillingDetails. Subclasses of an abstract class represent the various billing strategies (allowing future extension).  The application may nest a Category inside another Category, and so on. A recursive association, from the Category entity to itself, expresses this relationship. Note that a single Category may have multiple child categories but at most one parent. Each Item belongs to at least one Category.  This representation isn't the complete domain model but only classes for which you need persistence capabilities. You'd like to store and load instances of Category, Item, User, and so on. We have simplified this high-level overview a little; we may introduce additional classes later or make minor modifications to them when needed for more complex examples.",
  "page137": "Implementing the domain model- You'll start with an issue that any implementation must deal with: the separation of concerns. The domain model implementation is usually a central, organizin\u00aeg component; it's reused heavily whenever you implement new application functionality. For this reason, you should be prepared to go to some lengths to ensure that concerns other than business aspects don't leak into the domain mo\u00aedel implementation. When concerns such as persistence, transaction management, or authorization start to appear in the domain model classes, this is an example of leakage of concerns. The domain model implementation is such an important piece of code that it shouldn't depend on orthogonal Java APIs. For example, code in the domain mod\u00aeel shouldn't perform JNDI lookups or call the database via the JDBC API, not directly and not through an intermediate abstraction. This allows you to reuse the domain model classes virtually anywhere: The presentation layer can access instances and attributes of domain model entities when rendering views. The controller components in the business layer can also access the state of domai\u00aen model entities and call methods of the entities to execute business logic. The persistence layer can load and store instances of domain model entities from and to the database, preserving their state.",
  "page138": "Most important, preventing leakage of concerns makes it easy to unit-test the domain model without the need for a particular runtime environment or container, or the need for mocking any service\u00aedependencies. You can write unit tests that verify the correct behavior of your domain model classes without any special test harness. (We aren't talking about testing \"load from the database\" and \"store in the database\u00ae\" aspects, but \"calculate the shipping cost and tax\" behavior.) The Java EE standard solves the problem of leaky concerns with metadata, as annotations within your code or externalized as XML descriptors. This approach allows the runtime container to implement some predefined cross-cutting concerns security, concurrency, pe\u00aersistence, transactions, and remoteness in a generic way, by intercepting calls to application components.  Hibernate isn't a Java EE runtime environment, and it's not an application server. It's an implementation of just one specification under the Java EE umbrella JPA and a solution for just one of these concerns: persistence. JPA defines the entity class as the primary pr\u00aeogramming artifact. This programming model enables transparent persistence, and a JPA provider such as Hibernate also offers automated persistence. Transparent and automated persistence- We use transparent to mean a complete separation of concerns between the persistent classes\u00aeof the domain model and the persistence layer. The persistent classes are unaware of and have no dependency on the persistence mechanism. We use automatic to refer to a persistence solution (your annotated domain, the layer, and mechanism) that relieves you of handling low-level mechanical details, such as writing most SQL statements and working with the JDBC API.",
  "page139": "The Item class of the CaveatEmptor domain model, for example, shouldn't have any runtime dependency on any Java Persistence or Hibernate API. Furthermore: JPA doesn't require that any\u00aespecial superclasses or interfaces be inherited or implemented by persistent classes. Nor are any special classes used to implement attributes and associations. (Of course, the option to use both techniques is always there.) You can reuse\u00aepersistent classes outside the context of persistence, in unit tests or in the presentation layer, for example. You can create instances in any runtime environment with the regular Java new operator, preserving testability and reusability. In a system with transparent persistence, instances of entities aren't aware of the underlying d\u00aeata store; they need not even be aware that they're being persisted or retrieved. JPA externalizes persistence concerns to a generic persistence manager API. Hence, most of your code, and certainly your complex business logic, doesn't have to concern itself with the current state of a domain model entity instance in a single thread of execution.",
  "page140": "We regard transparency as a requirement because it makes an application easier to build and maintain. Transparent persistence should be one of the primary goals of any ORM solution. Clearly, no\u00aeautomated persistence solution is completely transparent: Every automated persistence layer, including JPA and Hibernate, imposes some requirements on the persistent classes. For example, JPA requires that collectionvalued attributes be typ\u00aeed to an interface such as java.util.Set or java.util.List and not to an actual implementation such as java.util.HashSet (this is a good practice anyway). Or, a JPA entity class has to have a special attribute, called the database identifier (which is also less of a restriction but usually convenient).  You now know why the persistence me\u00aechanism should have minimal impact on how you implement a domain model, and that transparent and automated persistence are required. Our preferred programming model to archive this is POJO.",
  "page141": "Around 10 years ago, many developers started talking about POJO, a back-to-basics approach that essentially revives JavaBeans, a component model for UI development, and reapplies it to the other\u00aelayers of a system. Several revisions of the EJB and JPA specifications brought us new lightweight entities, and it would be appropriate to call them persistence-capable JavaBeans. Java engineers often use all these terms as synonyms for t\u00aehe same basic design approach.  You shouldn't be too concerned about what terms we use in this book; the ultimate goal is to apply the persistence aspect as transparently as possible to Java classes. Almost any Java class can be persistence-capable if you follow some simple practices. Let's see how this looks in code.  Writing\u00aepersistence-capable classes Working with fine-grained and rich domain models is a major Hibernate objective. This is a reason we work with POJOs. In general, using fine-grained objects means more classes than tables.  A persistence-capable plain-old Java class declares attributes, which represent state, and business methods, which define behavior. Some attributes represent associations to her persistence-capable classes.",
  "page142": "JPA doesn't require that persistent classes implement java.io.Serializable. But when instances are stored in an HttpSession or passed by value using RMI, serialization is necessary. Althoug\u00aeh this might not occur in your application, the class will be serializable without any additional work, and there are no downsides to declaring that. (We aren't going to declare it on every example, assuming that you know when it will\u00aebe necessary.)  The class can be abstract and, if needed, extend a non-persistent class or implement an interface. It must be a top-level class, not nested within another class. The persistence-capable class and any of its methods can't be final (a requirement of the JPA specification).  Unlike the JavaBeans specification, which req\u00aeuires no specific constructor, Hibernate (and JPA) require a constructor with no arguments for every persistent class. Alternatively, you might not write a constructor at all; Hibernate will then use the Java default constructor. Hibernate calls classes using the Java reflection API on such a no argument constructor to create instances. The constructor may not be public, but it has to be at l\u00aeeast package-visible if Hibernate will use runtime-generated proxies for performance optimization. Also, consider the requirements of other specifications: the EJB standard requires public visibility on session bean constructors, just like the JavaServer Faces (JSF) specificati\u00aeon requires for its managed beans. There are other situations when you'd want a public constructor to create an \"empty\" state: for example, query-by-example building",
  "page143": "The properties of the POJO implement the attributes of the business entities for example, the username of User. You usually implement properties as private or protected member fields, together w\u00aeith public or protected property accessor methods: for each field a method for retrieving its value and a method for setting the value. These methods are known as the getter and setter, respectively. The example POJO in listing 3.1 declares\u00aegetter and setter methods for the username property.  The JavaBean specification defines the guidelines for naming accessor methods; this allows generic tools like Hibernate to easily discover and manipulate property values. A getter method name begins with get, followed by the name of the property (the first letter in uppercase); a sett\u00aeer method name begins with set and similarly is followed by the name of the property. You may begin getter methods for Boolean properties with is instead of get. Hibernate doesn't require accessor methods. You can choose how the state of an instance of your persistent classes should be persisted. Hibernate will either directly access fields or call accessor methods. Your class design is\u00aen't disturbed much by these considerations. You can make some accessor methods non-public or completely remove them then configure Hibernate to rely on field access for these properties.",
  "page144": "Should property fields and accessor methods be private, protected, or package visible? Typically, you want to discourage direct access to the internal state of your class, so you don't mak\u00aee attribute fields public. If you make fields or methods private, you're effectively declaring that nobody should ever access them; only you're allowed to do that (or a service like Hibernate). This is a definitive statement. Ther\u00aee are often good reasons for someone to access your \"private\" internals usually to fix one of your bugs and you only make people angry if they have to fall back to reflection access in an emergency. Instead, you might assume or know that the engineer who comes after you has access to your code and knows what they're doing. \u00aeThe protected visibility then is a more reasonable default. You're forbidding direct public access, indicating that this particular member detail is internal, but allowing access by subclasses if need be. You trust the engineer who creates the subclass. Package visibility is rude: you're forcing someone to create code in the same package to access member fields and methods; this is\u00aeextra work for no good reason. Most important, these recommendations for visibility are relevant for environments without security policies and a runtime SecurityManager. If you have to keep your internal code private, make it private.",
  "page145": "Implementing POJO associations- You'll now see how to associate and create different kinds of relationships between objects: one-to-many, many-to-one, and bidirectional relationships. We\u00ae019ll look at the scaffolding code needed to create these associations, how to simplify relationship management, and how to enforce the integrity of these relationships. Shouldn't bids on an item be stored in a list? The first reaction i\u00aes often to preserve the order of elements as they're entered by users, because this may also be the order in which you will show them later. Certainly, in an auction application there has to be some defined order in which the user sees bids for an item for example, highest bid first or newest bid last. You might even work with a java.\u00aeutil.List in your user interface code to sort and display bids of an item. That doesn't mean this display order should be durable; data integrity isn't affected by the order in which bids are displayed. You need to store the amount of each bid, so you can find the highest bid, and you need to store a timestamp for each bid when it's created, so you can find the newest bid. When\u00aein doubt, keep your system flexible and sort the data when it's retrieved from the datastore (in a query) and/or shown to the user (in Java code), not when it's stored.",
  "page146": "The addBid() method not only reduces the lines of code when dealing with Item and Bid instances, but also enforces the cardinality of the association. You avoid errors that arise from leaving ou\u00aet one of the two required actions. You should always provide this kind of grouping of operations for associations, if possible. If you compare this with the relational model of foreign keys in an SQL database, you can easily see how a netwo\u00aerk and pointer model complicates a simple operation: instead of a declarative constraint, you need procedural code to guarantee data integrity.  Because you want addBid() to be the only externally visible mutator method for the bids of an item (possibly in addition to a removeBid() method), you can make the Item#setBids() method private o\u00aer drop it and configure Hibernate to directly access fields for persistence. Consider making the Bid#setItem() method package-visible, for the same reason.  The Item#getBids() getter method still returns a modifiable collection, so clients can use it to make changes that aren't reflected on the inverse side. Bids added directly to the collection wouldn't have a reference to an item\u00aean inconsistent state, according to your database constraints. To prevent this, you can wrap the internal collection before returning it from the getter method, with Collections.unmodifiableCollection(c) and Collections.unmodifiableSet(s). The client then gets an exception if\u00aeit tries to modify the collection; you therefore force every modification to go through the relationship management method that guarantees integrity. Note that in this case you'll have to configure Hibernate for field access, because the collection",
  "page147": "There are several problems with this approach. First, Hibernate can't call this constructor. You need to add a no-argument constructor for Hibernate, and it needs to be at least package-vis\u00aeible. Furthermore, because there is no setItem() method, Hibernate would have to be configured to access the item field directly. This means the field can't be final, so the class isn't guaranteed to be immutable.  In the example\u00aes in this book, we'll sometimes write scaffolding methods such as the Item#addBid() shown earlier, or we may have additional constructors for required values. It's up to you how many convenience methods and layers you want to wrap around the persistent association properties and/or fields, but we recommend being consistent and ap\u00aeplying the same strategy to all your domain model classes. For the sake of readability, we won't always show convenience methods, special constructors, and other such scaffolding in future code samples and assume you'll add them according to your own taste and requirements.  You now have seen domain model classes, how to represent their attributes, and the relationships between the\u00aem. Next, we'll increase the level of abstraction, adding metadata to the domain model implementation and declaring aspects such as validation and persistence rules.",
  "page148": "Domain model metadata- Metadata is data about data, so domain model metadata is information about your domain model. For example, when you use the Java reflection API to discover the names of cl\u00aeasses of your domain model or the names of their attributes, you're accessing domain model metadata. ORM tools also require metadata, to specify the mapping between classes and tables, properties and columns, associations and foreign\u00aekeys, Java types and SQL types, and so on. This object/relational mapping metadata governs the transformation between the different type systems and relationship representations in objectoriented and SQL systems. JPA has a metadata API, which you can call to obtain details about the persistence aspects of your domain model, such as the nam\u00aees of persistent entities and attributes. First, it's your job as an engineer to create and maintain this information. JPA standardizes two metadata options: annotations in Java code and externalized XML descriptor files. Hibernate has some extensions for native functionality, also available as annotations and/or XML descriptors. Usually we prefer either annotations or XML files as the\u00aeprimary source of mapping metadata. After reading this section, you'll have the background information to make an educated decision for your own project.",
  "page149": "We'll also discuss Bean Validation (JSR 303) and how it provides declarative validation for your domain model (or any other) classes. The reference implementation of this specification is t\u00aehe Hibernate Validator project. Most engineers today prefer Java annotations as the primary mechanism for declaring metadata. Applying Bean Validation rules Most applications contain a multitude of data-integrity checks. You've seen wh\u00aeat happens when you violate one of the simplest data-integrity constraints: you get a NullPointerException when you expect a value to be available. Other examples are a string-valued property that shouldn't be empty (remember, an empty string isn't null), a string that has to match a particular regular expression pattern, and a n\u00aeumber or date value that must be within a certain range.  These business rules affect every layer of an application: The user interface code has to display detailed and localized error messages. The business and persistence layers must check input values received from the client before passing them to the datastore. The SQL database has to be the final validator, ultimately guaranteeing the\u00aeintegrity of durable data.",
  "page150": "The idea behind Bean Validation is that declaring rules such as \"This property can't be null\" or \"This number has to be in the given range\" is much easier and less error\u00ae-prone than writing if-then-else procedures repeatedly. Furthermore, declaring these rules on the central component of your application, the domain model implementation, enables integrity checks in every layer of the system. The rules are t\u00aehen available to the presentation and persistence layers. And if you consider how dataintegrity constraints affect not only your Java application code but also your SQL database schema which is a collection of integrity rules you might think of Bean Validation constraints as additional ORM metadata You add two more attributes the name of a\u00aen item and the auctionEnd date when an auction concludes. Both are typical candidates for additional constraints: you want to guarantee that the name is always present and human readable (one-character item names don't make much sense), but it shouldn't be too long your SQL database will be most efficient with variable-length strings up to 255 characters, and your user interface als\u00aeo has some constraints on visible label space. The ending time of an auction obviously should be in the future. If you don't provide an error message, a default message will be used. Messages can be keys to external properties files, for internationalization.",
  "page151": "The validation engine will access the fields directly if you annotate the fields. If you prefer calls through accessor methods, annotate the getter method with validation constraints, not the se\u00aetter. Then constraints are part of the class's API and included in its Javadoc, making the domain model implementation easier to understand. Note that this is independent from access by the JPA provider; that is, Hibernate Validator ma\u00aey call accessor methods, whereas Hibernate ORM may call fields directly.  Bean Validation isn't limited to the built-in annotations; you can create your own constraints and annotations. With a custom constraint, you can even use class-level annotations and validate several attribute values at the same time on an instance of the class\u00ae. The following test code shows how you can manually check the integrity of an Item instance. We're not going to explain this code in detail but offer it for you to explore. You'll rarely write this kind of validation code; most of the time, this aspect is automatically handled by your user interface and persistence framework. It's therefore important to look for Bean Validatio\u00aen integration when selecting a UI framework. JSF version 2 and newer automatically integrates with Bean Validation, for example.  Hibernate, as required from any JPA provider, also automatically integrates with Hibernate Validator if the libraries are available on the classpat\u00aeh and offers the following features:",
  "page152": "You don't have to manually validate instances before passing them to Hibernate for storage. Hibernate recognizes constraints on persistent domain model classes and triggers validatio\u00aen before database insert or update operations. When validation fails, Hibernate throws a ConstraintViolationException, containing the failure details, to the code calling persistence-management operations. The Hibernate toolset for a\u00aeutomatic SQL schema generation understands many constraints and generates SQL DDL-equivalent constraints for you. For example, an @NotNull annotation translates into an SQL NOT NULL constraint, and an @Size(n) rule defines the number of characters in a VARCHAR(n)-typed column. You can control this behavior of Hibernate with the <validation\u00ae-mode> element in your persistence.xml configuration file. The default mode is AUTO, so Hibernate will only validate if it finds a Bean Validation provider (such as Hibernate Validator) on the classpath of the running application. With mode CALLBACK, validation will always occur, and you'll get a deployment error if you forget to bundle a Bean Validation provider. The NONE mode disables\u00aeautomatic validation by the JPA provider. You'll see Bean Validation annotations again later in this book; you'll also find them in the example code bundles. At this point we could write much more about Hibernate Validator, but we'd only repeat what is already av\u00aeailable in the project's excellent reference guide. Have a look, and find out more about features such as validation groups and the metadata API for discovery of constraints.  The Java Persistence and Bean Validation standards embrace annotations aggressively. The expert groups have been aware of the advantages of XML deployment descriptors in certain situations, especially for configuration metadata that changes with each deployment.",
  "page153": "This part is all about actual ORM, from classes and properties to tables and columns. Chapter 4 starts with regular class and property mappings and explains how you can map fine-grained Java dom\u00aeain models. Next, in chapter 5, you'll see how to map basic properties and embeddable components, and how to control mapping between Java and SQL types. In chapter 6, you'll map inheritance hierarchies of entities to the database\u00aeusing four basic inheritance-mapping strategies; you'll also map polymorphic associations. Chapter 7 is all about mapping collections and entity associations: you map persistent collections, collections of basic and embeddable types, and simple many-to-one and one-to-many entity associations. Chapter 8 dives deeper with advanced entit\u00aey association mappings like mapping one-to-one entity associations, one-to-many mapping options, and many-to-many and ternary entity relationships. Finally, you'll find chapter 9 most interesting if you need to introduce Hibernate in an existing application, or if you have to work with legacy database schemas and handwritten SQL. We'll also talk about customized SQL DDL for schema g\u00aeeneration in this chapter. After reading this part of the book, you'll be ready to create even the most complex mappings quickly and with the right strategy. You'll understand how the problem of inheritance mapping can be solved and how to map collections and associa\u00aetions. You'll also be able to tune and customize Hibernate for integration with any existing database schema or application.",
  "page154": "Fine-grained domain models- A major objective of Hibernate is support for fine-grained and rich domain models. It's one reason we work with POJOs. In crude terms, fine-grained means more cl\u00aeasses than tables.  For example, a user may have a home address in your domain model. In the database, you may have a single USERS table with the columns HOME_STREET, HOME_CITY, and HOME_ZIPCODE. (Remember the problem of SQL types we discu\u00aessed in section 1.2.1?)  In the domain model, you could use the same approach, representing the address as three string-valued properties of the User class. But it's much better to model this using an Address class, where User has a homeAddress property. This domain model achieves improved cohesion and greater code reuse, and it\u00ae9s more understandable than SQL with inflexible type systems. JPA emphasizes the usefulness of fine-grained classes for implementing type safety and behavior. For example, many people model an email address as a string-valued property of User. A more sophisticated approach is to define an EmailAddress class, which adds higher-level semantics and behavior it may provide a prepareMail() method (it\u00aeshouldn't have a sendMail() method, because you don't want your domain model classes to depend on the mail subsystem).  This granularity problem leads us to a distinction of central importance in ORM. In Java, all classes are of equal standing all instances have thei\u00aer own identity and life cycle. When you introduce persistence, some instances may not have their own identity and life cycle but depend on others. Let's walk through an example",
  "page155": "Distinguishing entities and value types- You may find it helpful to add stereotype (a UML extensibility mechanism) information to your UML class diagrams so you can immediately recognize entitie\u00aes and value types. This practice also forces you to think about this distinction for all your classes, which is a first step to an optimal mapping and well-performing persistence layer. The Item and User classes are obvious entities. They\u00aeeach have their own identity, their instances have references from many other instances (shared references), and they have independent lifespans.  Marking the Address as a value type is also easy: a single User instance references a particular Address instance. You know this because the association has been created as a composition, where\u00aethe User instance has been made fully responsible for the life cycle of the referenced Address instance. Therefore, Address instances can't be referenced by anyone else and don't need their own identity.  The Bid class could be a problem. In object-oriented modeling, this is marked as a composition (the association between Item and Bid with the diamond). Thus, an Item is the owner\u00aeof its Bid instances and holds a collection of references. At first, this seems reasonable, because bids in an auction system are useless when the item they were made for is gone.  But what if a future extension of the domain model requires a User#bids collection, containing\u00aeall bids made by a particular User? Right now, the association between Bid and User is unidirectional; a Bid has a bidder reference. What if this was bidirectional?",
  "page156": "Mapping entities with identity- Mapping entities with identity requires you to understand Java identity and equality before we can walk through an entity class example and its mapping. After tha\u00aet, we'll be able to dig in deeper and select a primary key, configure key generators, and finally go through identifier generator strategies. First, it's vital to understand the difference between Java object identity and object e\u00aequality before we discuss terms like database identity and the way JPA manages identity. Understanding Java identity and equality- Java developers understand the difference between Java object identity and equality. Object identity (=) is a notion defined by the Java virtual machine. Two references are identical if they point to the same\u00aememory location. On the other hand, object equality is a notion defined by a class's equals() method, sometimes also referred to as equivalence. Equivalence means two different (non-identical) instances have the same value the same state. Two different instances of String are equal if they represent the same sequence of characters, even though each has its own location in the memory spa\u00aece of the virtual machine. (If you're a Java guru, we acknowledge that String is a special case. Assume we used a different class to make the same point.) Persistence complicates this picture. With object/relational persistence, a persistent instance is an in-memory repres\u00aeentation of a particular row (or rows) of a database table (or tables). Along with Java identity and equality, we define database identity",
  "page157": "Objects are identical if they occupy the same memory location in the JVM. This can be checked with the a = b operator. This concept is known as object identity. Objects are equal if they\u00aehave the same state, as defined by the a.equals(Object b) method. Classes that don't explicitly override this method inherit the implementation defined by java.lang.Object, which compares object identity with ==. This concept is known\u00aeas object equality. Objects stored in a relational database are identical if they share the same table and primary key value. This concept, mapped into the Java space, is known as database identity. We now need to look at how database identity relates to object identity and how to express database identity in the mapping metadata. A\u00aes an example, you'll map an entity of a domain model. Every entity class has to have an @Id property; it's how JPA exposes database identity to the application. We don't show the identifier property in our diagrams; we assume that each entity class has one. In our examples, we always name the identifier property id. This is a good practice for your own project; use the same id\u00aeentifier property name for all your domain model entity classes. If you specify nothing else, this property maps to a primary key column named ID of the ITEM table in your database schema.",
  "page158": "Hibernate will use the field to access the identifier property value when loading and storing items, not getter or setter methods. Because @Id is on a field, Hibernate will now enable every fiel\u00aed of the class as a persistent property by default. The rule in JPA is this: if @Id is on a field, the JPA provider will access fields of the class directly and consider all fields part of the persistent state by default. You'll see ho\u00aew to override this later in this chapter in our experience, field access is often the best choice, because it gives you more freedom for accessor method design.  Should you have a (public) getter method for the identifier property? Well, the application often uses database identifiers as a convenient handle to a particular instance, even\u00aeoutside the persistence layer. For example, it's common for web applications to display the results of a search screen to the user as a list of summaries. When the user selects a particular element, the application may need to retrieve the selected item, and it's common to use a lookup by identifier for this purpose you've probably already used identifiers this way, even in app\u00aelications that rely on JDBC. Should you have a setter method? Primary key values never change, so you shouldn't allow modification of the identifier property value. Hibernate won't update a primary key column, and you shouldn't expose a public identifier setter m\u00aeethod on an entity.  The Java type of the identifier property, java.lang.Long in the previous example, depends on the primary key column type of the ITEM table and how key values are produced. This brings us to the @GeneratedValue annotation and primary keys in general.",
  "page159": "Selecting a primary key-Must primary keys be immutable? The relational model defines that a candidate key must be unique and irreducible (no subset of the key attributes has the uniqueness prope\u00aerty). Beyond that, picking a candidate key as the primary key is a matter of taste. But Hibernate expects a candidate key to be immutable when used as the primary key. Hibernate doesn't support updating primary key values with an API;\u00aeif you try to work around this requirement, you'll run into problems with Hibernate's caching and dirty-checking engine. If your database schema relies on updatable primary keys (and maybe uses ON UPDATE CASCADE foreign key constraints), you must change the schema before it will work with Hibernate. The database identifier of an\u00aeentity is mapped to some table primary key, so let's first get some background on primary keys without worrying about mappings. Take a step back and think about how you identify entities.  A candidate key is a column or set of columns that you could use to identify a particular row in a table. To become the primary key, a candidate key must satisfy the following requirements: The value\u00aeof any candidate key column is never null. You can't identify something with data that is unknown, and there are no nulls in the relational model. Some SQL products allow defining (composite) primary keys with nullable columns, so you must be careful. The value of the can\u00aedidate key column(s) is a unique value for any row. The value of the candidate key column(s) never changes; it's immutable.",
  "page160": "If a table has only one identifying attribute, it becomes, by definition, the primary key. But several columns or combinations of columns may satisfy these properties for a particular table; you\u00aechoose between candidate keys to decide the best primary key for the table. You should declare candidate keys not chosen as the primary key as unique keys in the database if their value is indeed unique (but maybe not immutable).  Many le\u00aegacy SQL data models use natural primary keys. A natural key is a key with business meaning: an attribute or combination of attributes that is unique by virtue of its business semantics. Examples of natural keys are the US Social Security Number and Australian Tax File Number. Distinguishing natural keys is simple: if a candidate key attri\u00aebute has meaning outside the database context, it's a natural key, regardless of whether it's automatically generated. Think about the application users: if they refer to a key attribute when talking about and working with the application, it's a natural key: \"Can you send me the pictures of item #123-abc?\" Experience has shown that natural primary keys usually cause\u00aeproblems in the end. A good primary key must be unique, immutable, and never null. Few entity attributes satisfy these requirements, and some that do can't be efficiently indexed by SQL databases (although this is an implementation detail and shouldn't be the decidin\u00aeg factor for or against a particular key). In addition, you should make certain that a candidate key definition never changes throughout the lifetime of the database. Changing the value (or even definition) of a primary key, and all foreign keys that refer to it, is a frustrating task. Expect your database schema to survive decades, even if your application won't.",
  "page161": "Furthermore, you can often only find natural candidate keys by combining several columns in a composite natural key. These composite keys, although certainly appropriate for some schema artifact\u00aes (like a link table in a many-to-many relationship), potentially make maintenance, ad hoc queries, and schema evolution much more difficult. We talk about composite keys later in the book, For these reasons, we strongly recommend that you\u00aeadd synthetic identifiers, also called surrogate keys. Surrogate keys have no business meaning they have unique values generated by the database or application. Application users ideally don't see or refer to these key values; they're part of the system internals. Introducing a surrogate key column is also appropriate in the comm\u00aeon situation when there are no candidate keys. In other words, (almost) every table in your schema should have a dedicated surrogate primary key column with only this purpose. There are a number of well-known approaches to generating surrogate key values. The aforementioned @GeneratedValue annotation is how you configure this. Configuring key generators- The @Id annotation is required to ma\u00aerk the identifier property of an entity class. Without the @GeneratedValue next to it, the JPA provider assumes that you'll take care of creating and assigning an identifier value before you save an instance. We call this an application-assigned identifier. Assigning an en\u00aetity identifier manually is necessary when you're dealing with a legacy database and/or natural primary keys. We have more to say about this kind of mapping in a dedicated section,",
  "page162": "Usually you want the system to generate a primary key value when you save an entity instance, so you write the GeneratedValue annotation next to @Id. JPA standardizes several value-generation st\u00aerategies with the javax.persistence.GenerationType enum, which you select with @GeneratedValue(strategy): GenerationType.AUTO Hibernate picks an appropriate strategy, asking the SQL dialect of your configured database what is best. This is\u00aeequivalent to @GeneratedValue() without any settings. GenerationType.SEQUENCE Hibernate expects (and creates, if you use the tools) a sequence named HIBERNATE_SEQUENCE in your database. The sequence will be called separately before every INSERT, producing sequential numeric values. GenerationType.IDENTITY Hibernate expects (and creates in\u00aetable DDL) a special auto-incremented primary key column that automatically generates a numeric value on INSERT, in the database. GenerationType.TABLE Hibernate will use an extra table in your database schema that holds the next numeric primary key value, one row for each entity class. This table will be read and updated accordingly, before INSERTs. The default table name is HIBERNATE_SEQUEN\u00aeCES with columns SEQUENCE_NAME and SEQUENCE_NEXT_HI_VALUE. (The internal implementation uses a more complex but efficient hi/lo generation algorithm; more on this later.)",
  "page163": "JPA has two built-in annotations you can use to configure named generators: @javax persistence.SequenceGenerator and @javax.persistence.TableGenerator. With these annotations, you can create a n\u00aeamed generator with your own sequence and table names. As usual with JPA annotations, you can unfortunately only use them at the top of a (maybe otherwise empty) class, and not in a package-info.java file. For this reason, and because the\u00aeJPA annotations don't give us access to the full Hibernate feature set, we prefer an alternative: the native @org.hibernate.annotations .GenericGenerator annotation. It supports all Hibernate identifier generator strategies and their configuration details. Unlike the rather limited JPA annotations, you can use the Hibernat\u00aee annotation in a package-info.java file, typically in the same package as your domain model classes. The next listing shows a recommended configuration. This Hibernate-specific generator configuration has the following advantages: The enhanced-sequence B strategy produces sequential numeric values. If your SQL dialect supports sequences, Hibernate will use an actual database sequence. If yo\u00aeur DBMS doesn't support native sequences, Hibernate will manage and use an extra \"sequence table,\" simulating the behavior of a sequence. This gives you real portability: the generator can always be called before performing an SQL INSERT, unlike, for\u00aeexample, auto-increment identity columns, which produce a value on INSERT that has to be returned to the application afterward.",
  "page164": "You can configure the sequence_name C. Hibernate will either use an existing sequence or create it when you generate the SQL schema automatically. If your DBMS doesn't support sequences, th\u00aeis will be the special \"sequence table\" name. You can start with an initial_value D that gives you room for test data. For example, when your integration test runs, Hibernate will make any new data insertions from test code with i\u00aedentifier values greater than 1000. Any test data you want to import before the test can use numbers 1 to 999, and you can refer to the stable identifier values in your tests: \"Load item with id 123 and run some tests on it.\" This is applied when Hibernate generates the SQL schema and sequence; it's a DDL option. You can sha\u00aere the same database sequence among all your domain model classes. There is no harm in specifying @GeneratedValue(generator \"ID_GENERATOR\") in all your entity classes. It doesn't matter if primary key values aren't contiguous for a particular entity, as long as they're unique within one table. If you're worried about contention, because the sequence has to be called prio\u00aer to every INSERT, we discuss a variation of this generator configuration later,",
  "page165": "Finally, you use java.lang.Long as the type of the identifier property in the entity class, which maps perfectly to a numeric database sequence generator. You could also use a long primitive. Th\u00aee main difference is what someItem.getId() returns on a new item that hasn't been stored in the database: either null or 0. If you want to test whether an item is new, a null check is probably easier to understand for someone else read\u00aeing your code. You shouldn't use another integral type such as int or short for identifiers. Although they will work for a while (perhaps even years), as your database size grows, you may be limited by their range. An Integer would work for almost two months if you generated a new identifier each millisecond with no gaps, and a\u00aeLong would last for about 300 million years. Although recommended for most applications, the enhanced-sequence strategy as shown in listing is just one of the strategies built into Hibernate. Identifier generator strategies Following is a list of all available Hibernate identifier generator strategies, their options, and our usage recommendations. If you don't want to read the whole lis\u00aet now, enable GenerationType.AUTO and check what Hibernate defaults to for your database dialect. It's most likely sequence or identity a good but maybe not the most efficient or portable choice. If you require consistent portable behavior, and identifier values to be avai\u00aelable before INSERTs, use enhanced-sequence, as shown in the previous section. This is a portable, flexible, and modern strategy, also offering various optimizers for large datasets.",
  "page166": "We also show the relationship between each standard JPA strategy and its native Hibernate equivalent. Hibernate has been growing organically, so there are now two sets of mappings between standa\u00aerd and native strategies; we call them Old and New in the list. You can switch this mapping with the hibernate.id.new_generator_mappings setting in your persistence.xml file. The default is true; hence the New mapping. Software doesn\\u00aeu2019t age quite as well as wine: native Automatically selects other strategies, such as sequence or identity, depending on the configured SQL dialect. You have to look at the Javadoc (or even the source) of the SQL dialect you configured in persistence.xml. Equivalent to JPA GenerationType.AUTO with the Old mapping. sequence Uses a\u00aenative database sequence named HIBERNATE_SEQUENCE. The sequence is called before each INSERT of a new row. You can customize the sequence name and provide additional DDL settings; see the Javadoc for the class org.hibernate.id.SequenceGenerator and its parent. enhanced-sequence Uses a native database sequence when supported; otherwise falls back to an extra database table with a single\u00aecolumn and row, emulating a sequence. Defaults to name HIBERNATE_SEQUENCE. Always calls the database \"sequence\" before an INSERT, providing the same behavior independently of whether the DBMS supports real sequences. Supports an org.hibernate .id.enhanced.Optimizer to\u00aeavoid hitting the database before each INSERT; defaults to no optimization and fetching a new value for each INSERT. You can find more examples in chapter 20. For all parameters, see the Javadoc for the class org.hibernate.id.enhanced.SequenceStyleGenerator. Equivalent to JPA GenerationType.SEQUENCE and GenerationType.AUTO with the New mapping enabled, most likely your best option of the built-in strategies.",
  "page167": "seqhilo Uses a native database sequence named HIBERNATE_SEQUENCE, optimizing calls before INSERT by combining hi/lo values. If the hi value retrieved from the sequence is 1, the next 9 ins\u00aeertions will be made with key values 11, 12, 13, \u2026, 19. Then the sequence is called again to obtain the next hi value (2 or higher), and the procedure repeats with 21, 22, 23, and so on. You can configure the maximum lo value (9\u00aeis the default) with the max_lo parameter. Unfortunately, due to a quirk in Hibernate's code, you can not configure this strategy in @GenericGenerator. The only way to use it is with JPA GenerationType.SEQUENCE and the Old mapping. You can configure it with the standard JPA @SequenceGenerator annotation on a (maybe otherwi\u00aese empty) class. See the Javadoc for the class org.hibernate.id.SequenceHiLoGenerator and its parent for more information. Consider using enhanced-sequence instead, with an optimizer hilo Uses an extra table named HIBERNATE_UNIQUE_KEY with the same algorithm as the seqhilo strategy. The table has a single column and row, holding the next value of the sequence. The default maximum lo val\u00aeue is 32767, so you most likely want to configure it with the max_lo parameter. See the Javadoc for the class org.hibernate.id.TableHiLoGenerator for more information. We don't recommend this legacy strategy; use enhanced-sequence instead with an optimizer.",
  "page168": "enhanced-table Uses an extra table named HIBERNATE_SEQUENCES, with one row by default representing the sequence, storing the next value. This value is selected and updated when an identifier val\u00aeue has to be generated. You can configure this generator to use multiple rows instead: one for each generator; see the Javadoc for org.hibernate.id.enhanced.TableGenerator. Equivalent to JPA GenerationType.TABLE with the New mapping enabled\u00ae. Replaces the outdated but similar org.hibernate.id.MultipleHiLoPerTableGenerator, which is the Old mapping for JPA GenerationType.TABLE. identity Supports IDENTITY and auto-increment columns in DB2, MySQL, MS SQL Server, and Sybase. The identifier value for the primary key column will be generated on INSERT of a row. Has no option\u00aes. Unfortunately, due to a quirk in Hibernate's code, you can not configure this strategy in @GenericGenerator. The only way to use it is with JPA GenerationType.IDENTITY and the Old or New mapping, making it the default for GenerationType.IDENTITY. increment At Hibernate startup, reads the maximum (numeric) primary key column value of each entity's table and increments the v\u00aealue by one each time a new row is inserted. Especially efficient if a non-clustered Hibernate application has exclusive access to the database; but don't use it in any other scenario.",
  "page169": "select Hibernate won't generate a key value or include the primary key column in an INSERT statement. Hibernate expects the DBMS to assign a (default in schema or by trigger) value to\u00aethe column on insertion. Hibernate then retrieves the primary key column with a SELECT query after insertion. Required parameter is key, naming the database identifier property (such as id) for the SELECT. This strategy isn't very eff\u00aeicient and should only be used with old JDBC drivers that can't return generated keys directly. uuid2 Produces a unique 128-bit UUID in the application layer. Useful when you need globally unique identifiers across databases (say, you merge data from several distinct production databases in batch runs every night into an archive). The\u00aeUUID can be encoded either as a java.lang.String, a byte[16], or a java .util.UUID property in your entity class. Replaces the legacy uuid and uuid .hex strategies. You configure it with an org.hibernate.id.UUIDGenerationStrategy; see the Javadoc for the class org.hibernate.id.UUIDGenerator for more details. guid Uses a globally unique identifier produced by the database, with an SQL f\u00aeunction available on Oracle, Ingres, MS SQL Server, and MySQL. Hibernate calls the database function before an INSERT. Maps to a java.lang.String identifier property. If you need full control over identifier generation, configure the strategy of @GenericGenerator with the\u00aefully qualified name of a class that implements the org.hibernate.id.IdentityGenerator interface.",
  "page170": "QUOTING SQL IDENTIFIERS- From time to time, especially in legacy databases, you'll encounter identifiers with strange characters or whitespace, or wish to force case sensitivity. Or, as in\u00aethe previous example, the automatic mapping of a class or property would require a table or column name that is a reserved keyword.  Hibernate 5 knows the reserved keywords of your DBMS through the configured database dialect. Hibern\u00aeate 5 can automatically put quotes around such strings when generating SQL. You can enable this automatic quoting with hibernate.auto_quote _keyword true in your persistence unit configuration. If you're using an older version of Hibernate, or you find that the dialect's information is incomplete, you must still apply quotes on n\u00aeames manually in your mappings if there is a conflict with a keyword.  If you quote a table or column name in your mapping with backticks, Hibernate always quotes this identifier in the generated SQL. This still works in latest versions of Hibernate, but JPA 2.0 standardized this functionality as delimited identifiers with double quotes. If you have to quote all SQL identifiers, creat\u00aee an orm.xml file and add the setting <delimited-identifiers/> to its <persistence-unit-defaults> section, as shown in listing 3.8. Hibernate then enforces quoted identifiers everywhere.  You should consider renaming tables or columns with reserved keyword names whenever possi\u00aeble. Ad hoc SQL queries are difficult to write in an SQL console if you have to quote and escape everything properly by hand.  Next, you'll see how Hibernate can help when you encounter organizations with strict conventions for database table and column names.To summarize, our recommendations on identifier generator strategies are as follows: In general, we prefer pre-insert generating names- Let's first talk about the naming of entity classes and tables. If you only specify @Entity on the persistence-capable class, the default mapped table name is the same as the class name. Note that we write SQL artifact names in UPPERCASE to make them easier to distinguish SQL is actually case insensitive. So the Java entity class Item maps to the ITEM table. You can override the table name with the JPA @Table annotation, as shown next.",
  "page171": "Entities are the coarser-grained classes of your system. Their instances have an independent life cycle and their own identity, and many other instances can reference them. Value types, o\u00aen the other hand, are dependent on a particular entity class. A value type instance is bound to its owning entity instance, and only one entity instance can reference it it has no individual identity. We looked at Java identity, object equ\u00aeality, and database identity, and at what makes good primary keys. You learned which generators for primary key values Hibernate provides out of the box, and how to use and extend this identifier system. We discussed some useful class mapping options, such as naming strategies and dynamic SQL generation. After spending the previous chapte\u00aer almost exclusively on entities and the respective class- and identity-mapping options, we now focus on value types in their various forms. We split value types into two categories: basic value-typed classes that come with the JDK, such as String, Date, primitives, and their wrappers; and developerdefined value-typed classes, such as Address and MonetaryAmount in CaveatEmptor. I\u00aen this chapter, we first map persistent properties with JDK types and learn the basic mapping annotations. You see how to work with various aspects of properties: overriding defaults, customizing access, and generated values. You also see how SQL is used with derived properties\u00aeand transformed column values. We wrap up basic properties with temporal properties and mapping enumerations. We then discuss custom value-typed classes and map them as embeddable components. You learn how classes relate to the database schema and make your classes embeddable, while allowing for overriding embedded attributes. We complete embeddable components by mapping nested components. Finally,",
  "page172": "This configuration by exception approach means you don't have to annotate a property to make it persistent; you only have to configure the mapping in an exceptional case. Several anno\u00aetations are available in JPA to customize and control basic property map. Overriding basic property defaults- You might not want all properties of an entity class to be persistent. For example, although it makes sense to have a persistent I\u00aetem#initialPrice property, an Item#totalPriceIncludingTax property shouldn't be persistent if you only compute and use its value at runtime, and hence shouldn't be stored in the database. To exclude a property, mark the field or the getter method of the property with the @javax.persistence.Transient annotation or use the Ja\u00aeva transient keyword. The transient keyword usually only excludes fields for Java serialization, but it's also recognized by JPA providers.  We'll come back to the placement of the annotation on fields or getter methods in a moment. Let's assume as we have before that Hibernate will access fields directly because @Id has been placed on a field. Therefore, all other JPA a\u00aend Hibernate mapping annotations are also on fields. We have to admit that this annotation isn't very useful. It only has two parameters: the one shown here, optional, marks the property as not optional at the Java object level. By default, all persistent properties\u00aeare nullable and optional; an Item may have an unknown initialPrice. Mapping the initialPrice property as non-optional makes sense if you have a NOT NULL constraint on the INITIALPRICE column in your SQL schema. If Hibernate is generating the SQL schema, it will include a NOT NULL constraint automatically for non-optional properties. Mapping basic properties- When you mapy isn't what you want, and you should always map Java classes instead of storing a heap of bytes in the database. Imagine maintaining a database with this binary information when the application is gone in a few years. Otherwise, Hibernate will throw an exception on startup, complaining that it doesn't understand the type of the property",
  "page173": "The Column annotation has a few other parameters, most of which control SQL-level details such as catalog and schema names. They're rarely needed, and we only show them throughout this book\u00aewhen necessary.  Property annotations aren't always on fields, and you may not want Hibernate to access fields directly. Customizing property access- The persistence engine accesses the properties of a class either directly through\u00aefields or indirectly through getter and setter methods. An annotated entity inherits the default from the position of the mandatory @Id annotation. For example, if you've declared @Id on a field, not a getter method, all other mapping annotations for that entity are expected on fields. Annotations are never on the setter methods.  Th\u00aee default access strategy isn't only applicable to a single entity class. Any @Embedded class inherits the default or explicitly declared access strategy of its owning root entity class. We cover embedded components later in this chapter. Furthermore, Hibernate accesses any @MappedSuperclass properties with the default or explicitly declared access strategy of the mapped enti\u00aety class. Inheritance is the topic of chapter 6.  The JPA specification offers the @Access annotation for overriding the default behavior, with the parameters AccessType.FIELD and AccessType.PROPERTY. If you set @Access on the class/entity level, Hibernate accesses all propert\u00aeies of the class according to the selected strategy. You then set any other mapping annotations, including the @Id, on either fields or getter methods, respectively. Now, when you store an Item and forget to set a value on the initialPrice field, Hibernate will complain with an exception before hitting the database with an SQL statement. Hibernate knows that a value is required to perform an INSERT or UPDATEautomatically.  The @Column annotation can also override the mapping of the property name to the database column:",
  "page174": "Hibernate reads your Java domain model classes and mapping metadata and generates schema DDL statements. You can export them into a text file or execute them directly on your database whenever y\u00aeou run integration tests. Because schema languages are mostly vendor-specific, every option you put in your mapping metadata has the potential to bind the metadata to a particular database product keep this in mind when using schema feature\u00aes. Hibernate creates the basic schema for your tables and constraints automatically; it even creates sequences, depending on the identifier generator you select. But there are some schema artifacts Hibernate can't and won't create automatically. These include all kinds of highly vendor-specific performance options and any other\u00aeartifacts that are relevant only for the physical storage of data (tablespaces, for example). Besides these physical concerns, your DBA will often supply custom additional schema statements to improve the generated schema. DBAs should get involved early and verify the automatically generated schema from Hibernate. Never go into production with an unchecked automatically generated schema. If y\u00aeour development process allows, changes made by the DBA can flow back into your Java systems, for you to add to mapping metadata. In many projects, the mapping metadata can contain all the necessary schema changes from a DBA. Then Hibernate can generate the final production sch\u00aeema during the regular build by including all comments, constraints, indexes, and so on. In the following sections, we show you how to customize the generated schema and how to add auxiliary database schema artifacts (we call them objects sometimes; we don't mean Java objects here). We discuss custom data types, additional integrity rules, indexes, and how you can replace some of the (sometimes ugly)",
  "page175": "This property defines when the create and drop scripts should be executed. Your custom SQL scripts will contain CREATE DOMAIN statements, which must be executed before the tables using these dom\u00aeains are created. With these settings, the schema generator runs the create script first before reading your ORM metadata (annotations, XML files) and creating the tables. The drop script executes after Hibernate drops the tables, giving yo\u00aeu a chance to clean up anything you created. Other options are metadata (ignore custom script sources) and script (only use a custom script source; ignore ORM metadata in annotations and XML files). D This is the location of the custom SQL script for creation of the schema. The path is (a) the location of the script resource on the classpa\u00aeth; (b) the location of the script as a file:// URL; or, if neither (a) nor (b) matches, (c) the absolute or relative file path on the local file system. This example uses (a). E This is the custom SQL script for dropping the schema. F This load script runs after the tables have been created. We've mentioned that DDL is usually highly vendor-specific. If your application has to support s\u00aeeveral database dialects, you may need several sets of create/drop/load scripts to customize the schema for each database dialect. You can solve this with several persistence unit configurations in the persistence.xml file. Alternatively, Hibernate has its own proprietary conf\u00aeiguration for schema customization in an hbm.xml mapping file, You can hook the following three types of custom SQL scripts into Hibernates schema-generation process: The create script executes when the schema is generated. A custom create script can run before, after, or instead of Hibernates automatically generated scripts. In other words, you can write an SQL script that runs before or",
  "page176": "Table constraints An integrity rule that applies to several columns or several rows is a table constraint. A typical declarative table constraint is UNIQUE: all rows are checked for duplicate va\u00aelues (for example, each user must have a distinct email address). A rule affecting only a single row but multiple columns is \"the auction end time has to be after the auction start time.\" Database constraints If a rule applies to\u00aemore than one table, it has database scope. You should already be familiar with the most common database constraint, the foreign key. This rule guarantees the integrity of references between rows, usually in separate tables, but not always (self-referencing foreign key constraints aren't uncommon). Other database constraints involving\u00aeseveral tables aren't uncommon: for example, a bid can only be stored if the auction end time of the referenced item hasn't been reached. Most (if not all) SQL database management systems support these kinds of constraints and the most important options of each. In addition to simple keywords such as NOT NULL and UNIQUE, you can usually also declare more complex rules with the CHEC\u00aeK constraint that applies an arbitrary SQL expression. Still, integrity constraints are one of the weak areas in the SQL standard, and solutions from vendors can differ significantly. Furthermore, non-declarative and procedural constraints are possible with database triggers t\u00aehat intercept data-modification operations. A trigger can then implement the constraint procedure directly or call an existing stored procedure. Integrity constraints can be checked immediately when a data-modification statement is executed, or the check can be deferred until the end of a transaction. The violation response in SQL databases is usually rejection without any possibilitCHAR data type can hold character strings:",
  "page177": "This constraint restricts valid username values to a maximum length of 15 characters, and the string can't begin with admin to avoid confusion. You can call any SQL functions supported by y\u00aeour DBMS; the column Definition is always passed through into the exported schema. Note that you have a choice: creating and using a domain or adding a single column constraint has the same effect. Domains are usually easier to maintain an\u00aed avoid duplicating. At the time of writing, Hibernate doesn't support its proprietary annotation @org.hibernate.annotations.Check on individual properties; you use it for table level constraints. Hibernate appends table constraints to the generated CREATE TABLE statement, which can contain arbitrary SQL expressions. You can impleme\u00aent multirow table constraints with expressions that are more complex. You may need a sub select in the expression to do this, which may not be supported by your DBMS. But there are some common multirow table constraints, like UNIQUE, that you can add directly in the mappings. You've already seen the @Column(unique true | false) option in the previous section. Now all pairs of USERNAME an\u00aed EMAIL must be unique, for all rows in the USERS table. If you don't provide a name for the constraint here, UNQ_USERNAME_EMAIL an automatically generated and probably ugly name is used. The last kinds of constraints we discuss are database-wide rules that span several t\u00aeables. Hibernate passes on database constraint violations in error exceptions; check whether the exception in your transaction has a cause, somewhere in the exception chain, of type org.hibernate.exception.ConstraintViolationException. This exception can provide more information about the error, such as the name of the failed database constraint. The SQL standard includes domains, which unfortunately are rattly, so be careful with database-specific SQL",
  "page178": "This statement declares the foreign key constraint for the ITEM_ID column in the BID table, referencing the primary key column of the ITEM table. You can customize the name of the constraint wit\u00aeh the foreign Key option in an @JoinColumn mapping A foreign Key attribute is also supported in @PrimaryKeyJoinColumn, @MapKeyJoinColumn, @JoinTable, @CollectionTable, and @AssociationOverride mappings. The @ForeignKey annotation has some\u00aerarely needed options we haven't shown: You can write your own foreign Key Definition, an SQL fragment such as FOREIGN KEY ([column]) REFERENCES [table]([column]) ON UPDATE [action]. Hibernate will use this SQL fragment instead of the provider-generated fragment, it can be in the SQL dialect supported by your DBMS. The Constraint Mod\u00aee setting is useful if you want to disable foreign key generation completely, with the value NO_CONSTRAINT. You can then write the foreign key constraint yourself with an ALTER TABLE statement, probably in a load script as we've shown. Naming constraints properly is not only good practice, but also helps significantly when you have to read exception messages. This completes our discussi\u00aeon of database integrity rules. Next, we look at some optimization you might want to include in your schema for performance reasons. Indexes are a key feature when optimizing the performance of a database application. The query optimizer in a DBMS can use indexes to avoid exce\u00aessive scans of the data tables. Because they relevant only in the physical implementation of a database, indexes aren't part of the SQL standard, and the DDL and available indexing options are product specific. You can, however, embed the most common schema artifacts for typical indexes in mapping metadata A user can only make bids until an auction ends. Your database should guarantee thatntial integrity rules. They're widely known as foreign keys, which are a combination of two things: a key value copy from a related row and a constraint that guarantees that the referenced value exists. Hibernate creates foreign key constraints automatically for all foreign key columns in association mappings. If you check the schema produced by Hibernate, you'll notice that these constraints also have automatically generated database identifiers names that aren't easy to read and that make debugging more difficult. You see this kind of statement in the generated schema",
  "page179": "If you encounter a USERS table in a legacy schema, it's likely that USERNAME is the primary key. In this case, you have no surrogate identifier that Hibernate generates automatically. Inste\u00aead, your application has to assign the identifier value when saving an instance of the User class Hibernate calls the protected no-argument constructor when it loads a User from the database and then assigns the username field value directl\u00aey. When you instantiate a User, call the public constructor with a username. If you don't declare an identifier generator on the @Id property, Hibernate expects the application to take care of the primary key value assignment. Composite (natural) primary keys require a bit more work Suppose the primary key of the USERS table is a com\u00aeposite of the two columns USERNAME and DEPARTMENTNR. You write a separate composite identifier class that declares just the key properties and call this class User Id This class has to be @Embeddable and Serializable any type used as an identifier type in JPA must be Serializable. C You don't have to mark the properties of the composite key as @NotNull; their database columns are automat\u00aeically NOT NULL when embedded as the primary key of an entity. D The JPA specification requires a public no-argument constructor for an embeddable identifier class. Hibernate accepts protected visibility. E The only public constructor should have the key values as arguments. F\u00aeYou have to override the equals() and hashCode() methods with the same semantics the composite key has in your database. In this case, this is a straightforward comparison of the username and department values. Many queries in Caveat Emptor will probably involve the username of a User entity. You can speed up these queries by creating an index for the column of this property. Another candidate for an index impact performance. Unfortunately, many legacy schemas use (natural) composite keys heavily; and for the reason we discourage the use of composite keys, it may be difficult to change the legacy schema to use non-composite natural or surrogate keys. Therefore, JPA supports natural and composite primary and foreign keys.",
  "page180": "We've already shown the @SecondaryTable annotation in an inheritance mapping in section 6.5. It helped to break out properties of a particular subclass into a separate table. This generic f\u00aeunctionality has more uses but be aware that a properly designed system should have, simplified, more classes than tables. Suppose that in a legacy schema, you aren't keeping a user's billing address information with the other us\u00aeer details in the USERS main entity table, but in a separate table. Figure 9.5 shows this schema. The user's home address is stored in the columns STREET, ZIPCODE, and CITY of the USERS table. The user's billing address is stored in the BILLING_ADDRESS table, which has the primary key column USER_ID, which is also a foreign key c\u00aeonstraint referencing the ID primary key of the USERS table The User class has two properties of embedded type: home Address and billing Address. The first is a regular embedded mapping, and the Address class is @Embeddable. Just as in section 5.2.3, you can use the @AttributeOverrides annotation to override the mapping of embedded properties. Then, @Column maps the individual properties to\u00aethe BILLING_ADDRESS table, with its table option. Remember that an @AttributeOverride replaces all mapping information for a property: any annotations on the Address fields are ignored if you override. Therefore, you have to specify nullability and length again in the @Column\u00aeoverride. We've shown you a secondary table mapping example with an embeddable property. Of course, you could also break out simple basic properties like the username string in a secondary table. Keep in mind that reading and maintaining these mappings can be a problem, though; you should only map legacy unchangeable schemas with secondary tables. A foreign key constraint on the SELLER column in the ITEM tabName attribute of @JoinColumn to declare this relationship.",
  "page181": "strategies for runtime data management. These strategies are crucial to the performance and correct behavior of your applications. In this chapter, we discuss the life cycle of entity instances\u00aehow an instance becomes persistent, and how it stops being considered persistent and the method calls and management operations that trigger these transitions. The JPA Entity Manager is your primary interface for accessing data. Before we\u00aelook at the API, let's start with entity instances, their life cycle, and the events that trigger a change of state. Although some of the material may be formal, a solid understanding of the persistence life cycle is essential. Because JPA is a transparent persistence mechanism classes are unaware of their own persistence capability\u00aeit's possible to write application logic that's unaware whether the data it operates on represents persistent state or temporary state that exists only in memory. The application shouldn't necessarily need to care that an instance is persistent when invoking its methods. You can, for example, invoke the Item #calculateTotalPrice() business method without having to consider pers\u00aeistence at all (for example, in a unit test). Any application with persistent state must interact with the persistence service whenever it needs to propagate state held in memory to the database (or vice versa). In other words, you have to call the Java Persistence interfaces\u00aeto store and load data. When interacting with the persistence mechanism that way, the application must concern itself with the state and life cycle of an entity instance with respect to persistence. We refer to this as the persistence life cycle: the states an entity instance goes through during its life. We also use the term unit of work: a set of (possibly) state changing operations consideredning to and intercepting events, auditing and versioning with Hibernate Envers, and filtering data dynamically.",
  "page182": "A persistent entity instance has a representation in the database. It's stored in the database or it will be stored when the unit of work completes. It's an instance with a database id\u00aeentity, as defined in section 4.2; its database identifier is set to the primary key value of the database representation. The application may have created instances and then made them persistent by calling Entity Manager #persist(). There\u00aemay be instances that became persistent when the application created a reference to the object from another persistent instance that the JPA provider already manages. A persistent entity instance may be an instance retrieved from the database by execution of a query, by an identifier lookup, or by navigating the object graph starting fro\u00aem another persistent instance. Persistent instances are always associated with a persistence context. You see more about this in a moment You can delete a persistent entity instance from the database in several ways: For example, you can remove it with Entity Manager #remove(). It may also become available for deletion if you remove a reference to it from a mapped collection with orphan remo\u00aeval enabled. An entity instance is then in the removed state: the provider will delete it at the end of a unit of work. You should discard any references you may hold to it in the application after you finish working with it for example, after you've rendered the removal\u00aeconfirmation screen your users see Think of the persistence context as a service that remembers all the modifications and state changes you made to data in a particular unit of work (this is somewhat simplified, but it's a good starting point). We now dissect all these terms: entity states, persistence contexts, and managed scope. You're probably more accustomed to thinking about what SQL statemenst like new Long() and new Big Decimal(). Hibernate doesn't provide any rollback functionality for transient instances; if you modify the price of a transient Item, you can't automatically undo the change. For an entity instance to transition from transient to persistent state, to become managed, requires either a call to the Entity Manager #persist() method or the creation of a reference from an already-persistent instance and enabled cascading of ",
  "page183": "The persistence context acts as a first-level cache; it remembers all entity instances you've handled in a particular unit of work. For example, if you ask Hibernate to load an entity insta\u00aence using a primary key value (a lookup by identifier), Hibernate can first check the current unit of work in the persistence context. If Hibernate finds the entity instance in the persistence context, no database hit occurs this is a repea\u00aetable read for an application. Consecutive em.find(Item. Class, ITEM_ID) calls with the same persistence context will yield the same result. This cache also affects results of arbitrary queries, executed for example with the javax.persistence.Query API. Hibernate reads the SQL result set of a query and transforms it into entity instances.\u00aeThis process first tries to resolve every entity instance in the persistence context by identifier lookup. Only if an instance with the same identifier value can't be found in the current persistence context does Hibernate read the rest of the data from the result-set row. Hibernate ignores any potentially newer data in the result set, due to read-committed transaction isolation at the\u00aedatabase level, if the entity instance is already present in the persistence context. The persistence context cache is always on it can't be turned off. It ensures the following The persistence layer isn't vulnerable to stack overflows in the case of circular referen\u00aeces in an object graph. There can never be conflicting representations of the same database row at the end of a unit of work. The provider can safely write all changes made to an entity instance to the database. Likewise, changes made in a particular persistence context are always immediately visible to all other code executed inside that unit of work and its persistence context. JPA guarantees repeatable ease the state of instances monitored by a persistence context, either automatically or on demand. Typically, when a unit of work completes, the provider propagates state held in memory to the database through the execution of SQL INSERT, UPDATE, and DELETE statements (all part of the Data Modification Language [DML]) This flushing procedure may also occur at other times. For example, Hibernate may synchronize with the database before execution of a query. This ensures that queries are aware of changes made earlier during the unit of work.",
  "page184": "Hibernate joins the in-progress system transaction when an Entity Manager is created and waits for the transaction to commit. When Hibernate is notified (by the JTA engine) of the commit, it per\u00aeforms dirty checking of the persistence context and synchronizes with the database. You can also force dirty checking synchronization manually by calling EntityManager#flush() at any time during a transaction. You decide the scope of the p\u00aeersistence context by choosing when to close() the Entity Manager. You have to close the persistence context at some point, so always place the close() call in a finally block. How long should the persistence context be open? Let's assume for the following examples that you're writing a server, and each client request will be pr\u00aeocessed with one persistence context and system transaction in a multithreaded environment. If you're familiar with servlets, imagine the code in listing 10.1 embedded in a servlet's service() method. Within this unit of work, you access the Entity Manager to load and store data. A new transient Item is instantiated as usual. Of course, you may also instantiate it before creating th\u00aee EntityManager. A call to persist() makes the transient instance of Item persistent. It's now managed by and associated with the current persistence context. To store the Item instance in the database, Hibernate has to execute an SQL INSERT statement. When the transactio\u00aen of this unit of work commits, Hibernate flushes the persistence context, and the INSERT occurs at that time. Hibernate may even batch the INSERT at the JDBC level with other statements. When you call persist(), only the identifier value of the Item is assigned. Alternatively, if your identifier generator isn't pre-insert, the INSERT statement will be executed immediately when persist() is write empty catch clauses in your code, though you'll have to roll back the transaction and handle exceptions. Creating an Entity Manager starts its persistence context.",
  "page185": "You can modify the Item instance, and the persistence context will detect these changes and record them in the database automatically. When Hibernate flushes the persistence context during commi\u00aet, it executes the necessary SQL DML statements to synchronize the changes with the database. Hibernate propagates state changes to the database as late as possible, toward the end of the transaction. DML statements usually create locks in\u00aethe database that are held until the transaction completes, so Hibernate keeps the lock duration in the database as short as possible. Hibernate writes the new Item#name to the database with an SQL UPDATE. By default, Hibernate includes all columns of the mapped ITEM table in the SQL UPDATE statement, updating unchanged columns to their o\u00aeld values. Hence, Hibernate can generate these basic SQL statements at startup, not at runtime. If you want to include only modified (or non-nullable for INSERT) columns in SQL statements, you can enable dynamic SQL generation as discussed in section 4.3.2. Hibernate detects the changed name by comparing the Item with a snapshot copy it took before, when the Item was loaded from the database\u00ae. If your Item is different from the snapshot, an UPDATE is necessary. This snapshot in the persistence context consumes memory. Dirty checking with snapshots can also be time consuming, because Hibernate has to compare all instances in the persistence context with their snapsh\u00aeot during flushing. It's better (but not required) to fully initialize the Item instance before managing it with a persistence context. The SQL INSERT statement contains the values that were held by the instance at the point when persist() was called. If you don't set the name of the Item before making it persistent, a NOT NULL constraint may be violated. You can modify the Item after c9s a generic method, and its return type is set as a side effect of the first parameter. The retrieved entity instance is in persistent state, and you can now modify it inside the unit of work. If no persistent instance with the given identifier value can be found, find() returns null. The find() operation always hits the database if there was no hit for the",
  "page186": "As soon as you call any method such as Item#getName() on the proxy, a SELECT is executed to fully initialize the placeholder. The exception to this rule is a mapped database identifier getter me\u00aethod, such as getId(). A proxy may look like the real thing, but it's only a placeholder carrying the identifier value of the entity instance it represents. If the database record no longer exists when the proxy is initialized, an Enti\u00aetyNotFoundException is thrown. Note that the exception can be thrown when Item#getName() is called. E Hibernate has a convenient static initialize() method that loads the proxy's data. F After the persistence context is closed, item is in detached state. If you don't initialize the proxy while the persistence context is still ope\u00aen, you get a LazyInitializationException if you access the proxy. You can't load data on demand once the persistence context is closed. The solution is simple: load the data before you close the persistence context. We'll have much more to say about proxies, lazy loading, and on-demand fetching in chapter 12. If you want to remove the state of an entity instance from the database,\u00aeyou have to make it transient. To make an entity instance transient and delete its database representation, call the remove() method on the EntityManager If you call find(), Hibernate executes a SELECT to load the Item. If you call getReference(), Hibernate attempts to avoid t\u00aehe SELECT and returns a proxy. C Calling remove() queues the entity instance for deletion when the unit of work completes; it's now in removed state. If remove() is called on a proxy, Hibernate executes a SELECT to load the data. You may want to customize how Hibernate detects dirty state, using an extension point. Set the property hibernate.entity_dirtiness_strategy in your persistence.xml configuratioference() without hitting the database. Furthermore, if no persistent instance with that identifier is currently managed, Hibernate produces a hollow placeholder: a proxy. This means getReference() won't access the database, and it doesn't return null, unlike find(). C JPA offers PersistenceUnitUtil helper methods such as isLoaded() to detect whether you're working with an uninitialized proxy.",
  "page187": "Java Persistence also offers bulk operations that translate into direct SQL DELETE statements without life cycle interceptors in the application. We'll discuss these operations in section 2\u00ae0.1. Let's say you load an entity instance from the database and work with the data. For some reason, you know that another application or maybe another thread of your application has updated the underlying row in the database. Next,\u00aewe'll see how to refresh the data held in memory. After you load the entity instance, you realize (how isn't important) that someone else changed the data in the database. Calling refresh() causes Hibernate to execute a SELECT to read and marshal a whole result set, overwriting changes you already made to the persistent instance\u00aein application memory. If the database row no longer exists (someone deleted it), Hibernate throws an EntityNotFoundException on refresh(). Most applications don't have to manually refresh in-memory state; concurrent modifications are typically resolved at transaction commit time. The best use case for refreshing is with an extended persistence context, which might span several request\u00ae/response cycles and/or system transactions. While you wait for user input with an open persistence context, data gets stale, and selective refreshing may be required depending on the duration of the conversation and the dialogue between the user and the system. Refreshing can\u00aebe useful to undo changes made in memory during a conversation, if the user cancels the dialogue. We'll have more to say about refreshing in a conversation in section. An entity instance must be fully initialized during life cycle transitions. You may have life cycle callback methods or an entity listener enabled (see section 13.2), and the instance must pass through these interceptorslue after removal of an entity instance. In the previous code example, the identifier value is reset to the default value of null (it's a Long). The Item is now the same as in transient state, and you can save it again in a new persistence context.",
  "page188": "The persistence context is a cache of persistent instances. Every entity instance in persistent state is associated with the persistence context. Many Hibernate users who ignore this simple fac\u00aet run into an OutOfMemoryException. This is typically the case when you load thousands of entity instances in a unit of work but never intend to modify them. Hibernate still has to create a snapshot of each instance in the persistence conte\u00aext cache, which can lead to memory exhaustion. (Obviously, you should execute a bulk data operation if you modify thousands of rows we'll get back to this kind of unit of work in section 20.1.) The persistence context cache never shrinks automatically. Keep the size of your persistence context to the necessary minimum. Often, many pe\u00aersistent instances in your context are there by accident for example, because you needed only a few items but queried for many. Extremely large graphs can have a serious performance impact and require significant memory for state snapshots. Check that your queries return only data you need, and consider the following ways to control Hibernate's caching behavior. You can call EntityManag\u00aeer#detach(i) to evict a persistent instance manually from the persistence context. You can call EntityManager#clear() to detach all persistent entity instances, leaving you with an empty persistence context. The native Session API has some extra operations you might find usefu\u00ael. You can set the entire persistence context to read-only mode. This disables state snapshots and dirty checking, and Hibernate won't write modifications to the database. Replication is useful, for example, when you need to retrieve data from one database and store it in another. Replication takes detached instances loaded in one persistence context and makes them persistent in another persistence context. Ynt databases. An example case is a product upgrade: if the new version of your application requires a new database (schema), you may want to migrate and replicate the existing data once. The persistence context does many things for you: automatic dirty checking, guaranteed scope of object identity, and so on. It's equally important that you know some of the details of its management, and that you sometimes influence what goes on behind the scenes. ",
  "page189": "Next, we look at the detached entity state. We already mentioned some issues you'll see when entity instances aren't associated with a persistence context anymore, such as disabled laz\u00aey initialization. Let's explore the detached state with some examples, so you know what to expect when you work with data outside of a persistence context. If a reference leaves the scope of guaranteed identity, we call it a reference\u00aeto a detached entity instance. When the persistence context is closed, it no longer provides an identity-mapping service. You'll run into aliasing problems when you work with detached entity instances, so make sure you understand how to handle the identity of detached instances. If you look up data using the same database identifier\u00aevalue in the same persistence context, the result is two references to the same in-memory instance on the JVM heap. Consider the two units of work shown next. In the first unit of work at begin() B, you start by creating a persistence context C and loading some entity instances. Because references a and b are obtained from the same persistence context, they have the same Java identity D. They\u00ae're equal from the same persistence context, they have the same Java identity D. They're equal E because by default equals() relies on Java identity comparison. They obviously have the same database identity F. They reference the same Item instance, in persistent state, managed\u00aeby the persistence context for that unit of work. The first part of this example finishes by committing the transaction. By default, Hibernate flushes the persistence context of an EntityManager and synchronizes changes with the database whenever the joined transaction is committed. All the previous code examples, except some in the last section, have used that strategy. JPA allows itabase by calling EntityManager#flush(). This concludes our discussion of the transient, persistent, and removed entity states, and the basic usage of the EntityManager API. Mastering these state transitions and API methods is essential; every JPA application is built with these operations.",
  "page190": "When you begin a journey, it's a good idea to have a mental map of the terrain you'll be passing through. The same is true for an intellectual journey, such as learning to write comput\u00aeer programs. In this case, you'll need to know the basics of what computers are and how they work. You'll want to have some idea of what a computer program is and how one is created. Since you will be writing programs in the Java\u00aeprogramming language, you'll want to know something about that language in particular and about the modern, networked computing environment for which Java is designed. As you read this chapter, don't worry if you can't understand everything in detail. (In fact, it would be impossible for you to learn all the details from the\u00aebrief expositions in this chapter.) Concentrate on learning enough about the big ideas to orient yourself, in preparation for the rest of the book. Most of what is covered in this chapter will be covered in much greater detail later in the book. A computer is a complex system consisting of many different components. But at the heart or the brain, if you want of the computer is a single compo\u00aenent that does the actual computing. This is the Central Processing Unit, or CPU. In a modern desktop computer, the CPU is a single \"chip\" on the order of one square inch in size. The job of the CPU is to execute programs. A program is simply a list of unambiguous ins\u00aetructions meant to be followed mechanically by a computer. A computer is built to carry out instructions that are written in a very simple type of language called machine language. Each type of computer has its own machine language, and the computer can directly execute a program only if the program is expressed in that language. (It can execute programs written in other languages if they are first trachine language instructions in main memory. It does this by repeatedly reading, or fetching, an instruction from memory and then carrying out, or executing, that instruction. This process fetch an instruction, execute it, fetch another instruction, execute it, and so on forever is called the fetch-and-execute cycle.",
  "page191": "Machine language instructions are expressed as binary numbers. A binary number is made up of just two possible digits, zero and one. So, a machine language instruction is just a sequence of zero\u00aes and ones. Each particular sequence encodes some particular instruction. The data that the computer manipulates is also encoded as binary numbers. A computer can work directly with binary numbers because switches can readily represent such\u00aenumbers: Turn the switch on to represent a one; turn it off to represent a zero. Machine language instructions are stored in memory as patterns of switches turned on or off, When a machine language instruction is loaded into CPU, all that happens is that certain switches are turned on or off in the pattern that encodes that particular ins\u00aetruction. The CPU is built to respond to this pattern by executing the instruction it encodes; it does this simply because of the way all the other switches in the CPU are wired together. So, you should understand this much about how computers work: Main memory holds machine language programs and data. These are encoded as binary numbers. The CPU fetches machine language instructions from mem\u00aeory one after another and executes them. It does this mechanically, without thinking about or understanding what it does-and therefore the program it executes must be perfect, complete in all details, and unambiguous because the CPU can do nothing but execute it exactly as writ\u00aeten. Here is a schematic view of this first-stage understanding of the computer Asynchronous Events: Polling Loops and Interrupts The CPU spends almost all of its time fetching instructions from memory and executing them. However, the CPU and main memory are only two out of many components in areal computer system, A complete system contains other devices such as: A hard disk for storing programs and data files.(Nere is a device driver, which consists of software that the CPU executes when it has to deal with the device. Installing a new device on a system generally has two steps: plugging the device physically into the computer, and installing the device driver software. Without the device driver, the actual physical device would be us",
  "page192": "Again, you should understand that this is a purely mechanical process: A device signals an interrupt simply by turning on a wire. The CPU is built so that when that wire is turned on, the CPU sa\u00aeves enough information about what it is currently doing so that it can return to the same state later. This information consists of the contents of important internal registers such as the program counter. Then the CPU jumps to some predete\u00aermined memory location and begins executing the instructions stored there. Those instructions make up an interrupt handler that does the processing necessary to respond to the interrupt. (This interrupt handler is part of the device driver software for the device that signalled the interrupt.) At the end of the interrupt handler is an inst\u00aeruction that tells the CPU to jump back to what it was doing; it does that by restoring its previously saved state. Interrupts allow the CPU to deal with asynchronous events. In the regular fetch-and execute cycle, things happen in a predetermined order; everything that happens is \"synchronized\" with everything else. Interrupts make it possible for the CPU to deal efficiently with e\u00aevents that happen \"asynchronously,\" that is, at unpredictable times. As another example of how interrupts are used, consider what happens when the CPU need to access data that is stored on the hard disk. The CPU can access data directly only if it is in main memory. D\u00aeata on the disk has to be copied into memory before it can be accessed. Unfortunately, on the scale of speed at which the CPU operates, the disk drive is extremely slow. When the CPU needs data from the disk, it sends a signal to the disk drive telling it to locate the data and get it ready. (This signal is sent synchronously, under the control of a regular program.) Then, instead of just waiting the long analled a thread. (Or a process; there are technical differences between threads and processes, but they are not important here.) At any given time, only one thread can actually be executed by a CPU. The CPU will continue running the same thread until one of several things happens The thread might voluntarily yield control,",
  "page193": "The Java Virtual Machine Machine language consists of very simple instructions that can be executed directly by the CPU of a computer. Almost all programs, though, are written in high-level prog\u00aeramming languages such as Java, Pascal, or C++. A program written in a high-level language cannot be run directly on any computer. First, it has to be translated into machine language. This translation can be done by a program called a comp\u00aeiler. A compiler takes a high-level-language program and translates it into an executable machine-language program. Once the translation is done, the machine-language program can be run any number of times, but of course it can only be run on one type of computer (since each type of computer has its own individual machine language). If the\u00aeprogram is to run on another type of computer it has to be re-translated, using a different compiler, into the appropriate machine language. There is an alternative to compiling a high-level language program. Instead of using a compiler, which translates the program all at once, you can use an interpreter, which translates it instruction-by-instruction, as necessary. An interpreter is a prog\u00aeram that acts much like a CPU, with a kind of fetch-and-execute cycle. In order to execute a program, the interpreter runs in a loop in which it repeatedly reads one instruction from the program, decides what is necessary to carry out that instruction, and then performs the app\u00aeropriate machine-language commands to do so. one use of interpreters is to execute high-level language programs. For example, the programming language Lisp is usually executed by an interpreter rather than a compiler. However, interpreters have another purpose: they can let you use a machine-language program meant for one type of computer on a completely different type of computer. For example, there is a program puter. However, one of the main selling points of Java is that it can actually be used on any computer. All that the computer needs is an interpreter for Java bytecode. Such an interpreter simulates the Java virtual machine in the same way that Virtual PC simulates a PC computer. Of course, a different Java bytecode interpreterct-oriented language. I should also note that the really hard part of platform-independence is providing a \"Graphical User Interface\" with windows, buttons, etc. that will work on all the platforms that support Java.",
  "page194": "Programs must be designed. No one can just sit down at the computer and compose a program of any complexity. The discipline called software engineering is concerned with the construction of corr\u00aeect, working, well-written programs. The software engineer tends to use accepted and proven methods for analyzing the problem to be solved and for designing a program to solve that problem. During the 1970s and into the 80s, the primary sof\u00aetware engineering methodology was structured programming. The structured programming approach to program design was based on the following advice: To solve a large problem, break the problem into several pieces and work on each piece separately; to solve each piece, treat it as a new problem which can itself be broken down into smaller pro\u00aeblems; eventually, you will work your way down to problems that can be solved directly, without further decomposition. This approach is called top-down programming. There is nothing wrong with top-down programming. It is a valuable and often-used approach to problem-solving. However, it is incomplete. For one thing, it deals almost entirely with producing the instructions necessary to solve a\u00aeproblem. But as time went on, people realized that the design of the data structures for a program was as least as important as the design of subroutines and control structures. Top-down programming doesn't give adequate consideration to the data that the program manipula\u00aetes. Another problem with strict top-down programming is that it makes it difficult to reuse work done for other projects. By starting with a particular problem and subdividing it into convenient pieces, top-down programming tends to produce a design that is unique to that problem. It is unlikely that you will be able to take a large chunk of programming from another program and fit it into your project, at least not witware modules is to contain some data, along with some subroutines for manipulating that data. For example, a mailing-list module might contain a list of names and addresses along with a subroutine for adding a new name, a subroutine for printing mailing labels, and so forth.",
  "page195": "For example, consider a drawing program that lets the user draw lines, rectangles, ovals, polygons, and curves on the screen. In the program, each visible object on the screen could be represent\u00aeed by a software object in the program. There would be five classes of objects in the program, one for each type of visible object that can be drawn. All the lines would belong to one class, all the rectangles to another class, and so on. T\u00aehese classes are obviously related; all of them represent \"drawable objects.\" They would, for example, all presumably be able to respond to a \"draw yourself\" message. Another level of grouping, based on the data to represent each type of object, is less obvious, but would be very useful in a program: We can group polyg\u00aeons and curves together as \"multipoint objects,\" while lines, rectangles, and ovals are \"two-point objects.\" (A line is determined by its endpoints, a rectangle by two of its corners, and an oval by two corners of the rectangle that contains it.) We could diagram these relationships as follows DrawableObject, MultipointObject, and TwoPointObject would be classes in the pro\u00aegram. MultipointObject and TwoPointObject would be subclasses of DrawableObject. The class Line would be a subclass of TwoPointObject and (indirectly) of DrawableObject. A subclass of a class is said to inherit the properties of that class. The subclass can add to its inheritan\u00aece and it can even \"override\" part of that inheritance (by defining a different response to some method). Nevertheless, lines, rectangles, and so on are drawable objects, and the class DrawableObject expresses this relationship. Inheritance is a powerful means for organizing a program. It is also related to the problem of reusing software components. A class is the ultimate reusable component. Not only c the computer types back its response. Early personal computers also used typed commands and responses, except that there was only one person involved at a time. This type of interaction between a user and a computer is called a command-line interface. Today, of course, most people interact with computers in a completely",
  "page196": "The use of the term \"message\" here is deliberate. Messages, as you saw in the previous section, are sent to objects. In fact, Java GUI components are implemented as objects. Java inclu\u00aedes many predefined classes that represent various types of GUI components. Some of these classes are subclasses of others. Here is a diagram showing some of Swing's GUI classes and their relationships. Don't worry about the detai\u00aels for now, but try to get some feel about how object-oriented programming and inheritance are used here. Note that all the GUI classes are subclasses, directly or indirectly, of a class called JComponent, which represents general properties that are shared by all Swing components. Two of the direct subclasses of JComponent themselves have\u00aesubclasses. The classes JTextArea and JTextField, which have certain behaviors in common, are grouped together as subclasses of JTextComponent. Similarly JButton and JToggleButton are subclasses of JAbstractButton, which represents properties common to both buttons and checkboxes. (JComboBox, by the way, is the Swing class that represents pop-up menus.) Just from this brief discussion, perha\u00aeps you can see how GUI programming can make effective use of object-oriented design. In fact, GUI's, with their \"visible objects,\" are probably a major factor contributing to the popularity of OOP. The Internet and the World-Wide Web - Computers can be connected\u00aetogether on networks. A computer on a network can communicate with other computers on the same network by exchanging data and files or by sending and receiving messages. Computers on a network can even work together on a large computation. Today, millions of computers throughout the world are connected to a single huge network called the Internet. New computers are being connected to the Internet every day. some data being sent from one computer to another, along with addressing information that indicates where on the Internet that data is supposed to go. Think of a packet as an envelope with an address on the outside and a message on the inside. (The message is the data.) The packet also includes a \"return address,",
  "page197": "On a basic level (the level of machine language), a computer can perform only very simple operations. A computer performs complex tasks by stringing together large numbers of such operations. Su\u00aech tasks must be \"scripted\" in complete and perfect detail by programs. Creating complex programs will never be really easy, but the difficulty can be handled to some extent by giving the program a clear overall structure. The des\u00aeign of the overall structure of a program is what I call \"programming in the large.\" Programming in the small, which is sometimes called coding, would then refer to filling in the details of that design. The details are the explicit, step-by-step instructions for performing fairly small-scale tasks. When you do coding, you are wo\u00aerking fairly \"close to the machine,\" with some of the same concepts that you might use in machine language: memory locations, arithmetic operations, loops and branches. In a high-level language such as Java, you get to work with these concepts on a level several steps above machine language. However, you still have to worry about getting all the details exactly right. This chapter a\u00aend the next examine the facilities for programming in the small in the Java programming language. Don't be misled by the term \"programming in the small\" into thinking that this material is easy or unimportant. This material is an essential foundation for all type\u00aes of programming. If you don't understand it, you can't write programs, no matter how good you get at designing their large-scale structure. The Basic Java Application - A program is a sequence of instructions that a computer can execute to perform some task. A simple enough idea, but for the computer to make any use of the instructions, they must be written in a form that the computer can use. Thiect program is one that does what you want it to. Furthermore, a program can be syntactically and semantically correct but still be a pretty bad program. Using the language correctly is not the same as using it well. For example, a good program has \"style.\" It is written in a way that will make it easy for peopl detailswill use some command to try to compile the file.",
  "page198": "Here is a Java program to display the message \"Hello World!\". Don't expect to understand what's going on here just yet some of it you won't really understand until a few\u00aechapters from now: // A program to display the message // \\\"Hello World!\\\" on standard output public class HelloWorld { public static void main(String[] args) { System.out.println(\\\"Hello World!\\\"); } } // end of class HelloWorld T\u00aehe command that actually displays the message is: System.out.println(\\\"Hello World!\\\"); This command is an example of a subroutine call statement. It uses a \"built-in subroutine\" named System.out.println to do the actual work. Recall that a subroutine consists of the instructions for performing some task, chunked together and\u00aegiven a name. That name can be used to \"call\" the subroutine whenever that task needs to be performed. A built-in subroutine is one that is already defined as part of the language and therefore automatically available for use in any program. When you run this program, the message \"Hello World!\" (without the quotes) will be displayed on standard output. Unfortunately, I ca\u00aen't say exactly what that means! Java is meant to run on many different platforms, and standard output will mean different things on different platforms. However, you can expect the message to show up in some convenient place. (If you use a command-line interface, like tha\u00aet in Sun Microsystem's Java Development Kit, you type in a command to tell the computer to run the program. The computer will type the output from the program, Hello World!, on the next line.) You must be curious about all the other stuff in the above program. Part of it consists of comments. Comments in a program are entirely ignored by the computer; they are there for human readers only. This dohe main() subroutine, and the statements that it contains are executed. These statements make up the script that tells the computer exactly what to do when the program is executed. The main() routine can call subroutines that are defined in the same class or even in other classes, but it is the main() routine that determitics of",
  "page199": "Variables and the Primitive Types - Names are fundamental to programming. In programs, names are used to refer to many different sorts of things. In order to use those things, a programmer must\u00aeunderstand the rules for giving names to things and the rules for using the names to work with those things. That is, the programmer must understand the syntax and the semantics of names. According to the syntax rules of Java, a name is a s\u00aeequence of one or more characters. It must begin with a letter or underscore and must consist entirely of letters, digits, and underscores. (\"Underscore\" refers to the character ' '.) For example, here are some legal names: No spaces are allowed in identifiers; HelloWorld is a legal identifier, but \"Hello World\u00ae1d is not. Upper case and lower case letters are considered to be different, so that HelloWorld, helloworld, HELLOWORLD, and hElloWorLD are all distinct names. Certain names are reserved for special uses in Java, and cannot be used by the programmer for other purposes. These reserved words include: class, public, static, if, else, while, and several dozen other words. Java is actually pretty libe\u00aeral about what counts as a letter or a digit. Java uses the Unicode character set, which includes thousands of characters from many different languages and different alphabets, and many of these characters count as letters or digits. However, I will be sticking to what can be t\u00aeyped on a regular English keyboard. The pragmatics of naming includes style guidelines about how to choose names for things. For example, it is customary for names of classes to begin with upper case letters, while names of variables and of subroutines begin with lower case letters; you can avoid a lot of confusion by following the same convention in your own programs. Most Java programmers do not use underscoimple or compound that can be used to refer to something in Java. (Note that the reserved words are not identifiers, since they can't be used as names for things.) Variables - Programs manipulate data that are stored in memory. In machine language, data can only be referred to by giving the numerical address of the locatiocuting atever was there before. Now, consider the following more complicated assignment statement, which might come later in the same program.",
  "page200": "Types and Literals- A variable in Java is designed to hold only one particular type of data; it can legally hold that type of data and no other. The compiler will consider it to be a syntax erro\u00aer if you try to violate this rule. We say that Java is a strongly typed language because it enforces this rule. There are eight so-called primitive types built into Java. The primitive types are named byte, short, int, long, float, double,\u00aechar, and boolean. The first four types hold integers (whole numbers such as 17, -38477, and 0). The four integer types are distinguished by the ranges of integers they can hold. The float and double types hold real numbers (such as 3.6 and -145.99). Again, the two real types are distinguished by their range and accuracy. A variable of typ\u00aee char holds a single character from the Unicode character set. And a variable of type boolean holds one of the two logical values true or false. Any data value stored in the computer's memory must be represented as a binary number, that is as a string of zeros and ones. A single zero or one is called a bit. A string of eight bits is called a byte. Memory is usually measured in terms of\u00aebytes. Not surprisingly, the byte data type refers to a single byte of memory. A variable of type byte holds a string of eight bits, which can represent any of the integers between -128 and 127, inclusive. (There are 256 integers in that range; eight bits can represent 256 two\u00aeraised to the power eight different values.) As for the other integer types, short corresponds to two bytes (16 bits). Variables of type short have values in the range -32768 to 32767. int corresponds to four bytes (32 bits). Variables of type int have values in the range -2147483648 to 2147483647. long corresponds to eight bytes (64 bits). Variables of type long have values in the range -9223372036854775808 to 92233720rogram, it must be surrounded by single quotes; for example: 'A', '*', or 'x'. Without the quotes, A would be an identifier and * would be a multiplication operator. The quotes are not part of the value and are not stored in the variable; they are just a convention for naming a particular chae for ree-16) literals. I don't want to cover base-8 and base-16 in detail, but in case you run into them in other people's programs,",
  "page201": "Introduction to Threads- Like people, computers can multitask. That is, they can be working on several different tasks at the same time. A computer that has just a single central processing unit\u00aecan't literally do two things at the same time, any more than a person can, but it can still switch its attention back and forth among several tasks. Furthermore, it is increasingly common for computers to have more than one processin\u00aeg unit, and such computers can literally work on several tasks simultaneously. It is likely that from now on, most of the increase in computing power will come from adding additional processors to computers rather than from increasing the speed of individual processors. To use the full power of these multiprocessing computers, a programmer\u00aemust do parallel programming, which means writing a program as a set of several tasks that can be executed simultaneously. Even on a single-processor computer, parallel programming techniques can be useful, since some problems can be tackled most naturally by breaking the solution into a set of simultaneous tasks that cooperate to solve the problem. In Java, a single task is called a thread.\u00aeThe term \"thread\" refers to a \"thread of control\" or \"thread of execution,\" meaning a sequence of instructions that are executed one after another the thread extends through time, connecting each instruction to the next. In a multithreaded program\u00ae, there can be many threads of control, weaving through time in parallel and forming the complete fabric of the program. (Ok, enough with the metaphor, already!) Every Java program has at least one thread; when the Java virtual machine runs your program, it creates a thread that is responsible for executing the main routine of the program. This main thread can in turn create other threads that can continue even after the main thread has terminated. In a GUI program, there is at least one additional thread, which is responsible for handling events and drawing components on the screen. This GUI thread is created when the first window is opened. So in fact, you have already done parallel programming! When a main routine opens a window, both the main thread and the GUI thread can continue to run in parallel. Of course, parallel programming can be used in much more interesting ways.",
  "page202": "Unfortunately, parallel programming is even more difficult than ordinary, single-threaded programming. When several threads are working together on a problem, a whole new category of errors is p\u00aeossible. This just means that techniques for writing correct and robust programs are even more important for parallel programming than they are for normal programming. (That's one excuse for having this section in this chapter another\u00aeis that we will need threads at several points in future chapters, and I didn't have another place in the book where the topic fits more naturally.) Since threads are a difficult topic, you will probably not fully understand everything in this section the first time through the material. Your understanding should improve as you encoun\u00aeter more examples of threads in future sections. Creating and Running Threads- In Java, a thread is represented by an object belonging to the class java.lang.Thread (or to a subclass of this class). The purpose of a Thread object is to execute a single method. The method is executed in its own thread of control, which can run in parallel with other threads. When the execution of the method is\u00aefinished, either because the method terminates normally or because of an uncaught exception, the thread stops running. Once this happens, there is no way to restart the thread or to use the same Thread object to start another thread. Operations on Threads- The Thread class inc\u00aeludes several useful methods in addition to the start() method that was discussed above. I will mention just a few of them. If thrd is an object of type Thread, then the boolean-valued function thrd.isAlive() can be used to test whether or not the thread is alive. A thread is \"alive\" between the time it is started and the time when it terminates. After the thread has terminated it is said to be \"dead\". (The rather gruesome metaphor is also used when we refer to \"killing\" or \"aborting\" a thread.)",
  "page203": "The static method Thread.sleep(milliseconds) causes the thread that executes this method to \"sleep\" for the specified number of milliseconds. A sleeping thread is still alive, but it i\u00aes not running. While a thread is sleeping, the computer will work on any other runnable threads (or on other programs). Thread.sleep() can be used to insert a pause in the execution of a thread. The sleep method can throw an exception of ty\u00aepe InterruptedException, which is an exception class that requires mandatory exception handling. In practice, this means that the sleep method is usually used in a trycatch statement that catches the potential InterruptedException: try { Thread.sleep(lengthOfPause); } catch (InterruptedException e) { } One thread can interrupt another thre\u00aead to wake it up when it is sleeping or paused for some other reason. A Thread, thrd, can be interrupted by calling its method thrd.interrupt(), but you are not likely to do this until you start writing rather advanced applications, and you are not likely to need to do anything in response to an InterruptedException (except to catch it). It's unfortunate that you have to worry about it a\u00aet all, but that's the way that mandatory exception handling works. Mutual Exclusion with \"synchronized\" Programming several threads to carry out independent tasks is easy. The real difficulty arises when threads have to interact in some way. One way that threads\u00aeinteract is by sharing resources. When two threads need access to the same resource, such as a variable or a window on the screen, some care must be taken that they don't try to use the same resource at the same time. Otherwise, the situation could be something like this: Imagine several cooks sharing the use of just one measuring cup, and imagine that Cook A fills the measuring cup with milk, only to have Cook B grab the cup before Cook A has a chance to empty the milk into his bowl. There has to be some way for Cook A to claim exclusive rights to the cup while he performs the two operations: Add-Milk-To-Cup and Empty-Cup-Into-Bowl.",
  "page204": "Wait and Notify- Threads can interact with each other in other ways besides sharing resources. For example, one thread might produce some sort of result that is needed by another thread. This im\u00aeposes some restriction on the order in which the threads can do their computations. If the second thread gets to the point where it needs the result from the first thread, it might have to stop and wait for the result to be produced. Since\u00aethe second thread can't continue, it might as well go to sleep. But then there has to be some way to notify the second thread when the result is ready, so that it can wake up and continue its computation. Java, of course, has a way to do this kind of waiting and notification: That name can be used to \"call\" the subroutine w\u00aehenever that task needs to be performed. A built-in subroutine is one that is already defined as part of the language and therefore automatically available for use in any program. When you run this program, the message \"Hello World!\" (without the quotes) will be displayed on standard output. Unfortunately, I can't say exactly what that means! Java is meant to run on many differ\u00aeent platforms, and standard output will mean different things on different platforms. However, you can expect the message to show up in some convenient place. (If you use a command-line interface, like that in Sun Microsystem's Java Development Kit, you type in a command t\u00aeo tell the computer to run the program. The computer will type the output from the program, Hello World!, on the next line.) You must be curious about all the other stuff in the above program. Part of it consists of comments. Comments in a program are entirely ignored by the computer; they are there for human readers only. This dohe main() subroutine, and the statements that it contains are executed. These statements make up the script that tells the computer exactly what to do when the program is executed. The main() routine can call subroutines that are defined in the same class or even in other classes, but it is the main() routine that determitics of",
  "page205": "It is still possible to use a shared variable outside of synchronized code, but in that case, the variable must be declared to be volatile. The volatile keyword is a modifier that can be added t\u00aeo a variable declaration, as in private volatile int count; If a variable is declared to be volatile, no thread will keep a local copy of that variable in its cache. Instead, the thread will always use the official, main copy of the variabl\u00aee. This means that any change made to the variable will immediately be available to all threads. This makes it safe for threads to refer to volatile shared variables even outside of synchronized code. (Remember, though, that synchronization is still the only way to prevent race conditions.) When the volatile modifier is applied to an objec\u00aet variable, only the variable itself is declared to be volatile, not the contents of the object that the variable points to. For this reason, volatile is generally only used for variables of simple types such as primitive types and enumerated types.Analysis of Algorithms- This chapter has concentrated mostly on correctness of programs. In practice, another issue is also important: efficiency.\u00aeWhen analyzing a program in terms of efficiency, we want to look at questions such as, \"How long does it take for the program to run?\" and \"Is there another approach that will get the answer more quickly?\" Efficiency will always be less important than corre\u00aectness; if you don't care whether a program works correctly, you can make it run very quickly indeed, but no one will think it's much of an achievement! On the other hand, a program that gives a correct answer after ten thousand years isn't very useful either, so efficiency is often an important issue.The term \"efficiency\" can refer to efficient use of almost any resource, including time, computer memory, disk space, or network bandwidth. In this section, however, we will deal exclusively with time efficiency, and the major question that we want to ask about a program is, how long does it take to perform its task?",
  "page206": "it really makes little sense to classify an individual program as being \"efficient\" or \"inefficient.\" It makes more sense to compare two (correct) programs that perform the s\u00aeame task and ask which one of the two is \"more efficient,\" that is, which one performs the task more quickly. However, even here there are difficulties. The running time of a program is not well-defined. The run time can be differ\u00aeent depending on the number and speed of the processors in the computer on which it is run and, in the case of Java, on the design of the Java Virtual Machine which is used to interpret the program. It can depend on details of the compiler which is used to translate the program from high-level language to machine language. Furthermore, the\u00aerun time of a program depends on the size of the problem which the program has to solve. It takes a sorting program longer to sort 10000 items than it takes it to sort 100 items. When the run times of two programs are compared, it often happens that Program A solves small problems faster than Program B, while Program B solves large problems faster than Program A, so that it is simply not the\u00aecase that one program is faster than the other in all cases. the efficiency of programs. The field is known as Analysis of Algorithms. The focus is on algorithms, rather than on programs as such, to avoid having to deal with multiple implementations of the same algorithm writt\u00aeen in different languages, compiled with different compilers, and running on different computers. Analysis of Algorithms is a mathematical field that abstracts away from these down-and-dirty details. Still, even though it is a theoretical field, every working programmer should be aware of some of its techniques and results. This section is a very brief introduction to some of those techniques and results. Because this is not a mathematics book, the treatment will be rather informal.",
  "page207": "One of the main techniques of analysis of algorithms is asymptotic analysis. The term \"asymptotic\" here means basically \"the tendency in the long run.\" An asymptotic analysis\u00aeof an algorithm's run time looks at the question of how the run time depends on the size of the problem. The analysis is asymptotic because it only considers what happens to the run time as the size of the problem increases without li\u00aemit; it is not concerned with what happens for problems of small size or, in fact, for problems of any fixed finite size. Only what happens in the long run, as the problem size increases without limit, is important. Showing that Algorithm A is asymptotically faster than Algorithm B doesn't necessarily mean that Algorithm A will run fa\u00aester than Algorithm B for problems of size 10 or size 1000 or even size 1000000 it only means that if you keep increasing the problem size, you will eventually come to a point where Algorithm A is faster than Algorithm B. An asymptotic analysis is only a first approximation, but in practice it often gives important and useful information. Central to asymptotic analysis is Big-Oh notation. Usi\u00aeng this notation, we might say, for example, that an algorithm has a running time that is O(n2 ) or O(n) or O(log(n)). These notations are read \"Big-Oh of n squared,\" \"Big-Oh of n,\" and \"Big-Oh of log n\" (where log is a logarithm function). More ge\u00aenerally, we can refer to O(f(n)) (\"Big-Oh of f of n\"), where f(n) is some function that assigns a positive real number to every positive integer n. The \"n\" in this notation refers to the size of the problem. Before you can even begin an asymptotic analysis, you need some way to measure problem size. Usually, this is not a big issue. For example, if the problem is to sort a list of items, then the problem size can be taken to be the number of items in the list. When the input to an algorithm is an integer, as in the case of algorithm that checks whether a given positive integer is prime, the usual measure of the size of a problem is the number of bits in the input integer rather than the integer itself. More generally, the number of bits in the input to a problem is often a good measure of the size of the problem.",
  "page208": "To say that the running time of an algorithm is O(f(n)) means that for large values of the problem size, n, the running time of the algorithm is no bigger than some constant times f(n). (More ri\u00aegorously, there is a number C and a positive integer M such that whenever n is greater than M, the run time is less than or equal to C*f(n).) The constant takes into account details such as the speed of the computer on which the algorithm i\u00aes run; if you use a slower computer, you might have to use a bigger constant in the formula, but changing the constant won't change the basic fact that the run time is O(f(n)). The constant also makes it unnecessary to say whether we are measuring time in seconds, years, CPU cycles, or any other unit of measure; a change from one unit\u00aeof measure to another is just multiplication by a constant. Note also that O(f(n)) doesn't depend at all on what happens for small problem sizes, only on what happens in the long run as the problem size increases without limit. To look at a simple example, consider the problem of adding up all the numbers in an array. The problem size, n, is the length of the array. Using A as the name\u00aeof the array, the algorithm can be expressed in Java as: total = 0; for (int i = 0; i < n; i++) total = total + A[i]; This algorithm performs the same operation, total = total + A[i], n times. The total time spent on this operation is a*n, where a is the time it takes to perfor\u00aem the operation once. time spent on this operation is a*n, where a is the time it takes to perform the operation once. Now, this is not the only thing that is done in the algorithm. The value of i is incremented and is compared to n each time through the loop. This adds an additional time of b*n to the run time, for some constant b. Furthermore, i and total both have to be initialized to zero; this adds some constant amount c to the running time. The exact running time would then be (a+b)*n+c, where the constants a, b, and c depend on factors such as how the code is compiled and what computer it is run on.",
  "page209": "Using the fact that c is less than or equal to c*n for any positive integer n, we can say that the run time is less than or equal to (a+b+c)*n. That is, the run time is less than or equal to a c\u00aeonstant times n. By definition, this means that the run time for this algorithm is O(n). If this explanation is too mathematical for you, we can just note that for large values of n, the c in the formula (a+b)*n+c is insignificant compared\u00aeto the other term, (a+b)*n. We say that c is a \"lower order term.\" When doing asymptotic analysis, lower order terms can be discarded. A rough, but correct, asymptotic analysis of the algorithm would go something like this: Each iteration of the for loop takes a certain constant amount of time. There are n iterations of the loop,\u00aeso the total run time is a constant times n, plus lower order terms (to account for the initialization). Disregarding lower order terms, we see that the run time is O(n). Note that to say that an algorithm has run time O(f(n)) is to say that its run time is no bigger than some constant times n (for large values of n). O(f(n)) puts an upper limit on the run time. However, the run time could b\u00aee smaller, even much smaller. For example, if the run time is O(n), it would also be correct to say that the run time is O(n2 ) or even O(n10). If the run time is less than a constant times n, then it is certainly less than the same constant times n 2 or n10 Of course, sometime\u00aes it's useful to have a lower limit on the run time. That is, we want to be able to say that the run time is greater than or equal to some constant times f(n) (for large values of n). The notation for this is Ohm(f(n)), read \"Omega of f of n.\" \"Omega\" is the name of a letter in the Greek alphabet, and Ohm is the upper case version of that letter. (To be technical, saying that the run time of an algorithm is Ohm(f(n)) means that there is a positive number C and a positive integer M such that whenever n is greater than M, the run time is greater than or equal to C*f(n).) O(f(n)) tells you something about the maximum amount of time that you might have to wait for an algorithm to finish; Ohm(f(n)) tells you something about the minimum time.",
  "page210": "The algorithm for adding up the numbers in an array has a run time that is Ohm(n) as well as O(n). When an algorithm has a run time that is both Ohm(f(n)) and O(f(n)), its run time is said\u00aeto be Theta(f(n)), read \"Theta of f of n.\" (Theta is another letter from the Greek alphabet.) To say that the run time of an algorithm is Theta(f(n)) means that for large values of n, the run time is between a*f(n) and b*f(n),\u00aewhere a and b are constants (with b greater than a, and both greater than 0).So far, my analysis has ignored an important detail. We have looked at how run time depends on the problem size, but in fact the run time usually depends not just on the size of the problem but on the specific data that has to be processed. For example, the run ti\u00aeme of a sorting algorithm can depend on the initial order of the items that are to be sorted, and not just on the number of items. To account for this dependency, we can consider either the worst case run time analysis or the average case run time analysis of an algorithm. For a worst case run time analysis, we consider all possible problems of size n and look at the longest possible run time\u00aefor all such problems. For an average case analysis, we consider all possible problems of size n and look at the average of the run times for all such problems. Usually, the average case analysis assumes that all problems of size n are equally likely to be encountered, althoug\u00aeh this is not always realistic or even possible in the case where there is an infinite number of different problems of a given size. In many cases, the average and the worst case run times are the same to within a constant multiple. This means that as far as asymptotic analysis is concerned, they are the same. That is, if the average case run time is O(f(n)) or Theta(f(n)), then so is the worst case. However, later in the book, we will encounter a few cases where the average and worst case asymptotic analyses differ.",
  "page211": "So, what do you really have to know about analysis of algorithms to read the rest of this book? We will not do any rigorous mathematical analysis, but you should be able to follow informal discu\u00aession of simple cases such as the examples that we have looked at in this section. Most important, though, you should have a feeling for exactly what it means to say that the running time of an algorithm is O(f(n)) or Theta(f(n)) for some\u00aecommon functions f(n). The main point is that these notations do not tell you anything about the actual numerical value of the running time of the algorithm for any particular case. They do not tell you anything at all about the running time for small values of n. What they do tell you is something about the rate of growth of the running t\u00aeime as the size of the problem increases. Suppose you compare two algorithm that solve the same problem. The run time of one algorithm is Theta(n2), while the run time of the second algorithm is Theta(n3). What does this tell you? If you want to know which algorithm will be faster for some particular problem of size, say, 100, nothing is certain. As far as you can tell just from the asympto\u00aetic analysis, either algorithm could be faster for that particular case or in any particular case. But what you can say for sure is that if you look at larger and larger problems, you will come to a point where the Theta(n2) algorithm is faster than the Theta(n3) algorithm. F\u00aeurthermore, as you continue to increase the problem size, the relative advantage of the Theta(n2) algorithm will continue to grow. There will be values of n for which the Theta(n2) algorithm is a thousand times faster, a million times faster, a billion times faster, and so on. This is because for any positive constants a and b, the function a*n3 grows faster than the function b*n2 as n gets larger. (Mathematically, the limit of the ratio of a*n3 to b*n2 is infinite as n approaches infinity.) This means that for \"large\" problems, a Theta(n2) algorithm will definitely be faster than a Theta(n3) algorithm. You just don't know based on the asymptotic analysis alone exactly how large \"large\" has to be. In practice, in fact, it is likely that the Theta(n2) algorithm will be faster even for fairly small values of n, and absent other information you would generally prefer a Theta(n2) algorithm to a Theta(n3) algorithm.",
  "page212": "Recursion- At one time or another, you've probably been told that you can't define something in terms of itself. Nevertheless, if it's done right, defining something at least part\u00aeially in terms of itself can be a very powerful technique. A recursive definition is one that uses the concept or thing that is being defined as part of the definition. For example: An \"ancestor\" is either a parent or an ancestor\u00aeof a parent. A \"sentence\" can be, among other things, two sentences joined by a conjunction such as \"and.\" A \"directory\" is a part of a disk drive that can hold files and directories. In mathematics, a \"set\" is a collection of elements, which can themselves be sets. A \"statement\" in Java ca\u00aen be a while statement, which is made up of the word \"while\", a boolean-valued condition, and a statement. Recursive definitions can describe very complex situations with just a few words. A definition of the term \"ancestor\" without using recursion might go something like \"a parent, or \"and so on\" is not very rigorous. (I've often thought that recursion\u00aeis really just a rigorous way of saying \"and so on.\") You run into the same problem if you try to define a \"directory\" as \"a file that is a list of files, where some of the files can be lists of files, where some of those files can be lists of files, a\u00aend so on.\" Trying to describe what a Java statement can look like, without using recursion in the definition, would be difficult and probably pretty comical. Recursion can be used as a programming technique. A recursive subroutine is one that calls itself, either directly or indirectly. To say that a subroutine calls itself directly means that its definition contains a subroutine call statement that calls the subroutine that is being defined. To say that a subroutine calls itself indirectly means that it calls a second subroutine which in turn calls the first subroutine (either directly or indirectly). A recursive subroutine can define a complex task in just a few lines of code. In the rest of this section, we'll look at a variety of examples, and we'll see other examples in the rest of the book.",
  "page213": "Recursive Binary Search- Binary search is used to find a specified value in a sorted list of items (or, if it does not occur in the list, to determine that fact). The idea is to test the element\u00aein the middle of the list. If that element is equal to the specified value, you are done. If the specified value is less than the middle element of the list, then you should search for the value in the first half of the list. Otherwise, yo\u00aeu should search for the value in the second half of the list. The method used to search for the value in the first or second half of the list is binary search. That is, you look at the middle element in the half of the list that is still under consideration, and either you've found the value you are looking for, or you have to apply b\u00aeinary search to one half of the remaining elements. And so on! This is a recursive description, and we can write a recursive subroutine to implement it. Before we can do that, though, there are two considerations that we need to take into account. Each of these illustrates an important general fact about recursive subroutines. First of all, the binary search algorithm begins by looking at the\u00ae\"middle element of the list.\" But what if the list is empty? If there are no elements in the list, then it is impossible to look at the middle element. In the terminology of Subsection 8.2.1, having a non-empty list is a \"precondition\" for looking at the mi\u00aeddle element, and this is a clue that we have to modify the algorithm to take this precondition into account. What should we do if we find ourselves searching for a specified value in an empty list? The answer is easy: If the list is empty, we can be sure that the value does not occur in the list, so we can give the answer without any further work. An empty list is a base case for the binary search algorithm. A base case for a recursive algorithm is a case that is handled directly, rather than by applying the algorithm recursively. The binary search algorithm actually has another type of base case: If we find the element we are looking for in the middle of the list, we are done. There is no need for further recursion. The second consideration has to do with the parameters to the subroutine.",
  "page214": "Linked Data Structures- Every useful object contains instance variables. When the type of an instance variable is given by a class or interface name, the variable can hold a reference to another\u00aeobject. Such a reference is also called a pointer, and we say that the variable points to the object. (Of course, any variable that can contain a reference to an object can also contain the special value null, which points to nowhere.) Whe\u00aen one object contains an instance variable that points to another object, we think of the objects as being \"linked\" by the pointer. Data structures of great complexity can be constructed by linking objects together. Recursive Linking- Something interesting happens when an object contains an instance variable that can refer to ano\u00aether object of the same type. In that case, the definition of the object's class is recursive. Such recursion arises naturally in many cases. For example, consider a class designed to represent employees at a company. Suppose that every employee except the boss has a supervisor, who is another employee of the company. As the while loop is executed, runner points in turn to the original e\u00aemployee, emp, then variable is incremented each time runner \"visits\" a new employee. The loop ends when runner.supervisor is null, which indicates that runner has reached the boss. At that point, count has counted the number of steps between emp and the boss. In this\u00aeexample, the supervisor variable is quite natural and useful. In fact, data structures that are built by linking objects together are so useful that they are a major topic of study in computer science. We'll be looking at a few typical examples. In this section and the next, we'll be looking at linked lists. A linked list consists of a chain of objects of the same type, linked together by pointers from one object to the next. This is much like the chain of supervisors between emp and the boss in the above example. It's also possible to have more complex situations, in which one object can contain links to several other objects.",
  "page215": "Linked Lists- For most of the examples in the rest of this section, linked lists will be constructed out of objects belonging to the class Node which is defined as follows: class Node { String i\u00aetem; Node next; } The term node is often used to refer to one of the objects in a linked data structure. Objects of type Node can be chained together as shown in the top part of the above picture. Each node holds a String and a pointer to t\u00aehe next node in the list (if any). The last node in such a list can always be identified by the fact that the instance variable next in the last node holds the value null instead of a pointer to another node. The purpose of the chain of nodes is to represent a list of strings. The first string in the list is stored in the first node, the s\u00aeecond string is stored in the second node, and so on. The pointers and the node objects are used to build the structure, but the data that we are interested in representing is the list of strings. Of course, we could just as easily represent a list of integers or a list of JButtons or a list of any other type of data by changing the type of the item that is stored in each node. Although the N\u00aeodes in this example are very simple, we can use them to illustrate the common operations on linked lists. Typical operations include deleting nodes from the list, inserting new nodes into the list, and searching for a specified String among the items in the list. We will look\u00aeat subroutines to perform all of these operations, among others. For a linked list to be used in a program, that program needs a variable that refers to the first node in the list. It only needs a pointer to the first node since all the other nodes in the list can be accessed by starting at the first node and following links along the list from one node to the next. In my examples, I will always use a variable named head, of type Node, that points to the first node in the linked list. When the list is empty, the value of head is null.",
  "page216": "Stacks, Queues, and ADTs- A linked list is a particular type of data structure, made up of objects linked together by pointers. In the previous section, we used a linked list to store an ordered\u00aelist of Strings, and we implemented insert, delete, and find operations on that list. However, we could easily have stored the list of Strings in an array or ArrayList, instead of in a linked list. We could still have implemented the same\u00aeoperations on the list. The implementations of these operations would have been different, but their interfaces and logical behavior would still be the same. The term abstract data type, or ADT, refers to a set of possible values and a set of operations on those values, without any specification of how the values are to be represented or h\u00aeow the operations are to be implemented. An \"ordered list of strings\" can be defined as an abstract data type. Any sequence of Strings that is arranged in increasing order is a possible value of this data type. The operations on the data type include inserting a new string, deleting a string, and finding a string in the list. There are often several different ways to implement the s\u00aeame abstract data type. For example, the \"ordered list of strings\" ADT can be implemented as a linked list or as an array. A program that only depends on the abstract definition of the ADT can use either implementation, interchangeably. In particular, the implementati\u00aeon of the ADT can be changed without affecting the program as a whole. This can make the program easier to debug and maintain, so ADTs are an important tool in software engineering. In this section, we'll look at two common abstract data types, stacks and queues. Both stacks and queues are often implemented as linked lists, but that is not the only possible implementation. You should think of the rest of this section partly as a discussion of stacks and queues and partly as a case study in ADTs.",
  "page217": "Stacks- A stack consists of a sequence of items, which should be thought of as piled one on top of the other like a physical stack of boxes or cafeteria trays. Only the top item on the stack is\u00aeaccessible at any given time. It can be removed from the stack with an operation called pop. An item lower down on the stack can only be removed after all the items on top of it have been popped off the stack. A new item can be added to the\u00aetop of the stack with an operation called push. We can make a stack of any type of items. If, for example, the items are values of type int, then the push and pop operations can be implemented as instance methods void push (int newItem) Add newItem to top of stack. int pop() Remove the top int from the stack and return it. It is an error\u00aeto try to pop an item from an empty stack, so it is important to be able to tell whether a stack is empty. We need another stack operation to do the test, implemented as an instance method boolean isEmpty() Returns true if the stack is empty. To get a better handle on the difference between stacks and queues, consider the sample program DepthBreadth.java. I suggest that you run the program or\u00aetry the applet version that can be found in the on-line version of this section. The program shows a grid of squares. Initially, all the squares are white. When you click on a white square, the program will gradually mark all the squares in the grid, starting from the one wher\u00aee you click. To understand how the program does this, think of yourself in the place of the program. When the user clicks a square, you are handed an index card. The location of the square its row and column is written on the card. You put the card in a pile, which then contains just that one card. Then, you repeat the following: If the pile is empty, you are done. Otherwise, take an index card from the pile. The index card specifies a square. Look at each horizontal and vertical neighbor of that square. If the neighbor has not already been encountered, write its location on a new index card and put the card in the pile.",
  "page218": "While a square is in the pile, waiting to be processed, it is colored red; that is, red squares have been encountered but not yet processed. When a square is taken from the pile and processed, i\u00aets color changes to gray. Once a square has been colored gray, its color won't change again. Eventually, all the squares have been processed, and the procedure ends. In the index card analogy, the pile of cards has been emptied. The pr\u00aeogram can use your choice of three methods: Stack, Queue, and Random. In each case, the same general procedure is used. The only difference is how the \"pile of index cards\" is managed. For a stack, cards are added and removed at the top of the pile. For a queue, cards are added to the bottom of the pile and removed from the top.\u00aeIn the random case, the card to be processed is picked at random from among all the cards in the pile. The order of processing is very different in these three cases. You should experiment with the program to see how it all works. Try to understand how stacks and queues are being used. Try starting from one of the corner squares. While the process is going on, you can click on other white squ\u00aeares, and they will be added to the pile. When you do this with a stack, you should notice that the square you click is processed immediately, and all the red squares that were already waiting for processing have to wait. On the other hand, if you do this with a queue, the squa\u00aere that you click will wait its turn until all the squares that were already in the pile have been processed. Queues seem very natural because they occur so often in real life, but there are times when stacks are appropriate and even essential. For example, consider what happens when a routine calls a subroutine. The first routine is suspended while the subroutine is executed, and it will continue only when the subroutine returns. Now, suppose that the subroutine calls a second subroutine, and the second subroutine calls a third, and so on. Each subroutine is suspended while the subsequent subroutines are executed. The computer has to keep track of all the subroutines that are suspended. It does this with a stack.",
  "page219": "When a subroutine is called, an activation record is created for that subroutine. The activation record contains information relevant to the execution of the subroutine, such as its local variab\u00aeles and parameters. The activation record for the subroutine is placed on a stack. It will be removed from the stack and destroyed when the subroutine returns. If the subroutine calls another subroutine, the activation record of the second\u00aesubroutine is pushed onto the stack, on top of the activation record of the first subroutine. The stack can continue to grow as more subroutines are called, and it shrinks as those subroutines return. Postfix Expressions As another example, stacks can be used to evaluate postfix expressions. An ordinary mathematical expression such as 2+(1\u00ae5-12)*17 is called an infix expression. In an infix expression, an operator comes in between its two operands, as in \"2 + 2\". In a postfix expression, an operator comes after its two operands, as in \"2 2 +\". The infix expression \"2+(15-12)*17\" would be written in postfix form as \"2 15 12 - 17 * +\". The \"-\" operator in this expression applies t\u00aeo the two operands that precede it, namely \"15\" and \"12\". The \"*\" operator applies to the two operands that precede it, namely \"15 12 -\" and \"17\". And the \"+\" operator applies to \"2\" and \"15 12 - 17\u00aeThese are the same computations that are done in the original infix expression. Now, suppose that we want to process the expression \"2 15 12 - 17 * +\", from left to right and find its value. The first item we encounter is the 2, but what can we do with it? At this point, we don't know what operator, if any, will be applied to the 2 or what the other operand might be. We have to remember the 2 for later processing. We do this by pushing it onto a stack. Moving on to the next item, we see a 15, which is pushed onto the stack on top of the 2. Then the 12 is added to the stack. Now, we come to the operator, \"-\". This operation applies to the two operands that preceded it in the expression.",
  "page220": "These numbers are multiplied, and the result, 51 is pushed onto the stack. The next item in the expression is a \"+\" operator, which is processed by popping 51 and 2 from the stack, add\u00aeing them, and pushing the result, 53, onto the stack. Finally, we've come to the end of the expression. The number on the stack is the value of the entire expression, so all we have to do is pop the answer from the stack, and we are do\u00aene! The value of the expression is 53. Tree Traversal- Consider any node in a binary tree. Look at that node together with all its descendents (that is, its children, the children of its children, and so on). This set of nodes forms a binary tree, which is called a subtree of the original tree. For example, in the picture, nodes 2, 4, and\u00ae5 form a subtree. This subtree is called the left subtree of the root. Similarly, nodes 3 and 6 make up the right subtree of the root. We can consider any non-empty binary tree to be made up of a root node, a left subtree, and a right subtree. Either or both of the subtrees can be empty. This is a recursive definition, matching the recursive definition of the TreeNode class. So it should not\u00aebe a surprise that recursive subroutines are often used to process trees. Consider the problem of counting the nodes in a binary tree. (As an exercise, you might try to come up with a non-recursive algorithm to do the counting, but you shouldn't expect to find one.) The he\u00aeart of problem is keeping track of which nodes remain to be counted. It's not so easy to do this, and in fact it's not even possible without an auxiliary data structure such as a stack or queue. With recursion, however, the algorithm is almost trivial. Either the tree is empty or it consists of a root and two subtrees. If the tree is empty, the number of nodes is zero. (This is the base case of the recursion.) Otherwise, use recursion to count the nodes in each subtree. Add the results from the subtrees together, and add one to count the root.",
  "page221": "Binary Sort Trees- One of the examples in Section 9.2 was a linked list of strings, in which the strings were kept in increasing order. While a linked list works well for a small number of strin\u00aegs, it becomes inefficient for a large number of items. When inserting an item into the list, searching for that item's position requires looking at, on average, half the items in the list. Finding an item in the list requires a simila\u00aer amount of time. If the strings are stored in a sorted array instead of in a linked list, then searching becomes more efficient because binary search can be used. However, inserting a new item into the array is still inefficient since it means moving, on average, half of the items in the array to make a space for the new item. A binary tr\u00aeee can be used to store an ordered list of strings, or other items, in a way that makes both searching and insertion efficient. A binary tree used in this way is called a binary sort tree. A binary sort tree is a binary tree with the following property: For every node in the tree, the item in that node is greater than every item in the left subtree of that node, and it is less than or equal t\u00aeo all the items in the right subtree of that node. Here for example is a binary sort tree containing items of type String. (In this picture, I haven't bothered to draw all the pointer variables. Non-null pointers are shown as arrows.)",
  "page222": "Binary sort trees have this useful property: An inorder traversal of the tree will process the items in increasing order. In fact, this is really just another way of expressing the definition. F\u00aeor example, if an inorder traversal is used to print the items in the tree shown above, then the items will be in alphabetical order. The definition of an inorder traversal guarantees that all the items in the left subtree of \"judy\u00ae1d are printed before \"judy\", and all the items in the right subtree of \"judy\" are printed after \"judy\". But the binary sort tree property guarantees that the items in the left subtree of \"judy\" are precisely those that precede \"judy\" in alphabetical order, and all the items in the right subtre\u00aee follow \"judy\" in alphabetical order. So, we know that \"judy\" is output in its proper alphabetical position. But the same argument applies to the subtrees. \"Bill\" will be output after \"alice\" and before \"fred\" and its descendents. \"Fred\" will be output after \"dave\" and before \"jane\" and \"joe\". And so on\u00ae. Suppose that we want to search for a given item in a binary search tree. Compare that item to the root item of the tree. If they are equal, we're done. If the item we are looking for is less than the root item, then we need to search the left subtree of the root the righ\u00aet subtree can be eliminated because it only contains items that are greater than or equal to the root. Similarly, if the item we are looking for is greater than the item in the root, then we only need to look in the right subtree. In either case, the same procedure can then be applied to search the subtree. Inserting a new item is similar: Start by searching the tree for the position where the new item belongs. When that position is found, create a new node and attach it to the tree at that position.",
  "page223": "Searching and inserting are efficient operations on a binary search tree, provided that the tree is close to being balanced. A binary tree is balanced if for each node, the left subtree of that\u00aenode contains approximately the same number of nodes as the right subtree. In a perfectly balanced tree, the two numbers differ by at most one. Not all binary trees are balanced, but if the tree is created by inserting items in a random ord\u00aeer, there is a high probability that the tree is approximately balanced. (If the order of insertion is not random, however, it's quite possible for the tree to be very unbalanced.) During a search of any binary sort tree, every comparison eliminates one of two subtrees from further consideration. If the tree is balanced, that means cu\u00aetting the number of items still under consideration in half. This is exactly the same as the binary search algorithm, and the result, is a similarly efficient algorithm. In terms of asymptotic analysis, searching, inserting, and deleting in a binary search tree have average case run time Theta(log(n)). The problem size, n, is the number of items in the tree, and the average is taken over all\u00aethe different orders in which the items could have been inserted into the tree. As long the actual insertion order is random, the actual run time can be expected to be close to the average. However, the worst case run time for binary search tree operations is Theta(n), which\u00aeis much worse than Theta(log(n)). The worst case occurs for certain particular insertion orders. For example, if the items are inserted into the tree in order of increasing size, then every item that is inserted moves always to the right as it moves down the tree. The result is a \"tree\" that looks more like a linked list, since it consists of a linear string of nodes strung together by their right child pointers. Operations on such a tree have the same performance as operations on a linked list. Now, there are data structures that are similar to simple binary sort trees, except that insertion and deletion of nodes are implemented in a way that will always keep the tree balanced, or almost balanced. For these data structures, searching, inserting, and deleting have both average case and worst case run times that are Theta(log(n)).",
  "page224": "Backus-Naur Form- Natural and artificial languages are similar in that they have a structure known as grammar or syntax. Syntax can be expressed by a set of rules that describe what it means to\u00aebe a legal sentence or program. For programming languages, syntax rules are often expressed in BNF (Backus-Naur Form), a system that was developed by computer scientists John Backus and Peter Naur in the late 1950s. Interestingly, an equiva\u00aelent system was developed independently at about the same time by linguist Noam Chomsky to describe the grammar of natural language. BNF cannot express all possible syntax rules. For example, it can't express the fact that a variable must be defined before it is used. Furthermore, it says nothing about the meaning or semantics of the\u00aelangauge. The problem of specifying the semantics of a language even of an artificial programming langauge is one that is still far from being completely solved. However, BNF does express the basic structure of the language, and it plays a central role in the design of translation programs. Files- The data and programs in a computer's main memory survive only as long as the power is on.\u00aeFor more permanent storage, computers use files, which are collections of data stored on a hard disk, on a USB memory stick, on a CD-ROM, or on some other type of storage device. Files are organized into directories (sometimes called folders). A directory can hold other directo\u00aeries, as well as files. Both directories and files have names that are used to identify them. Programs can read data from existing files. They can create new files and can write data to files. In Java, such input and output can be done using streams. Human-readable character data is read from a file using an object belonging to the class FileReader, which is a subclass of Reader. Similarly, data is written to a file in human-readable format through an object of type FileWriter, a subclass of Writer. For files that store data in machine format, the appropriate I/O classes are FileInputStream and FileOutputStream. In this section, I will only discuss character oriented file I/O using the FileReader and FileWriter classes. However, FileInputStream and FileOutputStream are used in an exactly parallel fashion. All these classes are defined in the java.io package.",
  "page225": "Word Counting- The final example in this section also deals with storing information about words. The problem here is to make a list of all the words that occur in a file, along with the number\u00aeof times that each word occurs. The file will be selected by the user. The output of the program will consist of two lists. Each list contains all the words from the file, along with the number of times that the word occurred. One list is s\u00aeorted alphabetically, and the other is sorted according to the number of occurrences, with the most common words at the top and the least common at the bottom. The problem here is a generalization, which asked you to make an alphabetical list of all the words in a file, without counting the number of occurrences. Symbol Tables- We begin wi\u00aeth a straightforward but important application of maps. When a compiler reads the source code of a program, it encounters definitions of variables, subroutines, and classes. The names of these things can be used later in the program. The compiler has to remember the name is encountered later in the program. This is a natural application for a Map. The name can be used as a key in the map. The\u00aevalue associated to the key is the definition of the name, encoded somehow as an object. A map that is used in this way is called a symbol table. In a compiler, the values in a symbol table can be quite complicated, since the compiler has to deal with names for various sorts o\u00aef things, and it needs a different type of information for each different type of name. We will keep things simple by looking at a symbol table in another context. Suppose that we want a program that can evaluate expressions entered by the user, and suppose that the expressions can contain variables, in addition to operators, numbers, and parentheses. For this to make sense, we need some way of assigning values to variables. When a variable is used in an expression, we need to retrieve the variable's value. A symbol table can be used to store the data that we need. The keys for the symbol table are variable names. The value associated with a key is the value of that variable, which is of type double.",
  "page226": "Almost all applications require persistent data. Persistence is one of the fundamental concepts in application development. If an information system didn't preserve data when it was powered\u00aeoff, the system would be of little practical use. Object persistence means individual objects can outlive the application process; they can be saved to a data store and be re-created at a later point in time. When we talk about persistence\u00aein Java, we're normally talking about mapping and storing object instances in a database using SQL. We start by taking a brief look at the technology and how it's used in Java. Armed with this information, we then continue our discussion of persistence and how it's implemented in object-oriented applications. You, like mos\u00aet other software engineers, have probably worked with SQL and relational databases; many of us handle such systems every day. Relational database management systems have SQL-based application programming interfaces; hence, we call today's relational database products SQL database management systems (DBMS) or, when we're talking about particular systems, SQL databases. Relational te\u00aechnology is a known quantity, and this alone is sufficient reason for many organizations to choose it. But to say only this is to pay less respect than is due. Relational databases are entrenched because they're an incredibly flexible and robust approach to data management\u00ae. Due to the well-researched theoretical foundation of the relational data model, relational databases can guarantee and protect the integrity of the stored data, among other desirable characteristics. You may be familiar with E.F. Codd's four-decades-old introduction of the relational model, A Relational Model of Data for Large Shared Data Banks (Codd, 1970). A more recent compendium worth reading, with a focus on SQL, is C. J. Date's SQL and Relational Theory (Date, 2009).  Relational DBMSs aren't specific to Java, nor is an SQL database specific to a particular application. This important principle is known as data independence. In other words, and we can't stress this important fact enough, data lives longer than any application does. Relational technology provides a way of sharing data among\u00aeas well as files. Both directories and files have names that are used to identify them. Programs can read data from existing files. They can create new files and can write data to files. In Java, such input and output can be done using streams. Human-readable character data is read from a file using an object belonging to the class FileReader, which is a subclass of Reader. Similarly, data is written to a file in human-readable format through an object of type FileWriter, a subclass of Writer. For files that store data in machine format, the appropriate I/O classes are FileInputStream and FileOutputStream. In this section, I will only discuss character oriented file I/O using the FileReader and FileWriter classes. However, FileInputStream and FileOutputStream are used in an exactly parallel fashion. All these classes are defined in the java.io package.",
  "page227": "Before we go into more detail about the practical aspects of SQL databases, we have to mention an important issue: although marketed as relational, a database system providing only an SQL data l\u00aeanguage interface isn't really relational and in many ways isn't even close to the original concept. Naturally, this has led to confusion. SQL practitioners blame the relational data model for shortcomings in the SQL language, and\u00aerelational data management experts blame the SQL standard for being a weak implementation of the relational model and ideals. Application engineers are stuck somewhere in the middle, with the burden of delivering something that works. We highlight some important and significant aspects of this issue throughout this book, but generally we\u00aefocus on the practical aspects. If you're interested in more background material, we highly recommend Practical Issues in Database Management: A Reference for the Thinking Practitioner by Fabian Pascal (Pascal, 2000) and An Introduction to Database Systems by Chris Date (Date, 2003) for the theory, concepts, and ideals of (relational) database systems. The latter book is an excellent ref\u00aeerence (it's big) for all questions you may possibly have about databases and data management. To use Hibernate effectively, you must start with a solid understanding of the relational model and SQL. You need to understand the relational model and topics such as normaliza\u00aetion to guarantee the integrity of your data, and you'll need to use your knowledge of SQL to tune the performance of your Hibernate application. Hibernate automates many repetitive coding tasks, but your knowledge of persistence technology must extend beyond Hibernate itself if you want to take advantage of the full power of modern SQL databases. To dig deeper, consult the bibliography at the end of this book.\u00aeThese are the same computations that are done in the original infix expression. Now, suppose that we want to process the expression \"2 15 12 - 17 * +\", from left to right and find its value. The first item we encounter is the 2, but what can we do with it? At this point, we don't know what operator, if any, will be applied to the 2 or what the other operand might be. We have to remember the 2 for later processing. We do this by pushing it onto a stack. Moving on to the next item, we see a 15, which is pushed onto the stack on top of the 2. Then the 12 is added to the stack. Now, we come to the operator, \"-\". This operation applies to the two operands that preceded it in the expression.",
  "page228": "You've probably used SQL for many years and are familiar with the basic operations and statements written in this language. Still, we know from our own experience that SQL is sometimes hard\u00aeto remember, and some terms vary in usage.  Let's review some of the SQL terms used in this book. You use SQL as a data definition language (DDL) when creating, altering, and dropping artifacts such as tables and constraints in the c\u00aeatalog of the DBMS. When this schema is ready, you use SQL as a data manipulation language (DML) to perform operations on data, including insertions, updates, and deletions. You retrieve data by executing queries with restrictions, projections, and Cartesian products. For efficient reporting, you use SQL to join, aggregate, and group data\u00aeas necessary. You can even nest SQL statements inside each other a technique that uses subselects. When your business requirements change, you'll have to modify the database schema again with DDL statements after data has been stored; this is known as schema evolution.  If you're an SQL veteran and you want to know more about optimization and how SQL is executed, get a copy of the\u00aeexcellent book SQL Tuning, by Dan Tow (Tow, 2003). For a look at the practical side of SQL through the lens of how not to use SQL, SQL Antipatterns: Avoiding the Pitfalls of Database Programming (Karwin, 2010) is a good resource.  Although the SQL database is one part of ORM,\u00aethe other part, of course, consists of the data in your Java application that needs to be persisted to and loaded from the database.o\u00aef things, and it needs a different type of information for each different type of name. We will keep things simple by looking at a symbol table in another context. Suppose that we want a program that can evaluate expressions entered by the user, and suppose that the expressions can contain variables, in addition to operators, numbers, and parentheses. For this to make sense, we need some way of assigning values to variables. When a variable is used in an expression, we need to retrieve the variable's value. A symbol table can be used to store the data that we need. The keys for the symbol table are variable names. The value associated with a key is the value of that variable, which is of type double.\u00aeten. Here is a schematic view of this first-stage understanding of the computer Asynchronous Events: Polling Loops and Interrupts The CPU spends almost all of its time fetching instructions from memory and executing them. However, the CPU and main memory are only two out of many components in areal computer system, A complete system contains other devices such as: A hard disk for storing programs and data files.(Nere is a device driver, which consists of software that the CPU executes when it has to deal with the device. Installing a new device on a system generally has two steps: plugging the device physically into the computer, and installing the device driver software. Without the device driver, the actual physical device would be us",
  "page229": "Using SQL in Java- When you work with an SQL database in a Java application, you issue SQL statements to the database via the Java Database Connectivity (JDBC) API. Whether the SQL was written b\u00aey hand and embedded in the Java code or generated on the fly by Java code, you use the JDBC API to bind arguments when preparing query parameters, executing the query, scrolling through the query result, retrieving values from the result se\u00aet, and so on. These are low-level data access tasks; as application engineers, we're more interested in the business problem that requires this data access. What we'd really like to write is code that saves and retrieves instances of our classes, relieving us of this lowlevel drudgery. Because these data access tasks are often s\u00aeo tedious, we have to ask, are the relational data model and (especially) SQL the right choices for persistence in objectoriented applications? We answer this question unequivocally: yes! There are many reasons why SQL databases dominate the computing industry relational database management systems are the only proven generic data management technology, and they're almost always a requir\u00aeement in Java projects.  Note that we aren't claiming that relational technology is always the best solution. There are many data management requirements that warrant a completely different approach. For example, internet-scale distributed systems (web search engines, con\u00aetent distribution networks, peer-to-peer sharing, instant messaging) have to deal with exceptional transaction volumes. Many of these systems don't require that after a data update completes, all processes see the same updated data (strong transactional consistency). Users might be happy with weak consistency; after an update, there might be a window of inconsistency before all processes see the updated data. Some scientific applications work with enormous but very specialized datasets. Such systems and their unique challenges typically require equally unique and often custom-made persistence solutions. Generic data management tools such as ACID-compliant transactional SQL databases, JDBC, and Hibernate would play only a minor role. The Item class of the CaveatEmptor domain model, for example, shouldn't have any runtime dependency on any Java Persistence or Hibernate API. Furthermore: JPA doesn't require that any\u00aespecial superclasses or interfaces be inherited or implemented by persistent classes. Nor are any special classes used to implement attributes and associations. (Of course, the option to use both techniques is always there.) You can reuse\u00aepersistent classes outside the context of persistence, in unit tests or in the presentation layer, for example. You can create instances in any runtime environment with the regular Java new operator, preserving testability and reusability. In a system with transparent persistence, instances of entities aren't aware of the underlying d\u00aeata store; they need not even be aware that they're being persisted or retrieved. JPA externalizes persistence concerns to a generic persistence manager API. Hence, most of your code, and certainly your complex business logic, doesn't have to concern itself with the current state of a domain model entity instance in a single thread of execution.",
  "page230": "ORM and JPA- In a nutshell, object/relational mapping is the automated (and transparent) persistence of objects in a Java application to the tables in an SQL database, using metadata that descri\u00aebes the mapping between the classes of the application and the schema of the SQL database. In essence, ORM works by transforming (reversibly) data from one representation to another. Before we move on, you need to understand what Hibernate\u00aecan't do for you. A supposed advantage of ORM is that it shields developers from messy SQL. This view holds that object-oriented developers can't be expected to understand SQL or relational databases well and that they find SQL somehow offensive. On the contrary, we believe that Java developers must have a sufficient level of fa\u00aemiliarity with and appreciation of relational modeling and SQL in order to work with Hibernate. ORM is an advanced technique used by developers who have already done it the hard way. To use Hibernate effectively, you must be able to view and interpret the SQL statements it issues and understand their performance implications Let's look at some of the benefits of Hibernate: Productivity H\u00aeibernate eliminates much of the grunt work (more than you'd expect) and lets you concentrate on the business problem. No matter which application-development strategy you prefer top-down, starting with a domain model, or bottom-up, starting with an existing database schema\u00aeHibernate, used together with the appropriate tools, will significantly reduce development time. Maintainability Automated ORM with Hibernate reduces lines of code (LOC), making the system more understandable and easier to refactor. Hibernate provides a buffer between the domain model and the SQL schema, insulating each model from minor changes to the other. Performance Although hand-coded persistence might be faster in the same sense that assembly code can be faster than Java code, automated solutions like Hibernate allow the use of many optimizations at all times. One example of this is efficient and easily tunable caching in the application tier. This means developers can spend more energy hand-optimizing the few remaining real bottlenecks instead of prematurely optimizing everything. Vendor independence Hibernate can help mitigate some of the risks associated with vendor lock-in.",
  "page231": "The Hibernate approach to persistence was well received by Java developers, and the standard Java Persistence API was designed along similar lines. JPA became a key part of the simplifications\u00aeintroduced in recent EJB and Java EE specifications. We should be clear up front that neither Java Persistence nor Hibernate are limited to the Java EE environment; they're general-purpose solutions to the persistence problem that any\u00aetype of Java (or Groovy, or Scala) application can use.  The JPA specification defines the following: A facility for specifying mapping metadata how persistent classes and their properties relate to the database schema. JPA relies heavily on Java annotations in domain model classes, but you can also write mappings in XML files. APIs for\u00aeperforming basic CRUD operations on instances of persistent classes, most prominently javax.persistence.EntityManager to store and load data. A language and APIs for specifying queries that refer to classes and properties of classes. This language is the Java Persistence Query Language (JPQL) and looks similar to SQL. The standardized API allows for programmatic creation of criteria queries w\u00aeithout string manipulation. How the persistence engine interacts with transactional instances to perform dirty checking, association fetching, and other optimization functions. The latest JPA specification covers some basic caching strategies. Hibernate implements JPA and suppo\u00aerts all the standardized mappings, queries, and programming interfaces.",
  "page232": "With object persistence, individual objects can outlive their application process, be saved to a data store, and be re-created later. The object/relational mismatch comes into play when the data\u00aestore is an SQL-based relational database management system. For instance, a network of objects can't be saved to a database table; it must be disassembled and persisted to columns of portable SQL data types. A good solution for this\u00aeproblem is object/relational mapping (ORM). ORM isn't a silver bullet for all persistence tasks; its job is to relieve the developer of 95% of object persistence work, such as writing complex SQL statements with many table joins and copying values from JDBC result sets to objects or graphs of objects. A full-featured ORM middleware\u00aesolution may provide database portability, certain optimization techniques like caching, and other viable functions that aren't easy to hand-code in a limited time with SQL and JDBC. Better solutions than ORM might exist someday. We (and many others) may have to rethink everything we know about data management systems and their languages, persistence API standards, and application integ\u00aeration. But the evolution of today's systems into true relational database systems with seamless object-oriented integration remains pure speculation. We can't wait, and there is no sign that any of these issues will improve soon (a multibillion-dollar industry isn\u00ae019t very agile). ORM is the best solution currently available, and it's a timesaver for developers facing the object/relational mismatch every day.",
  "page233": "Hibernate is an ambitious project that aims to provide a complete solution to the problem of managing persistent data in Java. Today, Hibernate is not only an ORM service, but also a collection\u00aeof data management tools extending well beyond ORM. The Hibernate project suite includes the following: Hibernate ORM Hibernate ORM consists of a core, a base service for persistence with SQL databases, and a native proprietary API. Hibern\u00aeate ORM is the foundation for several of the other projects and is the oldest Hibernate project. You can use Hibernate ORM on its own, independent of any framework or any particular runtime environment with all JDKs. It works in every Java EE/J2EE application server, in Swing applications, in a simple servlet container, and so on. As long\u00aeas you can configure a data source for Hibernate, it works. Hibernate EntityManager This is Hibernate's implementation of the standard Java Persistence APIs, an optional module you can stack on top of Hibernate ORM. You can fall back to Hibernate when a plain Hibernate interface or even a JDBC Connection is needed. Hibernate's native features are a superset of the JPA persistence fe\u00aeatures in every respect. Hibernate Validator Hibernate provides the reference implementation of the Bean Validation (JSR 303) specification. Independent of other Hibernate projects, it provides declarative validation for your domain model (or any other) classes. Hibernat\u00aee Envers Envers is dedicated to audit logging and keeping multiple versions of data in your SQL database. This helps you add data history and audit trails to your application, similar to version control systems you might already be familiar with such as Subversion and Git. Hibernate Search Hibernate Search keeps an index of your domain model data up to date in an Apache Lucene database. It lets you query this database with a powerful and naturally integrated API. Many projects use Hibernate Search in addition to Hibernate ORM, adding full-text search capabilities. If you have a free text search form in your application's user interface, and you want happy users, work with Hibernate Search. Hibernate Search isn't covered in this book; you can find more information in Hibernate Search in Action by Emmanuel Bernard (Bernard, 2008). Hibernate OGM The most recent Hibernate project is the object/grid mapper.",
  "page234": "The \"Hello World\" example in the previous chapter introduced you to Hibernate; certainly, it isn't useful for understanding the requirements of real-world applications with comple\u00aex data models. For the rest of the book, we use a much more sophisticated example application CaveatEmptor, an online auction system to demonstrate Hibernate and Java Persistence. (Caveat emptor means \"Let the buyer beware\".) We\u00ae2019ll start our discussion of the application by introducing a layered application architecture. Then, you'll learn how to identify the business entities of a problem domain. You'll create a conceptual model of these entities and their attributes, called a domain model, and you'll implement it in Java by creating persistent c\u00aelasses. We'll spend some time exploring exactly what these Java classes should look like and where they fit within a typical layered application architecture. We'll also look at the persistence capabilities of the classes and how this aspect influences the design and implementation. We'll add Bean Validation, which helps to automatically verify the integrity of the domain model\u00aedata not only for persistent information but all business logic.  We'll then explore mapping metadata options the ways you tell Hibernate how your persistent classes and their properties relate to database tables and columns. This can be as simple as adding annotations d\u00aeirectly in the Java source code of the classes or writing XML documents that you eventually deploy along with the compiled Java classes that Hibernate accesses at runtime. After reading this chapter, you'll know how to design the persistent parts of your domain model in complex real-world projects, and what mapping metadata option you'll primarily prefer and use. Let's start with the example application.",
  "page235": "The example CaveatEmptor application- The CaveatEmptor example is an online auction application that demonstrates ORM techniques and Hibernate functionality. You can download the source code for\u00aethe application from www.jpwh.org. We won't pay much attention to the user interface in this book (it could be web based or a rich client); we'll concentrate instead on the data access code. When a design decision about data acce\u00aess code that has consequences for the user interface has to be made, we'll naturally consider both.  In order to understand the design issues involved in ORM, let's pretend the CaveatEmptor application doesn't yet exist and that you're building it from scratch. Let's start by looking at the architecture. A layered\u00aearchitecture- With any nontrivial application, it usually makes sense to organize classes by concern. Persistence is one concern; others include presentation, workflow, and business logic. A typical object-oriented architecture includes layers of code that represent the concerns. A layered architecture defines interfaces between code that implements the various concerns, allowing changes to\u00aebe made to the way one concern is implemented without significant disruption to code in the other layers. Layering determines the kinds of inter-layer dependencies that occur. The rules are as follows: Layers communicate from top to bottom. A layer is dependent only on the int\u00aeerface of the layer directly below it. Each layer is unaware of any other layers except for the layer just below it.",
  "page236": "The CaveatEmptor domain model The CaveatEmptor site auctions many different kinds of items, from electronic equipment to airline tickets. Auctions proceed according to the English auction strate\u00aegy: users continue to place bids on an item until the bid period for that item expires, and the highest bidder wins.  In any store, goods are categorized by type and grouped with similar goods into sections and onto shelves. The auction ca\u00aetalog requires some kind of hierarchy of item categories so that a buyer can browse these categories or arbitrarily search by category and item attributes. Lists of items appear in the category browser and search result screens. Selecting an item from a list takes the buyer to an item-detail view where an item may have images attached to i\u00aet.  An auction consists of a sequence of bids, and one is the winning bid. User details include name, address, and billing information.  The result of this analysis, the high-level overview of the domain model, is shown in figure 3.3. Let's briefly discuss some interesting features of this model.  Each item can be auctioned only once, so you don't need to make Item distinct from\u00aeany auction entities. Instead, you have a single auction item entity named Item. Thus, Bid is associated directly with Item. You model the Address information of a User as a separate class, a User may have three addresses, for home, billing, and shipping. You do allow the user\u00aeto have many BillingDetails. Subclasses of an abstract class represent the various billing strategies (allowing future extension).  The application may nest a Category inside another Category, and so on. A recursive association, from the Category entity to itself, expresses this relationship. Note that a single Category may have multiple child categories but at most one parent. Each Item belongs to at least one Category.  This representation isn't the complete domain model but only classes for which you need persistence capabilities. You'd like to store and load instances of Category, Item, User, and so on. We have simplified this high-level overview a little; we may introduce additional classes later or make minor modifications to them when needed for more complex examples.",
  "page237": "Implementing the domain model- You'll start with an issue that any implementation must deal with: the separation of concerns. The domain model implementation is usually a central, organizin\u00aeg component; it's reused heavily whenever you implement new application functionality. For this reason, you should be prepared to go to some lengths to ensure that concerns other than business aspects don't leak into the domain mo\u00aedel implementation. When concerns such as persistence, transaction management, or authorization start to appear in the domain model classes, this is an example of leakage of concerns. The domain model implementation is such an important piece of code that it shouldn't depend on orthogonal Java APIs. For example, code in the domain mod\u00aeel shouldn't perform JNDI lookups or call the database via the JDBC API, not directly and not through an intermediate abstraction. This allows you to reuse the domain model classes virtually anywhere: The presentation layer can access instances and attributes of domain model entities when rendering views. The controller components in the business layer can also access the state of domai\u00aen model entities and call methods of the entities to execute business logic. The persistence layer can load and store instances of domain model entities from and to the database, preserving their state.",
  "page238": "Most important, preventing leakage of concerns makes it easy to unit-test the domain model without the need for a particular runtime environment or container, or the need for mocking any service\u00aedependencies. You can write unit tests that verify the correct behavior of your domain model classes without any special test harness. (We aren't talking about testing \"load from the database\" and \"store in the database\u00ae\" aspects, but \"calculate the shipping cost and tax\" behavior.) The Java EE standard solves the problem of leaky concerns with metadata, as annotations within your code or externalized as XML descriptors. This approach allows the runtime container to implement some predefined cross-cutting concerns security, concurrency, pe\u00aersistence, transactions, and remoteness in a generic way, by intercepting calls to application components.  Hibernate isn't a Java EE runtime environment, and it's not an application server. It's an implementation of just one specification under the Java EE umbrella JPA and a solution for just one of these concerns: persistence. JPA defines the entity class as the primary pr\u00aeogramming artifact. This programming model enables transparent persistence, and a JPA provider such as Hibernate also offers automated persistence. Transparent and automated persistence- We use transparent to mean a complete separation of concerns between the persistent classes\u00aeof the domain model and the persistence layer. The persistent classes are unaware of and have no dependency on the persistence mechanism. We use automatic to refer to a persistence solution (your annotated domain, the layer, and mechanism) that relieves you of handling low-level mechanical details, such as writing most SQL statements and working with the JDBC API.",
  "page239": "The Item class of the CaveatEmptor domain model, for example, shouldn't have any runtime dependency on any Java Persistence or Hibernate API. Furthermore: JPA doesn't require that any\u00aespecial superclasses or interfaces be inherited or implemented by persistent classes. Nor are any special classes used to implement attributes and associations. (Of course, the option to use both techniques is always there.) You can reuse\u00aepersistent classes outside the context of persistence, in unit tests or in the presentation layer, for example. You can create instances in any runtime environment with the regular Java new operator, preserving testability and reusability. In a system with transparent persistence, instances of entities aren't aware of the underlying d\u00aeata store; they need not even be aware that they're being persisted or retrieved. JPA externalizes persistence concerns to a generic persistence manager API. Hence, most of your code, and certainly your complex business logic, doesn't have to concern itself with the current state of a domain model entity instance in a single thread of execution.",
  "page240": "We regard transparency as a requirement because it makes an application easier to build and maintain. Transparent persistence should be one of the primary goals of any ORM solution. Clearly, no\u00aeautomated persistence solution is completely transparent: Every automated persistence layer, including JPA and Hibernate, imposes some requirements on the persistent classes. For example, JPA requires that collectionvalued attributes be typ\u00aeed to an interface such as java.util.Set or java.util.List and not to an actual implementation such as java.util.HashSet (this is a good practice anyway). Or, a JPA entity class has to have a special attribute, called the database identifier (which is also less of a restriction but usually convenient).  You now know why the persistence me\u00aechanism should have minimal impact on how you implement a domain model, and that transparent and automated persistence are required. Our preferred programming model to archive this is POJO.",
  "page241": "Around 10 years ago, many developers started talking about POJO, a back-to-basics approach that essentially revives JavaBeans, a component model for UI development, and reapplies it to the other\u00aelayers of a system. Several revisions of the EJB and JPA specifications brought us new lightweight entities, and it would be appropriate to call them persistence-capable JavaBeans. Java engineers often use all these terms as synonyms for t\u00aehe same basic design approach.  You shouldn't be too concerned about what terms we use in this book; the ultimate goal is to apply the persistence aspect as transparently as possible to Java classes. Almost any Java class can be persistence-capable if you follow some simple practices. Let's see how this looks in code.  Writing\u00aepersistence-capable classes Working with fine-grained and rich domain models is a major Hibernate objective. This is a reason we work with POJOs. In general, using fine-grained objects means more classes than tables. A persistence-capable plain-old Java class declares attributes, which represent state, and business methods, which define behavior. Some attributes represent associations to ot\u00aeher persistence-capable classes.",
  "page242": "JPA doesn't require that persistent classes implement java.io.Serializable. But when instances are stored in an HttpSession or passed by value using RMI, serialization is necessary. Althoug\u00aeh this might not occur in your application, the class will be serializable without any additional work, and there are no downsides to declaring that. (We aren't going to declare it on every example, assuming that you know when it will\u00aebe necessary.)  The class can be abstract and, if needed, extend a non-persistent class or implement an interface. It must be a top-level class, not nested within another class. The persistence-capable class and any of its methods can't be final (a requirement of the JPA specification).  Unlike the JavaBeans specification, which req\u00aeuires no specific constructor, Hibernate (and JPA) require a constructor with no arguments for every persistent class. Alternatively, you might not write a constructor at all; Hibernate will then use the Java default constructor. Hibernate calls classes using the Java reflection API on such a no argument constructor to create instances. The constructor may not be public, but it has to be at l\u00aeeast package-visible if Hibernate will use runtime-generated proxies for performance optimization. Also, consider the requirements of other specifications: the EJB standard requires public visibility on session bean constructors, just like the JavaServer Faces (JSF) specificati\u00aeon requires for its managed beans. There are other situations when you'd want a public constructor to create an \"empty\" state: for example, query-by-example building",
  "page243": "The properties of the POJO implement the attributes of the business entities for example, the username of User. You usually implement properties as private or protected member fields, together w\u00aeith public or protected property accessor methods: for each field a method for retrieving its value and a method for setting the value. These methods are known as the getter and setter, respectively. The example POJO in listing 3.1 declares\u00aegetter and setter methods for the username property.  The JavaBean specification defines the guidelines for naming accessor methods; this allows generic tools like Hibernate to easily discover and manipulate property values. A getter method name begins with get, followed by the name of the property (the first letter in uppercase); a sett\u00aeer method name begins with set and similarly is followed by the name of the property. You may begin getter methods for Boolean properties with is instead of get. Hibernate doesn't require accessor methods. You can choose how the state of an instance of your persistent classes should be persisted. Hibernate will either directly access fields or call accessor methods. Your class design is\u00aen't disturbed much by these considerations. You can make some accessor methods non-public or completely remove them then configure Hibernate to rely on field access for these properties.",
  "page244": "Should property fields and accessor methods be private, protected, or package visible? Typically, you want to discourage direct access to the internal state of your class, so you don't mak\u00aee attribute fields public. If you make fields or methods private, you're effectively declaring that nobody should ever access them; only you're allowed to do that (or a service like Hibernate). This is a definitive statement. Ther\u00aee are often good reasons for someone to access your \"private\" internals usually to fix one of your bugs and you only make people angry if they have to fall back to reflection access in an emergency. Instead, you might assume or know that the engineer who comes after you has access to your code and knows what they're doing. \u00aeThe protected visibility then is a more reasonable default. You're forbidding direct public access, indicating that this particular member detail is internal, but allowing access by subclasses if need be. You trust the engineer who creates the subclass. Package visibility is rude: you're forcing someone to create code in the same package to access member fields and methods; this is\u00aeextra work for no good reason. Most important, these recommendations for visibility are relevant for environments without security policies and a runtime SecurityManager. If you have to keep your internal code private, make it private.",
  "page245": "Implementing POJO associations- You'll now see how to associate and create different kinds of relationships between objects: one-to-many, many-to-one, and bidirectional relationships. We\u00ae019ll look at the scaffolding code needed to create these associations, how to simplify relationship management, and how to enforce the integrity of these relationships. Shouldn't bids on an item be stored in a list? The first reaction i\u00aes often to preserve the order of elements as they're entered by users, because this may also be the order in which you will show them later. Certainly, in an auction application there has to be some defined order in which the user sees bids for an item for example, highest bid first or newest bid last. You might even work with a java.\u00aeutil.List in your user interface code to sort and display bids of an item. That doesn't mean this display order should be durable; data integrity isn't affected by the order in which bids are displayed. You need to store the amount of each bid, so you can find the highest bid, and you need to store a timestamp for each bid when it's created, so you can find the newest bid. When\u00aein doubt, keep your system flexible and sort the data when it's retrieved from the datastore (in a query) and/or shown to the user (in Java code), not when it's stored.",
  "page246": "The addBid() method not only reduces the lines of code when dealing with Item and Bid instances, but also enforces the cardinality of the association. You avoid errors that arise from leaving ou\u00aet one of the two required actions. You should always provide this kind of grouping of operations for associations, if possible. If you compare this with the relational model of foreign keys in an SQL database, you can easily see how a netwo\u00aerk and pointer model complicates a simple operation: instead of a declarative constraint, you need procedural code to guarantee data integrity.  Because you want addBid() to be the only externally visible mutator method for the bids of an item (possibly in addition to a removeBid() method), you can make the Item#setBids() method private o\u00aer drop it and configure Hibernate to directly access fields for persistence. Consider making the Bid#setItem() method package-visible, for the same reason.  The Item#getBids() getter method still returns a modifiable collection, so clients can use it to make changes that aren't reflected on the inverse side. Bids added directly to the collection wouldn't have a reference to an item\u00aean inconsistent state, according to your database constraints. To prevent this, you can wrap the internal collection before returning it from the getter method, with Collections.unmodifiableCollection(c) and Collections.unmodifiableSet(s). The client then gets an exception if\u00aeit tries to modify the collection; you therefore force every modification to go through the relationship management method that guarantees integrity. Note that in this case you'll have to configure Hibernate for field access, because the collection",
  "page247": "There are several problems with this approach. First, Hibernate can't call this constructor. You need to add a no-argument constructor for Hibernate, and it needs to be at least package-vis\u00aeible. Furthermore, because there is no setItem() method, Hibernate would have to be configured to access the item field directly. This means the field can't be final, so the class isn't guaranteed to be immutable.  In the example\u00aes in this book, we'll sometimes write scaffolding methods such as the Item#addBid() shown earlier, or we may have additional constructors for required values. It's up to you how many convenience methods and layers you want to wrap around the persistent association properties and/or fields, but we recommend being consistent and ap\u00aeplying the same strategy to all your domain model classes. For the sake of readability, we won't always show convenience methods, special constructors, and other such scaffolding in future code samples and assume you'll add them according to your own taste and requirements.  You now have seen domain model classes, how to represent their attributes, and the relationships between the\u00aem. Next, we'll increase the level of abstraction, adding metadata to the domain model implementation and declaring aspects such as validation and persistence rules.",
  "page248": "Domain model metadata- Metadata is data about data, so domain model metadata is information about your domain model. For example, when you use the Java reflection API to discover the names of cl\u00aeasses of your domain model or the names of their attributes, you're accessing domain model metadata. ORM tools also require metadata, to specify the mapping between classes and tables, properties and columns, associations and foreign\u00aekeys, Java types and SQL types, and so on. This object/relational mapping metadata governs the transformation between the different type systems and relationship representations in objectoriented and SQL systems. JPA has a metadata API, which you can call to obtain details about the persistence aspects of your domain model, such as the nam\u00aees of persistent entities and attributes. First, it's your job as an engineer to create and maintain this information. JPA standardizes two metadata options: annotations in Java code and externalized XML descriptor files. Hibernate has some extensions for native functionality, also available as annotations and/or XML descriptors. Usually we prefer either annotations or XML files as the\u00aeprimary source of mapping metadata. After reading this section, you'll have the background information to make an educated decision for your own project.",
  "page249": "We'll also discuss Bean Validation (JSR 303) and how it provides declarative validation for your domain model (or any other) classes. The reference implementation of this specification is t\u00aehe Hibernate Validator project. Most engineers today prefer Java annotations as the primary mechanism for declaring metadata. Applying Bean Validation rules Most applications contain a multitude of data-integrity checks. You've seen wh\u00aeat happens when you violate one of the simplest data-integrity constraints: you get a NullPointerException when you expect a value to be available. Other examples are a string-valued property that shouldn't be empty (remember, an empty string isn't null), a string that has to match a particular regular expression pattern, and a n\u00aeumber or date value that must be within a certain range.  These business rules affect every layer of an application: The user interface code has to display detailed and localized error messages. The business and persistence layers must check input values received from the client before passing them to the datastore. The SQL database has to be the final validator, ultimately guaranteeing the\u00aeintegrity of durable data.",
  "page250": "The idea behind Bean Validation is that declaring rules such as \"This property can't be null\" or \"This number has to be in the given range\" is much easier and less error\u00ae-prone than writing if-then-else procedures repeatedly. Furthermore, declaring these rules on the central component of your application, the domain model implementation, enables integrity checks in every layer of the system. The rules are t\u00aehen available to the presentation and persistence layers. And if you consider how dataintegrity constraints affect not only your Java application code but also your SQL database schema which is a collection of integrity rules you might think of Bean Validation constraints as additional ORM metadata You add two more attributes the name of a\u00aen item and the auctionEnd date when an auction concludes. Both are typical candidates for additional constraints: you want to guarantee that the name is always present and human readable (one-character item names don't make much sense), but it shouldn't be too long your SQL database will be most efficient with variable-length strings up to 255 characters, and your user interface als\u00aeo has some constraints on visible label space. The ending time of an auction obviously should be in the future. If you don't provide an error message, a default message will be used. Messages can be keys to external properties files, for internationalization.",
  "page251": "The validation engine will access the fields directly if you annotate the fields. If you prefer calls through accessor methods, annotate the getter method with validation constraints, not the se\u00aetter. Then constraints are part of the class's API and included in its Javadoc, making the domain model implementation easier to understand. Note that this is independent from access by the JPA provider; that is, Hibernate Validator ma\u00aey call accessor methods, whereas Hibernate ORM may call fields directly.  Bean Validation isn't limited to the built-in annotations; you can create your own constraints and annotations. With a custom constraint, you can even use class-level annotations and validate several attribute values at the same time on an instance of the class\u00ae. The following test code shows how you can manually check the integrity of an Item instance. We're not going to explain this code in detail but offer it for you to explore. You'll rarely write this kind of validation code; most of the time, this aspect is automatically handled by your user interface and persistence framework. It's therefore important to look for Bean Validatio\u00aen integration when selecting a UI framework. JSF version 2 and newer automatically integrates with Bean Validation, for example.  Hibernate, as required from any JPA provider, also automatically integrates with Hibernate Validator if the libraries are available on the classpat\u00aeh and offers the following features:",
  "page252": "You don't have to manually validate instances before passing them to Hibernate for storage. Hibernate recognizes constraints on persistent domain model classes and triggers validatio\u00aen before database insert or update operations. When validation fails, Hibernate throws a ConstraintViolationException, containing the failure details, to the code calling persistence-management operations. The Hibernate toolset for a\u00aeutomatic SQL schema generation understands many constraints and generates SQL DDL-equivalent constraints for you. For example, an @NotNull annotation translates into an SQL NOT NULL constraint, and an @Size(n) rule defines the number of characters in a VARCHAR(n)-typed column. You can control this behavior of Hibernate with the <validation\u00ae-mode> element in your persistence.xml configuration file. The default mode is AUTO, so Hibernate will only validate if it finds a Bean Validation provider (such as Hibernate Validator) on the classpath of the running application. With mode CALLBACK, validation will always occur, and you'll get a deployment error if you forget to bundle a Bean Validation provider. The NONE mode disables\u00aeautomatic validation by the JPA provider. You'll see Bean Validation annotations again later in this book; you'll also find them in the example code bundles. At this point we could write much more about Hibernate Validator, but we'd only repeat what is already av\u00aeailable in the project's excellent reference guide. Have a look, and find out more about features such as validation groups and the metadata API for discovery of constraints.  The Java Persistence and Bean Validation standards embrace annotations aggressively. The expert groups have been aware of the advantages of XML deployment descriptors in certain situations, especially for configuration metadata that changes with each deployment.",
  "page253": "This part is all about actual ORM, from classes and properties to tables and columns. Chapter 4 starts with regular class and property mappings and explains how you can map fine-grained Java dom\u00aeain models. Next, in chapter 5, you'll see how to map basic properties and embeddable components, and how to control mapping between Java and SQL types. In chapter 6, you'll map inheritance hierarchies of entities to the database\u00aeusing four basic inheritance-mapping strategies; you'll also map polymorphic associations. Chapter 7 is all about mapping collections and entity associations: you map persistent collections, collections of basic and embeddable types, and simple many-to-one and one-to-many entity associations. Chapter 8 dives deeper with advanced entit\u00aey association mappings like mapping one-to-one entity associations, one-to-many mapping options, and many-to-many and ternary entity relationships. Finally, you'll find chapter 9 most interesting if you need to introduce Hibernate in an existing application, or if you have to work with legacy database schemas and handwritten SQL. We'll also talk about customized SQL DDL for schema g\u00aeeneration in this chapter. After reading this part of the book, you'll be ready to create even the most complex mappings quickly and with the right strategy. You'll understand how the problem of inheritance mapping can be solved and how to map collections and associa\u00aetions. You'll also be able to tune and customize Hibernate for integration with any existing database schema or application.",
  "page254": "Fine-grained domain models- A major objective of Hibernate is support for fine-grained and rich domain models. It's one reason we work with POJOs. In crude terms, fine-grained means more cl\u00aeasses than tables.  For example, a user may have a home address in your domain model. In the database, you may have a single USERS table with the columns HOME_STREET, HOME_CITY, and HOME_ZIPCODE. (Remember the problem of SQL types we discu\u00aessed in section 1.2.1?)  In the domain model, you could use the same approach, representing the address as three string-valued properties of the User class. But it's much better to model this using an Address class, where User has a homeAddress property. This domain model achieves improved cohesion and greater code reuse, and it\u00ae9s more understandable than SQL with inflexible type systems. JPA emphasizes the usefulness of fine-grained classes for implementing type safety and behavior. For example, many people model an email address as a string-valued property of User. A more sophisticated approach is to define an EmailAddress class, which adds higher-level semantics and behavior it may provide a prepareMail() method (it\u00aeshouldn't have a sendMail() method, because you don't want your domain model classes to depend on the mail subsystem).  This granularity problem leads us to a distinction of central importance in ORM. In Java, all classes are of equal standing all instances have thei\u00aer own identity and life cycle. When you introduce persistence, some instances may not have their own identity and life cycle but depend on others. Let's walk through an example",
  "page255": "Distinguishing entities and value types- You may find it helpful to add stereotype (a UML extensibility mechanism) information to your UML class diagrams so you can immediately recognize entitie\u00aes and value types. This practice also forces you to think about this distinction for all your classes, which is a first step to an optimal mapping and well-performing persistence layer. The Item and User classes are obvious entities. They\u00aeeach have their own identity, their instances have references from many other instances (shared references), and they have independent lifespans.  Marking the Address as a value type is also easy: a single User instance references a particular Address instance. You know this because the association has been created as a composition, where\u00aethe User instance has been made fully responsible for the life cycle of the referenced Address instance. Therefore, Address instances can't be referenced by anyone else and don't need their own identity.  The Bid class could be a problem. In object-oriented modeling, this is marked as a composition (the association between Item and Bid with the diamond). Thus, an Item is the owner\u00aeof its Bid instances and holds a collection of references. At first, this seems reasonable, because bids in an auction system are useless when the item they were made for is gone.  But what if a future extension of the domain model requires a User#bids collection, containing\u00aeall bids made by a particular User? Right now, the association between Bid and User is unidirectional; a Bid has a bidder reference. What if this was bidirectional?",
  "page256": "Mapping entities with identity- Mapping entities with identity requires you to understand Java identity and equality before we can walk through an entity class example and its mapping. After tha\u00aet, we'll be able to dig in deeper and select a primary key, configure key generators, and finally go through identifier generator strategies. First, it's vital to understand the difference between Java object identity and object e\u00aequality before we discuss terms like database identity and the way JPA manages identity. Understanding Java identity and equality- Java developers understand the difference between Java object identity and equality. Object identity (=) is a notion defined by the Java virtual machine. Two references are identical if they point to the same\u00aememory location. On the other hand, object equality is a notion defined by a class's equals() method, sometimes also referred to as equivalence. Equivalence means two different (non-identical) instances have the same value the same state. Two different instances of String are equal if they represent the same sequence of characters, even though each has its own location in the memory spa\u00aece of the virtual machine. (If you're a Java guru, we acknowledge that String is a special case. Assume we used a different class to make the same point.) Persistence complicates this picture. With object/relational persistence, a persistent instance is an in-memory repres\u00aeentation of a particular row (or rows) of a database table (or tables). Along with Java identity and equality, we define database identity",
  "page257": "Objects are identical if they occupy the same memory location in the JVM. This can be checked with the a = b operator. This concept is known as object identity. Objects are equal if they\u00aehave the same state, as defined by the a.equals(Object b) method. Classes that don't explicitly override this method inherit the implementation defined by java.lang.Object, which compares object identity with ==. This concept is known\u00aeas object equality. Objects stored in a relational database are identical if they share the same table and primary key value. This concept, mapped into the Java space, is known as database identity. We now need to look at how database identity relates to object identity and how to express database identity in the mapping metadata. A\u00aes an example, you'll map an entity of a domain model. Every entity class has to have an @Id property; it's how JPA exposes database identity to the application. We don't show the identifier property in our diagrams; we assume that each entity class has one. In our examples, we always name the identifier property id. This is a good practice for your own project; use the same id\u00aeentifier property name for all your domain model entity classes. If you specify nothing else, this property maps to a primary key column named ID of the ITEM table in your database schema.",
  "page258": "Hibernate will use the field to access the identifier property value when loading and storing items, not getter or setter methods. Because @Id is on a field, Hibernate will now enable every fiel\u00aed of the class as a persistent property by default. The rule in JPA is this: if @Id is on a field, the JPA provider will access fields of the class directly and consider all fields part of the persistent state by default. You'll see ho\u00aew to override this later in this chapter in our experience, field access is often the best choice, because it gives you more freedom for accessor method design.  Should you have a (public) getter method for the identifier property? Well, the application often uses database identifiers as a convenient handle to a particular instance, even\u00aeoutside the persistence layer. For example, it's common for web applications to display the results of a search screen to the user as a list of summaries. When the user selects a particular element, the application may need to retrieve the selected item, and it's common to use a lookup by identifier for this purpose you've probably already used identifiers this way, even in app\u00aelications that rely on JDBC. Should you have a setter method? Primary key values never change, so you shouldn't allow modification of the identifier property value. Hibernate won't update a primary key column, and you shouldn't expose a public identifier setter m\u00aeethod on an entity.  The Java type of the identifier property, java.lang.Long in the previous example, depends on the primary key column type of the ITEM table and how key values are produced. This brings us to the @GeneratedValue annotation and primary keys in general.",
  "page259": "Selecting a primary key-Must primary keys be immutable? The relational model defines that a candidate key must be unique and irreducible (no subset of the key attributes has the uniqueness prope\u00aerty). Beyond that, picking a candidate key as the primary key is a matter of taste. But Hibernate expects a candidate key to be immutable when used as the primary key. Hibernate doesn't support updating primary key values with an API;\u00aeif you try to work around this requirement, you'll run into problems with Hibernate's caching and dirty-checking engine. If your database schema relies on updatable primary keys (and maybe uses ON UPDATE CASCADE foreign key constraints), you must change the schema before it will work with Hibernate. The database identifier of an\u00aeentity is mapped to some table primary key, so let's first get some background on primary keys without worrying about mappings. Take a step back and think about how you identify entities.  A candidate key is a column or set of columns that you could use to identify a particular row in a table. To become the primary key, a candidate key must satisfy the following requirements: The value\u00aeof any candidate key column is never null. You can't identify something with data that is unknown, and there are no nulls in the relational model. Some SQL products allow defining (composite) primary keys with nullable columns, so you must be careful. The value of the can\u00aedidate key column(s) is a unique value for any row. The value of the candidate key column(s) never changes; it's immutable.",
  "page260": "If a table has only one identifying attribute, it becomes, by definition, the primary key. But several columns or combinations of columns may satisfy these properties for a particular table; you\u00aechoose between candidate keys to decide the best primary key for the table. You should declare candidate keys not chosen as the primary key as unique keys in the database if their value is indeed unique (but maybe not immutable).  Many le\u00aegacy SQL data models use natural primary keys. A natural key is a key with business meaning: an attribute or combination of attributes that is unique by virtue of its business semantics. Examples of natural keys are the US Social Security Number and Australian Tax File Number. Distinguishing natural keys is simple: if a candidate key attri\u00aebute has meaning outside the database context, it's a natural key, regardless of whether it's automatically generated. Think about the application users: if they refer to a key attribute when talking about and working with the application, it's a natural key: \"Can you send me the pictures of item #123-abc?\" Experience has shown that natural primary keys usually cause\u00aeproblems in the end. A good primary key must be unique, immutable, and never null. Few entity attributes satisfy these requirements, and some that do can't be efficiently indexed by SQL databases (although this is an implementation detail and shouldn't be the decidin\u00aeg factor for or against a particular key). In addition, you should make certain that a candidate key definition never changes throughout the lifetime of the database. Changing the value (or even definition) of a primary key, and all foreign keys that refer to it, is a frustrating task. Expect your database schema to survive decades, even if your application won't.",
  "page261": "Furthermore, you can often only find natural candidate keys by combining several columns in a composite natural key. These composite keys, although certainly appropriate for some schema artifact\u00aes (like a link table in a many-to-many relationship), potentially make maintenance, ad hoc queries, and schema evolution much more difficult. We talk about composite keys later in the book, For these reasons, we strongly recommend that you\u00aeadd synthetic identifiers, also called surrogate keys. Surrogate keys have no business meaning they have unique values generated by the database or application. Application users ideally don't see or refer to these key values; they're part of the system internals. Introducing a surrogate key column is also appropriate in the comm\u00aeon situation when there are no candidate keys. In other words, (almost) every table in your schema should have a dedicated surrogate primary key column with only this purpose. There are a number of well-known approaches to generating surrogate key values. The aforementioned @GeneratedValue annotation is how you configure this. Configuring key generators- The @Id annotation is required to ma\u00aerk the identifier property of an entity class. Without the @GeneratedValue next to it, the JPA provider assumes that you'll take care of creating and assigning an identifier value before you save an instance. We call this an application-assigned identifier. Assigning an en\u00aetity identifier manually is necessary when you're dealing with a legacy database and/or natural primary keys. We have more to say about this kind of mapping in a dedicated section,",
  "page262": "Usually you want the system to generate a primary key value when you save an entity instance, so you write the GeneratedValue annotation next to @Id. JPA standardizes several value-generation st\u00aerategies with the javax.persistence.GenerationType enum, which you select with @GeneratedValue(strategy): GenerationType.AUTO Hibernate picks an appropriate strategy, asking the SQL dialect of your configured database what is best. This is\u00aeequivalent to @GeneratedValue() without any settings. GenerationType.SEQUENCE Hibernate expects (and creates, if you use the tools) a sequence named HIBERNATE_SEQUENCE in your database. The sequence will be called separately before every INSERT, producing sequential numeric values. GenerationType.IDENTITY Hibernate expects (and creates in\u00aetable DDL) a special auto-incremented primary key column that automatically generates a numeric value on INSERT, in the database. GenerationType.TABLE Hibernate will use an extra table in your database schema that holds the next numeric primary key value, one row for each entity class. This table will be read and updated accordingly, before INSERTs. The default table name is HIBERNATE_SEQUEN\u00aeCES with columns SEQUENCE_NAME and SEQUENCE_NEXT_HI_VALUE. (The internal implementation uses a more complex but efficient hi/lo generation algorithm; more on this later.)",
  "page263": "JPA has two built-in annotations you can use to configure named generators: @javax persistence.SequenceGenerator and @javax.persistence.TableGenerator. With these annotations, you can create a n\u00aeamed generator with your own sequence and table names. As usual with JPA annotations, you can unfortunately only use them at the top of a (maybe otherwise empty) class, and not in a package-info.java file. For this reason, and because the\u00aeJPA annotations don't give us access to the full Hibernate feature set, we prefer an alternative: the native @org.hibernate.annotations .GenericGenerator annotation. It supports all Hibernate identifier generator strategies and their configuration details. Unlike the rather limited JPA annotations, you can use the Hibernat\u00aee annotation in a package-info.java file, typically in the same package as your domain model classes. The next listing shows a recommended configuration. This Hibernate-specific generator configuration has the following advantages: The enhanced-sequence B strategy produces sequential numeric values. If your SQL dialect supports sequences, Hibernate will use an actual database sequence. If yo\u00aeur DBMS doesn't support native sequences, Hibernate will manage and use an extra \"sequence table,\" simulating the behavior of a sequence. This gives you real portability: the generator can always be called before performing an SQL INSERT, unlike, for\u00aeexample, auto-increment identity columns, which produce a value on INSERT that has to be returned to the application afterward.",
  "page264": "You can configure the sequence_name C. Hibernate will either use an existing sequence or create it when you generate the SQL schema automatically. If your DBMS doesn't support sequences, th\u00aeis will be the special \"sequence table\" name. You can start with an initial_value D that gives you room for test data. For example, when your integration test runs, Hibernate will make any new data insertions from test code with i\u00aedentifier values greater than 1000. Any test data you want to import before the test can use numbers 1 to 999, and you can refer to the stable identifier values in your tests: \"Load item with id 123 and run some tests on it.\" This is applied when Hibernate generates the SQL schema and sequence; it's a DDL option. You can sha\u00aere the same database sequence among all your domain model classes. There is no harm in specifying @GeneratedValue(generator \"ID_GENERATOR\") in all your entity classes. It doesn't matter if primary key values aren't contiguous for a particular entity, as long as they're unique within one table. If you're worried about contention, because the sequence has to be called prio\u00aer to every INSERT, we discuss a variation of this generator configuration later,",
  "page265": "Finally, you use java.lang.Long as the type of the identifier property in the entity class, which maps perfectly to a numeric database sequence generator. You could also use a long primitive. Th\u00aee main difference is what someItem.getId() returns on a new item that hasn't been stored in the database: either null or 0. If you want to test whether an item is new, a null check is probably easier to understand for someone else read\u00aeing your code. You shouldn't use another integral type such as int or short for identifiers. Although they will work for a while (perhaps even years), as your database size grows, you may be limited by their range. An Integer would work for almost two months if you generated a new identifier each millisecond with no gaps, and a\u00aeLong would last for about 300 million years. Although recommended for most applications, the enhanced-sequence strategy as shown in listing is just one of the strategies built into Hibernate. Identifier generator strategies Following is a list of all available Hibernate identifier generator strategies, their options, and our usage recommendations. If you don't want to read the whole lis\u00aet now, enable GenerationType.AUTO and check what Hibernate defaults to for your database dialect. It's most likely sequence or identity a good but maybe not the most efficient or portable choice. If you require consistent portable behavior, and identifier values to be avai\u00aelable before INSERTs, use enhanced-sequence, as shown in the previous section. This is a portable, flexible, and modern strategy, also offering various optimizers for large datasets.",
  "page266": "We also show the relationship between each standard JPA strategy and its native Hibernate equivalent. Hibernate has been growing organically, so there are now two sets of mappings between standa\u00aerd and native strategies; we call them Old and New in the list. You can switch this mapping with the hibernate.id.new_generator_mappings setting in your persistence.xml file. The default is true; hence the New mapping. Software doesn\\u00aeu2019t age quite as well as wine: native Automatically selects other strategies, such as sequence or identity, depending on the configured SQL dialect. You have to look at the Javadoc (or even the source) of the SQL dialect you configured in persistence.xml. Equivalent to JPA GenerationType.AUTO with the Old mapping. sequence Uses a\u00aenative database sequence named HIBERNATE_SEQUENCE. The sequence is called before each INSERT of a new row. You can customize the sequence name and provide additional DDL settings; see the Javadoc for the class org.hibernate.id.SequenceGenerator and its parent. enhanced-sequence Uses a native database sequence when supported; otherwise falls back to an extra database table with a single\u00aecolumn and row, emulating a sequence. Defaults to name HIBERNATE_SEQUENCE. Always calls the database \"sequence\" before an INSERT, providing the same behavior independently of whether the DBMS supports real sequences. Supports an org.hibernate .id.enhanced.Optimizer to\u00aeavoid hitting the database before each INSERT; defaults to no optimization and fetching a new value for each INSERT. You can find more examples in chapter 20. For all parameters, see the Javadoc for the class org.hibernate.id.enhanced.SequenceStyleGenerator. Equivalent to JPA GenerationType.SEQUENCE and GenerationType.AUTO with the New mapping enabled, most likely your best option of the built-in strategies.",
  "page267": "seqhilo Uses a native database sequence named HIBERNATE_SEQUENCE, optimizing calls before INSERT by combining hi/lo values. If the hi value retrieved from the sequence is 1, the next 9 ins\u00aeertions will be made with key values 11, 12, 13, \u2026, 19. Then the sequence is called again to obtain the next hi value (2 or higher), and the procedure repeats with 21, 22, 23, and so on. You can configure the maximum lo value (9\u00aeis the default) with the max_lo parameter. Unfortunately, due to a quirk in Hibernate's code, you can not configure this strategy in @GenericGenerator. The only way to use it is with JPA GenerationType.SEQUENCE and the Old mapping. You can configure it with the standard JPA @SequenceGenerator annotation on a (maybe otherwi\u00aese empty) class. See the Javadoc for the class org.hibernate.id.SequenceHiLoGenerator and its parent for more information. Consider using enhanced-sequence instead, with an optimizer hilo Uses an extra table named HIBERNATE_UNIQUE_KEY with the same algorithm as the seqhilo strategy. The table has a single column and row, holding the next value of the sequence. The default maximum lo val\u00aeue is 32767, so you most likely want to configure it with the max_lo parameter. See the Javadoc for the class org.hibernate.id.TableHiLoGenerator for more information. We don't recommend this legacy strategy; use enhanced-sequence instead with an optimizer.",
  "page268": "enhanced-table Uses an extra table named HIBERNATE_SEQUENCES, with one row by default representing the sequence, storing the next value. This value is selected and updated when an identifier val\u00aeue has to be generated. You can configure this generator to use multiple rows instead: one for each generator; see the Javadoc for org.hibernate.id.enhanced.TableGenerator. Equivalent to JPA GenerationType.TABLE with the New mapping enabled\u00ae. Replaces the outdated but similar org.hibernate.id.MultipleHiLoPerTableGenerator, which is the Old mapping for JPA GenerationType.TABLE. identity Supports IDENTITY and auto-increment columns in DB2, MySQL, MS SQL Server, and Sybase. The identifier value for the primary key column will be generated on INSERT of a row. Has no option\u00aes. Unfortunately, due to a quirk in Hibernate's code, you can not configure this strategy in @GenericGenerator. The only way to use it is with JPA GenerationType.IDENTITY and the Old or New mapping, making it the default for GenerationType.IDENTITY. increment At Hibernate startup, reads the maximum (numeric) primary key column value of each entity's table and increments the v\u00aealue by one each time a new row is inserted. Especially efficient if a non-clustered Hibernate application has exclusive access to the database; but don't use it in any other scenario.",
  "page269": "select Hibernate won't generate a key value or include the primary key column in an INSERT statement. Hibernate expects the DBMS to assign a (default in schema or by trigger) value to\u00aethe column on insertion. Hibernate then retrieves the primary key column with a SELECT query after insertion. Required parameter is key, naming the database identifier property (such as id) for the SELECT. This strategy isn't very eff\u00aeicient and should only be used with old JDBC drivers that can't return generated keys directly. uuid2 Produces a unique 128-bit UUID in the application layer. Useful when you need globally unique identifiers across databases (say, you merge data from several distinct production databases in batch runs every night into an archive). The\u00aeUUID can be encoded either as a java.lang.String, a byte[16], or a java .util.UUID property in your entity class. Replaces the legacy uuid and uuid .hex strategies. You configure it with an org.hibernate.id.UUIDGenerationStrategy; see the Javadoc for the class org.hibernate.id.UUIDGenerator for more details. guid Uses a globally unique identifier produced by the database, with an SQL f\u00aeunction available on Oracle, Ingres, MS SQL Server, and MySQL. Hibernate calls the database function before an INSERT. Maps to a java.lang.String identifier property. If you need full control over identifier generation, configure the strategy of @GenericGenerator with the\u00aefully qualified name of a class that implements the org.hibernate.id.IdentityGenerator interface.",
  "page270": "QUOTING SQL IDENTIFIERS- From time to time, especially in legacy databases, you'll encounter identifiers with strange characters or whitespace, or wish to force case sensitivity. Or, as in\u00aethe previous example, the automatic mapping of a class or property would require a table or column name that is a reserved keyword.  Hibernate 5 knows the reserved keywords of your DBMS through the configured database dialect. Hibern\u00aeate 5 can automatically put quotes around such strings when generating SQL. You can enable this automatic quoting with hibernate.auto_quote _keyword true in your persistence unit configuration. If you're using an older version of Hibernate, or you find that the dialect's information is incomplete, you must still apply quotes on n\u00aeames manually in your mappings if there is a conflict with a keyword.  If you quote a table or column name in your mapping with backticks, Hibernate always quotes this identifier in the generated SQL. This still works in latest versions of Hibernate, but JPA 2.0 standardized this functionality as delimited identifiers with double quotes. If you have to quote all SQL identifiers, creat\u00aee an orm.xml file and add the setting <delimited-identifiers/> to its <persistence-unit-defaults> section, as shown in listing 3.8. Hibernate then enforces quoted identifiers everywhere.  You should consider renaming tables or columns with reserved keyword names whenever possi\u00aeble. Ad hoc SQL queries are difficult to write in an SQL console if you have to quote and escape everything properly by hand.  Next, you'll see how Hibernate can help when you encounter organizations with strict conventions for database table and column names.To summarize, our recommendations on identifier generator strategies are as follows:appings. Entity-mapping options- You've now mapped a persistent class with @Entity, using defaults for all other settings, such as the mapped SQL table name. The following section explores some classlevel options",
  "page271": "Entities are the coarser-grained classes of your system. Their instances have an independent life cycle and their own identity, and many other instances can reference them. Value types, o\u00aen the other hand, are dependent on a particular entity class. A value type instance is bound to its owning entity instance, and only one entity instance can reference it it has no individual identity. We looked at Java identity, object equ\u00aeality, and database identity, and at what makes good primary keys. You learned which generators for primary key values Hibernate provides out of the box, and how to use and extend this identifier system. We discussed some useful class mapping options, such as naming strategies and dynamic SQL generation. After spending the previous chapte\u00aer almost exclusively on entities and the respective class- and identity-mapping options, we now focus on value types in their various forms. We split value types into two categories: basic value-typed classes that come with the JDK, such as String, Date, primitives, and their wrappers; and developerdefined value-typed classes, such as Address and MonetaryAmount in CaveatEmptor. I\u00aen this chapter, we first map persistent properties with JDK types and learn the basic mapping annotations. You see how to work with various aspects of properties: overriding defaults, customizing access, and generated values. You also see how SQL is used with derived properties\u00aeand transformed column values. We wrap up basic properties with temporal properties and mapping enumerations. We then discuss custom value-typed classes and map them as embeddable components. You learn how classes relate to the database schema and make your classes embeddable, while allowing for overriding embedded attributes. We complete embeddable componentC level is much more efficient if there are fewer statements. How can Hibernate create an UPDATE statement on startup? After all, the columns to be updated aren't known at this time. The answer is that the generated SQL statement updates all columns, and if the value of a particular column isn't modified,:",
  "page272": "This configuration by exception approach means you don't have to annotate a property to make it persistent; you only have to configure the mapping in an exceptional case. Several anno\u00aetations are available in JPA to customize and control basic property map. Overriding basic property defaults- You might not want all properties of an entity class to be persistent. For example, although it makes sense to have a persistent I\u00aetem#initialPrice property, an Item#totalPriceIncludingTax property shouldn't be persistent if you only compute and use its value at runtime, and hence shouldn't be stored in the database. To exclude a property, mark the field or the getter method of the property with the @javax.persistence.Transient annotation or use the Ja\u00aeva transient keyword. The transient keyword usually only excludes fields for Java serialization, but it's also recognized by JPA providers.  We'll come back to the placement of the annotation on fields or getter methods in a moment. Let's assume as we have before that Hibernate will access fields directly because @Id has been placed on a field. Therefore, all other JPA a\u00aend Hibernate mapping annotations are also on fields. We have to admit that this annotation isn't very useful. It only has two parameters: the one shown here, optional, marks the property as not optional at the Java object level. By default, all persistent properties\u00aeare nullable and optional; an Item may have an unknown initialPrice. Mapping the initialPrice property as non-optional makes sense if you have a NOT NULL constraint on the INITIALPRICE column in your SQL schema. If Hibernate is generating the SQL schema, it will include a NOT NULL constraint automatically for non-optional properties. Mrty. Otherwise, if you annotate the class of the property as @Embeddable, or you map the property itself as @Embedded, Hibernate maps the property as an embedded component of the owning class. We discuss embedding of components later in this chapter, with the Address and MonetaryAmount embeddable classes of CaveatEmptor.",
  "page273": "The Column annotation has a few other parameters, most of which control SQL-level details such as catalog and schema names. They're rarely needed, and we only show them throughout this book\u00aewhen necessary.  Property annotations aren't always on fields, and you may not want Hibernate to access fields directly. Customizing property access- The persistence engine accesses the properties of a class either directly through\u00aefields or indirectly through getter and setter methods. An annotated entity inherits the default from the position of the mandatory @Id annotation. For example, if you've declared @Id on a field, not a getter method, all other mapping annotations for that entity are expected on fields. Annotations are never on the setter methods.  Th\u00aee default access strategy isn't only applicable to a single entity class. Any @Embedded class inherits the default or explicitly declared access strategy of its owning root entity class. We cover embedded components later in this chapter. Furthermore, Hibernate accesses any @MappedSuperclass properties with the default or explicitly declared access strategy of the mapped enti\u00aety class. Inheritance is the topic of chapter 6.  The JPA specification offers the @Access annotation for overriding the default behavior, with the parameters AccessType.FIELD and AccessType.PROPERTY. If you set @Access on the class/entity level, Hibernate accesses all propert\u00aeies of the class according to the selected strategy. You then set any other mapping annotations, including the @Id, on either fields or getter methods, respectively. Now, when you store an Item and forget to set a value on the initialPrice field, Hibernate will complain with an exception before hitting the database with an SQL statement. Hibernate knows that a valuennotation, the @Column annotation, and earlier with the Bean Validation @NotNull annotation in section 3.3.2. All have the same effect on the JPA provider: Hibernate does a null check when saving and generates a NOT NULL constraint in the database schema. We recommend the Bean Validation @NotNull annotation so you can manually validate an Item instance and/or have your user interface code in the presentation layer execute validation checks automatically.  The @Column annotation can also override the mapping of the property name to the database column:",
  "page274": "Hibernate reads your Java domain model classes and mapping metadata and generates schema DDL statements. You can export them into a text file or execute them directly on your database whenever y\u00aeou run integration tests. Because schema languages are mostly vendor-specific, every option you put in your mapping metadata has the potential to bind the metadata to a particular database product keep this in mind when using schema feature\u00aes. Hibernate creates the basic schema for your tables and constraints automatically; it even creates sequences, depending on the identifier generator you select. But there are some schema artifacts Hibernate can't and won't create automatically. These include all kinds of highly vendor-specific performance options and any other\u00aeartifacts that are relevant only for the physical storage of data (tablespaces, for example). Besides these physical concerns, your DBA will often supply custom additional schema statements to improve the generated schema. DBAs should get involved early and verify the automatically generated schema from Hibernate. Never go into production with an unchecked automatically generated schema. If y\u00aeour development process allows, changes made by the DBA can flow back into your Java systems, for you to add to mapping metadata. In many projects, the mapping metadata can contain all the necessary schema changes from a DBA. Then Hibernate can generate the final production sch\u00aeema during the regular build by including all comments, constraints, indexes, and so on. In the following sections, we show you how to customize the generated schema and how to add auxiliary database schema artifacts (we call them objects sometimes; we don't mean Java objects here). We discuss custom data types, additional integrity rules, indexes, and how yous you can start a project top-down. There is no existing database schema and maybe not even any data your application is completely new. Many developers like to let Hibernate automatically generate the scripts for a database schema. You'll probably also let Hibernate deploy the schema on the test database on your development machine or your continuous build systems for integration testing. ",
  "page275": "This property defines when the create and drop scripts should be executed. Your custom SQL scripts will contain CREATE DOMAIN statements, which must be executed before the tables using these dom\u00aeains are created. With these settings, the schema generator runs the create script first before reading your ORM metadata (annotations, XML files) and creating the tables. The drop script executes after Hibernate drops the tables, giving yo\u00aeu a chance to clean up anything you created. Other options are metadata (ignore custom script sources) and script (only use a custom script source; ignore ORM metadata in annotations and XML files). D This is the location of the custom SQL script for creation of the schema. The path is (a) the location of the script resource on the classpa\u00aeth; (b) the location of the script as a file:// URL; or, if neither (a) nor (b) matches, (c) the absolute or relative file path on the local file system. This example uses (a). E This is the custom SQL script for dropping the schema. F This load script runs after the tables have been created. We've mentioned that DDL is usually highly vendor-specific. If your application has to support s\u00aeeveral database dialects, you may need several sets of create/drop/load scripts to customize the schema for each database dialect. You can solve this with several persistence unit configurations in the persistence.xml file. Alternatively, Hibernate has its own proprietary conf\u00aeiguration for schema customization in an hbm.xml mapping file, You can hook the following three types of custom SQL scripts into Hibernates schema-generation process: The create script executes when the schema is generated. A custom create script can run before, after, or instead of Hibernates automatically generated scripts. In other words, you ction process is actually standardized; you configure it with JPA properties in persistence.xml for a persistence unit. By default, Hibernate expects one SQL statement per line in scripts. This switches to the more convenient multiline extractor. SQL statements in scripts are terminated with semicolon. You can write your own org.hibernate.tool.hbm2ddl.ImportSqlCommandExtractor implementation if you want to handle the SQL script in a different way.",
  "page276": "Table constraints An integrity rule that applies to several columns or several rows is a table constraint. A typical declarative table constraint is UNIQUE: all rows are checked for duplicate va\u00aelues (for example, each user must have a distinct email address). A rule affecting only a single row but multiple columns is \"the auction end time has to be after the auction start time.\" Database constraints If a rule applies to\u00aemore than one table, it has database scope. You should already be familiar with the most common database constraint, the foreign key. This rule guarantees the integrity of references between rows, usually in separate tables, but not always (self-referencing foreign key constraints aren't uncommon). Other database constraints involving\u00aeseveral tables aren't uncommon: for example, a bid can only be stored if the auction end time of the referenced item hasn't been reached. Most (if not all) SQL database management systems support these kinds of constraints and the most important options of each. In addition to simple keywords such as NOT NULL and UNIQUE, you can usually also declare more complex rules with the CHEC\u00aeK constraint that applies an arbitrary SQL expression. Still, integrity constraints are one of the weak areas in the SQL standard, and solutions from vendors can differ significantly. Furthermore, non-declarative and procedural constraints are possible with database triggers t\u00aehat intercept data-modification operations. A trigger can then implement the constraint procedure directly or call an existing stored procedure. Integrity constraints can be checked immediately when a data-modification statement is executed, or the check can be deferred until the end of a transaction. The violation response in SQL databases . The advantages of declarative rules are fewer possible errors in code and a chance for the DBMS to optimize data access. In SQL databases, we identify four kinds of rules: Domain constraints A domain is (loosely speaking, and in the database world) a data type in a database. Hence, a domain constraint defines the range of possible values a particular data type can handle. For example, an INTEGER data type is usable for integer values.",
  "page277": "This constraint restricts valid username values to a maximum length of 15 characters, and the string can't begin with admin to avoid confusion. You can call any SQL functions supported by y\u00aeour DBMS; the column Definition is always passed through into the exported schema. Note that you have a choice: creating and using a domain or adding a single column constraint has the same effect. Domains are usually easier to maintain an\u00aed avoid duplicating. At the time of writing, Hibernate doesn't support its proprietary annotation @org.hibernate.annotations.Check on individual properties; you use it for table level constraints. Hibernate appends table constraints to the generated CREATE TABLE statement, which can contain arbitrary SQL expressions. You can impleme\u00aent multirow table constraints with expressions that are more complex. You may need a sub select in the expression to do this, which may not be supported by your DBMS. But there are some common multirow table constraints, like UNIQUE, that you can add directly in the mappings. You've already seen the @Column(unique true | false) option in the previous section. Now all pairs of USERNAME an\u00aed EMAIL must be unique, for all rows in the USERS table. If you don't provide a name for the constraint here, UNQ_USERNAME_EMAIL an automatically generated and probably ugly name is used. The last kinds of constraints we discuss are database-wide rules that span several t\u00aeables. Hibernate passes on database constraint violations in error exceptions; check whether the exception in your transaction has a cause, somewhere in the exception chain, of type org.hibernate.exception.ConstraintViolationException. This exception can provide more information about the error, such as the name of the failed database constraint. The SQL standard incond is a UNIQUE column constraint; users can't have duplicate email addresses. At the time of writing, there was unfortunately no way to customize the name of this single-column unique constraint in Hibernate; it will get an ugly auto-generated name in your schema. Last, the column Definition refers to the domain you've added with your custom create script. This definition is an SQL fragment, exported into your schema directly, so be careful with database-specific SQL",
  "page278": "This statement declares the foreign key constraint for the ITEM_ID column in the BID table, referencing the primary key column of the ITEM table. You can customize the name of the constraint wit\u00aeh the foreign Key option in an @JoinColumn mapping A foreign Key attribute is also supported in @PrimaryKeyJoinColumn, @MapKeyJoinColumn, @JoinTable, @CollectionTable, and @AssociationOverride mappings. The @ForeignKey annotation has some\u00aerarely needed options we haven't shown: You can write your own foreign Key Definition, an SQL fragment such as FOREIGN KEY ([column]) REFERENCES [table]([column]) ON UPDATE [action]. Hibernate will use this SQL fragment instead of the provider-generated fragment, it can be in the SQL dialect supported by your DBMS. The Constraint Mod\u00aee setting is useful if you want to disable foreign key generation completely, with the value NO_CONSTRAINT. You can then write the foreign key constraint yourself with an ALTER TABLE statement, probably in a load script as we've shown. Naming constraints properly is not only good practice, but also helps significantly when you have to read exception messages. This completes our discussi\u00aeon of database integrity rules. Next, we look at some optimization you might want to include in your schema for performance reasons. Indexes are a key feature when optimizing the performance of a database application. The query optimizer in a DBMS can use indexes to avoid exce\u00aessive scans of the data tables. Because they're relevant only in the physical implementation of a database, indexes aren't part of the SQL standard, and the DDL and available indexing options are product specific. You can, however, embed the most common schema artifacts for typical indexes in mapping metadata A user can only make bids until an auctiodonomatically for all foreign key columns in association mappings. If you check the schema produced by Hibernate, you'll notice that these constraints also have automatically generated database identifiers names that aren't easy to read and that make debugging more difficult. You see this kind of statement in the generated schema",
  "page279": "If you encounter a USERS table in a legacy schema, it's likely that USERNAME is the primary key. In this case, you have no surrogate identifier that Hibernate generates automatically. Inste\u00aead, your application has to assign the identifier value when saving an instance of the User class Hibernate calls the protected no-argument constructor when it loads a User from the database and then assigns the username field value directl\u00aey. When you instantiate a User, call the public constructor with a username. If you don't declare an identifier generator on the @Id property, Hibernate expects the application to take care of the primary key value assignment. Composite (natural) primary keys require a bit more work Suppose the primary key of the USERS table is a com\u00aeposite of the two columns USERNAME and DEPARTMENTNR. You write a separate composite identifier class that declares just the key properties and call this class User Id This class has to be @Embeddable and Serializable any type used as an identifier type in JPA must be Serializable. C You don't have to mark the properties of the composite key as @NotNull; their database columns are automat\u00aeically NOT NULL when embedded as the primary key of an entity. D The JPA specification requires a public no-argument constructor for an embeddable identifier class. Hibernate accepts protected visibility. E The only public constructor should have the key values as arguments. F\u00aeYou have to override the equals() and hashCode() methods with the same semantics the composite key has in your database. In this case, this is a straightforward comparison of the username and department values. Many queries in Caveat Emptor will probably involve the username of a User entity. You can speed up these queries by creating an index for the column of thisingJPA supports natural and composite primary and foreign keys.",
  "page280": "We've already shown the @SecondaryTable annotation in an inheritance mapping in section 6.5. It helped to break out properties of a particular subclass into a separate table. This generic f\u00aeunctionality has more uses but be aware that a properly designed system should have, simplified, more classes than tables. Suppose that in a legacy schema, you aren't keeping a user's billing address information with the other us\u00aeer details in the USERS main entity table, but in a separate table. Figure 9.5 shows this schema. The user's home address is stored in the columns STREET, ZIPCODE, and CITY of the USERS table. The user's billing address is stored in the BILLING_ADDRESS table, which has the primary key column USER_ID, which is also a foreign key c\u00aeonstraint referencing the ID primary key of the USERS table The User class has two properties of embedded type: home Address and billing Address. The first is a regular embedded mapping, and the Address class is @Embeddable. Just as in section 5.2.3, you can use the @AttributeOverrides annotation to override the mapping of embedded properties. Then, @Column maps the individual properties to\u00aethe BILLING_ADDRESS table, with its table option. Remember that an @AttributeOverride replaces all mapping information for a property: any annotations on the Address fields are ignored if you override. Therefore, you have to specify nullability and length again in the @Column\u00aeoverride. We've shown you a secondary table mapping example with an embeddable property. Of course, you could also break out simple basic properties like the username string in a secondary table. Keep in mind that reading and maintaining these mappings can be a problem, though; you should only map legacy unchangeable schemas with secondary tables. A foreign key consmplse @JoinColumns instead as in the previous section. Fortunately, it's often straightforward to clean up such a schema by refactoring foreign keys to reference primary keys if you can make changes to the database that don't disturb other applications sharing the data. This completes our discussion of natural, composite, and foreign key-related problems you may have to deal with when you try to map a legacy schema. Let's move on to another interesting special strategy",
  "page281": "strategies for runtime data management. These strategies are crucial to the performance and correct behavior of your applications. In this chapter, we discuss the life cycle of entity instances\u00aehow an instance becomes persistent, and how it stops being considered persistent and the method calls and management operations that trigger these transitions. The JPA Entity Manager is your primary interface for accessing data. Before we\u00aelook at the API, let's start with entity instances, their life cycle, and the events that trigger a change of state. Although some of the material may be formal, a solid understanding of the persistence life cycle is essential. Because JPA is a transparent persistence mechanism classes are unaware of their own persistence capability\u00aeit's possible to write application logic that's unaware whether the data it operates on represents persistent state or temporary state that exists only in memory. The application shouldn't necessarily need to care that an instance is persistent when invoking its methods. You can, for example, invoke the Item #calculateTotalPrice() business method without having to consider pers\u00aeistence at all (for example, in a unit test). Any application with persistent state must interact with the persistence service whenever it needs to propagate state held in memory to the database (or vice versa). In other words, you have to call the Java Persistence interfaces\u00aeto store and load data. When interacting with the persistence mechanism that way, the application must concern itself with the state and life cycle of an entity instance with respect to persistence. We refer to this as the persistence life cycle: the states an entity instance goes through during its life. We also use the term unit of work: a set of (posmosd store objects efficiently.",
  "page282": "A persistent entity instance has a representation in the database. It's stored in the database or it will be stored when the unit of work completes. It's an instance with a database id\u00aeentity, as defined in section 4.2; its database identifier is set to the primary key value of the database representation. The application may have created instances and then made them persistent by calling Entity Manager #persist(). There\u00aemay be instances that became persistent when the application created a reference to the object from another persistent instance that the JPA provider already manages. A persistent entity instance may be an instance retrieved from the database by execution of a query, by an identifier lookup, or by navigating the object graph starting fro\u00aem another persistent instance. Persistent instances are always associated with a persistence context. You see more about this in a moment You can delete a persistent entity instance from the database in several ways: For example, you can remove it with Entity Manager #remove(). It may also become available for deletion if you remove a reference to it from a mapped collection with orphan remo\u00aeval enabled. An entity instance is then in the removed state: the provider will delete it at the end of a unit of work. You should discard any references you may hold to it in the application after you finish working with it for example, after you've rendered the removal\u00aeconfirmation screen your users see Think of the persistence context as a service that remembers all the modifications and state changes you made to data in a particular unit of work (this is somewhat simplified, but it's a good starting point). We now dissect all these terms: entity states, persistence contexts, and managed scope. You're probably more accs ttransient to persistent state, to become managed, requires either a call to the Entity Manager #persist() method or the creation of a reference from an already-persistent instance and enabled cascading of ",
  "page283": "The persistence context acts as a first-level cache; it remembers all entity instances you've handled in a particular unit of work. For example, if you ask Hibernate to load an entity insta\u00aence using a primary key value (a lookup by identifier), Hibernate can first check the current unit of work in the persistence context. If Hibernate finds the entity instance in the persistence context, no database hit occurs this is a repea\u00aetable read for an application. Consecutive em.find(Item. Class, ITEM_ID) calls with the same persistence context will yield the same result. This cache also affects results of arbitrary queries, executed for example with the javax.persistence.Query API. Hibernate reads the SQL result set of a query and transforms it into entity instances.\u00aeThis process first tries to resolve every entity instance in the persistence context by identifier lookup. Only if an instance with the same identifier value can't be found in the current persistence context does Hibernate read the rest of the data from the result-set row. Hibernate ignores any potentially newer data in the result set, due to read-committed transaction isolation at the\u00aedatabase level, if the entity instance is already present in the persistence context. The persistence context cache is always on it can't be turned off. It ensures the following The persistence layer isn't vulnerable to stack overflows in the case of circular referen\u00aeces in an object graph. There can never be conflicting representations of the same database row at the end of a unit of work. The provider can safely write all changes made to an entity instance to the database. Likewise, changes made in a particular persistence context are always immediately visible to all other code executed inside that unit of work and its persi anatements (all part of the Data Modification Language [DML]) This flushing procedure may also occur at other times. For example, Hibernate may synchronize with the database before execution of a query. This ensures that queries are aware of changes made earlier during the unit of work.",
  "page284": "Hibernate joins the in-progress system transaction when an Entity Manager is created and waits for the transaction to commit. When Hibernate is notified (by the JTA engine) of the commit, it per\u00aeforms dirty checking of the persistence context and synchronizes with the database. You can also force dirty checking synchronization manually by calling EntityManager#flush() at any time during a transaction. You decide the scope of the p\u00aeersistence context by choosing when to close() the Entity Manager. You have to close the persistence context at some point, so always place the close() call in a finally block. How long should the persistence context be open? Let's assume for the following examples that you're writing a server, and each client request will be pr\u00aeocessed with one persistence context and system transaction in a multithreaded environment. If you're familiar with servlets, imagine the code in listing 10.1 embedded in a servlet's service() method. Within this unit of work, you access the Entity Manager to load and store data. A new transient Item is instantiated as usual. Of course, you may also instantiate it before creating th\u00aee EntityManager. A call to persist() makes the transient instance of Item persistent. It's now managed by and associated with the current persistence context. To store the Item instance in the database, Hibernate has to execute an SQL INSERT statement. When the transactio\u00aen of this unit of work commits, Hibernate flushes the persistence context, and the INSERT occurs at that time. Hibernate may even batch the INSERT at the JDBC level with other statements. When you call persist(), only the identifier value of the Item is assigned. Alternatively, if your identifier generator isn't pre-insert, the INSERT statement willommu2019t obtain a JDBC Connection from the pool until SQL statements have to be executed. You can create and close an Entity Manager without hitting the database. Hibernate executes SQL statements when you look up or query data and when it flushes changes detected by the persistence context to the database. Hibernate joins the in-progress system transaction when an Entity Manager is created and waits for the transaction to commit. When Hibernate is notified (by the JTA engine) of the commit, it performs dirty checking of the persistence context and synchronizes with the database.",
  "page285": "You can modify the Item instance, and the persistence context will detect these changes and record them in the database automatically. When Hibernate flushes the persistence context during commi\u00aet, it executes the necessary SQL DML statements to synchronize the changes with the database. Hibernate propagates state changes to the database as late as possible, toward the end of the transaction. DML statements usually create locks in\u00aethe database that are held until the transaction completes, so Hibernate keeps the lock duration in the database as short as possible. Hibernate writes the new Item#name to the database with an SQL UPDATE. By default, Hibernate includes all columns of the mapped ITEM table in the SQL UPDATE statement, updating unchanged columns to their o\u00aeld values. Hence, Hibernate can generate these basic SQL statements at startup, not at runtime. If you want to include only modified (or non-nullable for INSERT) columns in SQL statements, you can enable dynamic SQL generation as discussed in section 4.3.2. Hibernate detects the changed name by comparing the Item with a snapshot copy it took before, when the Item was loaded from the database\u00ae. If your Item is different from the snapshot, an UPDATE is necessary. This snapshot in the persistence context consumes memory. Dirty checking with snapshots can also be time consuming, because Hibernate has to compare all instances in the persistence context with their snapsh\u00aeot during flushing. It's better (but not required) to fully initialize the Item instance before managing it with a persistence context. The SQL INSERT statement contains the values that were held by the instance at the point when persist() was called. If you don't set the name of the Item before making it persistent, a NOT NULL constraint may bscuvalue can be found, find() returns null. The find() operation always hits the database if there was no hit for the",
  "page286": "As soon as you call any method such as Item#getName() on the proxy, a SELECT is executed to fully initialize the placeholder. The exception to this rule is a mapped database identifier getter me\u00aethod, such as getId(). A proxy may look like the real thing, but it's only a placeholder carrying the identifier value of the entity instance it represents. If the database record no longer exists when the proxy is initialized, an Enti\u00aetyNotFoundException is thrown. Note that the exception can be thrown when Item#getName() is called. E Hibernate has a convenient static initialize() method that loads the proxy's data. F After the persistence context is closed, item is in detached state. If you don't initialize the proxy while the persistence context is still ope\u00aen, you get a LazyInitializationException if you access the proxy. You can't load data on demand once the persistence context is closed. The solution is simple: load the data before you close the persistence context. We'll have much more to say about proxies, lazy loading, and on-demand fetching in chapter 12. If you want to remove the state of an entity instance from the database,\u00aeyou have to make it transient. To make an entity instance transient and delete its database representation, call the remove() method on the EntityManager If you call find(), Hibernate executes a SELECT to load the Item. If you call getReference(), Hibernate attempts to avoid t\u00aehe SELECT and returns a proxy. C Calling remove() queues the entity instance for deletion when the unit of work completes; it's now in removed state. If remove() is called on a proxy, Hibernate executes a SELECT to load the data. You may want to customize how Hibernate detects dirty state, using an extension point. Set the property hibernate.entity_dirtiness_st is9t return null, unlike find(). C JPA offers PersistenceUnitUtil helper methods such as isLoaded() to detect whether you're working with an uninitialized proxy.",
  "page287": "Java Persistence also offers bulk operations that translate into direct SQL DELETE statements without life cycle interceptors in the application. We'll discuss these operations in section 2\u00ae0.1. Let's say you load an entity instance from the database and work with the data. For some reason, you know that another application or maybe another thread of your application has updated the underlying row in the database. Next,\u00aewe'll see how to refresh the data held in memory. After you load the entity instance, you realize (how isn't important) that someone else changed the data in the database. Calling refresh() causes Hibernate to execute a SELECT to read and marshal a whole result set, overwriting changes you already made to the persistent instance\u00aein application memory. If the database row no longer exists (someone deleted it), Hibernate throws an EntityNotFoundException on refresh(). Most applications don't have to manually refresh in-memory state; concurrent modifications are typically resolved at transaction commit time. The best use case for refreshing is with an extended persistence context, which might span several request\u00ae/response cycles and/or system transactions. While you wait for user input with an open persistence context, data gets stale, and selective refreshing may be required depending on the duration of the conversation and the dialogue between the user and the system. Refreshing can\u00aebe useful to undo changes made in memory during a conversation, if the user cancels the dialogue. We'll have more to say about refreshing in a conversation in section. An entity instance must be fully initialized during life cycle transitions. You may have life cycle callback methods or an entity listener enabled (see section 13.2), and the inalu",
  "page288": "The persistence context is a cache of persistent instances. Every entity instance in persistent state is associated with the persistence context. Many Hibernate users who ignore this simple fac\u00aet run into an OutOfMemoryException. This is typically the case when you load thousands of entity instances in a unit of work but never intend to modify them. Hibernate still has to create a snapshot of each instance in the persistence conte\u00aext cache, which can lead to memory exhaustion. (Obviously, you should execute a bulk data operation if you modify thousands of rows we'll get back to this kind of unit of work in section 20.1.) The persistence context cache never shrinks automatically. Keep the size of your persistence context to the necessary minimum. Often, many pe\u00aersistent instances in your context are there by accident for example, because you needed only a few items but queried for many. Extremely large graphs can have a serious performance impact and require significant memory for state snapshots. Check that your queries return only data you need, and consider the following ways to control Hibernate's caching behavior. You can call EntityManag\u00aeer#detach(i) to evict a persistent instance manually from the persistence context. You can call EntityManager#clear() to detach all persistent entity instances, leaving you with an empty persistence context. The native Session API has some extra operations you might find usefu\u00ael. You can set the entire persistence context to read-only mode. This disables state snapshots and dirty checking, and Hibernate won't write modifications to the database. Replication is useful, for example, when you need to retrieve data from one database and store it in another. Replication takes detached instances loaded in one persistence context and makes them pame checking, guaranteed scope of object identity, and so on. It's equally important that you know some of the details of its management, and that you sometimes influence what goes on behind the scenes. ",
  "page289": "Next, we look at the detached entity state. We already mentioned some issues you'll see when entity instances aren't associated with a persistence context anymore, such as disabled laz\u00aey initialization. Let's explore the detached state with some examples, so you know what to expect when you work with data outside of a persistence context. If a reference leaves the scope of guaranteed identity, we call it a reference\u00aeto a detached entity instance. When the persistence context is closed, it no longer provides an identity-mapping service. You'll run into aliasing problems when you work with detached entity instances, so make sure you understand how to handle the identity of detached instances. If you look up data using the same database identifier\u00aevalue in the same persistence context, the result is two references to the same in-memory instance on the JVM heap. Consider the two units of work shown next. In the first unit of work at begin() B, you start by creating a persistence context C and loading some entity instances. Because references a and b are obtained from the same persistence context, they have the same Java identity D. They\u00ae're equal from the same persistence context, they have the same Java identity D. They're equal E because by default equals() relies on Java identity comparison. They obviously have the same database identity F. They reference the same Item instance, in persistent state, managed\u00aeby the persistence context for that unit of work. The first part of this example finishes by committing the transaction. By default, Hibernate flushes the persistence context of an EntityManager and synchronizes changes with the database whenever the joined transaction is committed. All the previous code examples, except some in the last secthecation is built with these operations.",
  "page290": "When you begin a journey, it's a good idea to have a mental map of the terrain you'll be passing through. The same is true for an intellectual journey, such as learning to write comput\u00aeer programs. In this case, you'll need to know the basics of what computers are and how they work. You'll want to have some idea of what a computer program is and how one is created. Since you will be writing programs in the Java\u00aeprogramming language, you'll want to know something about that language in particular and about the modern, networked computing environment for which Java is designed. As you read this chapter, don't worry if you can't understand everything in detail. (In fact, it would be impossible for you to learn all the details from the\u00aebrief expositions in this chapter.) Concentrate on learning enough about the big ideas to orient yourself, in preparation for the rest of the book. Most of what is covered in this chapter will be covered in much greater detail later in the book. A computer is a complex system consisting of many different components. But at the heart or the brain, if you want of the computer is a single compo\u00aenent that does the actual computing. This is the Central Processing Unit, or CPU. In a modern desktop computer, the CPU is a single \"chip\" on the order of one square inch in size. The job of the CPU is to execute programs. A program is simply a list of unambiguous ins\u00aetructions meant to be followed mechanically by a computer. A computer is built to carry out instructions that are written in a very simple type of language called machine language. Each type of computer has its own machine language, and the computer can directly execute a program only if the program is expressed in that language. (It can execute programs writttioute it, and so on forever is called the fetch-and-execute cycle. With one exception, which will be covered in the next section, this is all that the CPU ever does. The details of the fetch-and-execute cycle are not terribly important, but",
  "page291": "Machine language instructions are expressed as binary numbers. A binary number is made up of just two possible digits, zero and one. So, a machine language instruction is just a sequence of zero\u00aes and ones. Each particular sequence encodes some particular instruction. The data that the computer manipulates is also encoded as binary numbers. A computer can work directly with binary numbers because switches can readily represent such\u00aenumbers: Turn the switch on to represent a one; turn it off to represent a zero. Machine language instructions are stored in memory as patterns of switches turned on or off, When a machine language instruction is loaded into CPU, all that happens is that certain switches are turned on or off in the pattern that encodes that particular ins\u00aetruction. The CPU is built to respond to this pattern by executing the instruction it encodes; it does this simply because of the way all the other switches in the CPU are wired together. So, you should understand this much about how computers work: Main memory holds machine language programs and data. These are encoded as binary numbers. The CPU fetches machine language instructions from mem\u00aeory one after another and executes them. It does this mechanically, without thinking about or understanding what it does-and therefore the program it executes must be perfect, complete in all details, and unambiguous because the CPU can do nothing but execute it exactly as writ\u00aeten. Here is a schematic view of this first-stage understanding of the computer Asynchronous Events: Polling Loops and Interrupts The CPU spends almost all of its time fetching instructions from memory and executing them. However, the CPU and main memory are only two out of many components in areal computer system, A complete system contains other devices such as: A hard imaare. Without the device driver, the actual physical device would be useless, since the CPU would not be able to communicate with it. A computer system consisting of many devices is typically organized by connecting those devices to one or more busses.",
  "page292": "Again, you should understand that this is a purely mechanical process: A device signals an interrupt simply by turning on a wire. The CPU is built so that when that wire is turned on, the CPU sa\u00aeves enough information about what it is currently doing so that it can return to the same state later. This information consists of the contents of important internal registers such as the program counter. Then the CPU jumps to some predete\u00aermined memory location and begins executing the instructions stored there. Those instructions make up an interrupt handler that does the processing necessary to respond to the interrupt. (This interrupt handler is part of the device driver software for the device that signalled the interrupt.) At the end of the interrupt handler is an inst\u00aeruction that tells the CPU to jump back to what it was doing; it does that by restoring its previously saved state. Interrupts allow the CPU to deal with asynchronous events. In the regular fetch-and execute cycle, things happen in a predetermined order; everything that happens is \"synchronized\" with everything else. Interrupts make it possible for the CPU to deal efficiently with e\u00aevents that happen \"asynchronously,\" that is, at unpredictable times. As another example of how interrupts are used, consider what happens when the CPU need to access data that is stored on the hard disk. The CPU can access data directly only if it is in main memory. D\u00aeata on the disk has to be copied into memory before it can be accessed. Unfortunately, on the scale of speed at which the CPU operates, the disk drive is extremely slow. When the CPU needs data from the disk, it sends a signal to the disk drive telling it to locate the data and get it ready. (This signal is sent synchronously, under the control of a regular program.ly veral things happens The thread might voluntarily yield control, to give other threads a chance to run. The thread might have to wait for some asynchronous event to occur. For example, the thread might request some data from the disk drive, or it mighven programming",
  "page293": "The Java Virtual Machine Machine language consists of very simple instructions that can be executed directly by the CPU of a computer. Almost all programs, though, are written in high-level prog\u00aeramming languages such as Java, Pascal, or C++. A program written in a high-level language cannot be run directly on any computer. First, it has to be translated into machine language. This translation can be done by a program called a comp\u00aeiler. A compiler takes a high-level-language program and translates it into an executable machine-language program. Once the translation is done, the machine-language program can be run any number of times, but of course it can only be run on one type of computer (since each type of computer has its own individual machine language). If the\u00aeprogram is to run on another type of computer it has to be re-translated, using a different compiler, into the appropriate machine language. There is an alternative to compiling a high-level language program. Instead of using a compiler, which translates the program all at once, you can use an interpreter, which translates it instruction-by-instruction, as necessary. An interpreter is a prog\u00aeram that acts much like a CPU, with a kind of fetch-and-execute cycle. In order to execute a program, the interpreter runs in a loop in which it repeatedly reads one instruction from the program, decides what is necessary to carry out that instruction, and then performs the app\u00aeropriate machine-language commands to do so. one use of interpreters is to execute high-level language programs. For example, the programming language Lisp is usually executed by an interpreter rather than a compiler. However, interpreters have another purpose: they can let you use a machine-language program meant for one type of computer on a completely different type ofcomulates a PC computer. Of course, a different Java bytecode interpreter is needed for each type of computer, but once a computer has a Java bytecode interpreter, it can run any Java bytecode program. And the same Java bytecode program can be run on anyct-oriented language. I should also note that the really hard part of platform-independence is providing a \"Graphical User Interface\" with windows, buttons, etc. that will work on all the platforms that support Java.",
  "page294": "Programs must be designed. No one can just sit down at the computer and compose a program of any complexity. The discipline called software engineering is concerned with the construction of corr\u00aeect, working, well-written programs. The software engineer tends to use accepted and proven methods for analyzing the problem to be solved and for designing a program to solve that problem. During the 1970s and into the 80s, the primary sof\u00aetware engineering methodology was structured programming. The structured programming approach to program design was based on the following advice: To solve a large problem, break the problem into several pieces and work on each piece separately; to solve each piece, treat it as a new problem which can itself be broken down into smaller pro\u00aeblems; eventually, you will work your way down to problems that can be solved directly, without further decomposition. This approach is called top-down programming. There is nothing wrong with top-down programming. It is a valuable and often-used approach to problem-solving. However, it is incomplete. For one thing, it deals almost entirely with producing the instructions necessary to solve a\u00aeproblem. But as time went on, people realized that the design of the data structures for a program was as least as important as the design of subroutines and control structures. Top-down programming doesn't give adequate consideration to the data that the program manipula\u00aetes. Another problem with strict top-down programming is that it makes it difficult to reuse work done for other projects. By starting with a particular problem and subdividing it into convenient pieces, top-down programming tends to produce a design that is unique to that problem. It is unlikely that you will be able to take a large chunk of programming from another program anntels, and so forth. In such modules, the data itself is often hidden inside the module; a program that uses the module can then manipulate the data only indirectly, by calling the subroutines provided by the module. more objects are created using that class as a template.) But objects can be similar without being in exactly the same class.",
  "page295": "For example, consider a drawing program that lets the user draw lines, rectangles, ovals, polygons, and curves on the screen. In the program, each visible object on the screen could be represent\u00aeed by a software object in the program. There would be five classes of objects in the program, one for each type of visible object that can be drawn. All the lines would belong to one class, all the rectangles to another class, and so on. T\u00aehese classes are obviously related; all of them represent \"drawable objects.\" They would, for example, all presumably be able to respond to a \"draw yourself\" message. Another level of grouping, based on the data to represent each type of object, is less obvious, but would be very useful in a program: We can group polyg\u00aeons and curves together as \"multipoint objects,\" while lines, rectangles, and ovals are \"two-point objects.\" (A line is determined by its endpoints, a rectangle by two of its corners, and an oval by two corners of the rectangle that contains it.) We could diagram these relationships as follows DrawableObject, MultipointObject, and TwoPointObject would be classes in the pro\u00aegram. MultipointObject and TwoPointObject would be subclasses of DrawableObject. The class Line would be a subclass of TwoPointObject and (indirectly) of DrawableObject. A subclass of a class is said to inherit the properties of that class. The subclass can add to its inheritan\u00aece and it can even \"override\" part of that inheritance (by defining a different response to some method). Nevertheless, lines, rectangles, and so on are drawable objects, and the class DrawableObject expresses this relationship. Inheritance is a powerful means for organizing a program. It is also related to the problem of reusing software components. A class is danToday, of course, most people interact with computers in a completely different way. They use a Graphical User Interface, or GUI. The computer draws interface components on the screen. The components include things this example, the applet has been programmed to respond to each event by displaying a message in the text area.",
  "page296": "The use of the term \"message\" here is deliberate. Messages, as you saw in the previous section, are sent to objects. In fact, Java GUI components are implemented as objects. Java inclu\u00aedes many predefined classes that represent various types of GUI components. Some of these classes are subclasses of others. Here is a diagram showing some of Swing's GUI classes and their relationships. Don't worry about the detai\u00aels for now, but try to get some feel about how object-oriented programming and inheritance are used here. Note that all the GUI classes are subclasses, directly or indirectly, of a class called JComponent, which represents general properties that are shared by all Swing components. Two of the direct subclasses of JComponent themselves have\u00aesubclasses. The classes JTextArea and JTextField, which have certain behaviors in common, are grouped together as subclasses of JTextComponent. Similarly JButton and JToggleButton are subclasses of JAbstractButton, which represents properties common to both buttons and checkboxes. (JComboBox, by the way, is the Swing class that represents pop-up menus.) Just from this brief discussion, perha\u00aeps you can see how GUI programming can make effective use of object-oriented design. In fact, GUI's, with their \"visible objects,\" are probably a major factor contributing to the popularity of OOP. The Internet and the World-Wide Web - Computers can be connected\u00aetogether on networks. A computer on a network can communicate with other computers on the same network by exchanging data and files or by sending and receiving messages. Computers on a network can even work together on a large computation. Today, millions of computers throughout the world are connected to a single huge network called the Internet. New computers are tos the data.) The packet also includes a \"return address,\" that is, the address of the sender. A packet can hold only a limited amount of data; longer messages must be divided among several packets, as a web browser.",
  "page297": "On a basic level (the level of machine language), a computer can perform only very simple operations. A computer performs complex tasks by stringing together large numbers of such operations. Su\u00aech tasks must be \"scripted\" in complete and perfect detail by programs. Creating complex programs will never be really easy, but the difficulty can be handled to some extent by giving the program a clear overall structure. The des\u00aeign of the overall structure of a program is what I call \"programming in the large.\" Programming in the small, which is sometimes called coding, would then refer to filling in the details of that design. The details are the explicit, step-by-step instructions for performing fairly small-scale tasks. When you do coding, you are wo\u00aerking fairly \"close to the machine,\" with some of the same concepts that you might use in machine language: memory locations, arithmetic operations, loops and branches. In a high-level language such as Java, you get to work with these concepts on a level several steps above machine language. However, you still have to worry about getting all the details exactly right. This chapter a\u00aend the next examine the facilities for programming in the small in the Java programming language. Don't be misled by the term \"programming in the small\" into thinking that this material is easy or unimportant. This material is an essential foundation for all type\u00aes of programming. If you don't understand it, you can't write programs, no matter how good you get at designing their large-scale structure. The Basic Java Application - A program is a sequence of instructions that a computer can execute to perform some task. A simple enough idea, but for the computer to make any use of the instructions, they must be writtu fe.\" It is written in a way that will make it easy for people to read and to understand. It follows conventions that will be familiar to other programmers. And it has an overall design that will make sensteps for you, but you can be sure that the same three steps are being done in the background.",
  "page298": "Here is a Java program to display the message \"Hello World!\". Don't expect to understand what's going on here just yet some of it you won't really understand until a few\u00aechapters from now: // A program to display the message // \\\"Hello World!\\\" on standard output public class HelloWorld { public static void main(String[] args) { System.out.println(\\\"Hello World!\\\"); } } // end of class HelloWorld T\u00aehe command that actually displays the message is: System.out.println(\\\"Hello World!\\\"); This command is an example of a subroutine call statement. It uses a \"built-in subroutine\" named System.out.println to do the actual work. Recall that a subroutine consists of the instructions for performing some task, chunked together and\u00aegiven a name. That name can be used to \"call\" the subroutine whenever that task needs to be performed. A built-in subroutine is one that is already defined as part of the language and therefore automatically available for use in any program. When you run this program, the message \"Hello World!\" (without the quotes) will be displayed on standard output. Unfortunately, I ca\u00aen't say exactly what that means! Java is meant to run on many different platforms, and standard output will mean different things on different platforms. However, you can expect the message to show up in some convenient place. (If you use a command-line interface, like tha\u00aet in Sun Microsystem's Java Development Kit, you type in a command to tell the computer to run the program. The computer will type the output from the program, Hello World!, on the next line.) You must be curious about all the other stuff in the above program. Part of it consists of comments. Comments in a program are entirely ignored by the computer; theboveven in other classes, but it is the main() routine that determines how and in what order the other subroutines are used. The word \"public\" in the first line of main() means that this routine can be called from outside the program. Thitics of the language. The computer doesn't care about layout you could run the entire program together on one line as far as it is concerned. However, layout is important to human readers, and there are certain style guidelines for layout that are followed by most programmers. These style guidelines are part of the pragmatics of the Java programming language.",
  "page299": "Variables and the Primitive Types - Names are fundamental to programming. In programs, names are used to refer to many different sorts of things. In order to use those things, a programmer must\u00aeunderstand the rules for giving names to things and the rules for using the names to work with those things. That is, the programmer must understand the syntax and the semantics of names. According to the syntax rules of Java, a name is a s\u00aeequence of one or more characters. It must begin with a letter or underscore and must consist entirely of letters, digits, and underscores. (\"Underscore\" refers to the character ' '.) For example, here are some legal names: No spaces are allowed in identifiers; HelloWorld is a legal identifier, but \"Hello World\u00ae1d is not. Upper case and lower case letters are considered to be different, so that HelloWorld, helloworld, HELLOWORLD, and hElloWorLD are all distinct names. Certain names are reserved for special uses in Java, and cannot be used by the programmer for other purposes. These reserved words include: class, public, static, if, else, while, and several dozen other words. Java is actually pretty libe\u00aeral about what counts as a letter or a digit. Java uses the Unicode character set, which includes thousands of characters from many different languages and different alphabets, and many of these characters count as letters or digits. However, I will be sticking to what can be t\u00aeyped on a regular English keyboard. The pragmatics of naming includes style guidelines about how to choose names for things. For example, it is customary for names of classes to begin with upper case letters, while names of variables and of subroutines begin with lower case letters; you can avoid a lot of confusion by following the same convention in your own programst.pcan only be referred to by giving the numerical address of the location in memory where it is stored. In a high-level language such as Java, names are used instead of numbers to refer to data. It is the job of the computer to keep track of where in mecuting a program, it evaluates the expression and puts the resulting data value into the variable. For example, consider the simple assignment statement rate = 0.07; The variable in this assignment statement is rate, and the expression is the number 0.07.",
  "page300": "Types and Literals- A variable in Java is designed to hold only one particular type of data; it can legally hold that type of data and no other. The compiler will consider it to be a syntax erro\u00aer if you try to violate this rule. We say that Java is a strongly typed language because it enforces this rule. There are eight so-called primitive types built into Java. The primitive types are named byte, short, int, long, float, double,\u00aechar, and boolean. The first four types hold integers (whole numbers such as 17, -38477, and 0). The four integer types are distinguished by the ranges of integers they can hold. The float and double types hold real numbers (such as 3.6 and -145.99). Again, the two real types are distinguished by their range and accuracy. A variable of typ\u00aee char holds a single character from the Unicode character set. And a variable of type boolean holds one of the two logical values true or false. Any data value stored in the computer's memory must be represented as a binary number, that is as a string of zeros and ones. A single zero or one is called a bit. A string of eight bits is called a byte. Memory is usually measured in terms of\u00aebytes. Not surprisingly, the byte data type refers to a single byte of memory. A variable of type byte holds a string of eight bits, which can represent any of the integers between -128 and 127, inclusive. (There are 256 integers in that range; eight bits can represent 256 two\u00aeraised to the power eight different values.) As for the other integer types, short corresponds to two bytes (16 bits). Variables of type short have values in the range -32768 to 32767. int corresponds to four bytes (32 bits). Variables of type int have values in the range -2147483648 to 2147483647. long corresponds to eight bytes (64 bits). Variables of type long have values in thvariable; they are just a convention for naming a particular character constant in a program. A name for a constant value is called a literal. A literal is what you have to type in a program to represent a value. 'A' and '*' e for real numbers.) Even for integer literals, there are some complications. Ordinary integers such as 177777 and -32 are literals of type byte, short, or int, depending on their size. You can make a literal of type long by adding \"L\" as a suffix. For example: 17L or 728476874368L.",
  "page301": "All software problems can be termed as bugs. A software bug usually occurs when the software does not do what it is intended to do or does something that it is not intended to do. Flaws in speci\u00aefications, design, code or other reasons can cause these bugs. Identifying and fixing bugs in the early stages of the software is very important as the cost of fixing bugs grows over time. So, the goal of a software tester is to find bugs a\u00aend find them as early as possible and make sure they are fixed. Testing is context-based and risk-driven. It requires a methodical and disciplined approach to finding bugs. A good software tester needs to build credibility and possess the attitude to be explorative, troubleshooting, relentless, creative, diplomatic and persuasive. As again\u00aest the perception that testing starts only after the completion of coding phase, it actually begins even before the first line of code can be written. In the life cycle of the conventional software product, testing begins at the stage when the specifications are written, i.e. from testing the product specifications or product spec. Finding bugs at this stage can save huge amounts of time and\u00aemoney. Once the specifications are well understood, you are required to design and execute the test cases. Selecting the appropriate technique that reduces the number of tests that cover a feature is one of the most important things that you need to take into consideration whil\u00aee designing these test cases. Test cases need to be designed to cover all aspects of the software, i.e. security, database, functionality (critical and general) and the user interface. Bugs originate when the test cases are executed. As a tester you might have to perform testing under different circumstances, i.e. the application could be in , keeping pace with the latest developments in the field will augment your career as a software test engineer.",
  "page302": "Software is a series of instructions for the computer that perform a particular task,called a program; the two major categories of software are system software andapplication software. System so\u00aeftware is made up of control programs. Applicationsoftware is any program that processes data for the user (spreadsheet, wordprocessor, payroll, etc.).A software product should only be released after it has gone through a properprocess of d\u00aeevelopment, testing and bug fixing. Testing looks at areas such asperformance, stability and error handling by setting up test scenarios undercontrolled conditions and assessing the results. This is why exactly any software hasto be tested. It is important to note that software is mainly tested to see that it meetsthe customers' needs\u00aeand that it conforms to the standards. It is a usual norm thatsoftware is considered of good quality if it meets the user requirements.Quality can briefly be defined as \"a degree of excellence\". High quality softwareusually conforms to the user requirements. A customer's idea of quality may cover abreadth of features - conformance to specifications, good performance onplatform\u00ae(s)/configurations, completely meets operational requirements (even if notspecified!), compatibility to all the end-user equipment, no negative impact onexisting end-user base at introduction time.Quality software saves good amount of time and money. Because software will havef\u00aeewer defects, this saves time during testing and maintenance phases. Greaterreliability contributes to an immeasurable increase in customer satisfaction as well aslower maintenance costs. Because maintenance represents a large portion of allsoftware costs, the overall cost of the project will most likely be lower than similarprojects.Following are two cases thgn flaws at an early stage beforefailures occur in production, or in the field Promote continual improvement",
  "page303": "As software engineering is now being considered as a technical engineeringprofession, it is important that the software test engineer's posses certain traits witha relentless attitude to ma\u00aeke them stand out. Here are a few.Know the technology. Knowledge of the technology in which the application isdeveloped is an added advantage to any tester. It helps design better and powerfultest cases basing on the weakness or flaws of th\u00aee technology. Good testers knowwhat it supports and what it doesn't, so concentrating on these lines will help thembreak the application quickly. Perfectionist and a realist. Being a perfectionist will help testers spot the problemand being a realist helps know at the end of the day which problems are reallyimportant problems. You wil\u00ael know which ones require a fix and which ones don't.Tactful, diplomatic and persuasive. Good software testers are tactful and knowhow to break the news to the developers. They are diplomatic while convincing thedevelopers of the bugs and persuade them when necessary and have their bug(s)fixed. It is important to be critical of the issue and not let the person who developedthe applicatio\u00aen be taken aback of the findings. An explorer. A bit of creativity and an attitude to take risk helps the testersventure into unknown situations and find bugs that otherwise will be looked over.Troubleshoot. Troubleshooting and figuring out why something doesn't workhelps\u00aetesters be confident and clear in communicating the defects to the developers. Posses people skills and tenacity. Testers can face a lot of resistance fromprogrammers. Being socially smart and diplomatic doesn't mean being indecisive. Thebest testers are both-socially adept and tenacious where it matters.Organized. Best testers very well realize that they too can maat a later stage. Defects can cause serious problems if not managedproperly. Learning from defects helps - prevention of future problems, trackimprovements, improve prediction and estimation.",
  "page304": "Testing can't show that bugs don't exist. An important reason for testing is toprevent defects. You can perform your tests, find and report bugs, but at no point canyou guarantee that\u00aethere are no bugs. It is impossible to test a program completely. Unfortunately this is not possibleeven with the simplest program because - the number of inputs is very large, numberof outputs is very large, number of paths through th\u00aee software is very large, and thespecification is subjective to frequent changes. You can't guarantee quality. As a software tester, you cannot test everything andare not responsible for the quality of the product. The main way that a tester can failis to fail to report accurately a defect you have observed. It is important to remembe\u00aerthat we seldom have little control over quality. Target environment and intended end user. Anticipating and testing theapplication in the environment user is expected to use is one of the major factors thatshould be considered. Also, considering if the application is a single user system ormulti user system is important for demonstrating the ability for immediate readinesswhen necessary. The\u00aeerror case of Disney's Lion King illustrates this. Disney Companyreleased its first multimedia CD-ROM game for children, The Lion King AnimatedStorybook. It was highly promoted and the sales were huge. Soon there were reportsthat buyers were unable to get the software to\u00aework. It worked on a few systems -likely the ones that the Disney programmers used to create the game - but not on themost common systems that the general public used.No application is 100% bug free. It is more reasonable to recognize there arepriorities, which may leave some less critical problems unsolved or unidentified.Simple case is ",
  "page305": " Build your credibility. Credibility is like quality that includes reliability, knowledge,consistency, reputation, trust, attitude and attention to detail. It is not instant butshould be built o\u00aever time and gives voice to the testers in the organization. Your keysto build credibility - identify your strengths and weaknesses, build good relations,demonstrate competency, and be willing to admit mistakes, re-assess and adjust. T\u00aeest what you observe. It is very important that you test what you can observeand have access to. Writing creative test cases can help only when you have the opportunity to observe the results. So, assume nothing.Not all bugs you find will be fixed. Deciding which bugs will be fixed and whichwon't is a risk-based decision. Several reas\u00aeons why your bug might not be fixed iswhen there is no enough time, the bug is dismissed for a new feature, fixing it mightbe very risky or it may not be worth it because it occurs infrequently or has a workaround where the user can prevent or avoid the bug. Making a wrong decision can bedisastrous.Review competitive products. Gaining a good insight into various products of thesame kind and g\u00aeetting to know their functionality and general behavior will help youdesign different test cases and to understand the strengths and weaknesses of yourapplication. This will also enable you to add value and suggest new features andenhancements to your product.Follow standards a\u00aend processes. As a tester, your need to conform to thestandards and guidelines set by the organization. These standards pertain toreporting hierarchy, coding, documentation, testing, reporting bugs, using automatedtools etc. The software life cycle typically includes the following: requirements analysis, design,coding, testing, installation and maintenance. In betweto identify which requirementsshould be allocated to which components.Design and Specifications. The outcome of requirements analysis is therequirements specification. Using this, the overall design for the intended softwareis developed.Activities in this phase - Perform Architectural Design for the software, DesignDatabase (If applicable), Design User Interfaces, Select or Develop Algorithms (IfApplicable), Perform Detailed Design.",
  "page306": "Coding. The development process tends to run iteratively through these phasesrather than linearly; several models (spiral, waterfall etc.) have been proposed todescribe this process.Activities i\u00aen this phase - Create Test Data, Create Source, Generate Object Code,Create Operating Documentation, Plan Integration, Perform IntegrationTesting. The process of using the developed system with the intent to find errors.Defects/flaws/bugs f\u00aeound at this stage will be sent back to the developer for a fixand have to be re-tested. This phase is iterative as long as the bugs are fixed to meetthe requirements.Activities in this phase - Plan Verification and Validation, Execute Verification andvalidation Tasks, Collect and Analyze Metric Data, Plan Testing, Develop TestRequirements\u00ae, Execute TestsInstallation. The so developed and tested software will finally need to be installed atthe client place. Careful planning has to be done to avoid problems to the user afterinstallation is done Activities in this phase - Plan Installation, Distribution of Software, Installation ofSoftware, Accept Software in Operational Environment.Operation and Support. Support activities are u\u00aesually performed by theorganization that developed the software. Both the parties usually decide on theseactivities before the system is developed.Activities in this phase - Operate the System, Provide Technical Assistance andConsulting, Maintain Support Request Log.Maintenance\u00ae. The process does not stop once it is completely implemented andinstalled at user place; this phase undertakes development of new features,enhancements etc.Activities in this phase - Reapplying Software Life Cycle.The way you approach a particular application for testing greatly depends on the lifecycle model it follows. This is because, each life cyclenning High Level Test plan, QA plan (quality goals), identify - reportingprocedures, problem classification, acceptance criteria, databases for testing,measurement criteria (defect quantities/severity level and defect origin), projectmetrics and finally begin the schedule for project testing. Also, plan to maintain all test cases (manual or automated) in a database.",
  "page307": "Analysis. Involves activities that - develop functional validation based on BusinessRequirements (writing test cases basing on these details), develop test case format(time estimates and priorit\u00aey assignments), develop test cycles (matrices andtimelines), identify test cases to be automated (if applicable), define area of stressand performance testing, plan the test cycles required for the project and regressiontesting, define proc\u00aeedures for data maintenance (backup, restore, validation), reviewdocumentation.Design. Activities in the design phase - Revise test plan based on changes, revise testcycle matrices and timelines, verify that test plan and cases are in a database orrequisite, continue to write test cases and add new ones based on changes, developRisk Assess\u00aement Criteria, formalize details for Stress and Performance testing, finalizetest cycles (number of test case per cycle based on time estimates per test case andpriority), finalize the Test Plan, (estimate resources to support development in unittesting).Construction (Unit Testing Phase). Complete all plans, complete Test Cycle matricesand timelines, complete all test cases (manual), begin St\u00aeress and Performance testing,test the automated testing system and fix bugs, (support development in unittesting), run QA acceptance test suite to certify software is ready to turn over to QA.Test Cycle(s) / Bug Fixes (Re-Testing/System Testing Phase). Run the test cases (front\u00aeand back end), bug reporting, verification, and revise/add test cases as required.Final Testing and Implementation (Code Freeze Phase). Execution of all front end testcases - manual and automated, execution of all back end test cases - manual andautomated, execute all Stress and Performance tests, provide on-going defecttracking metrics, provide onror that causes an unexpected defect,fault, flaw, or imperfection in a computer program. In other words, if a program doesnot perform as intended, it is most likely a bug.There are bugs in software due to unclear or constantly changing requirements,software complexity, programming errors, timelines, errors in bug tracking,communication gap, documentation errors, deviation from standards etc.",
  "page308": "Constantly changing software requirements cause a lot of confusion and pressureboth on the development and testing teams. Often, a new feature added or existingfeature removed can be linked to t\u00aehe other modules or components in the software.Overlooking such issues causes bugs. Also, fixing a bug in one part/component of the software might arise another in adifferent or same component. Lack of foresight in anticipating such issues\u00aecan causeserious problems and increase in bug count. This is one of the major issues because ofwhich bugs occur since developers are very often subject to pressure related totimelines; frequently changing requirements, increase in the number of bugs etc. Designing and re-designing, UI interfaces, integration of modules, databasemanagement\u00aeall these add to the complexity of the software and the system as awhole. Fundamental problems with software design and architecture can cause problemsin programming. Developed software is prone to error as programmers can makemistakes too. As a tester you can check for, data reference/declaration errors, controlflow errors, parameter errors, input/output errors etc. Rescheduling of resources\u00ae, re-doing or discarding already completed work,changes in hardware/software requirements can affect the software too. Assigning anew developer to the project in midway can cause bugs. This is possible if propercoding standards have not been followed, improper code documentatio\u00aen, ineffectiveknowledge transfer etc. Discarding a portion of the existing code might just leave itstrail behind in other parts of the software; overlooking or not eliminating such codecan cause bugs. Serious bugs can especially occur with larger projects, as it getstougher to identify the problem area. Programmers usually tend to rush as the deadline apreate problemselsewhere. In most of the cases, the life cycle gets very complicated and difficult totrack making it imperative to have a bug/defect tracking system in place.See Chapter 7 - Defect TrackingFollowing are the different phases of a Bug Life Cycle:Open: A bug is in Open state when a tester identifies a problem areaAccepted: The bug is then assigned to a developer for a fix. The developer thenaccepts if valid.",
  "page309": "Not Accepted/Won't fix: If the developer considers the bug as low level or does notaccept it as a bug, thus pushing it into Not Accepted/Won't fix state.Such bugs will be assigned to t\u00aehe project manager who will decide if the bug needs afix. If it needs, then assigns it back to the developer, and if it doesn't, then assigns itback to the tester who will have to close the bug.Pending: A bug accepted by the developer\u00aemay not be fixed immediately. In suchcases, it can be put under Pending state.Fixed: Programmer will fix the bug and resolves it as Fixed.Close: The fixed bug will be assigned to the tester who will put it in the Close state.Re-Open: Fixed bugs can be re-opened by the testers in case the fix producesproblems elsewhere.Costs are logarithmic\u00ae; they increase in size tenfold as the time increases. A bug foundand fixed during the early stages - requirements or product spec stage can be fixed bya brief interaction with the concerned and might cost next to nothing.During coding, a swiftly spotted mistake may take only very less effort to fix. Duringintegration testing, it costs the paperwork of a bug report and a formally documen\u00aetedfix, as well as the delay and expense of a re-test.During system testing it costs even more time and may delay delivery. Finally, duringoperations it may cause anything from a nuisance to a system failure, possibly withcatastrophic consequences in a safety-critical system su\u00aech as an aircraft or anemergency service.It is difficult to determine when exactly to stop testing. Here are a few commonfactors that help you decide when you can stop or reduce testing: Deadlines (release deadlines, testing deadlines, etc.) Test cases completed with certain percentage passed Test budget depleted Coverage of code/functionality/requirements reaches a specified point Bug rate falls below a certain level Beta or alpha testing period ends There are basically three levels of testing i.e. Unit Testing, Integration Testing andSystem Testing. Various types of testing come under these levels.Unit TestingTo verify a single program or a section of a single programIntegration TestingTo verify interaction between system componentsPrerequisite: unit testing completed on all components that compose a system.System TestingTo verify and validate behaviors of the entire system against the original systemobjectives.",
  "page310": "Software testing is a process that identifies the correctness, completeness, andquality of software.Following is a list of various types of software testing and their definitions in arandom orde\u00aer: Formal Testing: Performed by test engineers Informal Testing: Performed by the developers Manual Testing: That part of software testing that requires human input, analysis,or evaluation. Automated Testing: Software testing that utilizes\u00aea variety of tools to automatethe testing process. Automated testing still requires a skilled quality assuranceprofessional with knowledge of the automation tools and the software being testedto set up the test cases. Black box Testing: Testing software without any knowledge of the back-end of thesystem, structure or language of the module\u00aebeing tested. Black box test cases arewritten from a definitive source document, such as a specification or requirementsdocument. White box Testing: Testing in which the software tester has knowledge of theback-end, structure and language of the software, or at least its purpose. Unit Testing: Unit testing is the process of testing a particular complied program,i.e., a window, a report, an i\u00aenterface, etc. independently as a stand-alonecomponent/program. The types and degrees of unit tests can vary among modifiedand newly created programs. Unit testing is mostly performed by the programmerswho are also responsible for the creation of the necessary unit test data. I\u00aencremental Testing: Incremental testing is partial testing of an incompleteproduct. The goal of incremental testing is to provide an early feedback to softwaredevelopers. System Testing: System testing is a form of black box testing. The purpose ofsystem testing is to validate an application's accuracy and completeness inperforming the functions as designed.Integration Testing: Testing two or more modules or functions together with theintent of finding interface defects between the modules/functions. System Integration Testing: Testing of software components that have beendistributed across multiple platforms (e.g., client, web server, application server, anddatabase server) to produce failures caused by system integration defects (i.e. defectsinvolving distribution and back- Functional Testing:",
  "page311": "Acceptance Testing: Testing the system with the intent of confirming readiness ofthe product and customer acceptance. Also known as User Acceptance Testing. Adhoc Testing: Testing without a form\u00aeal test plan or outside of a test plan. Withsome projects this type of testing is carried out as an addition to formal testing.Sometimes, if testing occurs very late in the development cycle, this will be the onlykind of testing that can be\u00aeperformed - usually done by skilled testers. Sometimes adhoc testing is referred to as exploratory testing. Configuration Testing: Testing to determine how well the product works with abroad range of hardware/peripheral equipment configurations as well as on differentoperating systems and software. Load Testing: Testing with the inte\u00aent of determining how well the product handlescompetition for system resources. The competition may come in the form of networktraffic, CPU utilization or memory allocation. Stress Testing: Testing done to evaluate the behavior when the system is pushedbeyond the breaking point. The goal is to expose the weak links and to determine ifthe system manages to recover gracefully. Performance Testi\u00aeng: Testing with the intent of determining how efficiently aproduct handles a variety of events. Automated test tools geared specifically to testand fine-tune performance are used most often for this type of testing. Usability Testing: Usability testing is testing for 'user-fri\u00aeendliness'. A way toevaluate and measure how users interact with a software product or site. Tasks aregiven to users and observations are made. Installation Testing: Testing with the intent of determining if the product iscompatible with a variety of platforms and how easily it installs. Recovery/Error Testing: Testing how well a system recovers from crashes,hardware failures, or other catastrophic problems. Security Testing: Testing of database and network software in order to keepcompany data and resources secure from mistaken/accidental users, hackers, andother malevolent attackers. Penetration Testing: Penetration testing is testing how well the system isprotected against unauthorized internal or external access, or willful damage. Thistype of testing usually requires sophisticated testing techniques.Compatibility Testing: Testing used to determine whether other system softwarecomponents such as browsers, utilities, and competing software will contion, ",
  "page312": "Following are the most common software errors that aid you in software testing. Thishelps you to identify errors systematically and increases the efficiency andproductivity of software testing.U\u00aeser Interface Errors: Missing/Wrong Functions Doesn't do what the user expects,Missing information, Misleading, Confusing information, Wrong content in Help text,Inappropriate error messages. Performance issues - Poor responsiveness, C\u00aean'tredirect output, Inappropriate use of key board Error Handling: Inadequate - protection against corrupted data, tests of userinput, version control; Ignores - overflow, data comparison, Error recovery - abortingerrors, recovery from hardware problems. Boundary related errors: Boundaries in loop, space, time, memory, mishandli\u00aeng ofcases outside boundary. Calculation errors: Bad Logic, Bad Arithmetic, Outdated constants, Calculationerrors, incorrect conversion from one data representation to another, Wrongformula, incorrect approximation. Initial and Later states: Failure to - set data item to zero, to initialize a loopcontrol variable, or re-initialize a pointer, to clear a string or flag, Incorrectinitialization.\u00aeControl flow errors: Wrong returning state assumed, Exception handling basedexits, Stack underflow/overflow, Failure to block or un-block interrupts, Comparisonsometimes yields wrong result, Missing/wrong default, and Data Type errors. Errors in Handling or Interpreting Data:\u00aeUn-terminated null strings, overwriting afile after an error exit or user abort. Race Conditions: Assumption that one event or task finished before anotherbegins, Resource races, Tasks starts before its prerequisites are met, Messages crosor don't arrive in the order sent.Load Conditions: Required resources are not available, No available large memoryarea, Low priority tasks not put off, doesn't erase old files from mass storage, anddoesn't return unused memory.Hardware: Wrong Device, Device unavailable, Underutilizing device intelligence,Misunderstood status or return code, Wrong operation or instruction codes. Source, Version and ID Control: No Title or version ID, Failure to update multiplecopies of data or program files.Testing Errors: Failure to notice/report a problem, Failure to use the mostpromising test case, Corrupted data files, Misinterpreted specifications ordocumentation, Failure to make it clear how to",
  "page313": "Test Policy - A document characterizing the organization's philosophy towardssoftware testing.Test Strategy - A high-level document defining the test phases to be performed andthe testing w\u00aeithin those phases for a program. It defines the process to be followed ineach project. This sets the standards for the processes, documents, activities etc. thatshould be followed for each project.For example, if a product is given for tes\u00aeting, you should decide if it is better to useblack-box testing or white-box testing and if you decide to use both, when will youapply each and to which part of the software? All these details need to be specified inthe Test Strategy.Project Test Plan - a document defining the test phases to be performed and thetesting within those phases\u00aefor a particular project.A Test Strategy should cover more than one project and should address the followingissues: An approach to testing high risk areas first, Planning for testing, How toimprove the process based on previous testing, Environments/data used, Testmanagement - Configuration management, Problem management, What Metrics arefollowed, Will the tests be automated and if so which t\u00aeools will be used, What are theTesting Stages and Testing Methods, Post Testing Review process, Templates.Test planning needs to start as soon as the project requirements are known. The firstdocument that needs to be produced then is the Test Strategy/Testing Approach thatsets\u00aethe high level approach for testing and covers all the other elements mentionedabove.Once the approach is understood, a detailed test plan can be written. Usually, thistest plan can be written in different styles. Test plans can completely differ fromproject to project in the same organization.PurposeTo describe the scope, approach, resources, and schedule of the testing activities. Toidentify the items being tested, the features to be tested, the testing tasks to beperformed, the personnel responsible for each task, and the risks associated with thisplan.OUTLINEA test plan shall have the following structure: Test plan identifier. A unique identifier assign to the test plan. Introduction: Summarized the software items and features to be tested andthe need for them to be included. Test items: Identify the test items, their transmittal media which impact their Features to be tested Features not to be testedApproachItem pass/fail criteriaSuspension crite contingencies Approvals",
  "page314": "Like any other process in software testing, the major tasks in test planning are to -Develop Test Strategy, Critical Success Factors, Define Test Objectives, IdentifyNeeded Test Resources,\u00aePlan Test Environment, Define Test Procedures, IdentifyFunctions To Be Tested, Identify Interfaces With Other Systems or Components, WriteTest Scripts, Define Test Cases, Design Test Data, Build Test Matrix, Determine TestSchedules, Assembl\u00aee Information, Finalize the Plan.A test case is a detailed procedure that fully tests a feature or an aspect of a feature.While the test plan describes what to test, a test case describes how to perform aparticular test. You need to develop test cases for each test listed in the test planAs a tester, the best way to determine the complianc\u00aee of the software torequirements is by designing effective test cases that provide a thorough test of aunit. Various test case design techniques enable the testers to develop effective testcases. Besides, implementing the design techniques, every tester needs to keep inmind general guidelines that will aid in test case design:a. The purpose of each test case is to run the test in the simplest\u00aeway possible.[Suitable techniques - Specification derived tests, Equivalence partitioning]b. Concentrate initially on positive testing i.e. the test case should show that thesoftware does what it is intended to do. [Suitable techniques - Specification derivedtests, Equivalence\u00aepartitioning, State-transition testing]c. Existing test cases should be enhanced and further test cases should be designedto show that the software does not do anything that it is not specified to do i.e.Negative Testing [Suitable techniques - Error guessing, Boundary value analysis,Internal boundary value testing, State-transition testing]d. Where appropriate, test cases should be designed to address issues such asperformance, safety requirements and security requirements [Suitable techniques -Specification derived tests]e. Further test cases can then be added to the unit test specification to achievespecific test coverage objectives. Once coverage tests have been designed, the testprocedure can be developed and the tests executed [Suitable techniques - Branchtesting, Condition testing, Data definition-use testing, State-transition testing]The manner in which a test case is depicted varies between organizations. Anyhow,many test case tem categories:",
  "page315": "For example, if a program accepts integer values only from 1 to 10. The possible testcases for such a program would be the range of all integers. In such a program, allintegers up to 0 and above\u00ae10 will cause an error. So, it is reasonable to assume that if11 will fail, all values above it will fail and vice versa.If an input condition is a range of values, let one valid equivalence class is the range (0or 10 in this example). Let\u00aethe values below and above the range be two respectiveinvalid equivalence values (i.e. -1 and 11). Therefore, the above three partition valuescan be used as test cases for the above example.Boundary Value AnalysisThis is a selection technique where the test data are chosen to lie along theboundaries of the input domain or the output range\u00ae. This technique is often called asstress testing and incorporates a degree of negative testing in the test design byanticipating that errors will occur at or around the partition boundaries.For example, a field is required to accept amounts of money between $0 and $10. Asa tester, you need to check if it means up to and including $10 and $9.99 and if $10 isacceptable. So, the boundary values\u00aeare $0, $0.01, $9.99 and $10.Now, the following tests can be executed. A negative value should be rejected, 0should be accepted (this is on the boundary), $0.01 and $9.99 should be accepted,null and $10 should be rejected. In this way, it uses the same concept of partitions as\u00aeequivalence partitioning.State Transition TestingAs the name suggests, test cases are designed to test the transition between thestates by creating the events that cause the transition.Branch TestingIn branch testing, test cases are designed to exercise control flow branches ordecision points in a unit. This is usually aimed at achieving a target level of DecisionCoverage. Branch Coverage, need to test both branches of IF and ELSE. All branchesand compound conditions (e.g. loops and array handling) within the branch should beexercised at least once.Condition TestingThe object of condition testing is to design test cases to show that the individualcomponents of logical conditions and combinations of the individual components arecorrect. Test cases are designed to test the individual elements of logical expressions,both within branch conditions and within other expressions in a unit.Data Definition - Use TestingData des set.",
  "page316": "Internal Boundary Value TestingIn many cases, partitions and their boundaries can be identified from a functionalspecification for a unit, as described under equivalence partitioning and boundar\u00aeyvalue analysis above. However, a unit may also have internal boundary values thatcan only be identified from a structural specification.Error GuessingIt is a test case design technique where the testers use their experience to guess thepos\u00aesible errors that might occur and design test cases accordingly to uncover them.Using any or a combination of the above described test case design techniques; youcan develop effective test cases.A use case describes the system's behavior under various conditions as it responds toa request from one of the users. The user initiates an i\u00aenteraction with the system toaccomplish some goal. Different sequences of behavior, or scenarios, can unfold,depending on the particular requests made and conditions surrounding the requests.The use case collects together those different scenarios.Use cases are popular largely because they tell coherent stories about how thesystem will behave in use. The users of the system get to see just wh\u00aeat this newsystem will be and get to react early.As discussed earlier, defect is the variance from a desired product attribute (it can bea wrong, missing or extra data). It can be of two types - Defect from the product or avariance from customer/user expectations. It is a\u00aeflaw in the software system and hasno impact until it affects the user/customer and operational system.With the knowledge of testing so far gained, you can now be able to categorize thedefects you have found. Defects can be categorized into different types basing on thecore issues they address. Some defects address security or database issues whileothers may refer to functionality or UI issues.Security Defects: Application security defects generally involve improper handling ofdata sent from the user to the application. These defects are the most severe andgiven highest priority for a fix.Examples:Authentication: Accepting an invalid username/password Authorization: Accessibility to pages though permission not givenData Quality/Database Defects: Deals with improper handling of data in thedatabase.Examples: Values not deleted/inserted into the database properly Improper/wrong/null values inserted in place of the actual valuesCritical Functionality Defeaffect the functionality",
  "page317": "User Interface Defects: As the name suggests, the bugs deal with problems related toUI are usually considered less severe.Examples:Improper error/warning/UI messages Spelling mistakes Alignment\u00aeproblemsOnce the test cases are developed using the appropriate techniques, they areexecuted which is when the bugs occur. It is very important that these bugs bereported as soon as possible because, the earlier you report a bug, the more t\u00aeimeremains in the schedule to get it fixed.Simple example is that you report a wrong functionality documented in the Help file afew months before the product release, the chances that it will be fixed are very high.If you report the same bug few hours before the release, the odds are that it won't befixed. The bug is still the same th\u00aeough you report it few months or few hours beforethe release, but what matters is the time.It is not just enough to find the bugs; these should also be reported/communicatedclearly and efficiently, not to mention the number of people who will be reading thedefect.Defect tracking tools (also known as bug tracking tools, issue tracking tools orproblem trackers) greatly aid the testers in report\u00aeing and tracking the bugsfound in software applications. They provide a means of consolidating a keyelement of project information in one place. Project managers can then seewhich bugs have been fixed, which are outstanding and how long it is taking tofix defects. Senior manage\u00aement can use reports to understand the state of thedevelopment process.You should provide enough detail while reporting the bug keeping in mind the peoplewho will use it - test lead, developer, project manager, other testers, new testersassigned etc. This means that the report you will write should be concise, straight andclear. Following are the details your report should contain:Bug Title Bug identifier (number, ID, etc.) The application name or identifier and version The function, module, feature, object, screen, etc. where the bug occurredEnvironment (OS, Browser and its version)-Bug Type or Category/Severity/Priorityo Bug Category: Security, Database, Functionality (Critical/General), UIo Bug Severity: Severity with which the bug affects the application - Very High,High, Medium, Low, Very Lowo Bug Priority: Recommended priority to be given for a fix of this bug - P0, P1,P2, P3, P4, P5 (P0-Highest, P5-Lowest) Comments",
  "page318": "Once the reported defect is fixed, the tester needs to re-test to confirm the fix. This isusually done by executing the possible scenarios where the bug can occur. Onceretesting is completed, th\u00aee fix can be confirmed and the bug can be closed. Thismarks the end of the bug life cycle.The documents outlined in the IEEE Standard of Software Test Documentation coverstest planning, test specification, and test reporting.Test reporting\u00aecovers four document types: A Test Item Transmittal Report identifies the test items being transmitted fortesting from the development to the testing group in the event that a formalbeginning of test execution is desiredDetails to be included in the report - Purpose, Outline, Transmittal-Report Identifier,Transmitted Items, Location, Statu\u00aes, and Approvals.. A Test Log is used by the test team to record what occurred during test executionDetails to be included in the report - Purpose, Outline, Test-Log Identifier,Description, Activity and Event Entries, Execution Description, Procedure Results,Environmental Information, Anomalous Events, Incident-Report Identifiers A Test Incident report describes any event that occurs during t\u00aehe test executionthat requires further investigationDetails to be included in the report - Purpose, Outline, Test-Incident-Report Identifier,Summary, Impact A test summary report summarizes the testing activities associated with one ormore test-design specificationsDetails to b\u00aee included in the report - Purpose, Outline, Test-Summary-ReportIdentifier, Summary, Variances, Comprehensiveness Assessment, Summary of Results,Summary of Activities, and Approvals.Automating testing is no different from a programmer using a coding language towrite programs to automate any manual process. One of the problems with testinglarge systems is that it can go beyond the scope of small test teams. Because only asmall number of testers are available the coverage and depth of testing provided areinadequate for the task at hand.Expanding the test team beyond a certain size also becomes problematic withincrease in work over head. Feasible way to avoid this without introducing a loss ofquality is through appropriate use of tools that can expand individual's capacityenormously while maintaining the focus (depth) of testing upon the critical elements.Consider the following factors that help determine the use of automated to make changes",
  "page319": "Fully manual testing has the benefit of being relatively cheap and effective. But asquality of the product improves the additional cost for finding further bugs becomesmore expensive. Large scal\u00aee manual testing also implies large scale testing teams withthe related costs of space overhead and infrastructure. Manual testing is also farmore responsive and flexible than automated testing but is prone to tester errorthrough fatigue.Fu\u00aelly automated testing is very consistent and allows the repetitions of similar testsat very little marginal cost. The setup and purchase costs of such automation are veryhigh however and maintenance can be equally expensive. Automation is alsorelatively inflexible and requires rework in order to adapt to changing requirements.Partial Autom\u00aeation incorporates automation only where the most benefits can beachieved. The advantage is that it targets specifically the tasks for automation andthus achieves the most benefit from them. It also retains a large component ofmanual testing which maintains the test team's flexibility and offers redundancy bybacking up automation with manual testing. The disadvantage is that it obviously\u00aedoes not provide as extensive benefits as either extreme solution.Take time to define the tool requirements in terms of technology, process,applications, people skills, and organization.During tool evaluation, prioritize which test types are the most critical to yoursuccess and\u00aejudge the candidate tools on those criteria. Understand the tools and their trade-offs. You may need to use a multi-tool solutionto get higher levels of test-type coverage. For example, you will need to combinethe capture/play-back tool with a load-test tool to cover your performance testcases. Involve potential users in the definition of tool requirements and evaluation criteria. Build an evaluation scorecard to compare each tool's performance against acommon set of criteria. Rank the criteria in terms of relative importance to theorganization. Buying the Wrong Tool Inadequate Test Team Organization Lack of Management Support Incomplete Coverage of Test Types by the selected tool Inadequate Tool Training Difficulty using the tool Lack of a Basic Test Process or Understanding of What to Test Lack of Configuration Management Processes Lack of Tool Compatibility and InteroperabilityLack of Tool Availability",
  "page320": " oversight, Software subcontractmanagement, Software quality assurance, Software configuration managementLevel 3 - Defined: Key practice areas - Organization process focus, Organizationproc\u00aeess definition, Training program, integrated software management, Softwareproduct engineering, intergroup coordination, Peer reviewsLevel 4 - Manageable: Key practice areas - Quantitative Process Management,Software Quality ManagementL\u00aeevel 5 - Optimizing: Key practice areas - Defect prevention, Technology changemanagement, Process change managementSix Sigma is a quality management program to achieve \"six sigma\" levels of quality. Itwas pioneered by Motorola in the mid-1980s and has spread too many othermanufacturing companies, notably General Electric Corporation\u00ae(GE).Six Sigma is a rigorous and disciplined methodology that uses data and statisticalanalysis to measure and improve a company's operational performance by identifyingand eliminating \"defects\" from manufacturing to transactional and from product toservice. Commonly defined as 3.4 defects per million opportunities, Six Sigma can bedefined and understood at three distinct levels: metric, m\u00aeethodology andphilosophy.Training Sigma processes are executed by Six Sigma Green Belts and Six Sigma BlackBelts, and are overseen by Six Sigma Master Black BeltsISO - International Organization for Standardization is a network of the nationalstandards institutes of 150 countri\u00aees, on the basis of one member per country, with aCentral Secretariat in Geneva, Switzerland, that coordinates the system. ISO is a nongovernmental organization. ISO has developed over 13, 000 International Standardson a variety of subjects.",
  "page321": "Capability Maturity git Model - Developed by the software community in 1986 withleadership from the SEI. The CMM describes the principles and practices underlyingsoftware process maturity. It is\u00aeintended to help software organizations improve thematurity of their software processes in terms of an evolutionary path from ad hoc,chaotic processes to mature, disciplined software processes. The focus is onidentifying key process areas\u00aeand the exemplary practices that may comprise adisciplined software process.What makes up the CMM? The CMM is organized into five maturity levels: Initial Repeatable Defined ManageableOptimizingExcept for Level 1, each maturity level decomposes into several key process areasthat indicate the areas an organization should focus on to improve\u00aeits softwareprocess.Level 1 - Initial Level: Disciplined process, Standard, Consistent process, Predictableprocess, Continuously Improving processLevel 2 - Repeatable: Key practice areas - Requirements management, Softwareproject planning, Software project tracking The two concepts are explainedas follows: Verification: This kind of activity helps us in evaluating a software systemby d\u00aeetermining whether the product of a given development phase satisfiesthe requirements established before the start of that phase. One may notethat a product can be an intermediate product, such as requirement specification, design specification, code, user manual, or even the f\u00aeinal product.Activities that check the correctness of a development phase are calledverification activities. Validation: Activities of this kind help us in confirming that a productmeets its intended use. Validation activities aim at confirming that a productmeets its customer's expectations. In other words, validation activities focuson the final product, which is extensively tested from the customer point ofview. Validation establishes whether the product meets overall expectationsof the users.Late execution of validation activities is often risky by leading tohigher development cost. Validation activities may be executed at earlystages of the software development cycle [28]. An example of early execution of validation activities can be found in the eXtreme Programming",
  "page322": "Following are some facts that can help you gain a better insight into the realities ofSoftware Engineering.1. The best programmers are up to 28 times better than the worst programmers.2. New too\u00aels/techniques cause an initial LOSS of productivity/quality.3. The answer to a feasibility study is almost always \"yes\".4. A May 2002 report prepared for the National Institute of Standards andTechnologies (NIST)(1) estimate the a\u00aennual cost of software defects in the UnitedStates as $59.5 billion.5. Reusable components are three times as hard to build6. For every 25% increase in problem complexity, there is a 100% increase in solutioncomplexity.7. 80% of software work is intellectual. A fair amount of it is creative. Little of it isclerical.8. Requirements errors a\u00aere the most expensive to fix during production.9. Missing requirements are the hardest requirement errors to correct.10. Error-removal is the most time-consuming phase of the life cycle.11. Software is usually tested at best at the 55-60% (branch) coverage level.12. 100% coverage is still far from enough.13. Rigorous inspections can remove up to 90% of errors before the first test case isrun.\u00ae15. Maintenance typically consumes 40-80% of software costs. It is probably the mostimportant life cycle phase of software.16. Enhancements represent roughly 60% of maintenance costs.17. There is no single best approach to software error removal.",
  "page323": "Testing plays an important role in achieving and assessing the quality of a softwareproduct [25]. On the one hand, we improve the quality of the products as we repeata test-find defects\u00ae13fix cycle during development. On the other hand, we assess howgood our system is when we perform system-level tests before releasing a product.Thus, as Friedman and Voas [26] have succinctly described, software testing is averification proces\u00aes for software quality assessment and improvement. Generallyspeaking, the activities for software quality assessment can be divided into twobroad categories, namely, static analysis and dynamic analysis. Static Analysis: As the term \"static\" suggests, it is based on the examination of a number of documents, namely requirements do\u00aecuments, softwaremodels, design documents, and source code. Traditional static analysisincludes code review, inspection, walk-through, algorithm analysis, andproof of correctness. It does not involve actual execution of the code underdevelopment. Instead, it examines code and reasons over all possible behaviors that might arise during run time. Compiler optimizations are standardstatic analys\u00aeis.Dynamic Analysis: Dynamic analysis of a software system involves actualprogram execution in order to expose possible program failures. The behavioral and performance properties of the program are also observed. Programs are executed with both typical and carefully chosen inp\u00aeut values.Often, the input set of a program can be impractically large. However, forpractical considerations, a finite subset of the input set can be selected.Therefore, in testing, we observe some representative program behaviorsand reach a conclusion about the quality of the system. Careful selectionof a finite test set is crucial to reaching a reliable conclusion.",
  "page324": "By performing static and dynamic analyses, practitioners want to identify as manyfaults as possible so that those faults are fixed at an early stage of the softwaredevelopment. Static analysis a\u00aend dynamic analysis are complementary in nature,and for better effectiveness, both must be performed repeatedly and alternated.Practitioners and researchers need to remove the boundaries between static anddynamic analysis and create a hybri\u00aed analysis that combines the strengths of bothapproachesTwo similar concepts related to software testing frequently used by practitioners areverification and validation. Both concepts are abstract in nature, and each can berealized by a set of concrete, executable activities. The two concepts are explainedas follows: Verification: This kin\u00aed of activity helps us in evaluating a software systemby determining whether the product of a given development phase satisfiesthe requirements established before the start of that phase. One may notethat a product can be an intermediate product, such as requirement specification, design specification, code, user manual, or even the final product.Activities that check the correctness of a dev\u00aeelopment phase are calledverification activities. Validation: Activities of this kind help us in confirming that a productmeets its intended use. Validation activities aim at confirming that a productmeets its customer's expectations. In other words, validation activities\u00aefocuson the final product, which is extensively tested from the customer point ofview. Validation establishes whether the product meets overall expectationsof the users.Late execution of validation activities is often risky by leading tohigher development cost. Validation activities may be executed at earlystages of the software development cycle [28]. An example of early execution of validation activities can be found in the eXtreme Programming",
  "page325": "(XP) software development methodology. In the XP methodology, the customer closely interacts with the software development group and conductsacceptance tests during each development iteration [2\u00ae9].The verification process establishes the correspondence of an implementationphase of the software development process with its specification, whereas validationestablishes the correspondence between a system and users' expectations.\u00aeOne cancompare verification and validation as follows: Verification activities aim at confirming that one is building the product correctly, whereas validation activities aim at confirming that one is buildingthe correct product [30]. Verification activities review interim work products, such as requirementsspecification, design, code, an\u00aed user manual, during a project life cycle toensure their quality. The quality attributes sought by verification activitiesare consistency, completeness, and correctness at each major stage of system development. On the other hand, validation is performed toward theend of system development to determine if the entire system meets thecustomer's needs and expectations. Verification activit\u00aeies are performed on interim products by applying mostlystatic analysis techniques, such as inspection, walkthrough, and reviews,and using standards and checklists. Verification can also include dynamicanalysis, such as actual program execution. On the other hand, validationis\u00aeperformed on the entire system by actually running the system in its realenvironment and using a variety of testsIn the literature on software testing, one can find references to the terms failure,error, fault, and defect. Although their meanings are related, there are importantdistinctions between these four concepts. In the following, we present first threeterms as they are understood in the fault-tolerant computing community",
  "page326": "Failure: A failure is said to occur whenever the external behavior of asystem does not conform to that prescribed in the system specification. Error: An error is a state of the system. In\u00aethe absence of any correctiveaction by the system, an error state could lead to a failure which wouldnot be attributed to any event subsequent to the error. Fault: A fault is the adjudged cause of an error.A fault may remain undetecte\u00aed for a long time, until some event activates it. Whenan event activates a fault, it first brings the program into an intermediate error state.If computation is allowed to proceed from an error state without any correctiveaction, the program eventually causes a failure. As an aside, in fault-tolerant computing, corrective actions can be ta\u00aeken to take a program out of an error state intoa desirable state such that subsequent computation does not eventually lead to afailure. The process of failure manifestation can therefore be succinctly representedas a behavior chain [31] as follows: faulterrorfailure. The behavior chaincan iterate for a while, that is, failure of one component can lead to a failure ofanother inter\u00aeacting component.The above definition of failure assumes that the given specification is acceptable to the customer. However, if the specification does not meet the expectationsof the customer, then, of course, even a fault-free implementation fails to satisfy thecustomer. It i\u00aes a difficult task to give a precise definition of fault, error, or failureof software, because of the \"human factor\" involved in the overall acceptance of asystem. In an article titled \"What Is Software Failure\" [32], Ram Chillarege commented that in modern software business software failure means \"the customer'sexpectation has not been met and/or the customer is unable to do useful work withproduct,\" p. 354.Roderick Rees [33] extended Chillarege's comments of software failure bypointing out that \"failure is a matter of function only [and is thus] related to purpose,not to whether an item is physically intact or not\" (p. 163). To substantiate this,Behrooz Parhami [34] provided three interesting examples to show the relevanceof such a view point in wider context. One of the examples is quoted here",
  "page327": "The above definition of failure assumes that the given specification is acceptable to the customer. However, if the specification does not meet the expectationsof the customer, then, of course,\u00aeeven a fault-free implementation fails to satisfy thecustomer. It is a difficult task to give a precise definition of fault, error, or failureof software, because of the \"human factor\" involved in the overall acceptance of asystem\u00ae. In an article titled \"What Is Software Failure\" [32], Ram Chillarege commented that in modern software business software failure means \"the customer'sexpectation has not been met and/or the customer is unable to do useful work withproduct. The two concepts are explainedas follows: Verification: This kind of activity h\u00aeelps us in evaluating a software systemby determining whether the product of a given development phase satisfiesthe requirements established before the start of that phase. One may notethat a product can be an intermediate product, such as requirement specification, design specification, code, user manual, or even the final product.Activities that check the correctness of a development phase\u00aeare calledverification activities. Validation: Activities of this kind help us in confirming that a productmeets its intended use. Validation activities aim at confirming that a productmeets its customer's expectations. In other words, validation activities focuson the fin\u00aeal product, which is extensively tested from the customer point ofview. Validation establishes whether the product meets overall expectationsof the users.Late execution of validation activities is often risky by leading tohigher development cost. Validation activities may be executed at earlystages of the software development cycle [28]. An example of early execution of validation activities can be found in the eXtreme Programming",
  "page328": " dissatisfactions are errors in the organization's state. The organization's personnel or departmentsprobably begin to malfunction as result of the errors, in turn causing an overall d\u00aeegradation of performance. The end result can be the organization's failure to achieveits goal.There is a fine difference between defects and faults in the above example, thatis, execution of a defective policy may lead to a faulty pro\u00aemotion. In a softwarecontext, a software system may be defective due to design issues; certain systemstates will expose a defect, resulting in the development of faults defined as incorrect signal values or decisions within the system. In industry, the term defect iswidely used, whereas among researchers the term fault is more prevalent. F\u00aeor allpractical purpose, the two terms are synonymous. In this book, we use the twoterms interchangeably as required. The two concepts are explainedas follows: Verification: This kind of activity helps us in evaluating a software systemby determining whether the product of a given development phase satisfiesthe requirements established before the start of that phase. One may notethat a produ\u00aect can be an intermediate product, such as requirement specification, design specification, code, user manual, or even the final product.Activities that check the correctness of a development phase are calledverification activities. Validation: Activities of this kind help us i\u00aen confirming that a productmeets its intended use. Validation activities aim at confirming that a productmeets its customer's expectations. In other words, validation activities focuson the final product, which is extensively tested from the customer point ofview. Validation establishes whether the product meets overall expectationsof the users.Late execution of validation activities is often risky by leading tohigher development cost. Validation activities may be executed at earlystages of the software development cycle [28]. An example of early execution of validation activities can be found in the eXtreme Programming",
  "page329": "Roderick Rees [33] extended Chillarege's comments of software failure bypointing out that \"failure is a matter of function only [and is thus] related to purpose,not to whether an item\u00aeis physically intact or not\" (p. 163). To substantiate this,Behrooz Parhami [34] provided three interesting examples to show the relevanceof such a view point in wider context. One of the examples is quoted here (p. 451):Consider a sma\u00aell organization. Defects in the organization's staff promotion policies cancause improper promotions, viewed as faults. The resulting ineptitudes The two concepts are explainedas follows: Verification: This kind of activity helps us in evaluating a software systemby determining whether the product of a given development phase satisfi\u00aeesthe requirements established before the start of that phase. One may notethat a product can be an intermediate product, such as requirement specification, design specification, code, user manual, or even the final product.Activities that check the correctness of a development phase are calledverification activities. Validation: Activities of this kind help us in confirming that a productmee\u00aets its intended use. Validation activities aim at confirming that a productmeets its customer's expectations. In other words, validation activities focuson the final product, which is extensively tested from the customer point ofview. Validation establishes whether the pro\u00aeduct meets overall expectationsof the users.Late execution of validation activities is often risky by leading tohigher development cost. Validation activities may be executed at earlystages of the software development cycle [28]. An example of early execution of validation activities can be found in the eXtreme Programming",
  "page330": "No matter how many times we run the test-find faults-fix cycle during softwareNo matter how many times we run the test-find faults-fix cycle during softwaredevelopment, some\u00aefaults are likely to escape our attention, and these will eventually surface at the customer site. Therefore, a quantitative measure that is usefulin assessing the quality of a software is its reliability [35]. Software reliability isdefine\u00aed as the probability of failure-free operation of a software system for a specified time in a specified environment. The level of reliability of a system depends onthose inputs that cause failures to be observed by the end users. Software reliabilitycan be estimated via random testing, as suggested by Hamlet [36]. Since the notionof reliab\u00aeility is specific to a \"specified environment,\" test data must be drawn fromthe input distribution to closely resemble the future usage of the system. Capturing the future usage pattern of a system in a general sense is described in a formcalled the operational profileThe stakeholders in a test process are the programmers, the test engineers, theproject managers, and the customers.\u00aeA stakeholder is a person or an organizationwho influences a system's behaviors or who is impacted by that system [39].Different stakeholders view a test process from different perspectives as explainedbelow",
  "page331": "It does work: While implementing a program unit, the programmer maywant to test whether or not the unit works in normal circumstances. Theprogrammer gets much confidence if the unit works to his\u00aeor her satisfaction. The same idea applies to an entire system as well once a systemhas been integrated, the developers may want to test whether or not thesystem performs the basic functions. Here, for the psychological reason,the objectiv\u00aee of testing is to show that the system works, rather than itdoes not work.It does not work: Once the programmer (or the development team) issatisfied that a unit (or the system) works to a certain degree, more testsare conducted with the objective of finding faults in the unit (or the system).Here, the idea is to try to make the unit (or\u00aethe system) fail.Reduce the risk of failure: Most of the complex software systems containfaults, which cause the system to fail from time to time. This concept of\"failing from time to time\" gives rise to the notion of failure rate. Asfaults are discovered and fixed while performing more and more tests, thefailure rate of a system generally decreases. Thus, a higher level objectiveof\u00aeperforming tests is to bring down the risk of failing to an acceptablelevel.Reduce the cost of testing: The different kinds of costs associated with atest process includethe cost of designing, maintaining, and executing test cases,the cost of analyzing the result of executing\u00aeeach test case,the cost of documenting the test cases, andthe cost of actually executing the system and documenting it.",
  "page332": "Therefore, the less the number of test cases designed, the less will be theassociated cost of testing. However, producing a small number of arbitrarytest cases is not a good way of saving cost.\u00aeThe highest level of objectiveof performing tests is to produce low-risk software with fewer numberof test cases. This idea leads us to the concept of effectiveness of testcases. Test engineers must therefore judiciously select fewer, effec\u00aetive testcasesIn its most basic form, a test case is a simple pair of < input, expected outcome >.If a program under test is expected to compute the square root of nonnegativenumbers, then four examples of test cases are as shown in Figure 1.3.In stateless systems, where the outcome depends solely on the current input,test cases are very s\u00aeimple in structure, as shown in Figure 1.3. A program tocompute the square root of nonnegative numbers is an example of a statelesssystem. A compiler for the C programming language is another example of astateless system. A compiler is a stateless system because to compile a program itdoes not need to know about the programs it compiled previously.In state-oriented systems, where the program\u00aeoutcome depends both on thecurrent state of the system and the current input, a test case may consist of asequence of < input, expected outcome > pairs. A telephone switching system andan automated teller machine (ATM) are examples of state-oriented systems. For anATM machine,\u00aea test case for testing the withdraw function is shown in Figure 1.4.Here, we assume that the user has already entered validated inputs, such as the cashcard and the personal identification number (PIN).",
  "page333": "In the test case TS1, \"check balance\" and \"withdraw\" in the first, second, andfourth tuples represent the pressing of the appropriate keys on the ATM keypad. It isassumed tha\u00aet the user account has $500.00 on it, and the user wants to withdraw anamount of $200.00. The expected outcome \"$200.00\" in the third tuple representsthe cash dispensed by the ATM. After the withdrawal operation, the user makessur\u00aee that the remaining balance is $300.00.For state-oriented systems, most of the test cases include some form of decision and timing in providing input to the system. A test case may include loopsand timers, which we do not show at this moment.An outcome of program execution is a complex entity that may include thefollowing:Values produced\u00aeby the program:Outputs for local observation (integer, text, audio, image)Outputs (messages) for remote storage, manipulation, or observationState change:State change of the programState change of the database (due to add, delete, and update operations) A sequence or set of values which must be interpreted together for theoutcome to be validAn important concept in test design is the concept o\u00aef an oracle. An oracleis any entity program, process, human expert, or body of data that tells us theexpected outcome of a particular test or set of tests [40]. A test case is meaningfulonly if it is possible to decide on the acceptability of the result produced by theprogram u\u00aender test.Ideally, the expected outcome of a test should be computed while designingthe test case. In other words, the test outcome is computed before the program is executed with the selected test input.",
  "page334": "The idea here is that one should be able tocompute the expected outcome from an understanding of the program's requirements. Precomputation of the expected outcome will eliminate any implem\u00aeentationbias in case the test case is designed by the developer.In exceptional cases, where it is extremely difficult, impossible, or evenundesirable to compute a single expected outcome, one should identify expectedoutcomes by examining th\u00aee actual test outcomes, as explained in the following:Execute the program with the selected input.Observe the actual outcome of program execution.Verify that the actual outcome is the expected outcome. Use the verified actual outcome as the expected outcome in subsequentruns of the test case It is not unusual to find people making claims s\u00aeuch as \"I have exhaustively testedthe program.\" Complete, or exhaustive, testing means there are no undiscoveredfaults at the end of the test phase. All problems must be known at the end ofcomplete testing. For most of the systems, complete testing is near impossiblebecause of the following reasons: The domain of possible inputs of a program is too large to be completelyused in test\u00aeing a system. There are both valid inputs and invalid inputs.The program may have a large number of states. There may be timingconstraints on the inputs, that is, an input may be valid at a certain timeand invalid at other times. An input value which is valid but is not properl\u00aeytimed is called an inopportune input. The input domain of a system canbe very large to be completely used in testing a program.",
  "page335": "The design issues may be too complex to completely test. The design mayhave included implicit design decisions and assumptions. For example,a programmer may use a global variable or a static var\u00aeiable to controlprogram execution. It may not be possible to create all possible execution environments of thesystem. This becomes more significant when the behavior of the softwaresystem depends on the real, outside world, such as weather,\u00aetemperature,altitude, pressure, and so on.We must realize that though the outcome of complete testing, that is, discovering allfaults, is highly desirable, it is a near-impossible task, and it may not be attempted.The next best thing is to select a subset of the input domain to test a program.Referring to Figure 1.5, let D be the input do\u00aemain of a program P. Suppose thatwe select a subset D1 of D, that is, D1 \u2282 D, to test program P. It is possible thatD1 exercises only a part P1, that is, P1 \u2282 P, of the execution behavior of P, inwhich case faults with the other part, P2, will go undetected.By selecting a subset of the input domain D1, the test engineer attemptsto deduce properties of an entire program P by observin\u00aeg the behavior of a partP1 of the entire behavior of P on selected inputs D1. Therefore, selection of thesubset of the input domain must be done in a systematic and careful manner sothat the deduction is as accurate and complete as possible. For example, the ideaof coverage is\u00aeconsidered while selecting test cases In order to test a program, a test engineer must perform a sequence of testingactivities. Most of these activities have been shown in Figure 1.6 and are explainedin the following. ",
  "page336": "Identify an objective to be tested: The first activity is to identify anobjective to be tested. The objective defines the intention, or purpose, ofdesigning one or more test cases to ensure that\u00aethe program supports theobjective. A clear purpose must be associated with every test caseSelect inputs: The second activity is to select test inputs. Selection of testinputs can be based on the requirements specification, the source code,\u00aeor our expectations. Test inputs are selected by keeping the test objectivein mind.Compute the expected outcome: The third activity is to compute theexpected outcome of the program with the selected inputs. In most cases,this can be done from an overall, high-level understanding of the testobjective and the specification of the program und\u00aeer test.Set up the execution environment of the program: The fourth step is toprepare the right execution environment of the program. In this step all theassumptions external to the program must be satisfied. A few examples ofassumptions external to a program are as follows:Initialize the local system, external to the program. This may includemaking a network connection available, making the\u00aeright databasesystem available, and so on.Initialize any remote, external system (e.g., remote partner process in adistributed application.) For example, to test the client code, we mayneed to start the server at a remote siteExecute the program: In the fifth step, the test eng\u00aeineer executes theprogram with the selected inputs and observes the actual outcome of theprogram. To execute a test case, inputs may be provided to the program atdifferent physical locations at different times. The concept of test coordination is used in synchronizing different components of a test case",
  "page337": "Analyze the test result: The final test activity is to analyze the result oftest execution. Here, the main task is to compare the actual outcome ofprogram execution with the expected outcome. Th\u00aee complexity of comparison depends on the complexity of the data to be observed. The observeddata type can be as simple as an integer or a string of characters or ascomplex as an image, a video, or an audio clip. At the end of the analysis\u00aestep, a test verdict is assigned to the program. There are three majorkinds of test verdicts, namely, pass, fail, and inconclusive, as explainedbelow.If the program produces the expected outcome and the purpose of thetest case is satisfied, then a pass verdict is assigned.If the program does not produce the expected outcome, then a fail ve\u00aerdictis assigned.However, in some cases it may not be possible to assign a clear passor fail verdict. For example, if a timeout occurs while executing atest case on a distributed application, we may not be in a position toassign a clear pass or fail verdict. In those cases, an inconclusive testverdict is assigned. An inconclusive test verdict means that furthertests are needed to be done to r\u00aeefine the inconclusive verdict into aclear pass or fail verdict A test report must be written after analyzing the test result. Themotivation for writing a test report is to get the fault fixed if the test revealeda fault. A test report contains the following items to be informa\u00aetive:Explain how to reproduce the failure.Analyze the failure to be able to describe it.A pointer to the actual outcome and the test case, complete with theinput, the expected outcome, and the execution environment.",
  "page338": "Testing is performed at different levels involving the complete system or parts ofit throughout the life cycle of a software product. A software system goes throughfour stages of testing before\u00aeit is actually deployed. These four stages are knownas unit, integration, system, and acceptance level testing. The first three levels oftesting are performed by a number of different stakeholders in the developmentorganization, where as ac\u00aeceptance testing is performed by the customers. The fourstages of testing have been illustrated in the form of what is called the classical Vmodel In unit testing, programmers test individual program units, such as a procedures, functions, methods, or classes, in isolation. After ensuring that individualunits work to a satisfactory extent,\u00aemodules are assembled to construct larger subsystems by following integration testing techniques. Integration testing is jointlyperformed by software developers and integration test engineers. The objective ofintegration testing is to construct a reasonably stable system that can withstandthe rigor of system-level testing. System-level testing includes a wide spectrumof testing, such as func\u00aetionality testing, security testing, robustness testing, loadtesting, stability testing, stress testing, performance testing, and reliability testing.System testing is a critical phase in a software development process because of theneed to meet a tight schedule close to delive\u00aery date, to discover most of the faults,and to verify that fixes are working and have not resulted in new faults. Systemtesting comprises a number of distinct activities: creating a test plan, designinga test suite, preparing test environments, executing the tests by following a clearstrategy, and monitoring the process of test execution.",
  "page339": "Regression testing is another level of testing that is performed throughout thelife cycle of a system. Regression testing is performed whenever a component ofthe system is modified. The key idea\u00aein regression testing is to ascertain that themodification has not introduced any new faults in the portion that was not subjectto modification. To be precise, regression testing is not a distinct level of testing.Rather, it is considered\u00aeas a subphase of unit, integration, and system-level testing,as illustrated in Figure 1.8 [41].In regression testing, new tests are not designed. Instead, tests are selected,prioritized, and executed from the existing pool of test cases to ensure that nothingis broken in the new version of the software. Regression testing is an expensivepr\u00aeocess and accounts for a predominant portion of testing effort in the industry. Itis desirable to select a subset of the test cases from the existing pool to reduce thecost. A key question is how many and which test cases should be selected so thatthe selected test cases are more likely to uncover new faults [42-44].After the completion of system-level testing, the product is delivered t\u00aeo thecustomer. The customer performs their own series of tests, commonly known asacceptance testing. The objective of acceptance testing is to measure the qualityof the product, rather than searching for the defects, which is objective of systemtesting. A key notion in acceptan\u00aece testing is the customer's expectations from thesystem. By the time of acceptance testing, the customer should have developedtheir acceptance criteria based on their own expectations from the system. Thereare two kinds of acceptance testing as explained in the following: User acceptance testing (UAT)Business acceptance testing (BAT)",
  "page340": "User acceptance testing is conducted by the customer to ensure that the systemsatisfies the contractual acceptance criteria before being signed off as meeting userneeds. On the other hand, BAT i\u00aes undertaken within the supplier's developmentorganization. The idea in having a BAT is to ensure that the system will eventuallypass the user acceptance test. It is a rehearsal of UAT at the supplier's premises.Designing test cas\u00aees has continued to stay in the foci of the research communityand the practitioners. A software development process generates a large body ofinformation, such as requirements specification, design document, and source code.In order to generate effective tests at a lower cost, test designers analyze thefollowing sources of information: Requ\u00aeirements and functional specificationsSource codeinput and output domainsOperational profileFault modelRequirements and Functional Specifications The process of software development begins by capturing user needs. The nature and amount of user needsidentified at the beginning of system development will vary depending on thespecific life-cycle model to be followed. Let us consider a few exampl\u00aees. In theWaterfall model [45] of software development, a requirements engineer tries tocapture most of the requirements. On the other hand, in an agile software development model, such as XP [29] or the Scrum [46-48], only a few requirementsare identified in the beginning\u00ae. A test engineer considers all the requirementsthe program is expected to meet whichever life-cycle model is chosen to test aprogram",
  "page341": "The requirements might have been specified in an informal manner, such asa combination of plaintext, equations, figures, and flowcharts. Though this form ofrequirements specification may be ambi\u00aeguous, it is easily understood by customers.For example, the Bluetooth specification consists of about 1100 pages of descriptions explaining how various subsystems of a Bluetooth interface is expected towork. The specification is written in\u00aeplaintext form supplemented with mathematical equations, state diagrams, tables, and figures. For some systems, requirementsmay have been captured in the form of use cases, entity-relationship diagrams,and class diagrams. Sometimes the requirements of a system may have been specified in a formal language or notation, such as Z, SDL,\u00aeEstelle, or finite-statemachine. Both the informal and formal specifications are prime sources of testcasesSource Code Whereas a requirements specification describes the intendedbehavior of a system, the source code describes the actual behavior of the system.High-level assumptions and constraints take concrete form in an implementation.Though a software designer may produce a detailed design\u00ae, programmers mayintroduce additional details into the system. For example, a step in the detaileddesign can be \"sort array A.\" To sort an array, there are many sorting algorithmswith different characteristics, such as iteration, recursion, and temporarily usinganothe\u00aer array. Therefore, test cases must be designed based on the program [50].Input and Output Domains Some values in the input domain of a programhave special meanings, and hence must be treated separately [5]. To illustrate thispoint, let us consider the factorial function.",
  "page342": "without considering the special case of n 0. The above wrong implementationwill produce the correct result for all positive values of n, but will fail for n = 0.Sometimes even some output value\u00aes have special meanings, and a programmust be tested to ensure that it produces the special values for all possible causes.In the above example, the output value 1 has special significance: (i) it is theminimum value computed by the factori\u00aeal function and (ii) it is the only valueproduced for two different inputs.In the integer domain, the values 0 and 1 exhibit special characteristicsif arithmetic operations are performed. These characteristics are 0 X x = 0 and1 X x = x for all values of x. Therefore, all the special values in the input andoutput domains of a pro\u00aegram must be considered while testing the program.Operational Profile As the term suggests, an operational profile is a quantitative characterization of how a system will be used. It was created to guide testengineers in selecting test cases (inputs) using samples of system usage. The notionof operational profiles, or usage profiles, was developed by Mills et al.The idea isto infer, from the\u00aeobserved test results, the future reliability of the software whenit is in actual use. To do this, test inputs are assigned a probability distribution, orprofile, according to their occurrences in actual operation. The ways test engineersassign probability and select test cases\u00aeto operate a system may significantly differfrom the ways actual users operate a system. However, for accurate estimationof the reliability of a system it is important to test a system by considering theways it will actually be used in the field.",
  "page343": "Fault Model Previously encountered faults are an excellent source of information in designing new test cases. The known faults are classified into differentclasses, such as initialization faults\u00ae, logic faults, and interface faults, and stored ina repository [55, 56]. Test engineers can use these data in designing tests to ensurethat a particular class of faults is not resident in the program.There are three types of fault-based te\u00aesting: error guessing, fault seeding,and mutation analysis. In error guessing, a test engineer applies his experienceto (i) assess the situation and guess where and what kinds of faults might exist,and (ii) design tests to specifically expose those kinds of faults. In fault seeding,known faults are injected into a program, and the test sui\u00aete is executed to assessthe effectiveness of the test suite. Fault seeding makes an assumption that a testsuite that finds seeded faults is also likely to find other faults. Mutation analysis issimilar to fault seeding, except that mutations to program statements are made inorder to determine the fault detection capability of the test suite. If the test cases arenot capable of revealing such\u00aefaults, the test engineer may specify additional testcases to reveal the faults. Mutation testing is based on the idea of fault simulation,whereas fault seeding is based on the idea of fault injection. In the fault injectionapproach, a fault is inserted into a program, and an o\u00aeracle is available to assert thatthe inserted fault indeed made the program incorrect. On the other hand, in faultsimulation, a program modification is not guaranteed to lead to a faulty program.In fault simulation, one may modify an incorrect program and turn it into a correctprogram.",
  "page344": "A key idea in Section was that test cases need to be designed by considering information from several sources, such as the specification, source code, andspecial properties of the program'\u00aes input and output domains. This is because allthose sources provide complementary information to test designers. Two broad concepts in testing, based on the sources of information for test design, are white-boxand black-box testing. White-\u00aebox testing techniques are also called structural testing techniques, whereas black-box testing techniques are called functional testingtechniques.In structural testing, one primarily examines source code with a focus on control flow and data flow. Control flow refers to flow of control from one instructionto another. Control passes from o\u00aene instruction to another instruction in a numberof ways, such as one instruction appearing after another, function call, messagepassing, and interrupts. Conditional statements alter the normal, sequential flowof control in a program. Data flow refers to the propagation of values from onevariable or constant to another variable. Definitions and uses of variables determinethe data flow aspect\u00aein a program.In functional testing, one does not have access to the internal details of aprogram and the program is treated as a black box. A test engineer is concernedonly with the part that is accessible outside the program, that is, just the inputand the externally visible o\u00aeutcome. A test engineer applies input to a program,observes the externally visible outcome of the program, and determines whetheror not the program outcome is the expected outcome. Inputs are selected fromthe program's requirements specification and properties of the program's input andoutput domains. A test engineer is concerned only with the functionality and thefeatures found in the program's specification",
  "page345": "At this point it is useful to identify a distinction between the scopes ofstructural testing and functional testing. One applies structural testing techniquesto individual units of a program, wh\u00aeereas functional testing techniques can beapplied to both an entire system and the individual program units. Since individualprogrammers know the details of the source code they write, they themselvesperform structural testing on the indivi\u00aedual program units they write. On the otherhand, functional testing is performed at the external interface level of a system,and it is conducted by a separate software quality assurance group.Let us consider a program unit U which is a part of a larger program P.A program unit is just a piece of source code with a well-defined objective an\u00aedwell-defined input and output domains. Now, if a programmer derives test casesfor testing U from a knowledge of the internal details of U , then the programmeris said to be performing structural testing. On the other hand, if the programmerdesigns test cases from the stated objective of the unit U and from his or herknowledge of the special properties of the input and output domains of U , t\u00aehen heor she is said to be performing functional testing on the same unit U .a of functional testing.Neither structural testing nor functional testing is by itself good enough todetect most of the faults. Even if one selects all possible inputs, a structural testingtechnique ca\u00aennot detect all faults if there are missing paths in a program. Intuitively,a path is said to be missing if there is no code to handle a possible condition.",
  "page346": "Similarly, without knowledge of the structural details of a program, many faultswill go undetected. Therefore, a combination of both structural and functionaltesting techniques must be used in p\u00aerogram testing.The purpose of system test planning, or simply test planning, is to get ready andorganized for test execution. A test plan provides a framework, scope, details ofresource needed, effort required, schedule of activities, and a\u00aebudget. A frameworkis a set of ideas, facts, or circumstances within which the tests will be conducted.The stated scope outlines the domain, or extent, of the test activities. The scopecovers the managerial aspects of testing, rather than the detailed techniques andspecific test cases.Test design is a critical phase of software testing. D\u00aeuring the test designphase, the system requirements are critically studied, system features to betested are thoroughly identified, and the objectives of test cases and the detailedbehavior of test cases are defined. Test objectives are identified from differentsources, namely, the requirement specification and the functional specification,and one or more test cases are designed for each test\u00aeobjective. Each test case isdesigned as a combination of modular test components called test steps. Thesetest steps can be combined together to create more complex, multistep tests. Atest case is clearly specified so that others can easily borrow, understand, andreuse it ",
  "page347": "Monitoring and measurement are two key principles followed in every scientific andengineering endeavor. The same principles are also applicable to the testing phasesof software development. It i\u00aes important to monitor certain metrics which trulyrepresent the progress of testing and reveal the quality level of the system. Basedon those metrics, the management can trigger corrective and preventive actions. Byputting a small but criti\u00aecal set of metrics in place the executive management willbe able to know whether they are on the right track [58]. Test execution metricscan be broadly categorized into two classes as follows: Metrics for monitoring test execution Metrics for monitoring defectsThe first class of metrics concerns the process of executing test cases, whereas\u00aethe second class concerns the defects found as a result of test execution. Thesemetrics need to be tracked and analyzed on a periodic basis, say, daily or weekly.In order to effectively control a test project, it is important to gather valid andaccurate information about the project. One such example is to precisely knowwhen to trigger revert criteria for a test cycle and initiate root cause\u00aeanalysis ofthe problems before more tests can be performed. By triggering such a revertcriteria, a test manager can effectively utilize the time of test engineers, and possibly money, by suspending a test cycle on a product with too many defects tocarry out a meaningful system\u00aetest. A management team must identify and monitor metrics while testing is in progress so that important decisions can be made",
  "page348": " It is important to analyze and understand the test metrics, rather than justcollect data and make decisions based on those raw data. Metrics are meaningful only if they enable the management to\u00aemake decisions which result in lowercost of production, reduced delay in delivery, and improved quality of softwaresystems.Quantitative evaluation is important in every scientific and engineering field.Quantitative evaluation is carried ou\u00aet through measurement. Measurement lets oneevaluate parameters of interest in a quantitative manner as follows: Evaluate the effectiveness of a technique used in performing a task. Onecan evaluate the effectiveness of a test generation technique by countingthe number of defects detected by test cases generated by following thetechnique and\u00aethose detected by test cases generated by other means.Evaluate the productivity of the development activities. One can keep trackof productivity by counting the number of test cases designed per day, thenumber of test cases executed per day, and so on. Evaluate the quality of the product. By monitoring the number of defectsdetected per week of testing, one can observe the quality level of th\u00aeesystem. Evaluate the product testing. For evaluating a product testing process, thefollowing two measurements are critical:incises in test design. The need for more testing occursas test engineers get new ideas while executing the planned testcases.Test effort effectiveness m\u00aeetric: It is important to evaluate the effectiveness of the testing effort in the development of a product. After aproduct is deployed at the customer's site, one is interested to knowthe effectiveness of testing that was performed",
  "page349": "A common measureof test effectiveness is the number of defects found by the customersthat were not found by the test engineers prior to the release of theproduct. These defects had escaped our t\u00aeest effort.In general, software testing is a highly labor intensive task. This is because test casesare to a great extent manually generated and often manually executed. Moreover,the results of test executions are manually analyzed. The dur\u00aeations of those taskscan be shortened by using appropriate tools. A test engineer can use a variety oftools, such as a static code analyzer, a test data generator, and a network analyzer,if a network-based application or protocol is under test. Those tools are useful inincreasing the efficiency and effectiveness of testing.Test automation\u00aeis essential for any testing and quality assurance division ofan organization to move forward to become more efficient. The benefits of testautomation are as follows: Increased productivity of the testers Better coverage of regression testing Reduced durations of the testing phases Reduced cost of software maintenance Increased effectiveness of test casesTest automation provides an opportunit\u00aey to improve the skills of the testengineers by writing programs, and hence their morale. They will be more focusedon developing automated test cases to avoid being a bottleneck in product deliveryto the market. Consequently, software testing becomes less of a tedious job.Test\u00aeautomation improves the coverage of regression testing because of accumulation of automated test cases over time. Automation allows an organization tocreate a rich library of reusable test cases and facilitates the execution of a consistent set of test cases. Here consistency means our ability to produce repeatedresults for the same set of tests",
  "page350": " It may be very difficult to reproduce test results inmanual testing, because exact conditions at the time and point of failure may notbe precisely known. In automated testing it is easier to se\u00aet up the initial conditionsof a system, thereby making it easier to reproduce test results. Test automationsimplifies the debugging work by providing a detailed, unambiguous log of activities and intermediate test steps. This leads to a mor\u00aee organized, structured, andreproducible testing approach.Automated execution of test cases reduces the elapsed time for testing, and,thus, it leads to a shorter time to market. The same automated test cases can beexecuted in an unsupervised manner at night, thereby efficiently utilizing the different platforms, such as hardware and config\u00aeuration. In short, automation increasestest execution efficiency. However, at the end of test execution, it is important toanalyze the test results to determine the number of test cases that passed or failed.And, if a test case failed, one analyzes the reasons for its failure.In the long run, test automation is cost-effective. It drastically reduces the software maintenance cost. In the susta\u00aeining phase of a software system, the regressiontests required after each change to the system are too many. As a result, regressiontesting becomes too time and labor intensive without automation.A repetitive type of testing is very cumbersome and expensive to performmanually,\u00aebut it can be automated easily using software tools. A simple repetitivetype of application can reveal memory leaks in a software. However, the applicationhas to be run for a significantly long duration, say, for weeks, to reveal memoryleaks. Therefore, manual testing may not be justified, whereas with automation itis easy to reveal memory leaks.",
  "page351": "For example, stress testing is a prime candidate forautomation. Stress testing requires a worst-case load for an extended period of time,which is very difficult to realize by manual means. Scala\u00aebility testing is anotherarea that can be automated. Instead of creating a large test bed with hundreds ofequipment, one can develop a simulator to verify the scalability of the system.Test automation is very attractive, but it comes with a\u00aeprice tag. Sufficienttime and resources need to be allocated for the development of an automated testsuite. Development of automated test cases need to be managed like a programmingproject. That is, it should be done in an organized manner; otherwise it is highlylikely to fail. An automated test suite may take longer to develop because th\u00aee testsuite needs to be debugged before it can be used for testing. Sufficient time andresources need to be allocated for maintaining an automated test suite and setting upa test environment. Moreover, every time the system is modified, the modificationmust be reflected in the automated test suite. Therefore, an automated test suiteshould be designed as a modular system, coordinated into reus\u00aeable libraries, andcross-referenced and traceable back to the feature being tested.It is important to remember that test automation cannot replace manual testing. Human creativity, variability, and observability cannot be mimicked throughautomation. Automation cannot detect som\u00aee problems that can be easily observedby a human being. Automated testing does not introduce minor variations the waya human can. Certain categories of tests, such as usability, interoperability, robustness, and compatibility, are often not suited for automation. It is too difficult toed.",
  "page352": "The objective of test automation is not to reduce the head counts in thetesting department of an organization, but to improve the productivity, quality, andefficiency of test execution. In fact,\u00aetest automation requires a larger head count inthe testing department in the first year, because the department needs to automatethe test cases and simultaneously continue the execution of manual tests. Even afterthe completion of the deve\u00aelopment of a test automation framework and test caselibraries, the head count in the testing department does not drop below its originallevel. The test organization needs to retain the original team members in order toimprove the quality by adding more test cases to the automated test case repository.Before a test automation project can pr\u00aeoceed, the organization must assessand address a number of considerations. The following list of prerequisites mustbe considered for an assessment of whether the organization is ready for testautomation:The test cases to be automated are well defined. Test tools and an infrastructure are in placeThe test automation professionals have prior successful experience inautomation. Adequate budget s\u00aehould have been allocated for the procurement of software tools.Testing is a distributed activity conducted at different levels throughout the lifecycle of a software. These different levels are unit testing, integration testing, system testing, and acceptance testing. It is lo\u00aegical to have different testing groups inan organization for each level of testing. However, it is more logical and is thecase in reality that unit-level tests be developed and executed by the programmersthemselves rather than an independent group of unit test engineers. The programmer who develops a software unit should take the ownership and responsibilityof producing good-quality software to his or her satisfaction.",
  "page353": "System integrationtesting is performed by the system integration test engineers. The integration testengineers involved need to know the software modules very well. This means thatall developmen\u00aet engineers who collectively built all the units being integratedneed to be involved in integration testing. Also, the integration test engineersshould thoroughly know the build mechanism, which is key to integrating largesystems.A team for\u00aeperforming system-level testing is truly separated from the development team, and it usually has a separate head count and a separate budget. Themandate of this group is to ensure that the system requirements have been met andthe system is acceptable. Members of the system test group conduct different categories of tests, such as function\u00aeality, robustness, stress, load, scalability, reliability,and performance. They also execute business acceptance tests identified in the useracceptance test plan to ensure that the system will eventually pass user acceptancetesting at the customer site. However, the real user acceptance testing is executedby the client's special user group. The user group consists of people from differen\u00aet backgrounds, such as software quality assurance engineers, business associates,and customer support engineers. It is a common practice to create a temporaryuser acceptance test group consisting of people with different backgrounds, suchas integration test engineers, system te\u00aest engineers, customer support engineers,and marketing engineers. Once the user acceptance is completed, the group is dismantled. It is recommended to have at least two test groups in an organization:integration test group and system test group.Hiring and retaining test engineers are challenging tasks. Interview is theprimary mechanism for evaluating applicants. Interviewing is a skill that improveswith practice. It is necessary to have a recruiting process in place in order to beeffective in hiring excellent test engineers. In order to retain test engineers, themanagement must recognize the importance of testing efforts at par with development efforts. The management should treat the test engineers as professionals andas a part of the overall team that delivers quality products",
  "page354": "With the above high-level introduction to quality and software testing, we are nowin a position to outline the remaining chapters. Each chapter in the book coverstechnical, process, and/or manag\u00aeerial topics related to software testing. The topicshave been designed and organized to facilitate the reader to become a software testspecialist. In Chapter 2 we provide a self-contained introduction to the theory andlimitations of softwar\u00aee testing.Chapters 3-6 treat unit testing techniques one by one, as quantitativelyas possible. These chapters describe both static and dynamic unit testing. Staticunit testing has been presented within a general framework called code review,rather than individual techniques called inspection and walkthrough. Dynamic unittesting, or ex\u00aeecution-based unit testing, focuses on control flow, data flow, anddomain testing. The JUnit framework, which is used to create and execute dynamicunit tests, is introduced. We discuss some tools for effectively performing unittesting.Chapter 7 discusses the concept of integration testing. Specifically, five kindsof integration techniques, namely, top down, bottom up, sandwich, big bang, andi\u00aencremental, are explained. Next, we discuss the integration of hardware and software components to form a complete system. We introduce a framework to developa plan for system integration testing. The chapter is completed with a brief discussion of integration testing of off-th\u00aee-shelf components.Chapters 8-13 discuss various aspects of system-level testing. These sixchapters introduce the reader to the technical details of system testing that is thepractice in industry. These chapters promote both qualitative and quantitative evaluation of a system testing process. The chapters emphasize the need for having anindependent system testing group. A process for monitoring and controlling system testing is clearly explained. Chapter 14 is devoted to acceptance testing, whichincludes acceptance testing criteria, planning for acceptance testing, and acceptancetest execution.",
  "page355": "execution.Chapter 15 contains the fundamental concepts of software reliability and theirapplication to software testing. We discuss the notion of operation profile and itsapplication in system t\u00aeesting. We conclude the chapter with the description of anexample and the time of releasing a system by determining the additional lengthof system testing. The additional testing time is calculated by using the idea ofsoftware reliability.I\u00aen Chapter 16, we present the structure of test groups and how these groupscan be organized in a software company. Next, we discuss how to hire and retaintest engineers by providing training, instituting a reward system, and establishingan attractive career path for them within the testing organization. We conclude thischapter with the desc\u00aeription of how to build and manage a test team with a focuson teamwork rather than individual gain.Chapters 17 and 18 explain the concepts of software quality and differentmaturity models. Chapter 17 focuses on quality factors and criteria and describesthe ISO 9126 and ISO 9000:2000 standards. Chapter 18 covers the CMM, whichwas developed by the SEI at Carnegie Mellon University. Two test-rel\u00aeated models,namely the TPI model and the TMM, are explained at the end of Chapter 18.We define the key words used in the book in a glossary at the end of the book.The reader will find about 10 practice exercises at the end of each chapter. A listof references is included at the\u00aeend of each chapter for a reader who would like tofind more detailed discussions of some of the topics. Finally, each chapter, exceptthis one, contains a literature review section that, essentially, provides pointers tomore advanced material related to the topics. The more advanced materials arebased on current research and alternate viewpoints.",
  "page356": "Any approach to testing is based on assumptions about the way program faultsoccur. Faults are due to two main reasons:Faults occur due to our inadequate understanding of all conditions withwhich\u00aea program must deal.Faults occur due to our failure to realize that certain combinations of conditions require special treatments.Goodenough and Gerhart classify program faults as follows:Logic Fault: This class of faults means a program p\u00aeroduces incorrectresults independent of resources required. That is, the program fails becauseof the faults present in the program and not because of a lack of resources.Logic faults can be further split into three categories:Requirements fault: This means our failure to capture the real requirements of the customer.Design fault: This repr\u00aeesents our failure to satisfy an understoodrequirement.Construction fault: This represents our failure to satisfy a design. Suppose that a design step says \"Sort array A.\" To sort the array with Nelements, one may choose one of several sorting algorithms. Let {:}be the desired for loop construct to sort the array. If a programmerwrites the for loop in the formfor (i = 0; i <= N; i \u00ae){:}then there is a construction error in the implementation.Performance Fault: This class of faults leads to a failure of the programto produce expected results within specified or desired resource limitations.A thorough test must be able to detect faults arising from any of t\u00aehe abovereasons. Test data selection criteria must reflect information derived from each stageof software development. Since each type of fault is manifested as an impropereffect produced by an implementation, it is useful to categorize the sources of faultsin implementation terms as folows: ",
  "page357": "Missing Control Flow Paths: Intuitively, a control flow path, or simply apath, is a feasible sequence of instructions in a program. A path may bemissing from a program if we fail to identify a c\u00aeondition and specify apath to handle that condition. An example of a missing path is our failureto test for a zero divisor before executing a division. If we fail to recognizethat a divisor can take a zero value, then we will not include a\u00aepiece ofcode to handle the special case. Thus, a certain desirable computation willbe missing from the program.Inappropriate Path Selection: A program executes an inappropriate path ifa condition is expressed incorrectly. we show a desiredbehavior and an implemented behavior. Both the behaviors are identicalexcept in the condition part of\u00aethe if statement. The if part of the implemented behavior contains an additional condition B. It is easy to see that both the desired part and the implemented part behave in the same wayfor all combinations of values of A and B except when A = 1 and B = 0.Inappropriate or Missing Action: There are three instances of this class offault One may calculate a value using a method that does not ne\u00aecessarilygive the correct result. For example, a desired expression is x = x X w,whereas it is wrongly written as x = x  w. These two expressionsproduce identical results for several combinations of x and w, such asx = 1.5 and w = 3, for example.Failing to assign a value\u00aeto a variable is an example of a missing action. Calling a function with the wrong argument list is a kind of inappropriateaction.",
  "page358": "The main danger due to an inappropriate or missing action is that the action isincorrect only under certain combinations of conditions. Therefore, one must dothe following to find test data that\u00aereliably reveal errors: Identify all the conditions relevant to the correct operation of a program.Select test data to exercise all possible combinations of these conditions.The above idea of selecting test data leads us to define the foll\u00aeowing terms:Test Data: Test data are actual values from the input domain of a programthat collectively satisfy some test selection criteria.Test Predicate: A test predicate is a description of conditions and combinations of conditions relevant to correct operation of the program:Test predicates describe the aspects of a program that are to\u00aebe tested.Test data cause these aspects to be tested.Test predicates are the motivating force for test data selection.Components of test predicates arise first and primarily from the specifications for a program.Further conditions and predicates may be added as implementations areconsidered.A set of test predicates must at least satisfy the following conditions to have anychance of being rel\u00aeiable. These conditions are key to meaningful testing:Every individual branching condition in a program must be represented byan equivalent condition in C.Every potential termination condition in the program, for example, an overflow, must be represented by a condition in C.Eve\u00aery condition relevant to the correct operation of the program that isimplied by the specification and knowledge of the data structure of theprogram must be represented as a condition in C.",
  "page359": " The concepts of reliability and validity have been defined with respect tothe entire input domain of a program. A criterion is guaranteed to be bothreliable and valid if and only if it selects\u00aethe entire domain as a single test.Since such exhaustive testing is impractical, one will have much difficultyin assessing the reliability and validity of a criterion.The concepts of reliability and validity have been defined with respect t\u00aeo aprogram. A test selection criterion that is reliable and valid for one programmay not be so for another program. The goodness of a test set should beindependent of individual programs and the faults therein.Neither validity nor reliability is preserved throughout the debugging process. In practice, as program failures are observed, the\u00aeprogram is debuggedto locate the faults, and the faults are generally fixed as soon as they arefound. During this debugging phase, as the program changes, so does theidealness of a test set. This is because a fault that was revealed beforedebugging is no more revealed after debugging and fault fixing. Thus,properties of test selection criteria are not even \"monotonic\" in the senseof\u00aebeing either always gained or preserved or always lost or preserved.A key problem in the theory of Goodenough and Gerhart is that the reliability andvalidity of a criterion depend upon the presence of faults in a program and theirtypes. Weyuker and Ostrand [18] provide a modif\u00aeied theory in which the validityand reliability of test selection criteria are dependent only on the program specification, rather than a program. They propose the concept of a uniformly ideal test",
  "page360": "An ideal goal in software development is to find out whether or not a program iscorrect, where a correct program is void of faults. Much research results have beenreported in the field of progra\u00aem correctness. However, due to the highly constrainednature of program verification techniques, no developer makes any effort to provethe correctness of even small programs of, say, a few thousand lines, let alonelarge programs with million\u00aes of lines of code. Instead, testing is accepted in theindustry as a practical way of finding faults in programs. The flip side of testingis that it cannot be used to settle the question of program correctness, which is theideal goal. Even though testing cannot settle the program correctness issue, thereis a need for a testing theory to en\u00aeable us to compare the power of different testmethods.To motivate a theoretical discussion of testing, we begin with an ideal processfor software development, which consists of the following steps:A customer and a development team specify the needs. The development team takes the specification and attempts to write a program to meet the specification. A test engineer takes both the specificat\u00aeion and the program and selectsa set of test cases. The test cases are based on the specification and theprogram.The program is executed with the selected test data, and the test outcomeis compared with the expected outcome. The program is said to have faults if some tests fail\u00ae.One can say the program to be ready for use if it passes all the test cases.We focus on the selection of test cases and the interpretation of their results.We assume that the specification is correct, and the specification is the sole arbiterof the correctness of the program. ",
  "page361": "Program Dependent: In this case, T :M (P), that is, test cases arederived solely based on the source code of a system. This is calledwhite-box testing. Here, a test method has complete knowledge\u00aeof theinternal details of a program. However, from the viewpoint of practicaltesting, a white-box method is not generally applied to an entire program.One applies such a method to small units of a given large system. A unitrefers to a func\u00aetion, procedure, method, and so on. A white-box methodallows a test engineer to use the details of a program unit. Effective use ofa program unit requires a thorough understanding of the unit. Therefore,white-box test methods are used by programmers to test their own code. Specification Dependent: In this case, T = M (S ), that is, test ca\u00aesesare derived solely based on the specification of a system. This is calledblack-box testing. Here, a test method does not have access to the internaldetails of a program. Such a method uses information provided in thespecification of a system. It is not unusual to use an entire specificationin the generation of test cases because specifications are much smaller insize than their correspondi\u00aeng implementations. Black-box methods aregenerally used by the development team and an independent system testgroup. Expectation Dependent: In practice, customers may generate test casesbased on their expectations from the product at the time of taking deliveryof the syst\u00aeem. These test cases may include continuous-operation tests,usability tests, and so on.",
  "page362": "Ideally, all programs should be correct, that is, there is no fault in a program. Dueto the impractical nature of proving even small programs to be correct, customersand software developers rely\u00aeon the efficacy of testing. In this section, we introducetwo main limitations of testing: Testing means executing a program with a generally small, proper subsetof the input domain of the program. A small, proper subset of the inputdomain\u00aeis chosen because cost may not allow a much larger subset tobe chosen, let alone the full input set. Testing with the full input set isknown as exhaustive testing. Thus, the inherent need to test a programwith a small subset of the input domain poses a fundamental limit on theefficacy of testing. The limit is in the form of our inability t\u00aeo extrapolate thecorrectness of results for a proper subset of the input domain to programcorrectness. In other words, even if a program passes a test set T, wecannot conclude that the program is correct. Once we have selected a subset of the input domain, we are faced with theproblem of verifying the correctness of the program outputs for individualtest input. That is, a program output is ex\u00aeamined to determine if the programperformed correctly on the test input. The mechanism which verifies thecorrectness of a program output is known as an oracle. The concept ofan oracle is discussed in detail in Chapter 9. Determining the correctnessof a program output is not a t\u00aerivial task. If either of the following twoconditions hold, a program is considered non testable [20]:There does not exist an oracle.",
  "page363": "Ideally, all programs should be correct, that is, there is no fault in a program. Dueto the impractical nature of proving even small programs to be correct, customersand software developers rely\u00aeon the efficacy of testing. In this section, we introducetwo main limitations of testing: Testing means executing a program with a generally small, proper subsetof the input domain of the program. A small, proper subset of the inputdomain\u00aeis chosen because cost may not allow a much larger subset tobe chosen, let alone the full input set. Testing with the full input set isknown as exhaustive testing. Thus, the inherent need to test a programwith a small subset of the input domain poses a fundamental limit on theefficacy of testing. The limit is in the form of our inability t\u00aeo extrapolate thecorrectness of results for a proper subset of the input domain to programcorrectness. In other words, even if a program passes a test set T, wecannot conclude that the program is correct. Once we have selected a subset of the input domain, we are faced with theproblem of verifying the correctness of the program outputs for individualtest input. That is, a program output is ex\u00aeamined to determine if the programperformed correctly on the test input. The mechanism which verifies thecorrectness of a program output is known as an oracle. The concept ofan oracle is discussed in detail in Chapter 9. Determining the correctnessof a program output is not a t\u00aerivial task. If either of the following twoconditions hold, a program is considered non testable [20]:There does not exist an oracle.",
  "page364": "In this chapter we consider the first level of testing, that is, unit testing. Unit testingrefers to testing program units in isolation. However, there is no consensus on thedefinition of a unit\u00ae. Some examples of commonly understood units are functions,procedures, or methods. Even a class in an object-oriented programming languagecan be considered as a program unit. Syntactically, a program unit is a piece ofcode, such as a functi\u00aeon or method of class, that is invoked from outside the unitand that can invoke other program units. Moreover, a program unit is assumed toimplement a well-defined function providing a certain level of abstraction to theimplementation of higher level functions. The function performed by a program unitmay not have a direct association with\u00aea system-level function. Thus, a programunit may be viewed as a piece of code implementing a \"low\"-level function. Inthis chapter, we use the terms unit and module interchangeably.Now, given that a program unit implements a function, it is only natural totest the unit before it is integrated with other units. Thus, a program unit is testedin isolation, that is, in a stand-alone mann\u00aeer. There are two reasons for testing aunit in a stand-alone manner. First, errors found during testing can be attributedto a specific unit so that it can be easily fixed. Moreover, unit testing removesdependencies on other program units. Second, during unit testing it is desir\u00aeableto verify that each distinct execution of a program unit produces the expectedresult. In terms of code details, a distinct execution refers to a distinct path in theunit.",
  "page365": "Second, during unit testing it is desirableto verify that each distinct execution of a program unit produces the expectedresult. In terms of code details, a distinct execution refers to a distin\u00aect path in theunit. Ideally, all possible or as much as possible distinct executions are to beconsidered during unit testing. This requires careful selection of input data for eachdistinct execution. A programmer has direct access to the in\u00aeput vector of the unit byexecuting a program unit in isolation. This direct access makes it easier to executeas many distinct paths as desirable or possible. If multiple units are put together fortesting, then a programmer needs to generate test input with indirect relationshipwith the input vectors of several units under test. The said in\u00aedirect relationshipmakes it difficult to control the execution of distinct paths in a chosen unit.Unit testing has a limited scope. A programmer will need to verify whetheror not a code works correctly by performing unit-level testing. Intuitively, a programmer needs to test a unit as follows:Execute every line of code. This is desirable because the programmer needsto know what happens when a\u00aeline of code is executed. In the absence ofsuch basic observations, surprises at a later stage can be expensive. Execute every predicate in the unit to evaluate them to true and falseseparately. Observe that the unit performs its intended function and ensure that itcontains no\u00aeknown errors.In spite of the above tests, there e is no guarantee that a satisfactorily tested unitis functionally correct from a systemwide perspective. ",
  "page366": "Even though it is not possible to find all errors in a program unit in isolation, itis still necessary to ensure that a unit performs satisfactorily before it is used byother program units. It s\u00aeerves no purpose to integrate an erroneous unit with otherunits for the following reasons: (i) many of the subsequent tests will be a wasteof resources and (ii) finding the root causes of failures in an integrated system ismore resource con\u00aesuming.Unit testing is performed by the programmer who writes the program unitbecause the programmer is intimately familiar with the internal details of the unit.The objective for the programmer is to be satisfied that the unit works as expected.Since a programmer is supposed to construct a unit with no errors in it, a unittest is performe\u00aed by him or her to their satisfaction in the beginning and to thesatisfaction of other programmers when the unit is integrated with other units. Thismeans that all programmers are accountable for the quality of their own work,which may include both new code and modifications to the existing code. The ideahere is to push the quality concept down to the lowest level of the organization andempow\u00aeer each programmer to be responsible for his or her own quality. Therefore,it is in the best interest of the programmer to take preventive actions to minimizethe number of defects in the code. The defects found during unit testing are internalto the software development group a\u00aend are not reported up the personnel hierarchyto be counted in quality measurement metrics. The source code of a unit is notused for interfacing by other group members until the programmer completes unit ",
  "page367": "testing and checks in the unit to the version control system.Unit testing is conducted in two complementary phases: Static unit testingDynamic unit testingIn static unit testing, a programmer do\u00aees not execute the unit; instead, the code isexamined over all possible behaviors that might arise during run time. Static unittesting is also known as non-execution-based unit testing, whereas dynamic unittesting is execution based. In sta\u00aetic unit testing, the code of each unit is validatedagainst requirements of the unit by reviewing the code. During the review process,potential issues are identified and resolved. For example, in the C programminglanguage the two program-halting instructions are abort() and exit(). While the twoare closely related, they have different effe\u00aects as explained below:Abort(): This means abnormal program termination. By default, a call toabort() results in a run time diagnostic and program self-destruction. Theprogram destruction may or may not flush and close opened files or removetemporary files, depending on the implementation. Exit(): This means graceful program termination. That is, the exit() callcloses the opened files and ret\u00aeurns a status code to the execution environment.Whether to use abort() or exit() depends on the context that can be easilydetected and resolved during static unit testing. More issues caught earlier lead tofewer errors being identified in the dynamic test phase and result in fe\u00aewer defectsin shipped products. Moreover, performing static tests is less expensive than performing dynamic tests. Code review is one component of the defect minimizationprocess and can help detect problems that are common to software development.After a round of code review, dynamic unit testing is conducted.",
  "page368": "In dynamic unittesting, a program unit is actually executed and its outcomes are observed. Dynamicunit testing means testing the code by actually running it. It may be noted that staticunit test\u00aeing is not an alternative to dynamic unit testing. A programmer performsboth kinds of tests. In practice, partial dynamic unit testing is performed concurrently with static unit testing. If the entire dynamic unit testing has been performed\u00aeand a static unit testing identifies significant problems, the dynamic unit testingmust be repeated. As a result of this repetition, the development schedule may beaffected. To minimize the probability of such an event, it is required that static unittesting be performed prior to the final dynamic unit testingStatic unit testing is conduct\u00aeed as a part of a larger philosophical belief that asoftware product should undergo a phase of inspection and correction at eachmilestone in its life cycle. At a certain milestone, the product need not be in itsfinal form. For example, completion of coding is a milestone, even though codingof all the units may not make the desired product. After coding, the next milestoneis testing all or a s\u00aeubstantial number of units forming the major components of theproduct. Thus, before units are individually tested by actually executing them, thoseare subject to usual review and correction as it is commonly understood. The ideabehind review is to find the defects as close to t\u00aeheir points of origin as possible sothat those defects are eliminated with less effort, and the interim product containsfewer defects before the next task is undertaken.",
  "page369": "Inspection: It is a step-by-step peer group review of a work product, witheach step checked against predetermined criteria. Walkthrough: It is a review where the author leads the team through am\u00aeanual or simulated execution of the product using predefined scenarios.Regardless of whether a review is called an inspection or a walkthrough, it isa systematic approach to examining source code in detail. The goal of such anexercise is to\u00aeassess the quality of the software in question, not the quality of theprocess used to develop the product [3]. Reviews of this type are characterizedby significant preparation by groups of designers and programmers with varyingdegree of interest in the software development project. Code examination can betime consuming. Moreover, no exami\u00aenation process is perfect. Examiners may takeshortcuts, may not have adequate understanding of the product, and may accept aproduct which should not be accepted. Nonetheless, a well-designed code reviewprocess can find faults that may be missed by execution-based testing. The key tothe success of code review is to divide and conquer, that is, having an examinerinspect small parts of the unit\u00aein isolation, while making sure of the following:(i) nothing is overlooked and (ii) the correctness of all examined parts of themodule implies the correctness of the whole module. The decomposition of thereview into discrete steps must assure that each step is simple enough tha\u00aet it canbe carried out without detailed knowledge of the others.The objective of code review is to review the code, not to evaluate the authorof the code",
  "page370": "review is to review the code, not to evaluate the authorof the code. A clash may occur between the author of the code and the reviewers,and this may make the meetings unproductive. Therefore, co\u00aede review must beplanned and managed in a professional manner. There is a need for mutual respect,openness, trust, and sharing of expertise in the group. The general guidelines forperforming code review consists of six steps as outlined in\u00aeFigure 3.1: readiness,preparation, examination, rework, validation, and exit. The input to the readinessstep is the criteria that must be satisfied before the start of the code review process,and the process produces two types of documents, a change request (CR) and areport. These steps and documents are explained in the following. Readine\u00aess The author of the unit ensures that the unit under test isready for review. A unit is said to be ready if it satisfies the followingcriteria. Completeness: All the code relating to the unit to be reviewed must beavailable. This is because the reviewers are going to read the code andtry to understand it. It is unproductive to review partially written codeor code that is going to be signific\u00aeantly modified by the programmer Minimal Functionality: The code must compile and link. Moreover,the code must have been tested to some extent to make sure that itperforms its basic functionalities.Readability: Since code review involves actual reading of code byother programme\u00aers, it is essential that the code is highly readable.Some code characteristics that enhance readability are proper formatting, using meaningful identifier names, straightforward use of programming language constructs, and an appropriate level of abstractionusing function calls. ",
  "page371": "Complexity: There is no need to schedule a group meeting to reviewstraightforward code which can be easily reviewed by the programmer.The code to be reviewed must be of sufficient complexity to\u00aewarrantgroup review. Here, complexity is a composite term referring to thenumber of conditional statements in the code, the number of input dataelements of the unit, the number of output data elements produced bythe unit, real-time processi\u00aeng of the code, and the number of other unitswith which the code communicates.Requirements and Design Documents: The latest approved versionof the low-level design specification or other appropriate descriptions Hierarchy of System Document of program requirements (see Table 3.1) should be available. Thesedocuments help the reviewers in ve\u00aerifying whether or not the codeunder review implements the expected functionalities. If the low-leveldesign document is available, it helps the reviewers in assessing whetheror not the code appropriately implements the design.All the people involved in the review process are informed of thegroup review meeting schedule two or three days before the meeting.They are also given a copy of the wor\u00aek package for their perusal. Reviewsare conducted in bursts of 1-2 hours. Longer meetings are less and lessproductive because of the limited attention span of human beings. Therate of code review is restricted to about 125 lines of code (in a high-levellanguage) per hour.\u00aeReviewing complex code at a higher rate will resultin just glossing over the code, thereby defeating the fundamental purposeof code review. The composition of the review group involves a numberof people with different roles. These roles are explained as follows",
  "page372": "Moderator: A review meeting is chaired by the moderator. The moderator is a trained individual who guides the pace of the review process.The moderator selects the reviewers and schedules the rev\u00aeiew meetings.Myers suggests that the moderator be a member of a group from anunrelated project to preserve objectivity Author: This is the person who has written the code to be reviewed.Presenter: A presenter is someone other than the autho\u00aer of the code.The presenter reads the code beforehand to understand it. It is thepresenter who presents the author's code in the review meeting forthe following reasons: (i) an additional software developer will understand the work within the software organization; (ii) if the originalprogrammer leaves the company with a short notice,\u00aeat least one otherprogrammer in the company knows what is being done; and (iii) theoriginal programmer will have a good feeling about his or her work, ifsomeone else appreciates their work. Usually, the presenter appreciatesthe author's work. Recordkeeper: The recordkeeper documents the problems found during the review process and the follow-up actions suggested. The personshould be dif\u00aeferent than the author and the moderator.Reviewers: These are experts in the subject area of the code underreview. The group size depends on the content of the material underreview. As a rule of thumb, the group size is between 3 and 7. Usuallythis group does not have manager t\u00aeo whom the author reports. Thisis because it is the author's ongoing work that is under review, andneither a completed work nor the author himself is being reviewed.Observers: These are people who want to learn about the code underreview. These people do not participate in the review process but aresimply passive observers.",
  "page373": " Preparation Before the meeting, each reviewer carefully reviews thework package. It is expected that the reviewers read the code and understand its organization and operation before the review\u00aemeeting. Eachreviewer develops the following: List of Questions: A reviewer prepares a list of questions to be asked,if needed, of the author to clarify issues arising from his or her reading.A general guideline of what to examine while rea\u00aeding the code isoutlined in Table 3.2. Potential CR: A reviewer may make a formal request to make achange. These are called change requests rather than defect reports.At this stage, since the programmer has not yet made the code public, it is more appropriate to make suggestions to the author to makechanges, rather than report a defect. Th\u00aeough CRs focus on defects inthe code, these reports are not included in defect statistics related tothe productSuggested Improvement Opportunities: The reviewers may suggesthow to fix the problems, if there are any, in the code under review.Since reviewers are experts in the subject area of the code, it is notunusual for them to make suggestions for improvements. Examination The examination p\u00aerocess consists of the followingactivities: The author makes a presentation of the procedural logic used in thecode, the paths denoting major computations, and the dependency ofthe unit under review on other units.The presenter reads the code line by line. The reviewers may rai\u00aesequestions if the code is seen to have defects. However, problems are notresolved in the meeting. The reviewers may make general suggestionson how to fix the defects, but it is up to the author of the code to takecorrective measures after the meeting ends.",
  "page374": " The recordkeeper documents the change requests and the suggestionsfor fixing the problems, if there are any. A CR includes the followingdetails Give a brief description of the issue or action i\u00aetem. Assign a priority level (major or minor) to a CR.Assign a person to follow up the issue. Since a CR documents apotential problem, there is a need for interaction between the author of the code and one of the reviewers, possibly the rev\u00aeiewer whomade the CR.Set a deadline for addressing a CR.The moderator ensures that the meeting remains focused on the reviewprocess. The moderator makes sure that the meeting makes progress ata certain rate so that the objective of the meeting is achieved.At the end of the meeting, a decision is taken regarding whether ornot to call anothe\u00aer meeting to further review the code. If the reviewprocess leads to extensive rework of the code or critical issues areidentified in the process, then another meeting is generally convened.Otherwise, a second meeting is not scheduled, and the author is giventhe responsibility of fixing the CRs Rework At the end of the meeting, the recordkeeper produces a summary of the meeting that includes t\u00aehe following information: A list of all the CRs, the dates by which those will be fixed, and thenames of the persons responsible for validating the CRs A list of improvement opportunitiesThe minutes of the meeting (optional) ",
  "page375": "A copy of the report is distributed to all the members of the review group.After the meeting, the author works on the CRs to fix the problems. Theauthor documents the improvements made to the co\u00aede in the CRs. Theauthor makes an attempt to address the issues within the agreed-upontime frame using the prevailing coding conventions Validation The CRs are independently validated by the moderatoror another person designated for this pu\u00aerpose. The validation processinvolves checking the modified code as documented in the CRs andensuring that the suggested improvements have been implementedcorrectly. The revised and final version of the outcome of the reviewmeeting is distributed to all the group members.Exit Summarizing the review process, it is said to be complete if all\u00aeofthe following actions have been taken: Every line of code in the unit has been inspected. If too many defects are found in a module, the module is once againreviewed after corrections are applied by the author. As a rule of thumb,if more than 5% of the total lines of code are thought to be contentious,then a second review is scheduled. The author and the reviewers reach a consensus that wh\u00aeen correctionshave been applied the code will be potentially free of defects. All the CRs are documented and validated by the moderator or someoneelse. The author's follow-up actions are documented. A summary report of the meeting including the CRs is distributed toall the\u00aemembers of the review group.",
  "page376": "The effectiveness of static testing is limited by the ability of a reviewer tofind defects in code by visual means. However, if occurrences of defects depend onsome actual values of variables, t\u00aehen it is a difficult task to identify those defectsby visual means. Therefore, a unit must be executed to observe its behaviors inresponse to a variety of inputs. Finally, whatever may be the effectiveness of statictests, one cannot feel c\u00aeonfident without actually running the code.Code Review Metrics It is important to collect measurement data pertinent toa review process, so that the review process can be evaluated, made visible tothe upper management as a testing strategy, and improved to be more effective.Moreover, collecting metrics during code review facilitates estima\u00aetion of reviewtime and resources for future projects. Thus, code review is a viable testing strategythat can be effectively used to improve the quality of products at an early stage.The following metrics can be collected from a code review:Number of lines of code (LOC) reviewed per hour Number of CRs generated per thousand lines of code (KLOC) Number of CRs generated per hour Total number of\u00aeCRs generated per project Total number of hours spent on code review per projectIt is in the best interest of the programmers in particular and the company in generalto reduce the number of CRs generated during code review. This is because CRs areindications of potential proble\u00aems in the code, and those problems must be resolvedbefore different program units are integrated. Addressing CRs means spending moreresources and potentially delaying the project. Therefore, it is essential to adoptthe concept of defect prevention during code development.",
  "page377": "Code Review Metrics It is important to collect measurement data pertinent toa review process, so that the review process can be evaluated, made visible tothe upper management as a testing strate\u00aegy, and improved to be more effective.Moreover, collecting metrics during code review facilitates estimation of reviewtime and resources for future projects. Thus, code review is a viable testing strategythat can be effectively used to impr\u00aeove the quality of products at an early stage.The following metrics can be collected from a code review: Number of lines of code (LOC) reviewed per hourNumber of CRs generated per thousand lines of code (KLOC)Number of CRs generated per hour Total number of CRs generated per project Total number of hours spent on code review per projectIt\u00aeis in the best interest of the programmers in particular and the company in generalto reduce the number of CRs generated during code review. This is because CRs areindications of potential problems in the code, and those problems must be resolvedbefore different program units are integrated. Addressing CRs means spending moreresources and potentially delaying the project. Therefore, it is ess\u00aeential to adoptthe concept of defect prevention during code development. In practice, defectsare inadvertently introduced by programmers. Those accidents can be reduced bytaking preventive measures. It is useful to develop a set of guidelines to constructcode for defect minimiz\u00aeation as explained in the following. These guidelines focuson incorporating suitable mechanisms into the code: ",
  "page378": "Build internal diagnostic tools, also known as instrumentation code, intothe units. Instrumentation codes are useful in providing information aboutthe internal states of the units. These codes a\u00aellow programmers to realizebuilt-in tracking and tracing mechanisms. Instrumentation plays a passiverole in dynamic unit testing. The role is passive in the sense of observingand recording the internal behavior without actively testing a un\u00aeit. Use standard controls to detect possible occurrences of error conditions.Some examples of error detection in the code are divides by zero and arrayindex out of bounds. Ensure that code exists for all return values, some of which may be invalid.Appropriate follow-up actions need to be taken to handle invalid returnvalues. Ensure that co\u00aeunter data fields and buffer overflow and underflow areappropriately handled. Provide error messages and help texts from a common source so thatchanges in the text do not cause inconsistency. Good error messagesidentify the root causes of the problems and help users in resolving theproblems .Validate input data, such as the arguments, passed to a function. Use assertions to detect impossible\u00aeconditions, undefined uses of data, andundesirable program behavior. An assertion is a Boolean statement whichshould never be false or can be false only if an error has occurred. In otherwords, an assertion is a check on a condition which is assumed to be true,but it can cause\u00aea problem if it not true. Assertion should be routinely usedto perform the following kinds of checks:",
  "page379": "Ensure that preconditions are satisfied before beginning to execute aunit. A precondition is a Boolean function on the states of a unit specifying our expectation of the state prior to initiatin\u00aeg an activity in thecode.Ensure that the expected postconditions are true while exiting from theunit. A postcondition is a Boolean function on the state of a unit specifying our expectation of the state after an activity has been completed.\u00aeThe postconditions may include an invariance. Ensure that the invariants hold. That is, check invariant states conditions which are expected not to change during the execution of apiece of code. Leave assertions in the code. You may deactivate them in the releasedversion of code in order to improve the operational performance of thesystem.\u00aeFully document the assertions that appear to be unclear. After every major computation, reverse-compute the input(s) from theresults in the code itself. Then compare the outcome with the actual inputsfor correctness. For example, suppose that a piece of code computes thesquare root of a positive number. Then square the output value and compare the result with the input. It may be needed to t\u00aeolerate a margin oferror in the comparison process. In systems involving message passing, buffer management is an importantinternal activity. Incoming messages are stored in an already allocatedbuffer. It is useful to generate an event indicating low buffer availabilitybefore t\u00aehe system runs out of buffer. Develop a routine to continuallymonitor the availability of buffer after every use, calculate the remainingspace available in the buffer, and call an error handling routine if theamount of available buffer space is too low",
  "page380": "Develop a timer routine which counts down from a preset time until iteither hits zero or is reset. If the software is caught in an infinite loop, thetimer will expire and an exception handler ro\u00aeutine can be invoked.Include a loop counter within each loop. If the loop is ever executed lessthan the minimum possible number of times or more than the maximumpossible number of times, then invoke an exception handler routine.Define a var\u00aeiable to indicate the branch of decision logic that will be taken.Check this value after the decision has been made and the right branch hassupposedly been taken. If the value of the variable has not been preset,there is probably a fall-through condition in the logic.Execution-based unit testing is referred to as dynamic unit testing. In t\u00aehis testing,a program unit is actually executed in isolation, as we commonly understand it.However, this execution differs from ordinary execution in the following way: A unit under test is taken out of its actual execution environment. The actual execution environment is emulated by writing more code(explained later in this section) so that the unit and the emulatedenvironment can be compile\u00aed togetherThe above compiled aggregate is executed with selected inputs. Theoutcome of such an execution is collected in a variety of ways, such asstraightforward observation on a screen, logging on files, and softwareinstrumentation of the code to reveal run time behavior. The\u00aeresultis compared with the expected outcome. Any difference between theactual and expected outcome implies a failure and the fault is inthe code",
  "page381": "An environment for dynamic unit testing is created by emulating the contextof the unit under test, as shown in Figure 3.2. The context of a unit test consistsof two parts: (i) a caller of the un\u00aeit and (ii) all the units called by the unit. Theenvironment of a unit is emulated because the unit is to be tested in isolationand the emulating environment must be a simple one so that any fault foundas a result of running the unit can be\u00aesolely attributed to the unit under test.The caller unit is known as a test driver, and all the emulations of the unitscalled by the unit under test are called stubs. The test driver and the stubs aretogether called scaffolding. The functions of a test driver and a stub are explained asfollows:Test Driver: A test driver is a program that\u00aeinvokes the unit under test.The unit under test executes with input values received from the driverand, upon termination, returns a value to the driver. The driver comparesthe actual outcome, that is, the actual value returned by the unit under test,with the expected outcome from the unit and reports the ensuing test result.The test driver functions as the main unit in the execution process.\u00aeThedriver not only facilitates compilation, but also provides input data to theunit under test in the expected format. Stubs: A stub is a \"dummy subprogram\" that replaces a unit that is calledby the unit under test. Stubs replace the units called by the unit under tes\u00aet.A stub performs two tasks. ",
  "page382": "First, it shows an evidence that the stub was, in fact, called. Such evidence can be shown by merely printing a message.Second, the stub returns a precomputed value to the caller so that the uni\u00aetunder test can continue its execution.The driver and the stubs are never discarded after the unit test is completed.Instead, those are reused in the future in regression testing of the unit if there issuch a need. For each unit, there shou\u00aeld be one dedicated test driver and severalstubs as required. If just one test driver is developed to test multiple units, thedriver will be a complicated one. Any modification to the driver to accommodatechanges in one of the units under test may have side effects in testing the otherunits. Similarly, the test driver should not depend on\u00aethe external input data filesbut, instead, should have its own segregated set of input data. The separate inputdata file approach becomes a very compelling choice for large amounts of testinput data. For example, if hundreds of input test data elements are required to testmore than one unit, then it is better to create a separate input test data file ratherthan to include the same set of inpu\u00aet test data in each test driver designed to testthe unit.The test driver should have the capability to automatically determine thesuccess or failure of the unit under test for each input test data. If appropriate,the driver should also check for memory leaks and problems in all\u00aeocation anddeallocation of memory. If the module opens and closes files, the test driver shouldcheck that these files are left in the expected open or closed state after each test.",
  "page383": "Mutation testing has a rich and long history. It can be traced back to the late 1970s[8-10]. Mutation testing was originally proposed by Dick Lipton, and the articleby DeMillo, Lipton, and\u00aeSayward [9] is generally cited as the seminal reference.Mutation testing is a technique that focuses on measuring the adequacy of test data(or test cases). The original intention behind mutation testing was to expose andlocate weaknesses in\u00aetest cases. Thus, mutation testing is a way to measure thequality of test cases, and the actual testing of program units is an added benefit.Mutation testing is not a testing strategy like control flow or data flow testing. Itshould be used to supplement traditional unit testing techniques.A mutation of a program is a modification of the\u00aeprogram created by introducing a single, small, legal syntactic change in the code. A modified program soobtained is called a mutant. The term mutant has been borrowed from biology.Some of these mutants are equivalent to the original program, whereas others arefaulty. A mutant is said to be killed when the execution of a test case causes it tofail and the mutant is considered to be dead.Some\u00aemutants are equivalent to the given program, that is, such mutantsalways produce the same output as the original program. In the real world, largeprograms are generally faulty, and test cases too contain faults. ",
  "page384": "The result of executing a mutant may be different from the expected result, but a test suite doesnot detect the failure because it does not have the right test case. In this scenariothe mutant i\u00aes called killable or stubborn, that is, the existing set of test casesis insufficient to kill it. A mutation score for a set of test cases is the percentage of nonequivalent mutants killed by the test suite. The test suite is said to bemuta\u00aetion adequate if its mutation score is 100%. Mutation analysis is a two-stepprocess: The adequacy of an existing test suite is determined to distinguish thegiven program from its mutants. A given test suite may not be adequateto distinguish all the nonequivalent mutants. As explained above, thosenonequivalent mutants that could not be iden\u00aetified by the given test suiteare called stubborn mutants. New test cases are added to the existing test suite to kill the stubbornmutants. The test suite enhancement process iterates until the test suitehas reached a desired level of mutation score.If we run the modified programs against the test suite, we will get the followingresults:Mutants 1 and 3: The programs will completely pass the t\u00aeest suite. In otherwords, mutants 1 and 3 are not killed.Mutant 2: The program will fail test case 2.Mutant 4: The program will fail test case 1 and test case 2.If we calculate the mutation score, we see that we created four mutants, andtwo of them were killed. This tells us th\u00aeat the mutation score is 50%, assumingthat mutants 1 and 3 are nonequivalent.The score is found to be low. It is low because we assumed that mutants 1 and3 are nonequivalent to the original program. ",
  "page385": "We have to show that either mutants 1 and 3 are equivalent mutants or those are killable. If those are killable, we needto add new test cases to kill these two mutants. First, let us analyze mut\u00aeant 1in order to derive a \"killer\" test. The difference between P and mutant 1 is thestarting point. Mutant 1 starts with i 1, whereas P starts with i = 2. There isno impact on the result r. Therefore, we conclude that mutant 1 i\u00aes an equivalentmutant. Second, we add a fourth test case as follows:Test case 4:Input: 2 2 1Then program P will produce the output \"Value of the rank is 1\" and mutant 3will produce the output \"Value of the rank is 2.\" Thus, this test data kills mutant 3,which give us a mutation score of 100%.In order to use the mutation\u00aetesting technique to build a robust test suite, thetest engineer needs to follow the steps that are outlined below:Step 1: Begin with a program P and a set of test cases T known to be correct.Step 2: Run each test case in T against the program P. If a test case fails, thatis, the output is incorrect, program P must be modified and retested. Ifthere are no failures, then continue with step 3.\u00aeStep 3: Create a set of mutants {Pi}, each differing from P by a simple, syntactically correct modification of P",
  "page386": "Execute each test case in T against each mutant Pi . If the output of themutant Pi differs from the output of the original program P, the mutantPi is considered incorrect and is said to be kille\u00aed by the test case. If Piproduces exactly the same results as the original program P for the testsin T, then one of the following is true: P and Pi are equivalent. That is, their behaviors cannot be distinguished by any set of test ca\u00aeses. Note that the general problem ofdeciding whether or not a mutant is equivalent to the original programis undecidable. Pi is killable. That is, the test cases are insufficient to kill the mutantPi . In this case, new test cases must be created.Step 5: Calculate the mutation score for the set of test cases T. The mutation score is\u00aethe percentage of nonequivalent mutants killed by the testdata, that is, Mutation score 100 X D/(N \u2212 E), where D is the deadmutants, N the total number of mutants, and E the number of equivalentmutants.Step 6: If the estimated mutation adequacy of T in step 5 is not sufficiently high,then design a new test case that distinguishes Pi from P, add the newtest case to T, and go to ste\u00aep 2. If the computed adequacy of T is morethan an appropriate threshold, then accept T as a good measure of thecorrectness of P with respect to the set of mutant programs Pi , and stopdesigning new test cases.",
  "page387": "Competent Programmer Hypothesis: This assumption states that programmers are generally competent, and they do not create \"random\" programs.Therefore, we can assume that for a given pro\u00aeblem a programmer will create a correct program except for simple errors. In other words, the mutantsto be considered are the ones falling within a small deviation from theoriginal program. In practice, such mutants are obtained by systemat\u00aeicallyand mechanically applying a set of transformations, called mutation operators, to the program under test. These mutation operators are expected tomodel programming errors made by programmers. In practice, this maybe only partly true. Coupling Effect: This assumption was first hypothesized in 1978 byDeMillo et al. [9]. The assumption\u00aecan be restated as complex faultsare coupled to simple faults in such a way that a test suite detectingall simple faults in a program will detect most of the complex faults.This assumption has been empirically supported by Offutt [11] andtheoretically demonstrated by Wah [12]. The fundamental premise ofmutation testing as coined by Geist et al. [13] is: If the software containsa fault, there\u00aewill usually be a set of mutants that can only be killed by atest case that also detect that faul Mutation testing helps the tester to inject, by hypothesis, different types offaults in the code and develop test cases to reveal them. In addition, comprehensivetesting can be per\u00aeformed by proper choice of mutant operations. However, a relatively large number of mutant programs need to be tested against many of the testcases before these mutants can be distinguished from the original program. Running the test cases, analyzing the results, identifying equivalent mutants [14], anddeveloping additional test cases to kill the stubborn mutants are all time consuming",
  "page388": "Robust automated testing tools such as Mothra can be used to expeditethe mutation testing process. Recently, with the availability of massive computing power, there has been a resurgence of mut\u00aeation testing processes within theindustrial community to use as a white-box methodology for unit testing [16, 17].Researchers have shown that with an appropriate choice of mutant programs mutation testing is as powerful as path testing, do\u00aemain testing [18], and data flowtesting The programmer, after a program failure, identifies the corresponding fault andfixes it. The process of determining the cause of a failure is known as debugging.Debugging occurs as a consequence of a test revealing a failure. Myers proposedthree approaches to debugging in his book The Art of Software\u00aeTesting [20]:Brute Force: The brute-force approach to debugging is preferred by manyprogrammers. Here, \"let the computer find the error\" philosophy is used. Print statements are scattered throughout the source code. These print statements provide a crude trace of the way the source code has executed.The availability of a good debugging tool makes these print statementsredundant . A\u00aedynamic debugger allows the software engineer to navigateby stepping through the code, observe which paths have executed, andobserve how values of variables change during the controlled execution.A good tool allows the programmer to assign values to several variablesand navigate step by step through the code",
  "page389": "Instrumentation code can bebuilt into the source code to detect problems and to log intermediate values of variables for problem diagnosis. One may use a memory dumpafter a failure has occurred\u00aeto understand the final state of the code beingdebugged. The log and memory dump are reviewed to understand whathappened and how the failure occurred. Cause Elimination: The cause elimination approach can be best describedas a process invol\u00aeving induction and deduction [21]. In the induction part,first, all pertinent data related to the failure are collected , such as whathappened and what the symptoms are. Next, the collected data are organized in terms of behavior and symptoms, and their relationship is studiedto find a pattern to isolate the causes. A cause hypothesis is d\u00aeevised, and theabove data are used to prove or disprove the hypothesis. In the deductionpart, a list of all possible causes is developed in order of their likelihoods,and tests are conducted to eliminate or substantiate each cause in decreasing order of their likelihoods. If the initial tests indicate that a particularhypothesis shows promise, test data are refined in an attempt to isolate th\u00aeeproblem as needed.Backtracking: In this approach, the programmer starts at a point in thecode where a failure was observed and traces back the execution to the pointwhere it occurred. This technique is frequently used by programmers, andthis is useful in small programs. Howeve\u00aer, the probability of tracing backto the fault decreases as the program size increases, because the numberof potential backward paths may become too large.",
  "page390": "Often, software engineers notice other previously undetected problems whiledebugging and applying a fix. These newly discovered faults should not be fixedalong with the fix in focus. This is bec\u00aeause the software engineer may not have afull understanding of the part of the code responsible for the new fault. The bestway to deal with such a situation is to file a CR. A new CR gives the programmer anopportunity to discuss the matter\u00aewith other team members and software architectsand to get their approval on a suggestion made by the programmer. Once the CR isapproved, the software engineer must file a defect in the defect tracking databaseand may proceed with the fix. This process is cumbersome, and it interrupts thedebugging process, but it is useful for very critical\u00aeprojects. However, programmersoften do not follow this because of a lack of a procedure to enforce it.A Debugging Heuristic The objective of debugging is to precisely identify thecause of a failure. Once the cause is identified, corrective measures are taken to fix the fault. Debugging is conducted by programmers, preferably by those whowrote the code, because the programmer is the best pers\u00aeon to know the source codewell enough to analyze the code efficiently and effectively. Debugging is usuallya time consuming and error-prone process, which is generally performed understress. Debugging involves a combination of systematic evaluation, intuition, and,sometimes, a\u00aelittle bit of luck. Given a symptom of a problem, the purpose is toisolate and determine its specific cause. The following heuristic may be followedto isolate and correct it",
  "page391": "Reproduce the symptom(s). Read the troubleshooting guide of the product. This guide may includeconditions and logs, produced by normal code, or diagnostics codespecifically written for troublesh\u00aeooting purpose that can be turned on. Try to reproduce the symptoms with diagnostics code turned on.Gather all the information and conduct causal analysis The goal ofcausal analysis is to identify the root cause of the problem and initiatea\u00aections so that the source of defects is eliminated.Step 2: Formulate some likely hypotheses for the cause of the problem based onthe causal analysis.Step 3: Develop a test scenario for each hypothesis to be proved or disproved.This is done by designing test cases to provide unambiguous resultsrelated to a hypothesis. The test cases may be\u00aestatic (reviewing code anddocumentation) and/or dynamic in nature. Preferably, the test cases arenondestructive, have low cost, and need minimum additional hardwareneeds. A test case is said to be destructive if it destroys the hardwaresetup. For example, cutting a cable during testing is called destructivetesting.Step 4: Prioritize the execution of test cases. Test cases corresponding to the\u00aehighly probable hypotheses are executed first. Also, the cost factor cannotbe overlooked. Therefore, it is desirable to execute the low-cost test casesfirst followed by the more expensive ones. The programmer needs toconsider both factors.Step 5: Execute the test cases in order\u00aeto find the cause of a symptom. Afterexecuting a test case, examine the result for new evidence. If the testresult shows that a particular hypothesis is promising, test data are refinedin an attempt to isolate the defect. If necessary, go back to earlier stepsor eliminate a particular hypothesis.",
  "page392": "any side effects (collateral damage) due to the changes effected in themodule. After a possible code review, apply the fix. Retest the unit to confirm that the actual cause of failur\u00aee had beenfound. The unit is properly debugged and fixed if tests show that theobserved failure does not occur any more. If there are no dynamic unit test cases that reveal the problem, thenadd a new test case to the dynamic unit test\u00aeing to detect possiblereoccurrences or other similar problems. For the unit under consideration, identify all the test cases that havepassed. Now, perform a regression test on the unit with those testcases to ensure that new errors have not been introduced. That is whyit is so important to have archived all the test cases that have b\u00aeeendesigned for a unit. Thus, even unit-level test cases must be managedin a systematic manner to reduce the cost of software development.Step 7: Document the changes which have been made. Once a defect is fixed,the following changes are required to be applied: Document the changes in the source code itself to reflect the change. Update the overall system documentation. Chan\u00aeges to the dynamic unit test cases. File a defect in the defect tracking database if the problem was foundafter the code was checked in to the version control system.",
  "page393": "A TDD approach to code development is used in the XP methodology [22, 23]. Thekey aspect of the TDD approach is that a programmer writes low-level tests beforewriting production code. This is re\u00aeferred to as test first [24] in software development. Writing test-driven units is an important concept in the XP methodology. InXP, a few unit tests are coded first, then a simple, partial system is implemented topass the tests. Then, one\u00aemore new unit test is created, and additional code is written to pass the new test, but not more, until a new unit test is created. The processis continued until nothing is left to test. The process is illustrated in Figure 3.3 andoutlined below:Step 1: Pick a requirement, that is, a story.Step 2: Write a test case that will verify a small\u00aepart of the story and assign afail verdict to it.Step 3: Write the code that implements a particular part of the story to pass thetest.Step 4: Execute all tests Step 5: Rework the code, and test the code until all tests pass.Step 6: Repeat steps 2-5 until the story is fully implemented.The simple cycle in Figure 3.3 shows that, at the beginning of each cycle,the intention is for all tes\u00aets to pass except the newly added test case. The new testcase is introduced to drive the new code development. At the end of the cycle, theprogrammer executes all the unit tests, ensuring that each one passes and, hence,the planned task of the code still works",
  "page394": "A TDD approach to code development is used in the XP methodology [22, 23]. Thekey aspect of the TDD approach is that a programmer writes low-level tests beforewriting production code. This is re\u00aeferred to as test first [24] in software development. Writing test-driven units is an important concept in the XP methodology. InXP, a few unit tests are coded first, then a simple, partial system is implemented topass the tests. Then, one\u00aemore new unit test is created, and additional code is written to pass the new test, but not more, until a new unit test is created. The processis continued until nothing is left to test. The process is illustrated in Figure 3.3 andoutlined below:Step 1: Pick a requirement, that is, a story.Step 2: Write a test case that will verify a small\u00aepart of the story and assign afail verdict to it.Step 3: Write the code that implements a particular part of the story to pass thetest.Step 4: Execute all tests Step 5: Rework the code, and test the code until all tests pass.Step 6: Repeat steps 2-5 until the story is fully implemented.The simple cycle in Figure 3.3 shows that, at the beginning of each cycle,the intention is for all tes\u00aets to pass except the newly added test case. The new testcase is introduced to drive the new code development. At the end of the cycle, theprogrammer executes all the unit tests, ensuring that each one passes and, hence,the planned task of the code still works",
  "page395": " One may not write production code unless the first failing unit test iswritten. One may not write more of a unit test than is sufficient to fail. One may not write more production code than is\u00aesufficient to make thefailing unit test pass.These three laws ensure that one must write a portion of a unit test that failsand then write just enough production code to make that unit test pass. The goalof these three laws is not to follow\u00aethem strictly it is to decrease the intervalbetween writing unit tests and production code.Creating unit tests helps a developer focus on what needs to be done. Requirements, that is, user stories, are nailed down firmly by unit tests. Unit tests arereleased into the code repository along with the code they test. Code without unittests ma\u00aey not be released. If a unit test is discovered to be missing, it must be created immediately. Creating unit tests independently before coding sets up checksand balances and improves the chances of getting the system right the first time.Unit tests provide a safety net of regression tests and validation tests so that XPprogrammers can refactor and integrate effectively.In XP, the code is bein\u00aeg developed by two programmers working together sideby side. The concept is called pair programming. The two programmers sit side byside in front of the monitor. One person develops the code tactically and the otherone inspects it methodically by keeping in mind the story they\u00aeare implementing.It is similar to the two-person inspection strategy proposed by Bisant and Lyle",
  "page396": "The JUnit is a unit testing framework for the Java programming language designedby Kent Beck and Erich Gamma. Experience gained with JUnit has motivated thedevelopment of the TDD [22] methodolog\u00aey. The idea in the JUnit framework hasbeen ported to other languages, including C# (NUnit), Python (PyUnit), Fortran(fUnit) and C  (CPPUnit). This family of unit testing frameworks is collectivelyreferred to as xUnit. This section will int\u00aeroduce the fundamental concepts of JUnitto the reader.Suppose that we want to test the individual methods of a class called PlanetClass. Let Move() be a method in PlanetClass such that Move() accepts only oneinput parameter of type integer and returns a value of type integer. One can followthe following steps, illustrated using pseudocode\u00aein Figure 3.4, to test Move(): Create an object instance of Planet lass. Let us call the instance Mars.Now we are interested in testing the method Move() by invoking it onobject Mars. Select a value for all the input parameters of Move() this function hasjust one input parameter. Let us represent the input value to Move() by x. Know the expected value to be returned by Move(). Let the expecte\u00aedreturned value be y Invoke method Move() on object Mars with input value x. Let z denotethe value returned by Move().Now compare y with z. If the two values are identical, then the methodMove() in object Mars passes the test. Otherwise, the test is said to havefailed",
  "page397": "In a nutshell, the five steps of unit testing are as follows:Create an object and select a method to execute. Select values for the input parameters of the method. Compute the expected values to\u00aebe returned by the method. Execute the selected method on the created object using the selected inputvalues. Verify the result of executing the method.Performing unit testing leads to a programmer consuming some resources,especially time.\u00aeTherefore, it is useful to employ a general programming frameworkto code individual test cases, organize a set of test cases as a test suite, initialize atest environment, execute the test suite, clean up the test environment, and recordthe result of execution of individual test cases. In the example shown in Figure 3.4,creating the object\u00aeMars is a part of the initialization process. The two print()statements are examples of recording the result of test execution. Alternatively,one can write the result of test execution to a file. The JUnit framework has been developed to make test writing simple. Theframework provides a basic class, called TestCase, to write test cases. Programmersneed to extend the TestCase class to write a\u00aeset of individual test cases. It may benoted that to write, for example, 10 test cases, one need not write 10 subclasses ofthe class Testcase. Rather, one subclass, say Testcase, of Testcase, can contain10 methods one for each test case. Programmers need to make assertions abo\u00aeut the state of objects while extending the Testcase class to write test cases. For example, in each test case it isrequired to compare the actual outcome of a computation with the expected outcome.",
  "page398": " Though an if() statement can be used to compare the equality of two valuesor two objects, it is seen to be more elegant to write an assert statement to achievethe same. The class Testcase exten\u00aeds a utility class called Assert in the JUnitframework. Essentially, the Assert class provides methods, as explained in the following, to make assertions about the state of objects created and manipulated whiletesting.assert True(Boolean co\u00aendition): This assertion passes if the condition is true;otherwise, it fails.assert Equals(Object expected, Object actual): This assertion passes if theexpected and the actual objects are equal according to the equals() method;otherwise, the assertion fails.assert Equals(int expected, int actual): This assertion passes if expected andactua\u00ael are equal according to the = operator; otherwise, the assertion fails. For each primitive type int, float, double, char, byte, long, short, andBoolean, the assertion has an overloaded version.assert Equals(double expected, double actual, double tolerance): This assertion passes if the absolute value of the difference between expected andactual is less than or equal to the tolerance value;\u00aeotherwise, the assertionfails. The assertion has an overloaded version for float inputs.assert Same(Object expected, Object actual): This assertion passes if theexpected and actual values refer to the same object in memory; otherwise,the assertion fails assert Null(Object testo\u00aebject): This assertion passes if testobject is null; otherwise the assertion fails.assert False(Boolean condition): This is the logical opposite of assert True()",
  "page399": "The reader may note that the above list of assertions is not exhaustive. Infact, one can build other assertions while extending the TestCase class. When anassertion fails, a programmer may want\u00aeto know immediately the nature of thefailure. This can be done by displaying a message when the assertion fails. Eachassertion method listed above accepts an optional first parameter of type String ifthe assertion fails, then the String val\u00aeue is displayed. This facilitates the programmerto display a desired message when the assertion fails. As an aside, upon failure,the assert Equals() method displays a customized message showing the expectedvalue and the actual value At this point it is interesting to note that only failed tests are reported. Failedtests can be reported by\u00aevarious means, such as displaying a message, displaying anidentifier for the test case, and counting the total number of failed test cases. Essentially, an assertion method throws an exception, called AssertionFailedError, whenthe assertion fails, and JUnit catches the exception. The code shown in Figure 3.5illustrates how the assert True() assertion works: When the JUnit framework catchesan\u00aeexception, it records the fact that the assertion failed and proceeds to the nexttest case. Having executed all the test cases, JUnit produces a list of all those teststhat have failed. MyTestSuite and invoke the two methods MyTest1() and MyTest2(). Whether ornot the two method\u00aes, namely Method1() and Method()2, are to be invoked on twodifferent instances of the class TestMe depends on the individual objectives ofthose two test cases. In other words, it is the programmer who decides whether ornot two instances of the class TestMe are to be created",
  "page400": "Programmers can benefit from using tools in unit testing by reducing testing timewithout sacrificing thoroughness. The well-known tools in everyday life are aneditor, a compiler, an operating sy\u00aestem, and a debugger. However, in some cases, the real execution environment of a unit may not be available to a programmerwhile the code is being developed. In such cases, an emulator of the environmentis useful in testing and debugging th\u00aee code. Other kinds of tools that facilitateeffective unit testing are as follows:1. Code Auditor: This tool is used to check the quality of software to ensurethat it meets some minimum coding standards. It detects violations of programming, naming, and style guidelines. It can identify portions of code that cannotbe ported between differe\u00aent operating systems and processors. Moreover, it cansuggest improvements to the structure and style of the source code. In addition, itcounts the number of LOC which can be used to measure productivity, that is, LOCproduced per unit time, and calculate defect density, that is, number of defects perKLOC.2. Bound Checker: This tool can check for accidental writes into the instruction areas of\u00aememory or to any other memory location outside the data storage areaof the application. This fills unused memory space with a signature pattern (distinct binary pattern) as a way of determining at a later time whether any of thismemory space has been overwritten. The tool can i\u00aessue diagnostic messages whenboundary violations on data items occur. It can detect violation of the boundaries of array, for example, when the array index or pointer is outside its allowedrange. ",
  "page401": "Documenters: These tools read source code and automatically generatedescriptions and caller/called tree diagram or data model from the source code. Interactive Debuggers: These tools assist soft\u00aeware developers in implementing different debugging approaches discussed in this chapter. These toolsshould have the trace-back and breakpoint capabilities to enable the programmers tounderstand the dynamics of program execution and to iden\u00aetify problem areas in thecode. Breakpoint debuggers are based on deductive logic. Breakpoints are placedaccording to a heuristic analysis of code [32]. Another popular kind of debuggeris known as omniscient debugger (ODB), in which there is no deduction. It simplyfollows the trail of \"bad\" values back to their source no \"gue\u00aessing\" where to putthe breakpoints. An ODB is like \"the snake in the grass,\" that is, if you see a snakein the grass and you pull its tail, sooner or later you get to its head. In contrast,breakpoint debuggers suffer from the \"lizard in the grass\" problem, that is, whenyou see the lizard and grab its tail, the lizard breaks off its tail and gets away [33].n-Circuit Em\u00aeulators: An in-circuit emulator, commonly known as ICE,is an invaluable software development tool in embedded system design. It providesa high-speed Ethernet connection between a host debugger and a target microprocessor, enabling developers to perform common source-level debug\u00aeging activities,such as watching memory and controlling large numbers of registers, in a matterof seconds. It is vital for board bring-up, solving complex problems, and manufacturing or testing of products. Many emulators have advanced features, such as ",
  "page402": "performance analysis, coverage analysis, buffering of traces, and advance triggerand breakpoint possibilities.6. Memory Leak Detectors: These tools test the allocation of memory to anapplication\u00aewhich requests for memory, but fails to deallocate. These detect thefollowing overflow problems in application programs:Illegal read, that is, accesses to memory which is not allocated to theapplication or which the application is not auth\u00aeorized to access. Reads memory which has not been initialized. Dynamic memory overwrites to a memory location that has not been allocated to the application. Reading from a memory location not allocated, or not initialized, prior tothe read operation.The tools watch the heap, keep track of heap allocations to applications, anddetect memory\u00aeleaks. The tools also build profiles of memory use, for example,which line-of-code source instruction accesses a particular memory address.7. Static Code (Path) Analyzer: These tools identify paths to test, basedon the structure of the code such as McCabe's cyclometric complexity measure(Table 3.3). Such tools are dependent on source language and require the sourcecode to be recompiled\u00aewith the tool. These tools can be used to improve productivity, resource management, quality, and predictability by providing complexitymeasurement metrics.8. Software Inspection Support: Tools can help schedule group inspections.These can also provide status of items reviewed\u00aeand follow-up actions and distributethe reports of problem resolution. They can be integrated with other tools, such asstatic code analyzers Test Coverage Analyzer: These tools measure internal test coverage, oftenexpressed in terms of the control structure of the test object, and report the coverage metric. Coverage analyzers track and report what paths were exercised duringdynamic unit testing. ",
  "page403": "Test coverage analyzers are powerful tools that increase confidence in product quality by assuring that tests cover all of the structural parts ofa unit or a program. An important aspect in test\u00aecoverage analysis is to identifyparts of source code that were never touched by any dynamic unit test. Feedbackfrom the coverage reports to the source code makes it easier to design new unittest cases to cover the specific untested paths.\u00aeTest Data Generator: These tools assist programmers in selecting test datathat cause a program to behave in a desired manner. Test data generators can offerseveral capabilities beyond the basics of data generation:They have generate a large number of variations of a desired data set basedon a description of the characteristics which has be\u00aeen fed into the tool. They can generate test input data from source code. They can generate equivalence classes and values close to the boundaries. They can calculate the desired extent of boundary value testing. They can estimate the likelihood of the test data being able to reveal faults. They can generate data to assist in mutation analysis.Automatic generation of test inputs is an active\u00aearea of research. Several tools,such as CUTE [34], DART [35], and EGT system [36], have been developed byresearchers to improve test coverage. Test Harness: This class of tools supports the execution of dynamic unittests by making it almost painless to (i) install the unit unde\u00aer test in a test environment, (ii) drive the unit under test with input data in the expected input format, (iii)generate stubs to emulate the behavior of subordinate modules, and (iv) capturethe actual outcome as generated by the unit under test and log or display it in ausable form. ",
  "page404": "Advanced tools may compare the expected outcome with the actualoutcome and log a test verdict for each input test data.Performance Monitors: The timing characteristics of software componentscan\u00aebe monitored and evaluated by these tools. These tools are essential for anyreal-time system in order to evaluate the performance characteristics of the system,such as delay and throughput. For example, in telecommunication systems, theseto\u00aeols can be used to calculate the end-to-end delay of a telephone call. Network Analyzers: Network operating systems such as software that runon routers, switches, and client/server systems are tested by network analyzers.These tools have the ability to analyze the traffic and identify problem areas.Many of these networking tools allow test\u00aeengineers to monitor performance metrics and diagnose performance problems across the networks. These tools areenhanced to improve the network security monitoring (NSM) capabilities to detectintrusion Simulators and Emulators: These tools are used to replace the real software and hardware that are currently not available. Both kinds of tools are usedfor training, safety, and economy reasons.\u00aeSome examples are flight simulators,terminal emulators, and emulators for base transceiver stations in cellular mobilenetworks. These tools are bundled with traffic generators and performance analyzersin order to generate a large volume of input data Traffic Generators: Large\u00aevolumes of data needed to stress the interfacesand the integrated system are generated by traffic generators. These produce streamsof transactions or data packets. For example, in testing routers, one needs a trafficthat simulates streams of varying size Internet Protocol (IP) packets arriving fromdifferent sources. These tools can set parameters for mean packet arrival rate,duration, and packet size. Operational profiles can be used to generate traffic forload and stability testing.",
  "page405": "Version Control: A version control system provides functionalities to storea sequence of revisions of the software and associated information files underdevelopment. A system release is a collec\u00aetion of the associated files from a version control tool perspective. These files may contain source code, compiled code,documentation, and environment information, such as version of the tool used towrite the software. The objective of ver\u00aesion control is to ensure a systematic andtraceable software development process in which all changes are precisely managed, so that a software system is always in a well-defined state. With most of theversion control tools, the repository is a central place that holds the master copyof all the files.The configuration management system (CM\u00aeS) extends the version control fromsoftware and documentation to control the changes made to hardware, firmware,software, documentation, test, test fixtures, test documentation, and execution environments throughout the development and operational life of a system. Therefore,configuration management tools are larger, better variations of version control tools.The characteristics of the versio\u00aen control and configuration management tools areas follows: Access Control: The tools monitor and control access to components.One can specify which users can access a component or group of components. One can also restrict access to components currently undergoingmodification\u00aeor testing.Cross Referencing: The tools can maintain linkages among related components, such as problem reports, components, fixes, and documentations. One can merge files and coordinate multiple updates from different versionsto produce one consolidated file.",
  "page406": "Tracking of Modifications: The tools maintain records of all modifications to components. These also allow merging of files and coordinatemultiple updates from different versions to produce one\u00aeconsolidated file.These can track similarities and differences among versions of code, documentation, and test libraries. They also provide an audit trail or history ofthe changes from version to version.Release Generation: The tools can au\u00aetomatically build new systemreleases and insulate the development, test, and shipped versions of theproduct. System Version Management: The tools allow sharing of common components across system versions and controlled use of system versions. Theysupport coordination of parallel development, maintenance, and integrationof multiple componen\u00aets among several programmers or project teams. Theyalso coordinate geographically dispersed development and test teams. Archiving: The tools support automatic archiving of retired componentsand system versions. This chapter began with a description of unit-level testing, which means identifyingfaults in a program unit analyzed and executed in isolation. Two complementarytypes of unit testing\u00aewere introduced: static unit testing and dynamic unit testing. Static unit testing involves visual inspection and analysis of code, whereas aprogram unit is executed in a controlled manner in dynamic unit testing.Next, we described a code review process, which comprises six ste\u00aeps: readiness, preparation, examination, rework, validation, and exit. The goal of codereview is to assess the quality of the software in question, not the quality of theprocess used to develop the product. We discussed a few basic metrics that canbe collected from the code review process. Those metrics facilitate estimation ofreview time and resources required for similar projects. Also, the metrics makecode review visible to the upper management and allow upper management to besatisfied with the viability of code review as a testing tool ",
  "page407": "We explained several preventive measures that can be taken during codedevelopment to reduce the number of faults in a program. The preventive measures were presented in the form of a set of guid\u00aeelines that programmers canfollow to construct code. Essentially, the guidelines focus on incorporating suitablemechanisms into the code.Next, we studied dynamic unit testing in detail. In dynamic unit testing, aprogram unit is actually exe\u00aecuted, and the outcomes of program execution areobserved. The concepts of test driver and stubs were explained in the contextof a unit under test. A test driver is a caller of the unit under test and all the \"dummy modules\" called by the unit are known as stubs. We described how mutation analysis can be used to locate weaknesses\u00aein test data used for unit testing.Mutation analysis should be used in conjunction with traditional unit testing techniques such as domain analysis or data flow analysis. That is, mutation testing isnot an alternative to domain testing or data flow analysis.With the unit test model in place to reveal defects, we examined how programmers can locate faults by debugging a unit. Debugging occurs\u00aeas a consequenceof a test revealing a defect. We discussed three approaches to debugging: bruteforce, cause elimination, and backtracking. The objective of debugging is to precisely identify the cause of a failure. Given the symptom of a problem, the purposeis to isolate and de\u00aetermine its specific cause. We explained a heuristic to performprogram debugging. Next, we explained dynamic unit testing is an integral part of the XP softwaredevelopment process. In the XP process, unit tests are created prior to coding thisis known as test first.",
  "page408": "The test-first approach sets up checks and balances to improvethe chances of getting things right the first time. We then introduced the JUnitframework, which is used to create and execute dynam\u00aeic unit tests.We concluded the chapter with a description of several tools that can be useful in improving the effectiveness of unit testing. These tools are of the followingtypes: code auditor, bound checker, documenters, interactive debug\u00aegers, in-circuitemulators, memory leak detectors, static code analyzers, tools for software inspection support, test coverage analyzers, test data generators, tools for creating testharness, performance monitors, network analyzers, simulators and emulators, trafficgenerators, and tools for version control.The Institute of Electrical and El\u00aeectronics Engineers (IEEE) standard 1028-1988(IEEE Standard for Software Reviews and Audits: IEEE/ANSI Standard) describesthe detailed examination process for a technical review, an inspection, a softwarewalkthrough, and an audit. For each of the examination processes, it includes anobjective, an abstract, special responsibilities, program input, entry criteria, procedures, exit criteria, out\u00aeput, and auditability.Several improvements on Fagan's inspection techniques have been proposedby researchers during the past three decades. Those proposals suggest ways toenhance the effectiveness of the review process or to fit specific applicationdomains. A number of exc\u00aeellent articles address various issues related to softwareinspection as follows Biffl, and M. Halling, \"Investigating the Defect Effectiveness and CostBenefit of Nominal Inspection Teams,\" IEEE Transactions on SoftwareEngineering, Vol. 29, No. 5, May 2003, pp. 385-397.A. A. Porter and P. M. Johnson, \"Assessing Software Review Meeting:Results of a Comparative Analysis of Two Experimental Studies,\" IEEE",
  "page409": "Transactions on Software Engineering, Vol. 23, No. 3, March 1997, pp.129-145.A. A. Porter, H. P. Say, C. A. Toman, and L. G. Votta, \"An Experimentto Assess the Cost-Benefits of Code In\u00aespection in Large Scale SoftwareDevelopment,\" IEEE Transactions on Software Engineering, Vol. 23, No.6, June 1997, pp. 329-346.A. A. Porter and L. G. Votta, \"What Makes Inspection Work,\" IEEE Software,Vol. 14, No. 5, May\u00ae1997, pp. 99-102.C. Sauer, D. Jeffery, L. Land, and P. Yetton, \"The Effectiveness of Software Development Technical Reviews: A Behaviorally Motivated Programof Search,\" IEEE Transactions on Software Engineering, Vol. 26, No. 1,January 2000, pp. 1-14.An alternative non-execution-based technique is formal verification of\u00aecode.Formal verification consists of mathematical proofs to show that a program iscorrect. The two most prominent methods for proving program properties are thoseof Dijkstra and Hoare:E. W. Dijkstra, A Discipline of Programming, Prentice-Hall, EnglewoodCliffs, NJ, 1976.C. A. R. Hoare, \"An Axiomatic Basis of Computer Programming,\" Communications of the ACM , Vol. 12, No. 10, October\u00ae1969, pp. 576-580. Hoare presented an axiomatic approach in which properties of program fragmentsare described using preconditions and postconditions. An example statement witha precondition and a postcondition is {PRE} P {POST}, where PRE is the precondition, POST is the\u00aepostcondition, and P is the program fragment. Both PREand POST are expressed in first-order predicate calculus, which means that theycan include the universal quantifier \"for all\" or \"for every\" (\"for all\") and existential quantifier \"there exists\" or \"for some\" (\"thereexists\"). The interpretation of the above statement is that if the program fragmentP starts executing in a state satisfying PRE, then if P terminates, P will do so in astate satisfying POST ",
  "page410": "Hoare's logic led to Dijkstra's closely related \"calculus of programs,\" whichis based on the idea of weakest preconditions. The weakest preconditions R withrespect to a progr\u00aeam fragment P and a postcondition POST is the set of all statesthat, when subject to P, will terminate and leave the state of computation in POST.The weakest precondition is written as WP(P, POST).While mutation testing systematically impla\u00aents faults in programs by applyingsyntactic transformations, perturbation testing is performed to test a program'srobustness by changing the values of program data during run time, so that thesubsequent execution will either fail or succeed. Program perturbation is based onthree parts of software hypothesis as explained in the followi\u00aeng: Execution: A fault must be executed. Infection: The fault must change the data state of the computation directlyafter the fault location. Propagation: The erroneous data state must propagate to an output variable.In the perturbation technique, the programmer injects faults in the data stateof an executing program and traces the injected faults on the program's output.A fault injecti\u00aeon is performed by applying a perturbation function that changesthe program's data state. A perturbation function is a mathematical function thattakes a data state as its input, changes the data state according to some specifiedcriteria, and produces a modified data state\u00aeas output. For the interested readers,two excellent references on perturbation testing are as follow",
  "page411": "M. A. Friedman and J. M. Voas, Software Assessment Reliability, Safety,Testability, Wiley, New York, 1995.J. M. Voas and G. McGraw, Software Fault Injection Inoculating ProgramsAgainst Errors, W\u00aeiley, New York, 1998.The paper by Steven J. Zeil (\"Testing for Perturbation of Program Statement,\" IEEE Transactions on Software Engineering, Vol. 9, No. 3, May 1983,pp. 335-346) describes a method for deducing sufficient pat\u00aeh coverage to ensurethe absence of prescribed errors in a program. It models the program computationand potential errors as a vector space. This enables the conditions for nondetectionof an error to be calculated. The above article is an advanced reading for studentswho are interested in perturbation analysis.Those readers actively involve\u00aed in software configuration management(SCM) systems or interested in a more sophisticated treatment of the topic mustread the article by Jacky Estublier, David Leblang, Andre V. Hoek, Reidar \u00b4Conradi, Geoffrey Clemm, Walter Tichy, and Darcy Wiborg-Weber (\"Impactof Software Engineering Research on the Practice of Software ConfigurationManagement,\" ACM Transactions on Software En\u00aegineering and Methodology,Vol. 14, No. 4, October 2005, pp. 383-430). The authors discussed the evolutionof software configuration management technology, with a particular emphasis onthe impact that university and industrial research has had along the way. Thisarticle crea\u00aetes a detailed record of the critical value of software configurationmanagement research and illustrates the research results that have shaped thefunctionality of SCM systems ",
  "page412": "Two kinds of basic statements in a program unit are assignment statements andconditional statements. An assignment statement is explicitly represented by usingan assignment symbol, such as x = 2\u00ae*y;, where x and y are variables.Program conditions are at the core of conditional statements, such as if(), for()loop, while() loop, and goto. As an example, in if(x! = y), we are testing for theinequality of x and y. In the absence of con\u00aeditional statements, program instructionsare executed in the sequence they appear. The idea of successive execution ofinstructions gives rise to the concept of control flow in a program unit. Conditionalstatements alter the default, sequential control flow in a program unit. In fact,even a small number of conditional statements can lead to\u00aea complex control flowstructure in a program.Function calls are a mechanism to provide abstraction in program design.A call to a program function leads to control entering the called function. Similarly,when the called function executes its return statement, we say that control exitsfrom the function. Though a function can have many return statements, for simplicity, one can restructure the\u00aefunction to have exactly one return. A program unit canbe viewed as having a well-defined entry point and a well-defined exit point. Theexecution of a sequence of instructions from the entry point to the exit point of aprogram unit is called a program path. There can be a large\u00ae, even infinite, numberof paths in a program unit. Each program path can be characterized by an inputand an expected output. A specific input value causes a specific program path to beexecuted; it is expected that the program path performs the desired computation,thereby producing the expected output value. ",
  "page413": "The overall idea of generating test input data for performing control flow testinghas been depicted in Figure 4.1. The activities performed, the intermediate resultsproduced by those activities,\u00aeand programmer preferences in the test generationprocess are explained below.Inputs: The source code of a program unit and a set of path selection criteriaare the inputs to a process for generating test data. In the following, twoexamples\u00aeof path selection criteria are given.Example. Select paths such that every statement is executed at least once.Example. Select paths such that every conditional statement, forexample, an if() statement, evaluates to true and false at least once ondifferent occasions. A conditional statement may evaluate to true in onepath and false in a se\u00aecond path.Generation of a Control Flow Graph: A control flow graph (CFG) is adetailed graphical representation of a program unit. The idea behind drawing a CFG is to be able to visualize all the paths in a program unit. Theprocess of drawing a CFG from a program unit will be explained in thefollowing section. If the process of test generation is automated, a compilercan be modified to produce\u00aea CFG. Selection of Paths: Paths are selected from the CFG to satisfy the path selection criteria, and it is done by considering the structure of the CFG.Generation of Test Input Data: A path can be executed if and only if acertain instance of the inputs to the program unit ca\u00aeuses all the conditionalstatements along the path to evaluate to true or false as dictated by thecontrol flow. Such a path is called a feasible path. Otherwise, the path issaid to be infeasible. ",
  "page414": "It is essential to identify certain values of the inputsfrom a given path for the path to execute.Feasibility Test of a Path: The idea behind checking the feasibility of aselected path is to mee\u00aet the path selection criteria. If some chosen pathsare found to be infeasible, then new paths are selected to meet the criteria A CFG is a graphical representation of a program unit. Three symbols are usedto construct a CFG, as shown in Fig\u00aeure 4.2. A rectangle represents a sequential computation. A maximal sequential computation can be represented either by asingle rectangle or by many rectangles, each corresponding to one statement in thesource code.We label each computation and decision box with a unique integer. The twobranches of a decision box are labeled with T and F t\u00aeo represent the true and falseevaluations, respectively, of the condition within the box. We will not label a mergenode, because one can easily identify the paths in a CFG even without explicitlyconsidering the merge nodes. Moreover, not mentioning the merge nodes in a pathwill make a path description shorter.We consider the open files() function shown in Figure 4.3 to illustrate theprocess o\u00aef drawing a CFG. The function has three statements: an assignment statement int i 0;, a conditional statement if(), and a return(i) statement. The readermay note that irrespective of the evaluation of the if(), the function performs thesame action, namely, null. In Figure 4.4,\u00aewe show a high-level representation of the control flow in openfiles() with three nodes numbered 1, 2, and 3. The flowgraph shows just two paths in open files().",
  "page415": "A closer examination of the condition part of the if() statement reveals thatthere are not only Boolean and relational operators in the condition part, but alsoassignment statements. Some of the\u00aeir examples are given below:Assignment statements: fptr1 fopen(\"file1\", \"r\") and i  Relational operator: fptr1! = NULLBoolean operators: Execution of the assignment statements in the condition part of the if statementd\u00aeepends upon the component conditions. For example, consider the following component condition in the if part:((( fptr1 fopen(\"file1\", \"r\")) != NULL) nThe above condition is executed as follows: Execute the assignment statement fptr1 fopen(\"file1\", \"r\"). Execute the relational operation fptr1! = NULL. If the above r\u00aeelational operator evaluates to false, skip the evaluation ofthe subsequent condition components (i ) ",
  "page416": "A CFG, such as the one shown in Figure 4.7, can have a large number of differentpaths. One may be tempted to test the execution of each and every path in a programunit. For a program unit with a\u00aesmall number of paths, executing all the paths may be desirable and achievable as well. On the other hand, for a program unit with alarge number of paths, executing every distinct path may not be practical. Thus,it is more productive for p\u00aerogrammers to select a small number of program pathsin an effort to reveal defects in the code. Given the set of all paths, one is facedwith a question \"What paths do I select for testing?\" The concept of path selectioncriteria is useful is answering the above question. In the following, we state theadvantages of selecting paths\u00aebased on defined criteria All program constructs are exercised at least once. The programmer needsto observe the outcome of executing each program construct, for example,statements, Boolean conditions, and returns. We do not generate test inputs which execute the same path repeatedly.Executing the same path several times is a waste of resources. However,if each execution of a program path pot\u00aeentially updates the state of thesystem, for example, the database state, then multiple executions of thesame path may not be identical.We know the program features that have been tested and those not tested.For example, we may execute an if statement only once so that it evalu\u00aeatesto true. If we do not execute it once again for its false evaluation, we are,at least, aware that we have not observed the outcome of the program witha false evaluation of the if statement.",
  "page417": "Now we explain the following well-known path selection criteria: Select all paths. Select paths to achieve complete statement coverage. Select paths to achieve complete branch coverage.Select pa\u00aeths to achieve predicate coverage If all the paths in a CFG are selected, then one can detect all faults, except thosedue to missing path errors. However, a program may contain a large number ofpaths, or even an infinite number of paths. Th\u00aee small, loop-free openfiles() functionshown in Figure 4.3 contains more than 25 paths. One does not know whether ornot a path is feasible at the time of selecting paths, though only eight of all thosepaths are feasible. If one selects all possible paths in a program, then we say thatthe all-path selection criterion has been satisfied.Let\u00aeus consider the example of the openfiles() function. This function tries toopen the three files file1, file2, and file3. The function returns an integer representingthe number of files it has successfully opened. A file is said to be successfullyopened with \"read\" access if the file exists. The existence of a file is either \"yes\"or \"no.\" Thus, the input domain of\u00aethe function consists of eight combinations ofthe existence of the three files, as shown in Table 4.2.We can trace a path in the CFG of Figure 4.5 for each input, that is, eachrow of Table 4.2. Ideally, we identify test inputs to execute a certain path in a program; this will\u00aebe explained later in this chapter. We give three examples of thepaths executed by the test inputs (Table 4.3). In this manner, we can identify eightpossible paths .The all-paths selection criterion is desirable since itcan detect faults; however, it is difficult to achieve in practice.",
  "page418": "Statement coverage refers to executing individual program statements and observing the outcome. We say that 100% statement coverage has been achieved if allthe statements have been executed at l\u00aeeast once. Complete statement coverage isthe weakest coverage criterion in program testing. Any test suite that achieves lessthan statement coverage for new software is considered to be unacceptable.All program statements are represented in\u00aesome form in a CFG. Referringto the ReturnAverage() method in Figure 4.6 and its CFG in Figure 4.7, the fourassignment statementsi 0;ti = 0;tv = 0;sum = 0;have been represented by node 2. The while statement has been represented as aloop, where the loop control condition(ti < AS value[i] ! -999) as been represented by nodes 3 and 4. Thus,\u00aecovering a statement in a programmeans visiting one or more nodes representing the statement, more precisely, selecting a feasible entry-exit path that includes the corresponding nodes. Since a singleentry-exit path includes many nodes, we need to select just a few paths to coverall the nodes of a CFG. Therefore, the basic problem is to select a few feasiblepaths to cover all the n\u00aeodes of a CFG in order to achieve the complete statementcoverage criterion. We follow these rules while selecting paths: Select short paths. Select paths of increasingly longer length. Unfold a loop several times ifthere is a need.Select arbitrarily long, \"complex\" pa\u00aeths.One can select the two paths shown in Figure 4.4 to achieve complete statementcoverage.",
  "page419": "Syntactically, a branch is an outgoing edge from a node. All the rectangle nodeshave at most one outgoing branch (edge). The exit node of a CFG does not have anoutgoing branch. All the diamond n\u00aeodes have two outgoing branches. Covering abranch means selecting a path that includes the branch. Complete branch coveragemeans selecting a number of paths such that every branch is included in at leastone path.In a preceding discussion, w\u00aee showed that one can select two paths, SCPath 1and SCPath 2 in Table 4.4, to achieve complete statement coverage. These twopaths cover all the nodes (statements) and most of the branches of the CFG shownin Figure 4.7. The branches which are not covered by these two paths have beenhighlighted by bold dashed lines in Figure 4.8. These uncov\u00aeered branches correspond to the three independent conditions evaluating to false. This means that as a programmer we have not observed theoutcome of the program execution as a result of the conditions evaluating to false.Thus, complete branch coverage means selecting enough number of paths such thatevery condition evaluates to true at least once and to false at least once.We need to select m\u00aeore paths to cover the branches highlighted by the bolddashed lines We refer to the partial CFG of Figure 4.9a to explain the concept of predicatecoverage. OB1, OB2, OB3, and OB are four Boolean variables. The programcomputes the values of the individual variables OB1, OB2, an\u00aed OB3 details oftheir computation are irrelevant to our discussion and have been omitted. Next, OBis computed as shown in the CFG. The CFG checks the value of OB and executeseither OBlock1 or OBlock2 depending on whether OB evaluates to true or false,respectively.",
  "page420": "We need to design just two test cases to achieve both statement coverageand branch coverage. We select inputs such that the four Boolean conditions inFigure 4.9a evaluate to the values shown in\u00aeTable 4.6. The reader may note thatwe have shown just one way of forcing OB to true. If we select inputs so that thesetwo cases hold, then we do not observe the effect of the computations taking placein nodes 2 and 3. There may be faults in\u00aethe computation parts of nodes 2 and 3such that OB2 and OB3 always evaluate to false . Therefore, there is a need to design test cases such that a path is executedunder all possible conditions. The False branch of node 5 (Figure 4.9a) is executedunder exactly one condition, namely, when OB1 False, OB2 = False, and OB3 =False, whereas th\u00aee true branch executes under seven conditions. If all possiblecombinations of truth values of the conditions affecting a selected path have beenexplored under some tests, then we say that predicate coverage has been achieved.Therefore, the path taking the true branch of node 5 in Figure 4.9a must be executedfor all seven possible combinations of truth values of OB1, OB2, and OB3 whichresult i\u00aen OB = True.A similar situation holds for the partial CFG shown in Figure 4.9b, whereAB1, AB2, AB3, and AB are Boolean variables. ",
  "page421": "In Section 4.5 we explained the concept of path selection criteria to cover certainaspects of a program with a set of paths. The program aspects we consideredwere all statements, true and false\u00aeevaluations of each condition, and combinationsof conditions affecting execution of a path. Now, having identified a path, thequestion is how to select input values such that when the program is executedwith the selected inputs, the chosen\u00aepaths get executed. In other words, we needto identify inputs to force the executions of the paths. In the following, we definea few terms and give an example of generating test inputs for a selected path.1. Input Vector: An input vector is a collection of all data entities read bythe routine whose values must be fixed prior to entering th\u00aee routine. Members ofan input vector of a routine can take different forms as listed below:Input arguments to a routine Global variables and constants Files Contents of registers in assembly language programming Network connections TimersA file is a complex input element. In one case, mere existence of a file can beconsidered as an input, whereas in another case, contents of the file are cons\u00aeidered to be inputs. Thus, the idea of an input vector is more general than the concept ofinput arguments of a function.Example. An input vector for openfiles() (Figure 4.3) consists of individual presence or absence of the files file1, file2, and file3.Example. The input vecto\u00aer of the ReturnAverage() method shown in Figure 4.6is < value [], AS, MIN, MAX > .2. Predicate: A predicate is a logical function evaluated at a decision point.Example. The construct ti < AS is the predicate in decision node 3 of Figure 4.7.",
  "page422": "he construct OB is the predicate in decision node 5 of Figure 4.9.3. Path Predicate: A path predicate is the set of predicates associated witha path.The path in Figure 4.10 indicates that nodes\u00ae3, 4, 6, 7, and 10 are decision nodes. The predicate associated with node 3 appears twice in the path; inthe first instance it evaluates to true and in the second instance it evaluates tofalse. The path predicate associated with the path un\u00aeder consideration is shown inFigure 4.11.We also specify the intended evaluation of the component predicates as foundin the path specification. For instance, we specify that value[i] ! 999 mustevaluate to true in the path predicate shown in Figure 4.11. We keep this additionalinformation for the following two reasons:In the absence of this\u00aeadditional information denoting the intended evaluation of a predicate, we will have no way to distinguish between the twoinstances of the predicate ti < AS, namely 3(T) and 3(F), associated withnode 3.We must know whether the individual component predicates of a pathpredicate evaluate to true or false in order to generate path forcing inputs.4. Predicate Interpretation: The path predicate s\u00aehown in Figure 4.11 is composed of elements of the input vector < value[], AS, MIN, MAX >, a vector oflocal variables < i, ti, tv >, and the constant \u2212999. The local variables are notvisible outside a function but are used to hold intermediate results, point to array eleme\u00aents, and control loop iterations. ",
  "page423": "In other words, they play no roles in selecting inputs that force the paths to execute.Therefore, we can easily substitute all the local variables in a predicate with theelements of the input ve\u00aector by using the idea of symbolic substitution. Let usconsider the method shown in Figure 4.12. The input vector for the method inFigure 4.12 is given by < x1, x2 > . The method defines a local variable y and alsouses the constants 7 and 0\u00ae.The predicatex1  y > 0can be rewritten asx1  x2  7 >= 0by symbolically substituting y with x 2  7. The rewritten predicatex1  x2  7 >= 0has been expressed solely in terms of the input vector < x1,x2 > and the constantvector < 0,7 > . Thus, predicate interpretation is defined as the process of symbolically substituting operations alo\u00aeng a path in order to express the predicates solelyin terms of the input vector and a constant vector.In a CFG, there may be several different paths leading up to a decision pointfrom the initial node, with each path doing different computations. Therefore, apredicate may have different interpretations depending on how control reaches thepredicate under consideration. Path Predicate Expressio\u00aen: An interpreted path predicate is called a pathpredicate expression. A path predicate expression has the following properties:It is void of local variables and is solely composed of elements of the inputvector and possibly a vector of constants.It is a set of constraints cons\u00aetructed from the elements of the input vectorand possibly a vector of constants.Path forcing input values can be generated by solving the set of constraintsin a path predicate expression.",
  "page424": "If the set of constraints cannot be solved, there exist no input which cancause the selected path to execute. In other words, the selected path is saidto be infeasible. An infeasible path does n\u00aeot imply that one or more components of a pathpredicate expression are unsatisfiable. It simply means that the total combination of all the components in a path predicate expression is unsatisfiable.Infeasibility of a path predicate express\u00aeion suggests that one considers otherpaths in an effort to meet a chosen path selection criterion.Example. Consider the path shown in Figure 4.10 from the CFG of Figure 4.7.Table 4.7 shows the nodes of the path in column 1, the corresponding descriptionof each node in column 2, and the interpretation of each node in column 3. The intended\u00aeevaluation of each interpreted predicate can be found in column 1 of thesame row.We show the path predicate expression of the path under consideration in Figure 4.13 for the sake of clarity. The rows of Figure 4.13 have beenobtained from Table 4.11 by combining each interpreted predicate in column 3 withits intended evaluation in column 1. Now the reader may compare Figures 4.11and 4.13 to no\u00aete that the predicates in Figure 4.13 are interpretations of the corresponding predicates in Figure 4.11 We show in Figure 4.14 an infeasible path appearing in the CFG ofFigure 4.7. The path predicate and its interpretation are shown in Table 4.8, and thepath predicate expressi\u00aeon is shown in Figure 4.15. The path predicate expression isunsolvable because the constraint 0 > 0 = True is unsatisfiable. Therefore, the pathshown in Figure 4.14 is an infeasible path.",
  "page425": "Generating Input Data from Path Predicate Expression: We must solvethe corresponding path predicate expression in order to generate input data whichcan force a program to execute a selected path\u00ae. Let us consider the path predicateexpression shown in Figure 4.13. We observe that constraint 1 is always satisfied.Constraints 1 and 5 must be solved together to obtain AS 1. Similarly, constraints2, 3, and 4 must be solved together. We\u00aenote that MIN < = value[0] < = MAXand value[0]! = \u2212999. Therefore, we have many choices to select values of MIN,MAX, and value[0]. An instance of the solutions of the constraints of Figure 4.13is shown in Figure 4.16 We give examples of selected test data to achieve complete statement and branchcoverage. We show four sets of test dat\u00aea in Table 4.9. The first two data sets coverall statements of the CFG in Figure 4.7. However, we need all four sets of testdata for complete branch coverage.If we execute the method ReturnAverage shown in Figure 4.6 with the foursets of test input data shown in Figure 4.9, then each statement of the method isexecuted at least once, and every Boolean condition evaluates once to true andonce t\u00aeo false. We have thoroughly tested the method in the sense of completebranch coverage. However, it is possible to introduce simple faults in the methodwhich can go undetected when the method with the above four sets of test data isexecuted. Two examples of fault insertion are g\u00aeiven below. in the method. Here the fault is that the method computes the average of thetotal number of inputs, denoted by ti, rather than the total number of valid inputs,denoted by tv.",
  "page426": "he JPA specification only allows properties/paths in the ORDER BY clause if the SELECT clause projects the same properties/paths. The following queries may be nonportable but work in Hibernate:\u00aeselect i.name from Item i order by i.buyNowPrice asc select i from Item i order by i.seller.username des Be careful with implicit inner joins in path expressions and ORDER BY: The last query returns only Item instances that have a seller. T\u00aehis may be unexpected, as the same query without the ORDER BY clause would retrieve all Item instances. (Ignoring for a moment that in our model the Item always has a seller, this issue is visible with optional references.) You'll find a more detailed discussion of inner joins and path. expressions later in this chapter. You now know\u00aehow to write the FROM, WHERE, and ORDER BY clauses. You know how. to select the entities, you want to retrieve instances of and the necessary expressions. and operations to restrict and order the result. All you need now is the ability to project the data of this result to what you need in your application. In simple terms, selection and restriction in a query is the process of declaring whi\u00aech. tables and rows you want to query. Projection is defining the \"columns\" you want returned to the application: the data you need. The SELECT clause in JPQL performs projections. As promised earlier, this criteria query shows how you can add several Roots by calling\u00aethe from() method several times. To add several elements to your projection, either call the tuple() method of CriteriaBuilder, or the shortcut multiselect().",
  "page427": " Because this is a product, the result contains every possible combination of Item. and Bid rows found in the two underlying tables. Obviously, this query isn't useful, but you shouldn\u00ae9t be surprised to receive a collection of Object  as a query result. Hibernate manages all Item and Bid entity instances in persistent state, in the persistence. context. Note how the HashSets filter out duplicate Item and Bid instances. Alte\u00aernatively, with the Tuple API, in criteria queries you get typed access to the result list. Start by calling createTupleQuery() to create a CriteriaQuery<Tuple>. Then, refine the query definition by adding aliases for the entity classes The Object  returned by this query contain a Long at index 0, a String at index 1, and an Address at in\u00aedex 2. The first two are scalar values; the third is an embedded class instance. None are managed entity instances! Therefore, these values aren't in any persistent state, like an entity instance would be. They aren't transactional and obviously aren't checked automatically for dirty state. We say that all of these values are transient. This is the kind of query you need to wri\u00aete for a simple reporting screen, showing all user names and their home addresses. You have now seen path expressions several times: using dot-notation, you can reference properties of an entity, such as User#username with u.username. For a nested embedded property, for exampl\u00aee, you can write the path u.homeAddress.city.zipcode. These are single-valued path expressions, because they don't terminate in a mapped collection property A more convenient alternative than Object[] or Tuple, especially for report queries, is dynamic instantiation in projections, which is next",
  "page428": "Let's say you have a reporting screen in your application where you need to show. some data in a list. You want to show all auction items and when each auction ends. You don't want to\u00aeload managed Item entity instances, because no data will be modified: you only read data. First, write a class called ItemSummary with a constructor that takes a Long for the item's identifier, a String for the item's name, and a\u00aeDate for the item's auction end timestamp: We sometimes call these kinds of classes data transfer objects (DTOs), because their main purpose is to shuttle data around in the application. The ItemSummary class isn't mapped to the database, and you can add arbitrary methods (getter, setter, printing of values) as needed by your re\u00aeporting user interface. Hibernate can directly return instances of Item Summary from a query with the new keyword in JPQL and the construct() method in criteria We sometimes call these kinds of classes data transfer objects (DTOs), because their main purpose is to shuttle data around in the application. The ItemSummary class isn't mapped to the database, and you can add arbitrary method\u00aes (getter, setter, printing of values) as needed by your reporting user interface. Hibernate can directly return instances of ItemSummary from a query with the new keyword in JPQL and the construct() method in criteria ",
  "page429": "your DTO class doesn't have the right constructor, and you want to populate it from a query result through setter methods or fields, apply a ResultTransformer, as shown in in section 16.1.3\u00ae. Later, we have more examples of aggregation and grouping. Next, we're going to look at an issue with projection that is frequently confusing for many engineers: handling duplicates When you create a projection in a query, the elemen\u00aets of the result aren't guaranteed. to be unique. For example, item names aren't unique, so the following query may return the same name more than once: It's difficult to see how it could be meaningful to have two identical rows in a query result, so if you think duplicates are likely, you normally apply the DISTINCT keyword\u00aeor distinct() method This eliminates duplicates from the returned list of Item descriptions and translates. directly into the SQL DISTINCT operator. The filtering occurs at the database level. Later in this chapter, we show you that this isn't always the case. Earlier, you saw function calls in restrictions, in the WHERE clause. You can also call functions in projections, to modify the\u00aereturned data within the query If an Item doesn't have a buyNowPrice, a BigDecimal for the value zero is returned instead of null. Similar to coalesce() but more powerful are case/when expressions. The following query returns the username of each User and an additional\u00aeString with either \"Germany\", \"Switzerland For the built-in standard functions, refer to the tables in the previous section. Unlike function calls in restrictions, Hibernate won't pass on an unknown function call in a projection to the database as a plain direct SQL function call. Any function you'd like to call in a projection must be known to Hibernate and/or invoked with the special function() operation of JPQL.",
  "page430": " This projection returns the name of each auction Item and the number of days between item creation and auction end, calling the SQL datediff() function of the H2 database If instead you want to\u00aecall a function directly, you give Hibernate the function's return type, so it can parse the query. You add functions for invocation in projections by extending your configured org.hibernate.Dialect. The datediff() function is already\u00aeregistered for you in the H2 dialect. Then, you can either call it as shown with function(), which works in other JPA providers when accessing H2, or directly as datediff(), which most likely only works in Hibernate. Check the source code of the dialect for your database; you'll probably find many other proprietary SQL functions. alr\u00aeeady registered there. Furthermore, you can add SQL functions programmatically on boot to Hibernate by calling the method applySqlFunction() on a Hibernate MetadataBuilder See the Javadoc of SQLFunction and its subclasses for more information.  Next, we look at aggregation functions, which are the most useful functions in reporting queries. Reporting queries take advantage of the database\u00ae2019s ability to perform efficient grouping and aggregation of data. For example, a typical report query would retrieve the highest initial item price in a given category. This calculation can occur in the database, and you don't have to load many Item entity ins\u00aetances into memory. The aggregation functions standardized in JPA are count(), min(), max(), sum(), and avg(). This query returns a BigDecimal, because the amount property is of type BigDecimal. The sum() function also recognizes the BigInteger property type and returns Long for all other numeric property types",
  "page431": "When you call an aggregation function in the SELECT clause, without specifying any grouping in a GROUP BY clause, you collapse the results down to a single row, containing. the aggregated value(\u00aes). This means (in the absence of a GROUP BY clause) any SELECT. clause that contains an aggregation function must contain only aggregation function for more advanced statistics and for reporting, you need to be able to perform. grouping, w\u00aehich is up next JPA standardizes several features of SQL that are most commonly used for reporting although they're also used for other things. In reporting queries, you write the SELECT. clause for projection and the GROUP BY and HAVING clauses for aggregation. Just like in SQL, any property or alias that appears outside of an aggr\u00aeegate function. in the SELECT clause must also appear in the GROUP BY clause in this example, the u.lastname property isn't inside an aggregation function, so projected data has to be \"grouped by\" u.lastname. You also don't need to specify the property you want to count; the count(u)expression is automatically translated into. count(u.id) When grouping, you may run into a\u00aeHibernate limitation. The following query is specification compliant but not properly handled in Hibernate The JPA specification allows grouping by an entity path expression, group by i. But Hibernate doesn't automatically expand the properties of Item in the generated SQL\u00ae GROUP BY clause, which then doesn't match the SELECT clause. You have to expand the The JPA specification allows grouping by an entity path expression, group by i. But Hibernate doesn't automatically expand the properties of Item in the generated SQL GROUP BY clause, which then doesn't match the SELECT clause. You have to expand the fix",
  "page432": "Join operations combine data in two (or more) relations. Joining data in a query also enables you to fetch several associated instances and collections in a single query: for example, to load an\u00aeItem and all its bids in one round trip to the database. We now show you how basic join operations work and how to use them to write such dynamic fetching strategies. Let's first look at how joins work in SQL queries, without JPA Let\u00ae's start with the example we already mentioned: joining the data in the ITEM and BID tables, as shown in figure 15.1. The database contains three items: the first has three bids, the second has one bid, and the third has no bids. Note that we don't show. all columns; hence the dotted lines. What most people think of when they hea\u00aer the word join in the context of SQL databases is an inner join. An inner join is the most important of several types of joins and the easiest to understand. Consider the SQL statement and result in figure 15.2. This SQL statement contains an ANSI-style inner join in the FROM clause. If you join the ITEM and BID tables with an inner join, with the condition that the ID of an ITEM row must m\u00aeatch the ITEM_ID value of a BID row, you get items combined with their bids in the result. Note that the result of this operation contains only items that have bids",
  "page433": "You can think of a join as working as follows: first you take a product of the two. tables, by taking all possible combinations of ITEM rows with BID rows. Second, you filter these combined rows\u00aewith a join condition: the expression in the ON clause. (Any good database engine has much more sophisticated algorithms to evaluate a join; it usually doesn't build a memory-consuming product and then filter out rows.) The join condi\u00aetion is a Boolean expression that evaluates to true if the combined row is to be included in the result. It's crucial to understand that the join condition can be any expression that evaluates to true. You can join data in arbitrary ways; you aren't limited to comparisons of identifier values. For example, the join conditi\u00aeon on i.ID b.ITEM_ID and amount > 100 would only include rows from the BID table that also have an AMOUNT greater than 100. that a BID has a reference to an ITEM row. This doesn't mean you can only join by comparing primary and foreign key columns. Key columns are of course the most common operands in a join condition, because you often want to retrieve related information together. I\u00aef you want all items, not just the ones which have related bids, and NULL instead of bid data when there is no corresponding bid, then you write a (left) outer join",
  "page434": " In case of the left outer join, each row in the (left) ITEM table that never satisfies the join condition is also included in the result, with NULL returned for all columns of BID. Right outer\u00aejoins are rarely used; developers always think from left to right and put the \"driving\" table of a join operation first. In figure 15.4, you can see the same result with BID instead of ITEM as the driving table, and a right outer\u00aejoin. In SQL, you usually specify the join condition explicitly. Unfortunately, it isn't possible. to use the name of a foreign key constraint to specify how two tables are to be joined: select * from ITEM join BID on FK_BID_ITEM_ID doesn't work. You specify the join condition in the ON clause for an ANSI-style join or in the WH\u00aeERE. clause for a so-called theta-style join: select * from ITEM, BID b where i.ID b.ITEM_ID. This is an inner join; here you see that a product is created first in the FROM clause. We now discuss JPA join options. Remember that Hibernate eventually translates all queries into SQL, so even if the syntax is slightly different, you should always refer to the illustrations shown in this section\u00aeand verify that you understand what the resulting SQL and result set looks like JPA provides four ways of expressing (inner and outer) joins in queries: An implicit association join with path expressions. An ordinary join in the FROM clause with the join operator A fetch joi\u00aen in the FROM clause with the join operator and the fetch keyword for eager fetching A theta-style join in the WHERE clause. Let's start with implicit association joins. In JPA queries, you don't have to specify a join condition explicitly. Rather, you specify the name of a mapped Java class association. This is the same feature we'd prefer to have in SQL: a join condition expressed with a foreign key constraint name. Because you've mapped most, if not all, foreign key relationships of your database schema, you can use the names of these mapped associations in the query language. This is syntactical sugar, but it's convenient",
  "page435": " For example, the Bid entity class has a mapped many-to-one association named item, with the Item entity class. If you refer to this association in a query, Hibernate has enough information to d\u00aeeduce the join expression with a key column comparison. This helps make queries less verbose and more readable. Earlier in this chapter, we showed you property path expressions, using dot-notation: single-valued path expressions such\u00aeas user.homeAddress.zipcode and collectionvalued path expressions such as item.bids. You can create a path expression in an implicit inner join query The path b.item.name creates an implicit join on the many-to-one associations from Bid to Item the name of this association is item. Hibernate knows that you mapped this association wi\u00aeth the ITEM_ID foreign key in the BID table and generates the SQL join condition accordingly. Implicit joins are always directed along many-to-one or one-to-one associations, never through a collection-valued association (you can't write item.bids.amount). This query joins rows from the BID, the ITEM, and the USER tables. We frown on the use of this syntactic sugar for more complex quer\u00aeies. SQL joins are important, and especially when optimizing queries, you need to be able to see at a glance exactly how many of them there are How many joins are required to express such a query in SQL? Even if you get the answer right, it takes more than a few seconds to figu\u00aere out. The answer is two. The generated SQL looks something like this: Alternatively, instead of joins with such complex path expressions, you can write ordinary joins explicitly in the FROM clause",
  "page436": "JPA differentiates between purposes you may have for joining. Suppose you're querying items; there are two possible reasons you may be interested in joining them with bids. You may want to\u00aelimit the items returned by the query based on some criterion to apply to their bids. For example, you may want all items that have a bid of more than 100, which requires an inner join. Here, you aren't interested in items that have n\u00aeo bids. On the other hand, you may be primarily interested in the items but may want to execute an outer join just because you want to retrieve all bids for the queried items in a single SQL statement, something we called eager join fetching earlier. Remember that you prefer to map all associations lazily by default, so an eager fetch que\u00aery will override the default fetching strategy at runtime for a particular use case. Let's first write some queries that use joins for the purpose of restriction. If you want to retrieve Item instances and restrict the result to items that have bids with a certain amount, you have to assign an alias to a joined association. Then you refer to the alias in a WHERE clause to restrict the d\u00aeata you want This query assigns the alias b to the collection bids and limits the returned Item instances to those with Bid#amount greater than 100. So far, you've only written inner joins. Outer joins are mostly used for dynamic fetching, which we discuss soon. Sometimes\u00ae, you want to write a simple query with an outer join without applying a dynamic fetching strategy. For example, the following query and retrieves items that have no bids, and items with bids of a minimum bid amount:",
  "page437": "This query returns ordered pairs of Item and Bid, in a List<Object[]>. The first thing that is new in this query is the LEFT keyword and JoinType.LEFT in the criteria query. Optionally you can\u00aewrite LEFT OUTER JOIN and RIGHT OUTER JOIN in JPQL, but we usually prefer the short form. The second change is the additional join condition following the ON keyword. If instead you place the amount > 100 expression into the WHERE clause,\u00aeyou restrict the result to Item instances that have bids. This isn't what you want here: you want to retrieve items and bids, and even items that don't have bids. If an item has bids, the bid amount must be greater than 100. By adding an additional join condition in the FROM clause, you can restrict the Bid instances and still re\u00aetrieve all Item instances, whether they have bids or not The SQL query will always contain the implied join condition of the mapped association, i.ID b.ITEM_ID. You can only append additional expressions to the join condition. JPA and Hibernate don't support arbitrary outer joins without a mapped entity association or collection. Hibernate has a proprietary WITH keyword, it'\u00aes the same as the ON keyword in JPQL. You may see it in older code examples, because JPA only recently standardized ON. You can write a query returning the same data with a right outer join, switching the driving table This right outer join query is more important than you may\u00aethink. Earlier in this book, we told you to avoid mapping a persistent collection whenever possible. If you don't have a one-to-many Item#bids collection, you need a right outer join to retrieve all Items and their Bid instances. You drive the query from the \"other\" side: the many-toone Bid#item. ",
  "page438": "All the queries you saw in the previous sections have one thing in common: the returned Item instances have a collection named bids. This @OneToMany collection, if mapped as FetchType.LAZY (the\u00aedefault for collections), isn't initialized, and an additional SQL statement is triggered as soon as you access it. The same is true for all single-valued associations, like the @ManyToOne association seller of each Item. B\u00aey default, Hibernate generates a proxy and loads the associated User instance lazily and only on demand. What options do you have to change this behavior? First, you can change the fetch. plan in your mapping metadata and declare a collection or single-valued association. as FetchType.EAGER. Hibernate then executes the necessary SQL to gu\u00aearantee that. the desired network of instances is loaded at all times. This also means a single JPA. query may result in several SQL operations! As an example, the simple query selects I from Item i may trigger additional SQL statements to load the bids of each Item, the seller of each Item, and so on. In chapter 12, we made the case for a lazy global fetch plan in mapping metadata, where yo\u00aeu shouldn't have FetchType.EAGER on association and collection mappings. Then, for a particular use case in your application, you dynamically override the lazy fetch plan and write a query that fetches the data you need as efficiently as possible. For example, there is no\u00aereason you need several SQL statements to fetch all Item instances and to initialize their bids collections, or to retrieve the seller for each Item. You can do this at the same time, in a single SQL statement, with a join operation. Eager fetching of associated data is possible with the FETCH keyword in JPQL and the fetch() method in the criteria query API",
  "page439": "You've already seen the SQL query this produces and the result set in figure 15.3. This query returns a List<Item>; each Item instance has its bids collection fully initialized. This is di\u00aefferent than the ordered pairs returned by the queries in the previous section! Be careful you may not expect the duplicate results from the previous query: Make sure you understand why these duplicates appear in the result List. Verify th\u00aee number of Item \"rows\" in the result set, as shown in figure 15.3. Hibernate preserves the rows as list elements; you may need the correct row count to make rendering a report table in the user interface easier. You can filter out duplicate Item instances by passing the result List through a LinkedHashSet, which doesn't al\u00aelow duplicate elements but preserves the order of elements. Alternatively, Hibernate can remove the duplicate elements with the DISTINCT operation and distinct() criteria method: Understand that in this case the DISTINCT operation does not execute in the database. There will be no DISTINCT keyword in the SQL statement. Conceptually, you can't remove the duplicate rows at the SQL ResultSe\u00aet level. Hibernate performs deduplication in memory, just as you would manually with a LinkedHashSet This query returns a List<Item>, and each Item has its bids collection initialized. The seller of each Item is loaded as well. Finally, the bidder of each Bid instance is loaded\u00ae. You can do this in one SQL query by joining rows of the ITEM, BID, and USERS tables. If you write JOIN FETCH without LEFT, you get eager loading with an inner join (also if you use INNER JOIN FETCH) ",
  "page440": "An eager inner join fetch makes sense if there must be a fetched value: an Item must have a seller, and a Bid must have a bidder. There are limits to how many associations you should eagerly lo\u00aead in one query and how much data you should fetch in one round trip. Consider the following query, which initializes the Item bids and Item images collections: This is a bad query, because it creates a Cartesian product of bids and images,\u00aewith a potentially extremely large result set. We covered this issue in section 12.2.2. To summarize, eager dynamic fetching in queries has the following caveats: Never assign an alias to any fetch-joined association or collection for further restriction or projection. The query left join fetch i.bids b where b.amount ... is invalid. Yo\u00aeu can't say, \"Load the Item instances and initialize their bids collections, but only with Bid instances that have a certain amount.\" You can assign an alias to a fetch-joined association for further fetching: for example, retrieving the bidder of each Bid: left join fetch i.bids b join fetch b.bidder. You shouldn't fetch more than one collection; otherwise, you crea\u00aete a Cartesian product. You can fetch as many single-valued associations as you like without creating a product Queries ignore any fetching strategy you've defined in mapping metadata with @org.hibernate.annotations.Fetch. For example, mapping the bids collection wit\u00aeh org.hibernate.annotations.FetchMode.JOIN has no effect on the queries you write. The dynamic fetching strategy of your query ignores the global fetching strategy. On the other hand, Hibernate doesn't ignore the mapped fetch plan: Hibernate always considers a FetchType.EAGER, and you may see several additional SQL statements when you execute your query.",
  "page441": "If you eager-fetch a collection, the List returned by Hibernate preserves the number of rows in the SQL result as duplicate references. You can filter out the duplicates in-memory either manuall\u00aey with a LinkedHashSet or with the special DISTINCT operation in the query. There is one more issue to be aware of, and it deserves some special attention. You can't paginate a result set at the database level if you eagerly fetch a co\u00aellection. For example, for the query select i from Item i fetch i.bids, how should Query#setFirstResult(21) and Query#setMaxResults(10) be handled? Clearly, you expect to get only 10 items, starting with item 21. But you also want to load all bids of each Item eagerly. Therefore, the database can't do the paging operation\u00ae; you can't limit the SQL result to 10 arbitrary rows. Hibernate will execute paging in-memory if a collection is eagerly fetched in a query. This means all Item instances will be loaded into memory, each with the bids collection fully initialized. Hibernate then gives you the requested page of items: for example, only items 21 to 30. Not all items might fit into memory, and you probabl\u00aey expected the paging to occur in the database before it transmitted the result to the application! Therefore, Hibernate will log a warning message if your query contains fetch [collectionPath] and you call setFirstResult() or setMaxResults(). We don't recommend the use o\u00aef fetch [collectionPath] with setMaxResults() or setFirstResult() options. Usually there is an easier query you can write to get the data you want to render and we don't expect that you load data page by page to modify it. For example, if you want to show several pages of items and for each item the number of bids, write a report query ",
  "page442": "In traditional SQL, a theta-style join is a Cartesian product together with a join condition in the WHERE clause, which is applied on the product to restrict the result. In JP queries, the theta\u00ae-style syntax is useful when your join condition isn't a foreign key relationship mapped to a class association. For example, suppose you store the User's name in log records instead of mapping an association from LogRecord to Us\u00aeer. The classes don't know anything about each other, because they aren't associated. You can then find all the Users and their Log Records with the following theta-style join The join condition here is a comparison of username, present as an attribute in both. classes. If both rows have the same username, they're joined (wi\u00aeth an inner join) in the result. The query result consists of ordered pairs You probably won't need to use the theta-style joins often. Note that it's currently not. possible in JPA to outer join two tables that don't have a mapped association thetastyle joins are inner joins. Another more common case for theta-style joins is comparisons of primary key or foreign key val\u00aeues to either query parameters or other primary or foreign key values in the WHERE clause: This query returns pairs of Item and Bid instances, where the bidder is also the seller. This is an important query in CaveatEmptor because it lets you detect people who bid on their own\u00aeitems. You probably should translate this query into a database constraint and not allow such a Bid instance to be stored.  The previous query also has an interesting comparison expression: i.seller b.bidder. This is an identifier comparison, our next topic",
  "page443": "In this query, i.seller refers to the SELLER_ID foreign key column of the ITEM table, referencing the USERS table. The alias u refers to the primary key of the USERS table. (on the ID column). H\u00aeence, this query has a theta-style join and is equivalent to the easier, readable alternative A path expression ending with id is special in Hibernate: the id name always refers to the identifier property of an entity. It doesn't matte\u00aer what the actual name of the property annotated with @Id is you can always reach it with entityAlias.id. That's why we recommend you always name the identifier property of your entity classes id, to avoid confusion in queries. Note that this isn't a requirement or standardized in JPA; only Hibernate treats an id path element spe\u00aecially. You may also want to compare a key value to a query parameter, perhaps to find all Items for a given seller (a User) The first query pair uses an implicit table join; the second has no joins at all!  This completes our discussion of queries that involve joins. Our final topic is nesting selects within selects: subselect Sub selects are an important and powerful feature of SQL. A sub\u00aeselect is a select query embedded in another query, usually in the SELECT, FROM, or WHERE clause. JPA supports subqueries in the WHERE clause. Subselects in the FROM clause aren't supported because the query languages doesn't have transitive closure. The result of a q\u00aeuery may not be usable for further selection in a FROM clause. The query language also doesn't support subselects in the SELECT clause, but you map can subselects to derived properties with @org.hibernate.annotations.Formula, as shown in section 5.1.3. Subselects can be either correlated with the rest of the query or uncorrelated.",
  "page444": "The result of a subquery may contain either a single row or multiple rows. Typically, subqueries that return single rows perform aggregation. The following subquery returns the total number of i\u00aetems sold by a user; the outer query returns all users who have sold more than one item: The subquery in this example returns the maximum bid amount in the entire system; the outer query returns all bids whose amount is within one (U.S. dol\u00aelar, Euro, and so on) of that amount. Note that in both cases, parentheses enclose the subquery in JPQL. This is always required Uncorrelated subqueries are harmless, and there is no reason not to use them when convenient. You can always rewrite them as two queries, because they don't. reference each other. You should think more caref\u00aeully about the performance impact. of correlated subqueries. On a mature database, the performance cost of a simple correlated subquery is similar to the cost of a join. But it isn't necessarily possible to rewrite a correlated subquery using several separate queries.  If a subquery returns multiple rows, you combine it with quantification The following quantifiers are standardized: ALL\u00aeThe expression evaluates to true if the comparison is true for all values in the result of the subquery. It evaluates to false if a single value of the subquery result fails the comparison test. ANY The expression evaluates to true if the comparison is true for some (any) val\u00aeue in the result of the subquery. If the subquery result is empty or no value satisfies the comparison, it evaluates to false. The keyword SOME is a synonym for ANY. EXISTS Evaluates to true if the result of the subquery consists of one or more values",
  "page445": "This chapter explains query options that you may consider optional or advanced: transforming query results, filtering collections, and the Hibernate criteria query API. First, we discuss Hiberna\u00aete's ResultTransformer API, with which you can apply a result transformer to a query result to filter or marshal the result with your own code instead of Hibernate's default behavior. In previous chapters, we always advised you t\u00aeo be careful when mapping collections, because it's rarely worth the effort. In this chapter, we introduce collection filters, a native Hibernate feature that makes persistent collections more valuable. Finally, we look at another proprietary Hibernate feature, the org.hibernate.Criteria query API, and some situations when you m\u00aeight prefer it to the standard JPA query-by-criteria. Let's start with the transformation of query results. Transforming query results You can apply a result transformer to a query result so that you can filter or marshal the result with your own procedure instead of the Hibernate default behavior. Hibernates default behavior provides a set of default transformers that you can replace\u00aeand/or customize. The result you're going to transform is that of a simple query, but you need to access the native Hibernate API org.hibernate.Query through the Session, as shown in the following listing Each object array is a \"row\" of the query result. Each el\u00aeement of that tuple can be accessed by index: here index 0 is a Long, index 1 a String, and index 2 a Date. The first result transformer we introduce instead returns a List of Lists",
  "page446": "In section 15.3.2, we showed how a query can return instances of a JavaBean dynamically by calling the ItemSummary constructor. In JPQL, you achieve this with the new operator. For criteria quer\u00aeies, you use the construct() method. The ItemSummary class must have a constructor that matches the projected query result. Alternatively, if your JavaBean doesn't have the right constructor, you can still instantiate and populate its\u00aevalues through setters and/or fields with the AliasToBeanResultTransformer.  You create the transformer with the JavaBean class you want to instantiate, here ItemSummary. Hibernate requires that this class either has no constructor or a public nonargument constructor. When transforming the query result, Hibernate looks for s\u00aeetter methods and fields with the same names as the aliases in the query. The ItemSummary class must either have the fields itemId, name, unauctioned, or the setter methods setItemId(), setName(), and setAuctionEnd(). The fields or setter method parameters must be of the right type. If you have fields that map to some query aliases and setter methods for the rest, that's fine too.  You\u00aeshould also know how to write your own ResultTransformer when none of the built-in ones suits you The built-in transformers in Hibernate aren't sophisticated; there isn't much difference between result tuples represented as lists, maps, or object arrays. Next, w\u00aee show you how to implement a ResultTransformer. Let's assume that you want a List<ItemSummary> returned from the query shown in listing 16.1, but you can't let Hibernate create an instance of ItemSummary through reflection on a constructor. Maybe your ItemSummary class is predefined and doesn't have the right constructor, fields, and setter methods. Instead, you have an ItemSummaryFactory to produce instances of ItemSummary",
  "page447": "For each result \"row,\" an Object[] tuple must be transformed into the desired result value for that row. Here you access each projection element by index in the tuple array and then ca\u00aell the ItemSummaryFactory to produce the query result value. Hibernate passes the method the aliases found in the query, for each tuple element. You don't need the aliases in this transformer, though. C You can wrap or modify the resul\u00aet list after transforming the tuples. Here you make. the returned List unmodifiable: ideal for a reporting screen where nothing should change the data. As you can see in the example, you transform query results in two steps: first you customize how to convert each \"row\" or tuple of the query result to whatever value you desire. T\u00aehen you work on the entire List of these values, wrapping or converting again. Next, we discuss another convenient Hibernate feature (where JPA doesn't have an equivalent): collection filters. In chapter 7, you saw reasons you should (or rather, shouldn't) map a collection in your Java domain model. The main benefit of a collection mapping is easier access to data: you can call it\u00aeem.getImages() or item.getBids() to access all images and bids associated with an Item. You don't have to write a JPQL or criteria query; Hibernate will execute the query for you when you start iterating through the collection elements. The most obvious problem with this\u00aeautomatic data access is that Hibernate will always write the same query, retrieving all images or bids for an Item. You can customize the order of collection elements, but even that is a static mapping. What would you do to render two lists of bids for an Item, in ascending and descending order by creation date? ",
  "page448": " Instead, you can use a Hibernate proprietary feature, collection filters, that makes writing these queries easier, using the mapped collection. Let's say you have a persistent Item instanc\u00aee in memory, probably loaded with the EntityManager API. You want to list all bids made for this Item but further restrict the result to bids made by a particular User. You also want the list sorted in descending order by Bid#amount.The ses\u00aesion.createFilter() method accepts a persistent collection and a JPQL query fragment. This query fragment doesn't require a select or from clause; here it only has a restriction with the where clause and an order by clause. The alias this always refers to elements of the collection, here Bid instances. The filter created is an ordinar\u00aey org.hibernate.Query, prepared with a bound parameter and executed with list(), as usual. Hibernate doesn't execute collection filters in memory. The Item bids collection may be uninitialized when you call the filter and, and if so, remains uninitialized. Furthermore, filters don't apply to transient collections or query results. You may only apply them to a mapped persistent coll\u00aeection currently referenced by an entity instance managed by the persistence context. The term filter is somewhat misleading, because the result of filtering is a completely new and different collection; the original collection isn't touched. To the great surprise of ever\u00aeyone, including the designer of this feature, even trivial filters turn out to be useful. For example, you can use an empty query to paginate collection elements:",
  "page449": "Here, Hibernate executes the query, loading the collection elements and limiting the returned rows to two, starting with row zero of the result. Usually, you'd use an order by with paginate\u00aed queries. You don't need a from clause in a collection filter, but you can have one if that's your style. A collection filter doesn't even need to return elements of the collection being filtered. This next filter returns\u00aeany Item sold by any of the bidders All this is a lot of fun, but the most important reason for the existence of collection filters is to allow your application to retrieve collection elements without initializing the entire collection. For large collections, this is important to achieve acceptable performance. The following query re\u00aetrieves all bids made for the Item with an amount greater or equal to 100: Again, this doesn't initialize the Item#bids collection but returns a new collection. Before JPA 2, query-by-criteria was only available as a proprietary Hibernate API. Today, the standardized JPA interfaces are equally as powerful as the old org.hibernate.Criteria API, so you'll rarely need it. But se\u00aeveral features are still only available in the Hibernate API, such as query-by-example and embedding of arbitrary SQL fragments. In the following section, you find a short overview of the org.hibernate .Criteria API and some of its unique options Using the org.hibernate.C\u00aeriteria and org.hibernate.Example interfaces, you can build queries programmatically by creating and combining org.hibernate.criterion.* instances. You see how to use these APIs and how to express selection, restriction, joins, and projection. We assume that you've read the previous chapter and that you know how these operations are translated into SQL.",
  "page450": ". All query examples shown here have an equivalent JPQL or JPA criteria example in the previous chapter, so you can easily flip back and forth if you need to compare all three APIs. Let's\u00aestart with some basic selection examples. When you're ready to execute the query, \"attach\" it to a Session with getExecutableCriteria(). Note that this is a unique feature of the Hibernate criteria API. With JPA, you a\u00aelways need at least an EntityManagerFactory to get a CriteriaBuilder. You can declare the order of the results, equivalent to an order by clause in JPQL. The following query loads all User instances sorted in ascending order by first and last name: In this example, the code is written in the fluent style (using method chaining); method's\u00aesuch as add Order() return the original org.hibernate.Criteria.  Next, we look at restricting the selected records The Restrictions interface is the factory for individual Criterion you can add to the Criteria. Attributes are addressed with simple strings, here Item#name with \"name\". You can also match substrings, similar to the like operator in JPQL. The following query loads all User in\u00aestances with username starting with \"j\" or \"J\" A unique feature of the Hibernate Criteria API is the ability to write plain SQL fragments in restrictions. This query loads all User instances with a username shorter than eight characters Hibernate sends\u00aethe SQL fragment to the database as is. You need the {alias} placeholder to prefix any table alias in the final SQL; it always refers to the table the root entity is mapped to (USERS, in this case). You also apply a position parameter (named parameters aren't supported by this API) and specify its type as StandardBasicTypes.INTEGER",
  "page451": "The result of this query is a List of Object[], one array for each tuple. Each array contains a Long (or whatever the type of the user's identifier is), a String, and an Address. Just as w\u00aeith restrictions, you can add arbitrary SQL expressions and function calls to projections This query returns a List of Strings, where strings have the form \"[Item name]:[Auction end date]\". The second parameter for the proje\u00aection is the name of the alias(es) you used in the query: Hibernate needs this to read the value of the ResultSet. The type of each projected element/alias is also needed: here, StandardBasicTypes.STRING. Hibernate supports grouping and aggregation. This query counts users' last names This query returns all Bid instances of any\u00aeItem sold by User \"johndoe\" that doesn't have a buyNowPrice. The first inner join of the Bid#item association is made with createCriteria(\"item\") on the root Criteria of the Bid. This nested Criteria now represents the association path, on which another inner join is made with createCriteria(\"seller\"). Further restrictions are placed on each join Criteria; they will\u00aebe combined with logical and in the where clause of the final SQL query. This query returns all Item instances, loads the Item#bids collection with an outer join, and loads Bid#bidder with an inner join. The Item#seller is also loaded: because it can't be null, it doesnt\u00aematter whether an inner or outer join is used. As always, don't fetch several collections in one query, or you'll create a Cartesian products (see section 15.4.5).  Next, you see that subqueries with criteria also work with nested Criteria instances.",
  "page452": "The DetachedCriteria is a query that returns the number of items sold restricted by a given User. The restriction relies on the alias u, so this is a correlated subquery. The \"outer\" q\u00aeuery then embeds the DetachedCriteria and provides the alias u. Note that the subquery is the right operand of the lt() (less than) operation, which translates into 1 < ([Result of count query]) in SQL. Again, the position of the operands d\u00aeictates that the comparison is based on geAll() (greater or equal than all) to find the bids with \"less or equal than 10\" amount. So far, there are a few good reasons to use the old org.hibernate.Criteria API. You really should use the standardized JPA query languages in new applications, though. The most interesting features of\u00aethe old proprietary API we've shown are embedded SQL expressions in restrictions and projections. Another Hibernate-only feature you may find interesting is query-by-example The idea behind example queries is that you provide an example entity instance, and Hibernate loads all entity instances that \"look like the example.\" This can be convenient if you have a complex search sc\u00aereen in your user interface, because you don't have to write extra classes to hold the entered search terms. Let's say you have a search form in your application where you can search for User instances by last name. You can bind the form field for \"last name\u00ae1d directly to the User#lastname property and then tell Hibernate to load \"similar\" User instances",
  "page453": "Create an \"empty\" instance of User as a template for your search, and set the property values you're looking for: people with the last name \"Doe\" Create an instance of E\u00aexample with the template. This API allows you to fine-tune the search. You want the case of the last name to be ignored, and a substring search, so \"Doe\", \"Dex\", or \"Doe Y\" will match. D The User class has a B\u00aeoolean property called activated. As a primitive, it can't be null, and its default value is false, so Hibernate would include it in the search and only return users that aren't activated. You want all users, so tell Hibernate to ignore that property. E The Example is added to a Criteria as a restriction. Because you've writ\u00aeten the User entity class following JavaBean rules, binding it to a UI form should be trivial. It has regular getter and setter methods, and you can create an \"empty\" instance with the public no-argument constructor (remember our discussion of constructor design in section 3.2.3.) One obvious disadvantage of the Example API is that any string-matching options, such as ignoreC\u00aease() and enableLike(), apply to all string-valued properties of the template. If you searched for both last name and first name, both would be case insensitive substring matches. nsensitive substring matches. By default, all non-null valued properties of the given entity temp\u00aelate are added to the restriction of the example query. As shown in the last code snippet, you can manually exclude properties of the entity template by name with excludeProperty",
  "page454": "Other exclusion options are exclusion of zero-valued properties (such as int or long) with excludeZeroes() and disabling exclusion altogether with excludeNone(). If no properties are excluded, a\u00aeny null property of the template is added to the restriction in the SQL query with an is null check. If you need more control over exclusion and inclusion of properties, you can extend Example and write your own PropertySelector: After ad\u00aeding an Example restriction to a Criteria, you can add further restrictions to the query. Alternatively, you can add multiple example restrictions to a single query. The following query returns all Item instances with names starting with \"B\" or \"b\" and a seller matching a User example: You used the ResultTransformer API\u00aeto write custom code to process a query result, returning a list of lists and a list of maps, and mapping aliases to bean properties. We covered Hibernate's collection-filtering interfaces as well as making better use of mapped persistent collections. You explored the older Hibernate Criteria query facility and when you might use it instead of the standardized criteria queries in JPA.\u00aeWe covered all the relational and Hibernate goodies using this API: selection and ordering, restriction, projection and aggregation, joins, subselects, and example queries. Each element of that tuple can be accessed by index: here index 0 is a Long, index 1 a String, and\u00aeindex 2 a Date. The first result transformer we introduce instead returns a List of Lists",
  "page455": "In this chapter, we cover customizing and embedding SQL in a Hibernate application. SQL was created in the 1970s, but ANSI didn't standardized it until 1986. Although each update of the SQL\u00aestandard has seen new (and many controversial) features, every DBMS product that supports SQL does so in its own unique way. The burden of portability is again on the database application developers. This is where Hibernate helps: its buil\u00aet-in query languages produce SQL that depends on the configured database dialect. Dialects also help produce all other automatically generated SQL (for example, when Hibernate has to retrieve a collection on demand). With a simple dialect switch, you can run your application on a different DBMS. Hibernate generates all SQL statements for y\u00aeou, for all create, read, update, and delete (CRUD) operations. Sometimes, though, you need more control than Hibernate and the Java Persistence API provide: you need to work at a lower level of abstraction. With Hibernate, you can write your own SQL statements: Fall back to the JDBC API, and work directly with the Connection, PreparedStatement, and ResultSet interfaces. Hibernate prov\u00aeides the Connection, so you don't have to maintain a separate connection pool, and your SQL statements execute within the same (current) transaction. Write plain SQL SELECT statements, and either embed them within your Java code or externalize them (in XML fil\u00aees or annotations) as named queries. You execute these SQL queries with the Java Persistence API, just like a regular JPQL query. Hibernate can then transform the query result according to your mapping. This also works with stored procedure calls.",
  "page456": "Replace SQL statements generated by Hibernate with your own hand-written SQL. For example, when Hibernate loads an entity instance with em.find() or loads a collection on-demand, your own SQL qu\u00aeery can perform the load. You can also write your own Data Manipulation Language (DML) statements, such as UPDATE, INSERT, and DELETE. You might even call a stored procedure to preform a CRUD operation. You can replace all SQL statements au\u00aetomatically generated by Hibernate with custom statements. We start with JDBC fallback usage and then discuss Hibernate's automatic result-mapping capabilities. Then, we show you how to override queries and DML statements in Hibernate. Last, we discuss integration with stored database procedures. Sometimes you want Hibernate to\u00aeget out of the way and directly access the database through the JDBC API. To do so, you need a java.sql.Connection interface to write and execute your own PreparedStatement and direct access to your statement ResultSet. Because Hibernate already knows how to obtain and close database connections, it can provide your application with a Connection and release it when you're done. Th\u00aeis functionality is available with the org.hibernate.jdbc.Work API, a callbackstyle interface. You encapsulate your JDBC \"work\" by implementing this interface; Hibernate calls your implementation providing a Connection. The following example executes an SQL\u00aeSELLCT and iterates through the ResultSet For this \"work,\" an item identifier is needed, enforced with the final field and the constructor paramet",
  "page457": "The execute() method is called by Hibernate with a JDBC Connection. You don't have to close the connection when you're done. D You have to close and release other resources you've\u00aeobtained, though, such as the PreparedStatement and ResultSet In this case, Hibernate has already enlisted the JDBC Connection it provides with the current system transaction. Your statements are committed when the system transaction is co\u00aemmitted, and all operations, whether executed with the EntityManager or Session API, are part of the same unit of work. Alternatively, if you want to return a value from your JDBC \"work\" to the application, implement the interface org.hibernate.jdbc.ReturningWork. There are no limits on the JDBC operations you can perform\u00aein a Work implementation. Instead of a PreparedStatement, you may use a CallableStatement and execute a stored procedure in the database; you have full access to the JDBC API. For simple queries and working with a ResultSet, such as the one in the previous example, a more convenient alternative is available. When you execute an SQL SELECT query with the JDBC API or execute a stored procedur\u00aees that returns a ResultSet, you iterate through each row of this result set and retrieve the data you need. This is a labor-intensive task, and you end up duplicating the same lines of code repeatedly. When you execute an SQL SELECT query with the JDBC API or execute a stored\u00aeprocedure that returns a ResultSet, you iterate through each row of this result set and retrieve the data you need. This is a labor-intensive task, and you end up duplicating the same lines of code repeatedly.",
  "page458": "The returned Item instances are in persistent state, managed by the current persistence context. The result is therefore the same as with the JPQL query select i from Item i. For this transform\u00aeation, Hibernate reads the result set of the SQL query and tries to discover the column names and types as defined in your entity mapping metadata. If the column AUCTIONEND is returned, and it's mapped to the Item#auctionEnd property,\u00aeHibernate knows how to populate that property and returns fully loaded entity instances. Note that Hibernate expects the query to return all columns required to create an instance of Item, including all properties, embedded components, and foreign key columns. If Hibernate can't find a mapped column (by name) in the result set, an ex\u00aeception is thrown. You may have to use aliases in SQL to return the same column names as defined in your entity mapping metadata. The interfaces javax.persistence.Query and org.hibernate.SQLQuery both support parameter binding. The following query returns only a single Item entity instance Although available in Hibernate for both APIs, the JPA specification doesn't consider named parame\u00aeter binding for native queries portable. Therefore, some JPA providers may not support named parameters for native queries.  If your SQL query doesn't return the columns as mapped in your Java entity class, and you can't rewrite the query with aliases to rename colum ns in the result, you must create a result-set mapping",
  "page459": "The following query returns a List of managed Item entity instances. All columns of the ITEM table are included in the SQL projection, as required for the construction of an Item instance. But t\u00aehe query renames the NAME column to EXTENDED_NAME with an alias in the projection Hibernate can no longer automatically match the result set fields to Item properties: the NAME column is missing from the result set. You therefore specify a\u00ae\"result mapping\" with You map all fields of the result set to properties of the entity class. Even if only one field/column doesn't match the already mapped column name (here EXTENDED_NAME), all other columns and properties have to be mapped as well SQL result mappings in annotations are difficult to read and as usual\u00aewith JPA annotations, they only work when declared on a class, not in a package-info.java metadata file. We prefer externalizing such mappings into XML files. The following provides the same mapping: If both result-set mappings have the same name, the mapping declared in XML overrides the one defined with annotations. You can also externalize the actual SQL query with @NamedNativeQuer\u00aey or <namednative-query>, as shown in section 14.4. In all following examples, we keep the SQL statement embedded in the Java code, because this will make it easier for you to understand what the code does. But most of the time, you'll see result-set mappings in the more succinct XML syntax.",
  "page460": "With the Hibernate API, you can perform the result-set mapping directly within the query through alias placeholders. When calling addEntity(), you provide an alias, here i. In the SQL string, yo\u00aeu then let Hibernate generate the actual aliases in the projection with placeholders such as {i.name} and {i.auctionEnd}, which refer to properties of the Item entity. No additional result-set mapping declaration is necessary; Hibernate gen\u00aeerates the aliases in the SQL string and knows how to read the property values from the query ResultSet. This is much more convenient than the JPA result-set mapping option. Or, if you can't or don't want to modify the SQL statement, use add Root() and add Property() on the org.hibernate.SQLQuery to perform the mapping This is e\u00aeffectively an eager fetch of the association Item#seller. Hibernate knows that each row contains the fields for an Item and a User entity instance, linked by the SELLER_ID. The duplicate columns in the result set would be i.ID and u.ID, which both have the same name. You've renamed them with an alias to ITEM_ID and USER_ID, so you have to map how the result set is to be transformed As b\u00aeefore, you have to map all fields of each entity result to column names, even if only two have different names as the original entity mapping. This query is much easier to map with the Hibernate API:  By default, all non-null valued properties of the given entity template are\u00aeadded to the restriction of the example query. As shown in the last code snippet, you can manually exclude properties of the entity template by name with excludeProperty",
  "page461": "Hibernate will add auto-generated unique aliases to the SQL statement for the {i.*} and {u.*} placeholders, so the query won't return duplicate column names.  You may have noticed the dot\u00aesyntax in the previous JPA result mapping for the home Address embedded component in a User. Let's look at this special case again We've shown this dot syntax several times before when discussing embedded components: you reference\u00aethe street property of home Address with homeAddress.street. For nested embedded components, you can write homeAddress.city.name if City isn't just a string but another embeddable class. Hibernate's SQL query API also supports the dot syntax in alias placeholders for component properties. Here are the same query and result-set\u00aemapping: The query (outer) joins the ITEM and BID tables. The projection returns all columns required to construct Item and Bid instances. The query renames duplicate columns such as ID with aliases, so field names are unique in the result. C Because of the renamed fields, you have to map each column to its respective entity property. D Add a Fetch Return for the bids collection with the alia\u00aes of the owning entity and map the key and element special properties to the foreign key column BID_ITEM_ID and the identifier of the Bid. Then the code maps each property of Bid to a field of the result set. Some fields are mapped twice, as required by Hibernate for construct\u00aeion of the collection The number of rows in the result set is a product: one item has three bids, one item has one bid, and the last item has no bids, for a total of five rows in the result. F The first element of the result tuple is the Item instance; Hibernate initialized the bids collection",
  "page462": "The second element of the result tuple is each Bid. Alternatively, if you don't have to manually map the result because the field names returned by your SQL query match the already-mapped c\u00aeolumns of the entities, you can let Hibernate insert aliases into your SQL statement with placeholders: Eager fetching of collections with dynamic SQL result mappings is only available with the Hibernate API; it's not standardized in J\u00aePA So far, you've seen SQL queries returning managed entity instances. You can also return transient instances of any class with the right constructor. The returned column types have to match the constructor parameter types; Hibernate would default to BigInteger for the ID column, so you map it to a Long with the class attribute. Th\u00aee Hibernate API gives you a choice. You can either use an existing result mapping for the query by name, or apply a result transformer, as you saw for JPQL queries in section 16.1: You can use an existing result mapping. C Alternatively, you can map the fields returned by the query as scalar values. Without a result transformer, you'd get an Object[] for each result row. D Apply a\u00aebuilt-in result transformer to turn the Object[] into instances of ItemSummary. As explained in section 15.3.2, Hibernate can use any class constructor with such a mapping. Instead of ItemSummary, you can construct Item instances. They will be in either transient or detached st\u00aeate, depending on whether you return and map an identifier value in your query.  You can also mix different kinds of result mappings or return scalar values directly.",
  "page463": "Hibernate usually generates CRUD SQL statements at startup. It caches the SQL statements internally for future use, thus avoiding any runtime costs of SQL generation forthe most common operation\u00aes. You've seen how you can override the R in CRUD, sonow, let's do the same for CUD. For each entity, you can define custom CUD SQL statements with the Hibernate annotations @SQLInsert, @SQLUpdate, and @SQLDelete,respectivelyYou n\u00aeeed to be careful with the binding of arguments for SQL statements and parameter placeholders (the ? in the statements). For CUD customization, Hibernate onlysupports positional parameters. What is the right order for the parameters? There's an internal order to howHibernate binds arguments to SQL parameters for CUD statements. The ea\u00aesiest way tofigure out the right SQL statement and parameter order is to let Hibernate generatean example for you. Without any custom SQL statements, enable DEBUG logging forthe org.hibernate.persister.entity category, and search the Hibernate startupoutput for lines like the following:These automatically generated SQL statements show the right parameter order, andHibernate always binds the v\u00aealues in that order. Copy the SQL statements you want tocustomize into annotations, and make the necessary changes. A special case is properties of an entity class mapped to another table with@SecondaryTable. The CUD customization statements we've shown so far only refer t\u00aeothe columns of the main entity table. Hibernate still executes automatically generatedSQL statements for insertion, updates, and deletion of rows in the secondary table(s).",
  "page464": "statements in an XML file. This also simplifies ad hoc testing, because you can copyand paste SQL statements between an XML file and your SQL database console. You've probably noticed that\u00aeall the SQL examples in the previous sections weretrivial. In fact, none of the examples required a query written in SQL we could haveused JPQL in each case. To make the next example moreinteresting, we write a query that can't be expr\u00aeessed inJPQL, only in SQL. This is the mapping of the association a regular@ManyToOne of the PARENT_ID foreign key column:Categories form a tree. The root of the tree is a Category node without a parent. Thedatabase data for the example tree is in figure 17.2. You can also represent this data as a tree diagram, as shown in figure 17.3. Alt\u00aeernatively, you can use a sequence of paths and the level of each node:Now, consider how your application loads Category instances. You may want to findthe root Category of the tree. This is a trivial JPQL query:You can easily query for the categories in a particular level of the tree, such as all children of the root:This query will only return direct children of the root: here, categories T\u00aewo and Three. How can you load the entire tree (or a subtree) in one query? This isn't possible withJPQL, because it would require recursion: \"Load categories at this level, then all the children on the next level, then all the children of those, and so on.\" In S\u00aeQL, you can writesuch a query, using a common table expression (CTE), a feature also known as subquery factoring.",
  "page465": "It's a complex query, and we won't spend too much time on it here. To understand it,read the last SELECT, querying the CATEGORY_LINK view. Each row in that view represents a node in th\u00aee tree. The view is declared here in the WITH() AS operation. TheCATEGORY_LINK view is a combined (union) result of two other SELECTs. You add additional information to the view during recursion, such as the PATH and the LEVEL ofeach node.T\u00aehe XML maps the ID, CAT_NAME, and PARENT_ID fields to properties of the Categoryentity. The mapping returns the PATH and LEVEL as additional scalar values. To execute the named SQL query and access the result, write the following:Each tuple contains a managed, persistent Category instance; its path in the tree as astring (such as /One, /On\u00aee/Two, and so on); and the tree level of the node. Alternatively, you can declare and map an SQL query in a Hibernate XML metadata file:We left out the SQL query in this snippet; it's the same as the SQL statement shownearlier in the JPA example. As mentioned in section 14.4, with regard to the execution in Java code, it doesn'tmatter which syntax you declare your named queries in:\u00aeXML file or annotations. Eventhe language doesn't matter it can be JPQL or SQL. Both Hibernate and JPA queryinterfaces have methods to \"get a named query\" and execute it independently fromhow you defined it. This concludes our discussion of SQL result mapping for\u00aequeries. The next subjectis customization of SQL statements for CRUD operations, replacing the SQL automatically generated by Hibernate for creating, reading, updating, and deleting data in thedatabase.",
  "page466": "The first custom SQL you write loads an entity instance of the User class. All the following code examples show the same SQL that Hibernate executes automatically bydefault, without much customi\u00aezation this helps you understand the mapping technique more quickly. You can customize retrieval of an entity instance with a loaderHibernate has two requirements when you override an SQL query to load an entityinstance: Write a named query\u00aethat retrieves the entity instance. We show an example inSQL, but as always, you can also write named queries in JPQL. For an SQL query,you may need a custom result mapping, as shown earlier in this chapter.Activate the query on an entity class with @org.hibernate.annotationsLoader. This enables the query as the replacement for the Hibern\u00aeate-generated query.Let's override how Hibernate loads an instance of the User entity, as shown in the following listing.Annotations declare the query to load an instance of User; you can also declare it inan XML file (JPA or Hibernate metadata). You can call this named query directly inyour data-access code when needed. The query must have exactly one parameter placeholder, which Hibern\u00aeate sets as theidentifier value of the instance to load. Here it's a positional parameter, but a namedparameter would also work. For this trivial query, you don't need a custom result-set mapping. The User class mapsall fields returned by the query. Hibernate can automatically transform the result.",
  "page467": "Setting the loader for an entity class to a named query enables the query for all operations that retrieve an instance of User from the database. There's no indication of thequery language\u00aeor where you declared it; this is independent of the loader declaration.In a named loader query for an entity, you have to SELECT (that is, perform a projection for) the following properties of the entity class: The value of the identifier\u00aeproperty or properties, if a composite primary key isused. All scalar properties of basic type. All properties of embedded components. An entity identifier value for each @JoinColumn of each mapped entity association such as @ManyToOne owned by the loaded entity class. All scalar properties, embedded component properties, and association j\u00aeoinreferences that are inside a @SecondaryTable annotation. If you enable lazy loading for some properties, through interception and bytecodeinstrumentation, you don't need to load the lazy properties (see section 12.1.3).Hibernate always calls the enabled loader query when a User has to be retrieved fromthe database by identifier. When you call em.find(User.class, USER_ID), your custom\u00aequery will execute. When you call someItem.getSeller().getUsername(), andthe Item#seller proxy has to be initialized, your custom query will load the data. You may also want to customize how Hibernate creates, updates, and deletes aninstance of User in the database.",
  "page468": "Hibernate usually generates CRUD SQL statements at startup. It caches the SQL statements internally for future use, thus avoiding any runtime costs of SQL generation forthe most common operation\u00aes. You've seen how you can override the R in CRUD, sonow, let's do the same for CUD. For each entity, you can define custom CUD SQL statements with the Hibernate annotations @SQLInsert, @SQLUpdate, and @SQLDelete,respectivelyYou n\u00aeeed to be careful with the binding of arguments for SQL statements and parameter placeholders (the ? in the statements). For CUD customization, Hibernate onlysupports positional parameters. What is the right order for the parameters? There's an internal order to howHibernate binds arguments to SQL parameters for CUD statements. The ea\u00aesiest way tofigure out the right SQL statement and parameter order is to let Hibernate generatean example for you. Without any custom SQL statements, enable DEBUG logging forthe org.hibernate.persister.entity category, and search the Hibernate startupoutput for lines like the following:These automatically generated SQL statements show the right parameter order, andHibernate always binds the v\u00aealues in that order. Copy the SQL statements you want tocustomize into annotations, and make the necessary changes. A special case is properties of an entity class mapped to another table with@SecondaryTable. The CUD customization statements we've shown so far only refer\u00aeto the columns of the main entity table. Hibernate still executes automatically generatedSQL statements for insertion, updates, and deletion of rows in the secondary table(s).",
  "page469": "You can customize this SQL by adding the @org.hibernate.annotations.Table annotation to your entity class and setting its sqlInsert, sqlUpdate, and sqlDelete attributes. If you prefer to have yo\u00aeur CUD SQL statements in XML, your only choice is to mapthe entire entity in a Hibernate XML metadata file. The elements in this proprietarymapping format for custom CUD statements are <sql-insert>, <sql-update>, and<sql-delete>. Fortunatel\u00aey, CUD statements are usually much more trivial than queries, so annotations are fine in most applications. You've now added custom SQL statements for CRUD operations of an entityinstance. Next, we show how to override SQL statements loading and modifying acollection.Let's override the SQL stafortement Hibernate uses when loadin\u00aeg the Item#images collection. This is a collection of embeddable components mapped with @ElementCollection; the procedure is the same for collections of basic types or many-valuedentity associations (@OneToMany or @ManyToMany)As before, you declare that a named query will load the collection. This time, however,you must declare and map the result of the query in a Hibernate XML metadata file,\u00aewhich is the only facility that supports mapping of query results to collection properties:The query has to have one (positional or named) parameter. Hibernate sets its valueto the entity identifier that owns the collection. Whenever Hibernate need to initializethe Item#images\u00aecollection, Hibernate now executes your custom SQL query.",
  "page470": "QUOTING SQL IDENTIFIERS- From time to time, especially in legacy databases, you'll encounter identifiers with strange characters or whitespace, or wish to force case sensitivity. Or, as in\u00aethe previous example, the automatic mapping of a class or property would require a table or column name that is a reserved keyword.  Hibernate 5 knows the reserved keywords of your DBMS through the configured database dialect. Hibern\u00aeate 5 can automatically put quotes around such strings when generating SQL. You can enable this automatic quoting with hibernate.auto_quote _keyword true in your persistence unit configuration. If you're using an older version of Hibernate, or you find that the dialect's information is incomplete, you must still apply quotes on n\u00aeames manually in your mappings if there is a conflict with a keyword.  If you quote a table or column name in your mapping with backticks, Hibernate always quotes this identifier in the generated SQL. This still works in latest versions of Hibernate, but JPA 2.0 standardized this functionality as delimited identifiers with double quotes. If you have to quote all SQL identifiers, creat\u00aee an orm.xml file and add the setting <delimited-identifiers/> to its <persistence-unit-defaults> section, as shown in listing 3.8. Hibernate then enforces quoted identifiers everywhere.  You should consider renaming tables or columns with reserved keyword names whenever\u00aepossible. Ad hoc SQL queries are difficult to write in an SQL console if you have to quote and escape everything properly by hand.  Next, you'll see how Hibernate can help when you encounter organizations with strict conventions for database table and column names.To summarize, our recommendations on identifier generator strategies are as follows: In general, we prefer pre-insert generation strategies that produce identifier values independently before INSERT.ng names- Let's first talk about the naming of entity classes and tables. If you only specify @Entity on the persistence-capable class, the default mapped table name is the same as the class name. Note that we write SQL artifact names in UPPERCASE to make them easier to distinguish SQL is actually case insensitive. So the Java entity class Item maps to the ITEM table. You can override the table name with the JPA @Table annotation, as shown next.",
  "page471": "Entities are the coarser-grained classes of your system. Their instances have an independent life cycle and their own identity, and many other instances can reference them. Value types, o\u00aen the other hand, are dependent on a particular entity class. A value type instance is bound to its owning entity instance, and only one entity instance can reference it it has no individual identity. We looked at Java identity, object equ\u00aeality, and database identity, and at what makes good primary keys. You learned which generators for primary key values Hibernate provides out of the box, and how to use and extend this identifier system. We discussed some useful class mapping options, such as naming strategies and dynamic SQL generation. After spending the previous chapte\u00aer almost exclusively on entities and the respective class- and identity-mapping options, we now focus on value types in their various forms. We split value types into two categories: basic value-typed classes that come with the JDK, such as String, Date, primitives, and their wrappers; and developerdefined value-typed classes, such as Address and MonetaryAmount in CaveatEmptor. I\u00aen this chapter, we first map persistent properties with JDK types and learn the basic mapping annotations. You see how to work with various aspects of properties: overriding defaults, customizing access, and generated values. You also see how SQL is used with derived properties\u00aeand transformed column values. We wrap up basic properties with temporal properties and mapping enumerations. We then discuss custom value-typed classes and map them as embeddable components. You learn how classes relate to the database schema and make your classes embeddable, while allowing for overriding embedded attributes. We complete embeddable components by mapping nested components. Finally, we discuss how to customize loading and storing of property values at a lowQL",
  "page472": "This configuration by exception approach means you don't have to annotate a property to make it persistent; you only have to configure the mapping in an exceptional case. Several anno\u00aetations are available in JPA to customize and control basic property map. Overriding basic property defaults- You might not want all properties of an entity class to be persistent. For example, although it makes sense to have a persistent I\u00aetem#initialPrice property, an Item#totalPriceIncludingTax property shouldn't be persistent if you only compute and use its value at runtime, and hence shouldn't be stored in the database. To exclude a property, mark the field or the getter method of the property with the @javax.persistence.Transient annotation or use the Ja\u00aeva transient keyword. The transient keyword usually only excludes fields for Java serialization, but it's also recognized by JPA providers.  We'll come back to the placement of the annotation on fields or getter methods in a moment. Let's assume as we have before that Hibernate will access fields directly because @Id has been placed on a field. Therefore, all other JPA a\u00aend Hibernate mapping annotations are also on fields. We have to admit that this annotation isn't very useful. It only has two parameters: the one shown here, optional, marks the property as not optional at the Java object level. By default, all persistent properties\u00aeare nullable and optional; an Item may have an unknown initialPrice. Mapping the initialPrice property as non-optional makes sense if you have a NOT NULL constraint on the INITIALPRICE column in your SQL schema. If Hibernate is generating the SQL schema, it will include a NOT NULL constraint automatically for non-optional properties. Mapping basic properties- When you map a persistent class, whether it's an entity or an embeddable type (my isn't what you want, and you should always map Java classes instead of storing a heap of bytes in the database. Imagine maintaining a database with this binary information when the application is gone in a few years. Otherwise, Hibernate will throw an exception on startup, complaining that it doesn't understand the type of the property",
  "page473": "The Column annotation has a few other parameters, most of which control SQL-level details such as catalog and schema names. They're rarely needed, and we only show them throughout this book\u00aewhen necessary.  Property annotations aren't always on fields, and you may not want Hibernate to access fields directly. Customizing property access- The persistence engine accesses the properties of a class either directly through\u00aefields or indirectly through getter and setter methods. An annotated entity inherits the default from the position of the mandatory @Id annotation. For example, if you've declared @Id on a field, not a getter method, all other mapping annotations for that entity are expected on fields. Annotations are never on the setter methods.  Th\u00aee default access strategy isn't only applicable to a single entity class. Any @Embedded class inherits the default or explicitly declared access strategy of its owning root entity class. We cover embedded components later in this chapter. Furthermore, Hibernate accesses any @MappedSuperclass properties with the default or explicitly declared access strategy of the mapped enti\u00aety class. Inheritance is the topic of chapter 6.  The JPA specification offers the @Access annotation for overriding the default behavior, with the parameters AccessType.FIELD and AccessType.PROPERTY. If you set @Access on the class/entity level, Hibernate accesses all properties\u00aeies of the class according to the selected strategy. You then set any other mapping annotations, including the @Id, on either fields or getter methods, respectively. Now, when you store an Item and forget to set a value on the initialPrice field, Hibernate will complain with an exception before hitting the database with an SQL statement. Hibernate knows that a value is required to perform an INSERT or UPDATE. If you don't mark the property as optional and try to save aautomatically.  The @Column annotation can also override the mapping of the property name to the database column:",
  "page474": "Hibernate reads your Java domain model classes and mapping metadata and generates schema DDL statements. You can export them into a text file or execute them directly on your database whenever y\u00aeou run integration tests. Because schema languages are mostly vendor-specific, every option you put in your mapping metadata has the potential to bind the metadata to a particular database product keep this in mind when using schema feature\u00aes. Hibernate creates the basic schema for your tables and constraints automatically; it even creates sequences, depending on the identifier generator you select. But there are some schema artifacts Hibernate can't and won't create automatically. These include all kinds of highly vendor-specific performance options and any other\u00aeartifacts that are relevant only for the physical storage of data (tablespaces, for example). Besides these physical concerns, your DBA will often supply custom additional schema statements to improve the generated schema. DBAs should get involved early and verify the automatically generated schema from Hibernate. Never go into production with an unchecked automatically generated schema. If y\u00aeour development process allows, changes made by the DBA can flow back into your Java systems, for you to add to mapping metadata. In many projects, the mapping metadata can contain all the necessary schema changes from a DBA. Then Hibernate can generate the final production\u00aeschema during the regular build by including all comments, constraints, indexes, and so on. In the following sections, we show you how to customize the generated schema and how to add auxiliary database schema artifacts (we call them objects sometimes; we don't mean Java objects here). We discuss custom data types, additional integrity rules, indexes, and how you can replace some of the (sometimes ugly) auto-generated artifact names produced by Hibernate.If your application caed scripts and write an improved and final schema for production deployment.",
  "page475": "This property defines when the create and drop scripts should be executed. Your custom SQL scripts will contain CREATE DOMAIN statements, which must be executed before the tables using these dom\u00aeains are created. With these settings, the schema generator runs the create script first before reading your ORM metadata (annotations, XML files) and creating the tables. The drop script executes after Hibernate drops the tables, giving yo\u00aeu a chance to clean up anything you created. Other options are metadata (ignore custom script sources) and script (only use a custom script source; ignore ORM metadata in annotations and XML files). D This is the location of the custom SQL script for creation of the schema. The path is (a) the location of the script resource on the classpa\u00aeth; (b) the location of the script as a file:// URL; or, if neither (a) nor (b) matches, (c) the absolute or relative file path on the local file system. This example uses (a). E This is the custom SQL script for dropping the schema. F This load script runs after the tables have been created. We've mentioned that DDL is usually highly vendor-specific. If your application has to support s\u00aeeveral database dialects, you may need several sets of create/drop/load scripts to customize the schema for each database dialect. You can solve this with several persistence unit configurations in the persistence.xml file. Alternatively, Hibernate has its own proprietary\u00aeconfiguration for schema customization in an hbm.xml mapping file, You can hook the following three types of custom SQL scripts into Hibernates schema-generation process: The create script executes when the schema is generated. A custom create script can run before, after, or instead of Hibernates automatically generated scripts. In other words, you can write an SQL script that runs before or after Hibernate generates tables, constraints, and so on from your mappin",
  "page476": "Table constraints An integrity rule that applies to several columns or several rows is a table constraint. A typical declarative table constraint is UNIQUE: all rows are checked for duplicate va\u00aelues (for example, each user must have a distinct email address). A rule affecting only a single row but multiple columns is \"the auction end time has to be after the auction start time.\" Database constraints If a rule applies to\u00aemore than one table, it has database scope. You should already be familiar with the most common database constraint, the foreign key. This rule guarantees the integrity of references between rows, usually in separate tables, but not always (self-referencing foreign key constraints aren't uncommon). Other database constraints involving\u00aeseveral tables aren't uncommon: for example, a bid can only be stored if the auction end time of the referenced item hasn't been reached. Most (if not all) SQL database management systems support these kinds of constraints and the most important options of each. In addition to simple keywords such as NOT NULL and UNIQUE, you can usually also declare more complex rules with the CHEC\u00aeK constraint that applies an arbitrary SQL expression. Still, integrity constraints are one of the weak areas in the SQL standard, and solutions from vendors can differ significantly. Furthermore, non-declarative and procedural constraints are possible with database triggers\u00aethat intercept data-modification operations. A trigger can then implement the constraint procedure directly or call an existing stored procedure. Integrity constraints can be checked immediately when a data-modification statement is executed, or the check can be deferred until the end of a transaction. The violation response in SQL databases is usually rejection without any possibility of customization. Foreign keys are special because you can typically dCHAR data type can hold character strings: for example, all characters defined in ASCII or some other encoding. Because we mostly use data types built-in the DBMS,",
  "page477": "This constraint restricts valid username values to a maximum length of 15 characters, and the string can't begin with admin to avoid confusion. You can call any SQL functions supported by y\u00aeour DBMS; the column Definition is always passed through into the exported schema. Note that you have a choice: creating and using a domain or adding a single column constraint has the same effect. Domains are usually easier to maintain an\u00aed avoid duplicating. At the time of writing, Hibernate doesn't support its proprietary annotation @org.hibernate.annotations.Check on individual properties; you use it for table level constraints. Hibernate appends table constraints to the generated CREATE TABLE statement, which can contain arbitrary SQL expressions. You can impleme\u00aent multirow table constraints with expressions that are more complex. You may need a sub select in the expression to do this, which may not be supported by your DBMS. But there are some common multirow table constraints, like UNIQUE, that you can add directly in the mappings. You've already seen the @Column(unique true | false) option in the previous section. Now all pairs of USERNAME an\u00aed EMAIL must be unique, for all rows in the USERS table. If you don't provide a name for the constraint here, UNQ_USERNAME_EMAIL an automatically generated and probably ugly name is used. The last kinds of constraints we discuss are database-wide rules that span several\u00aetables. Hibernate passes on database constraint violations in error exceptions; check whether the exception in your transaction has a cause, somewhere in the exception chain, of type org.hibernate.exception.ConstraintViolationException. This exception can provide more information about the error, such as the name of the failed database constraint. The SQL standard includes domains, which unfortunately are rather limited and often not supported by the DBMS. If your system supports tly, so be careful with database-specific SQL",
  "page478": "This statement declares the foreign key constraint for the ITEM_ID column in the BID table, referencing the primary key column of the ITEM table. You can customize the name of the constraint wit\u00aeh the foreign Key option in an @JoinColumn mapping A foreign Key attribute is also supported in @PrimaryKeyJoinColumn, @MapKeyJoinColumn, @JoinTable, @CollectionTable, and @AssociationOverride mappings. The @ForeignKey annotation has some\u00aerarely needed options we haven't shown: You can write your own foreign Key Definition, an SQL fragment such as FOREIGN KEY ([column]) REFERENCES [table]([column]) ON UPDATE [action]. Hibernate will use this SQL fragment instead of the provider-generated fragment, it can be in the SQL dialect supported by your DBMS. The Constraint Mod\u00aee setting is useful if you want to disable foreign key generation completely, with the value NO_CONSTRAINT. You can then write the foreign key constraint yourself with an ALTER TABLE statement, probably in a load script as we've shown. Naming constraints properly is not only good practice, but also helps significantly when you have to read exception messages. This completes our discussi\u00aeon of database integrity rules. Next, we look at some optimization you might want to include in your schema for performance reasons. Indexes are a key feature when optimizing the performance of a database application. The query optimizer in a DBMS can use indexes to avoid exce\u00aessive scans of the data tables. Because they're relevant only in the physical implementation of a database, indexes aren't part of the SQL standard, and the DDL and available indexing options are product specific. You can, however, embed the most common schema artifacts for typical indexes in mapping metadata A user can only make bids until an auction ends. Your database should guarantee that invalid bids can't be stored so that whenever a row is inserted intntial integrity rules. They're widely known as foreign keys, which are a combination of two things: a key value copy from a related row and a constraint that guarantees that the referenced value exists.",
  "page479": "If you encounter a USERS table in a legacy schema, it's likely that USERNAME is the primary key. In this case, you have no surrogate identifier that Hibernate generates automatically. Inste\u00aead, your application has to assign the identifier value when saving an instance of the User class Hibernate calls the protected no-argument constructor when it loads a User from the database and then assigns the username field value directl\u00aey. When you instantiate a User, call the public constructor with a username. If you don't declare an identifier generator on the @Id property, Hibernate expects the application to take care of the primary key value assignment. Composite (natural) primary keys require a bit more work Suppose the primary key of the USERS table is a com\u00aeposite of the two columns USERNAME and DEPARTMENTNR. You write a separate composite identifier class that declares just the key properties and call this class User Id This class has to be @Embeddable and Serializable any type used as an identifier type in JPA must be Serializable. C You don't have to mark the properties of the composite key as @NotNull; their database columns are automat\u00aeically NOT NULL when embedded as the primary key of an entity. D The JPA specification requires a public no-argument constructor for an embeddable identifier class. Hibernate accepts protected visibility. E The only public constructor should have the key values as arguments.\u00aeYou have to override the equals() and hashCode() methods with the same semantics the composite key has in your database. In this case, this is a straightforward comparison of the username and department values. Many queries in Caveat Emptor will probably involve the username of a User entity. You can speed up these queries by creating an index for the column of this property. Another candidate for an index is the combination of USERNAME and EMAIL columns, which you also use frequmpact performance. Unfortunately, many legacy schemas use (natural) composite keys heavily; and for the reason we discourage the use of composite keys, it may be difficult to change the legacy schema to use non-composite natural or surrogate keys. Therefore, JPA supports natural and composite primary and foreign keys.",
  "page480": "We've already shown the @SecondaryTable annotation in an inheritance mapping in section 6.5. It helped to break out properties of a particular subclass into a separate table. This generic f\u00aeunctionality has more uses but be aware that a properly designed system should have, simplified, more classes than tables. Suppose that in a legacy schema, you aren't keeping a user's billing address information with the other us\u00aeer details in the USERS main entity table, but in a separate table. Figure 9.5 shows this schema. The user's home address is stored in the columns STREET, ZIPCODE, and CITY of the USERS table. The user's billing address is stored in the BILLING_ADDRESS table, which has the primary key column USER_ID, which is also a foreign key c\u00aeonstraint referencing the ID primary key of the USERS table The User class has two properties of embedded type: home Address and billing Address. The first is a regular embedded mapping, and the Address class is @Embeddable. Just as in section 5.2.3, you can use the @AttributeOverrides annotation to override the mapping of embedded properties. Then, @Column maps the individual properties to\u00aethe BILLING_ADDRESS table, with its table option. Remember that an @AttributeOverride replaces all mapping information for a property: any annotations on the Address fields are ignored if you override. Therefore, you have to specify nullability and length again in the @Column\u00aeoverride. We've shown you a secondary table mapping example with an embeddable property. Of course, you could also break out simple basic properties like the username string in a secondary table. Keep in mind that reading and maintaining these mappings can be a problem, though; you should only map legacy unchangeable schemas with secondary tables. A foreign key constraint on the SELLER column in the ITEM table ensures that the seller of the item exists by requiring the same attribute of @JoinColumn to declare this relationship. Hibernate now knows that the referenced target column is a natural key, and not the primary key, and manages the foreign key relationship accordingly. If the target natural key is a composite key, use @JoinColumns instead as in the previous section. Fortunately,",
  "page481": "strategies for runtime data management. These strategies are crucial to the performance and correct behavior of your applications. In this chapter, we discuss the life cycle of entity instances\u00aehow an instance becomes persistent, and how it stops being considered persistent and the method calls and management operations that trigger these transitions. The JPA Entity Manager is your primary interface for accessing data. Before we\u00aelook at the API, let's start with entity instances, their life cycle, and the events that trigger a change of state. Although some of the material may be formal, a solid understanding of the persistence life cycle is essential. Because JPA is a transparent persistence mechanism classes are unaware of their own persistence capability\u00aeit's possible to write application logic that's unaware whether the data it operates on represents persistent state or temporary state that exists only in memory. The application shouldn't necessarily need to care that an instance is persistent when invoking its methods. You can, for example, invoke the Item #calculateTotalPrice() business method without having to consider pers\u00aeistence at all (for example, in a unit test). Any application with persistent state must interact with the persistence service whenever it needs to propagate state held in memory to the database (or vice versa). In other words, you have to call the Java Persistence interfaces\u00aeto store and load data. When interacting with the persistence mechanism that way, the application must concern itself with the state and life cycle of an entity instance with respect to persistence. We refer to this as the persistence life cycle: the states an entity instance goes through during its life. We also use the term unit of work: a set of (possibly) state changing operations considered one (usually atomic) group. Another piece of the puzzle is the persistenning to and intercepting events, auditing and versioning with Hibernate Envers, and filtering data dynamically. After reading this part, you'll know how to work with Hibernate and Java Persistence programming interfaces and how to load, modify, and store objects efficiently. You'll understand how transactions work and why conversational processing can open up new approaches for application design.",
  "page482": "A persistent entity instance has a representation in the database. It's stored in the database or it will be stored when the unit of work completes. It's an instance with a database id\u00aeentity, as defined in section 4.2; its database identifier is set to the primary key value of the database representation. The application may have created instances and then made them persistent by calling Entity Manager #persist(). There\u00aemay be instances that became persistent when the application created a reference to the object from another persistent instance that the JPA provider already manages. A persistent entity instance may be an instance retrieved from the database by execution of a query, by an identifier lookup, or by navigating the object graph starting fro\u00aem another persistent instance. Persistent instances are always associated with a persistence context. You see more about this in a moment You can delete a persistent entity instance from the database in several ways: For example, you can remove it with Entity Manager #remove(). It may also become available for deletion if you remove a reference to it from a mapped collection with orphan remo\u00aeval enabled. An entity instance is then in the removed state: the provider will delete it at the end of a unit of work. You should discard any references you may hold to it in the application after you finish working with it for example, after you've rendered the removal\u00aeconfirmation screen your users see Think of the persistence context as a service that remembers all the modifications and state changes you made to data in a particular unit of work (this is somewhat simplified, but it's a good starting point). We now dissect all these terms: entity states, persistence contexts, and managed scope. You're probably more accustomed to thinking about what SQL statements you have to manage to get stuff in and out of the database; but st like new Long() and new Big Decimal(). Hibernate doesn't provide any rollback functionality for transient instances; if you modify the price of a transient Item, you can't automatically undo the change. For an entity instance to transition from transient to persistent state, to become managed, requires either a call to the Entity Manager #persist() method or the creation of a reference from an already-persistent instance and enabled cascading of ",
  "page483": "The persistence context acts as a first-level cache; it remembers all entity instances you've handled in a particular unit of work. For example, if you ask Hibernate to load an entity insta\u00aence using a primary key value (a lookup by identifier), Hibernate can first check the current unit of work in the persistence context. If Hibernate finds the entity instance in the persistence context, no database hit occurs this is a repea\u00aetable read for an application. Consecutive em.find(Item. Class, ITEM_ID) calls with the same persistence context will yield the same result. This cache also affects results of arbitrary queries, executed for example with the javax.persistence.Query API. Hibernate reads the SQL result set of a query and transforms it into entity instances.\u00aeThis process first tries to resolve every entity instance in the persistence context by identifier lookup. Only if an instance with the same identifier value can't be found in the current persistence context does Hibernate read the rest of the data from the result-set row. Hibernate ignores any potentially newer data in the result set, due to read-committed transaction isolation at the\u00aedatabase level, if the entity instance is already present in the persistence context. The persistence context cache is always on it can't be turned off. It ensures the following The persistence layer isn't vulnerable to stack overflows in the case of circular referen\u00aeces in an object graph. There can never be conflicting representations of the same database row at the end of a unit of work. The provider can safely write all changes made to an entity instance to the database. Likewise, changes made in a particular persistence context are always immediately visible to all other code executed inside that unit of work and its persistence context. JPA guarantees repeatable entity-instance reads To understand detached entity instances, consider loase the state of instances monitored by a persistence context, either automatically or on demand. Typically, when a unit of work completes, the provider propagates state held in memory to the database through the execution of SQL INSERT, UPDATE, and DELETE statements (all part of the Data Modification Language [DML]) This flushing procedure may also occur at other times. For example, Hibernate may synchronize with the database before execution of a query.",
  "page484": "Hibernate joins the in-progress system transaction when an Entity Manager is created and waits for the transaction to commit. When Hibernate is notified (by the JTA engine) of the commit, it per\u00aeforms dirty checking of the persistence context and synchronizes with the database. You can also force dirty checking synchronization manually by calling EntityManager#flush() at any time during a transaction. You decide the scope of the p\u00aeersistence context by choosing when to close() the Entity Manager. You have to close the persistence context at some point, so always place the close() call in a finally block. How long should the persistence context be open? Let's assume for the following examples that you're writing a server, and each client request will be pr\u00aeocessed with one persistence context and system transaction in a multithreaded environment. If you're familiar with servlets, imagine the code in listing 10.1 embedded in a servlet's service() method. Within this unit of work, you access the Entity Manager to load and store data. A new transient Item is instantiated as usual. Of course, you may also instantiate it before creating th\u00aee EntityManager. A call to persist() makes the transient instance of Item persistent. It's now managed by and associated with the current persistence context. To store the Item instance in the database, Hibernate has to execute an SQL INSERT statement. When the transactio\u00aen of this unit of work commits, Hibernate flushes the persistence context, and the INSERT occurs at that time. Hibernate may even batch the INSERT at the JDBC level with other statements. When you call persist(), only the identifier value of the Item is assigned. Alternatively, if your identifier generator isn't pre-insert, the INSERT statement will be executed immediately when persist() is called. You may want to review section In Java SE and some EE architecturwrite empty catch clauses in your code, though you'll have to roll back the transaction and handle exceptions. Creating an Entity Manager starts its persistence context. Hibernate won't access the database until necessary; the Entity Manager doesn't obtain a JDBC Connection from the pool until SQL statements have to be executed. You can create and close an Entity Manager without hitting the database. Hibernate executes",
  "page485": "You can modify the Item instance, and the persistence context will detect these changes and record them in the database automatically. When Hibernate flushes the persistence context during commi\u00aet, it executes the necessary SQL DML statements to synchronize the changes with the database. Hibernate propagates state changes to the database as late as possible, toward the end of the transaction. DML statements usually create locks in\u00aethe database that are held until the transaction completes, so Hibernate keeps the lock duration in the database as short as possible. Hibernate writes the new Item#name to the database with an SQL UPDATE. By default, Hibernate includes all columns of the mapped ITEM table in the SQL UPDATE statement, updating unchanged columns to their o\u00aeld values. Hence, Hibernate can generate these basic SQL statements at startup, not at runtime. If you want to include only modified (or non-nullable for INSERT) columns in SQL statements, you can enable dynamic SQL generation as discussed in section 4.3.2. Hibernate detects the changed name by comparing the Item with a snapshot copy it took before, when the Item was loaded from the database\u00ae. If your Item is different from the snapshot, an UPDATE is necessary. This snapshot in the persistence context consumes memory. Dirty checking with snapshots can also be time consuming, because Hibernate has to compare all instances in the persistence context with their snapsh\u00aeot during flushing. It's better (but not required) to fully initialize the Item instance before managing it with a persistence context. The SQL INSERT statement contains the values that were held by the instance at the point when persist() was called. If you don't set the name of the Item before making it persistent, a NOT NULL constraint may be violated. You can modify the Item after calling persist(), and your changes will be propagated to the database wit9s a generic method, and its return type is set as a side effect of the first parameter. The retrieved entity instance is in persistent state, and you can now modify it inside the unit of work. If no persistent instance with the given identifier value can be found, find() returns null. The find() operation always hits the database if there was no hit for the",
  "page486": "As soon as you call any method such as Item#getName() on the proxy, a SELECT is executed to fully initialize the placeholder. The exception to this rule is a mapped database identifier getter me\u00aethod, such as getId(). A proxy may look like the real thing, but it's only a placeholder carrying the identifier value of the entity instance it represents. If the database record no longer exists when the proxy is initialized, an Enti\u00aetyNotFoundException is thrown. Note that the exception can be thrown when Item#getName() is called. E Hibernate has a convenient static initialize() method that loads the proxy's data. F After the persistence context is closed, item is in detached state. If you don't initialize the proxy while the persistence context is still ope\u00aen, you get a LazyInitializationException if you access the proxy. You can't load data on demand once the persistence context is closed. The solution is simple: load the data before you close the persistence context. We'll have much more to say about proxies, lazy loading, and on-demand fetching in chapter 12. If you want to remove the state of an entity instance from the database,\u00aeyou have to make it transient. To make an entity instance transient and delete its database representation, call the remove() method on the EntityManager If you call find(), Hibernate executes a SELECT to load the Item. If you call getReference(), Hibernate attempts to avoid t\u00aehe SELECT and returns a proxy. C Calling remove() queues the entity instance for deletion when the unit of work completes; it's now in removed state. If remove() is called on a proxy, Hibernate executes a SELECT to load the data. You may want to customize how Hibernate detects dirty state, using an extension point. Set the property hibernate.entity_dirtiness_strategy in your persistence.xml configuration file to a class name that implements org.hibernate.CustomEntityDirtinesference() without hitting the database. Furthermore, if no persistent instance with that identifier is currently managed, Hibernate produces a hollow placeholder: a proxy. This means getReference() won't access the database, and it doesn't return null, unlike find(). C JPA offers PersistenceUnitUtil helper methods such as isLoaded() to detect whether you're working with an uninitialized proxy.",
  "page487": "Java Persistence also offers bulk operations that translate into direct SQL DELETE statements without life cycle interceptors in the application. We'll discuss these operations in section 2\u00ae0.1. Let's say you load an entity instance from the database and work with the data. For some reason, you know that another application or maybe another thread of your application has updated the underlying row in the database. Next,\u00aewe'll see how to refresh the data held in memory. After you load the entity instance, you realize (how isn't important) that someone else changed the data in the database. Calling refresh() causes Hibernate to execute a SELECT to read and marshal a whole result set, overwriting changes you already made to the persistent instance\u00aein application memory. If the database row no longer exists (someone deleted it), Hibernate throws an EntityNotFoundException on refresh(). Most applications don't have to manually refresh in-memory state; concurrent modifications are typically resolved at transaction commit time. The best use case for refreshing is with an extended persistence context, which might span several request\u00ae/response cycles and/or system transactions. While you wait for user input with an open persistence context, data gets stale, and selective refreshing may be required depending on the duration of the conversation and the dialogue between the user and the system. Refreshing can\u00aebe useful to undo changes made in memory during a conversation, if the user cancels the dialogue. We'll have more to say about refreshing in a conversation in section. An entity instance must be fully initialized during life cycle transitions. You may have life cycle callback methods or an entity listener enabled (see section 13.2), and the instance must pass through these interceptors to complete its full life cycle. D An entity in removed state is no longlue after removal of an entity instance. In the previous code example, the identifier value is reset to the default value of null (it's a Long). The Item is now the same as in transient state, and you can save it again in a new persistence context.",
  "page488": "The persistence context is a cache of persistent instances. Every entity instance in persistent state is associated with the persistence context. Many Hibernate users who ignore this simple fac\u00aet run into an OutOfMemoryException. This is typically the case when you load thousands of entity instances in a unit of work but never intend to modify them. Hibernate still has to create a snapshot of each instance in the persistence conte\u00aext cache, which can lead to memory exhaustion. (Obviously, you should execute a bulk data operation if you modify thousands of rows we'll get back to this kind of unit of work in section 20.1.) The persistence context cache never shrinks automatically. Keep the size of your persistence context to the necessary minimum. Often, many pe\u00aersistent instances in your context are there by accident for example, because you needed only a few items but queried for many. Extremely large graphs can have a serious performance impact and require significant memory for state snapshots. Check that your queries return only data you need, and consider the following ways to control Hibernate's caching behavior. You can call EntityManag\u00aeer#detach(i) to evict a persistent instance manually from the persistence context. You can call EntityManager#clear() to detach all persistent entity instances, leaving you with an empty persistence context. The native Session API has some extra operations you might find usefu\u00ael. You can set the entire persistence context to read-only mode. This disables state snapshots and dirty checking, and Hibernate won't write modifications to the database. Replication is useful, for example, when you need to retrieve data from one database and store it in another. Replication takes detached instances loaded in one persistence context and makes them persistent in another persistence context. You usually open these contexts from two different EntityManagerFactory cont databases. An example case is a product upgrade: if the new version of your application requires a new database (schema), you may want to migrate and replicate the existing data once. The persistence context does many things for you: automatic dirty checking, guaranteed scope of object identity, and so on. It's equally important that you know some of the details of its management, and that you sometimes influence what goes on behind the scenes. ",
  "page489": "Next, we look at the detached entity state. We already mentioned some issues you'll see when entity instances aren't associated with a persistence context anymore, such as disabled laz\u00aey initialization. Let's explore the detached state with some examples, so you know what to expect when you work with data outside of a persistence context. If a reference leaves the scope of guaranteed identity, we call it a reference\u00aeto a detached entity instance. When the persistence context is closed, it no longer provides an identity-mapping service. You'll run into aliasing problems when you work with detached entity instances, so make sure you understand how to handle the identity of detached instances. If you look up data using the same database identifier\u00aevalue in the same persistence context, the result is two references to the same in-memory instance on the JVM heap. Consider the two units of work shown next. In the first unit of work at begin() B, you start by creating a persistence context C and loading some entity instances. Because references a and b are obtained from the same persistence context, they have the same Java identity D. They\u00ae're equal from the same persistence context, they have the same Java identity D. They're equal E because by default equals() relies on Java identity comparison. They obviously have the same database identity F. They reference the same Item instance, in persistent state, managed\u00aeby the persistence context for that unit of work. The first part of this example finishes by committing the transaction. By default, Hibernate flushes the persistence context of an EntityManager and synchronizes changes with the database whenever the joined transaction is committed. All the previous code examples, except some in the last section, have used that strategy. JPA allows implementations to synchronize the persistence context at other times, if tabase by calling EntityManager#flush(). This concludes our discussion of the transient, persistent, and removed entity states, and the basic usage of the EntityManager API. Mastering these state transitions and API methods is essential; every JPA application is built with these operations.",
  "page490": "When you begin a journey, it's a good idea to have a mental map of the terrain you'll be passing through. The same is true for an intellectual journey, such as learning to write comput\u00aeer programs. In this case, you'll need to know the basics of what computers are and how they work. You'll want to have some idea of what a computer program is and how one is created. Since you will be writing programs in the Java\u00aeprogramming language, you'll want to know something about that language in particular and about the modern, networked computing environment for which Java is designed. As you read this chapter, don't worry if you can't understand everything in detail. (In fact, it would be impossible for you to learn all the details from the\u00aebrief expositions in this chapter.) Concentrate on learning enough about the big ideas to orient yourself, in preparation for the rest of the book. Most of what is covered in this chapter will be covered in much greater detail later in the book. A computer is a complex system consisting of many different components. But at the heart or the brain, if you want of the computer is a single compo\u00aenent that does the actual computing. This is the Central Processing Unit, or CPU. In a modern desktop computer, the CPU is a single \"chip\" on the order of one square inch in size. The job of the CPU is to execute programs. A program is simply a list of unambiguous ins\u00aetructions meant to be followed mechanically by a computer. A computer is built to carry out instructions that are written in a very simple type of language called machine language. Each type of computer has its own machine language, and the computer can directly execute a program only if the program is expressed in that language. (It can execute programs written in other languages if they are first translated into machine language.) When the CPU executes a program, that prochine language instructions in main memory. It does this by repeatedly reading, or fetching, an instruction from memory and then carrying out, or executing, that instruction. This process fetch an instruction, execute it,",
  "page491": "Machine language instructions are expressed as binary numbers. A binary number is made up of just two possible digits, zero and one. So, a machine language instruction is just a sequence of zero\u00aes and ones. Each particular sequence encodes some particular instruction. The data that the computer manipulates is also encoded as binary numbers. A computer can work directly with binary numbers because switches can readily represent such\u00aenumbers: Turn the switch on to represent a one; turn it off to represent a zero. Machine language instructions are stored in memory as patterns of switches turned on or off, When a machine language instruction is loaded into CPU, all that happens is that certain switches are turned on or off in the pattern that encodes that particular ins\u00aetruction. The CPU is built to respond to this pattern by executing the instruction it encodes; it does this simply because of the way all the other switches in the CPU are wired together. So, you should understand this much about how computers work: Main memory holds machine language programs and data. These are encoded as binary numbers. The CPU fetches machine language instructions from mem\u00aeory one after another and executes them. It does this mechanically, without thinking about or understanding what it does-and therefore the program it executes must be perfect, complete in all details, and unambiguous because the CPU can do nothing but execute it exactly as writ\u00aeten. Here is a schematic view of this first-stage understanding of the computer Asynchronous Events: Polling Loops and Interrupts The CPU spends almost all of its time fetching instructions from memory and executing them. However, the CPU and main memory are only two out of many components in areal computer system, A complete system contains other devices such as: A hard disk for storing programs and data files.(Note that main memory holds only a comparatively small amount of informatiere is a device driver, which consists of software that the CPU executes when it has to deal with the device. Installing a new device on a system generally has two steps: plugging the device physically into the computer, and ins putting aside whatever",
  "page492": "Again, you should understand that this is a purely mechanical process: A device signals an interrupt simply by turning on a wire. The CPU is built so that when that wire is turned on, the CPU sa\u00aeves enough information about what it is currently doing so that it can return to the same state later. This information consists of the contents of important internal registers such as the program counter. Then the CPU jumps to some predete\u00aermined memory location and begins executing the instructions stored there. Those instructions make up an interrupt handler that does the processing necessary to respond to the interrupt. (This interrupt handler is part of the device driver software for the device that signalled the interrupt.) At the end of the interrupt handler is an inst\u00aeruction that tells the CPU to jump back to what it was doing; it does that by restoring its previously saved state. Interrupts allow the CPU to deal with asynchronous events. In the regular fetch-and execute cycle, things happen in a predetermined order; everything that happens is \"synchronized\" with everything else. Interrupts make it possible for the CPU to deal efficiently with e\u00aevents that happen \"asynchronously,\" that is, at unpredictable times. As another example of how interrupts are used, consider what happens when the CPU need to access data that is stored on the hard disk. The CPU can access data directly only if it is in main memory. D\u00aeata on the disk has to be copied into memory before it can be accessed. Unfortunately, on the scale of speed at which the CPU operates, the disk drive is extremely slow. When the CPU needs data from the disk, it sends a signal to the disk drive telling it to locate the data and get it ready. (This signal is sent synchronously, under the control of a regular program.) Then, instead of just waiting the long and unpredictalble amount of time that the disk drive will take to do this,alled a thread. (Or a process; there are technical differences between threads and processes, but they are not important here.) At any given time, only one thread can actually be executed by a CPU.",
  "page493": "The Java Virtual Machine Machine language consists of very simple instructions that can be executed directly by the CPU of a computer. Almost all programs, though, are written in high-level prog\u00aeramming languages such as Java, Pascal, or C++. A program written in a high-level language cannot be run directly on any computer. First, it has to be translated into machine language. This translation can be done by a program called a comp\u00aeiler. A compiler takes a high-level-language program and translates it into an executable machine-language program. Once the translation is done, the machine-language program can be run any number of times, but of course it can only be run on one type of computer (since each type of computer has its own individual machine language). If the\u00aeprogram is to run on another type of computer it has to be re-translated, using a different compiler, into the appropriate machine language. There is an alternative to compiling a high-level language program. Instead of using a compiler, which translates the program all at once, you can use an interpreter, which translates it instruction-by-instruction, as necessary. An interpreter is a prog\u00aeram that acts much like a CPU, with a kind of fetch-and-execute cycle. In order to execute a program, the interpreter runs in a loop in which it repeatedly reads one instruction from the program, decides what is necessary to carry out that instruction, and then performs the app\u00aeropriate machine-language commands to do so. one use of interpreters is to execute high-level language programs. For example, the programming language Lisp is usually executed by an interpreter rather than a compiler. However, interpreters have another purpose: they can let you use a machine-language program meant for one type of computer on a completely different type of computer. For example, there is a program called \"Virtual PC\" that runs on Macintosh computers. Virtual Pputer. However, one of the main selling points of Java is that it can actually be used on any computer. All that the computer needs is an interpreter for Java bytecode. Such an interpreter simulates the Java virtual machine in trous actions on the partct-oriented language. I should also note that the really hard part of platform-independence is providing a \"Graphical User Interface\" with windows, buttons, etc. that will work on all the platforms that support Java.",
  "page494": "Programs must be designed. No one can just sit down at the computer and compose a program of any complexity. The discipline called software engineering is concerned with the construction of corr\u00aeect, working, well-written programs. The software engineer tends to use accepted and proven methods for analyzing the problem to be solved and for designing a program to solve that problem. During the 1970s and into the 80s, the primary sof\u00aetware engineering methodology was structured programming. The structured programming approach to program design was based on the following advice: To solve a large problem, break the problem into several pieces and work on each piece separately; to solve each piece, treat it as a new problem which can itself be broken down into smaller pro\u00aeblems; eventually, you will work your way down to problems that can be solved directly, without further decomposition. This approach is called top-down programming. There is nothing wrong with top-down programming. It is a valuable and often-used approach to problem-solving. However, it is incomplete. For one thing, it deals almost entirely with producing the instructions necessary to solve a\u00aeproblem. But as time went on, people realized that the design of the data structures for a program was as least as important as the design of subroutines and control structures. Top-down programming doesn't give adequate consideration to the data that the program manipula\u00aetes. Another problem with strict top-down programming is that it makes it difficult to reuse work done for other projects. By starting with a particular problem and subdividing it into convenient pieces, top-down programming tends to produce a design that is unique to that problem. It is unlikely that you will be able to take a large chunk of programming from another program and fit it into your project, at least not without extensive modification. Producing high-quality programs is ditware modules is to contain some data, along with some subroutines for manipulating that data. For example, a mailing-list module might contain a list of names and addresses along with a subroutine for adding a new name, a subrong the objects involved d to be better modelse very different results, depending on the object it is sent to. This property of objects that different objects can respond without being in exactly the same class.",
  "page495": "For example, consider a drawing program that lets the user draw lines, rectangles, ovals, polygons, and curves on the screen. In the program, each visible object on the screen could be represent\u00aeed by a software object in the program. There would be five classes of objects in the program, one for each type of visible object that can be drawn. All the lines would belong to one class, all the rectangles to another class, and so on. T\u00aehese classes are obviously related; all of them represent \"drawable objects.\" They would, for example, all presumably be able to respond to a \"draw yourself\" message. Another level of grouping, based on the data to represent each type of object, is less obvious, but would be very useful in a program: We can group polyg\u00aeons and curves together as \"multipoint objects,\" while lines, rectangles, and ovals are \"two-point objects.\" (A line is determined by its endpoints, a rectangle by two of its corners, and an oval by two corners of the rectangle that contains it.) We could diagram these relationships as follows DrawableObject, MultipointObject, and TwoPointObject would be classes in the pro\u00aegram. MultipointObject and TwoPointObject would be subclasses of DrawableObject. The class Line would be a subclass of TwoPointObject and (indirectly) of DrawableObject. A subclass of a class is said to inherit the properties of that class. The subclass can add to its inheritan\u00aece and it can even \"override\" part of that inheritance (by defining a different response to some method). Nevertheless, lines, rectangles, and so on are drawable objects, and the class DrawableObject expresses this relationship. Inheritance is a powerful means for organizing a program. It is also related to the problem of reusing software components. A class is the ultimate reusable component. Not only can it be reused directly if it fits exactly into a program you are trying the computer types back its response. Early personal computers also used typed commands and responses, except that there was only one person involved at a time. This type of interaction between a user and a computer is cad a scrollbar component er, and is used in prwith the GUI components in this applet, an \"event\" is generated.",
  "page496": "The use of the term \"message\" here is deliberate. Messages, as you saw in the previous section, are sent to objects. In fact, Java GUI components are implemented as objects. Java inclu\u00aedes many predefined classes that represent various types of GUI components. Some of these classes are subclasses of others. Here is a diagram showing some of Swing's GUI classes and their relationships. Don't worry about the detai\u00aels for now, but try to get some feel about how object-oriented programming and inheritance are used here. Note that all the GUI classes are subclasses, directly or indirectly, of a class called JComponent, which represents general properties that are shared by all Swing components. Two of the direct subclasses of JComponent themselves have\u00aesubclasses. The classes JTextArea and JTextField, which have certain behaviors in common, are grouped together as subclasses of JTextComponent. Similarly JButton and JToggleButton are subclasses of JAbstractButton, which represents properties common to both buttons and checkboxes. (JComboBox, by the way, is the Swing class that represents pop-up menus.) Just from this brief discussion, perha\u00aeps you can see how GUI programming can make effective use of object-oriented design. In fact, GUI's, with their \"visible objects,\" are probably a major factor contributing to the popularity of OOP. The Internet and the World-Wide Web - Computers can be connected\u00aetogether on networks. A computer on a network can communicate with other computers on the same network by exchanging data and files or by sending and receiving messages. Computers on a network can even work together on a large computation. Today, millions of computers throughout the world are connected to a single huge network called the Internet. New computers are being connected to the Internet every day. Computers can join the Internet by using a modem to establish a connectio some data being sent from one computer to another, along with addressing information that indicates where on the Internet that data is supposed to go. Think of a packet as an envelope with an address on the outside and a messag messaging, file sharingAP, are used to fetchunderlying network protocols. The World-Wide Web is perhaps the most exciting of network services.",
  "page497": "On a basic level (the level of machine language), a computer can perform only very simple operations. A computer performs complex tasks by stringing together large numbers of such operations. Su\u00aech tasks must be \"scripted\" in complete and perfect detail by programs. Creating complex programs will never be really easy, but the difficulty can be handled to some extent by giving the program a clear overall structure. The des\u00aeign of the overall structure of a program is what I call \"programming in the large.\" Programming in the small, which is sometimes called coding, would then refer to filling in the details of that design. The details are the explicit, step-by-step instructions for performing fairly small-scale tasks. When you do coding, you are wo\u00aerking fairly \"close to the machine,\" with some of the same concepts that you might use in machine language: memory locations, arithmetic operations, loops and branches. In a high-level language such as Java, you get to work with these concepts on a level several steps above machine language. However, you still have to worry about getting all the details exactly right. This chapter a\u00aend the next examine the facilities for programming in the small in the Java programming language. Don't be misled by the term \"programming in the small\" into thinking that this material is easy or unimportant. This material is an essential foundation for all type\u00aes of programming. If you don't understand it, you can't write programs, no matter how good you get at designing their large-scale structure. The Basic Java Application - A program is a sequence of instructions that a computer can execute to perform some task. A simple enough idea, but for the computer to make any use of the instructions, they must be written in a form that the computer can use. This means that programs have to be written in programming languages. Prograect program is one that does what you want it to. Furthermore, a program can be syntactically and semantically correct but still be a pretty bad program. Using the language correctly is not the same as using it well. For example might seem like a trivi details here of how will use some command to try to compile the file. You'll either get a message that the program contains syntax errors,",
  "page498": "Here is a Java program to display the message \"Hello World!\". Don't expect to understand what's going on here just yet some of it you won't really understand until a few\u00aechapters from now: // A program to display the message // \\\"Hello World!\\\" on standard output public class HelloWorld { public static void main(String[] args) { System.out.println(\\\"Hello World!\\\"); } } // end of class HelloWorld T\u00aehe command that actually displays the message is: System.out.println(\\\"Hello World!\\\"); This command is an example of a subroutine call statement. It uses a \"built-in subroutine\" named System.out.println to do the actual work. Recall that a subroutine consists of the instructions for performing some task, chunked together and\u00aegiven a name. That name can be used to \"call\" the subroutine whenever that task needs to be performed. A built-in subroutine is one that is already defined as part of the language and therefore automatically available for use in any program. When you run this program, the message \"Hello World!\" (without the quotes) will be displayed on standard output. Unfortunately, I ca\u00aen't say exactly what that means! Java is meant to run on many different platforms, and standard output will mean different things on different platforms. However, you can expect the message to show up in some convenient place. (If you use a command-line interface, like tha\u00aet in Sun Microsystem's Java Development Kit, you type in a command to tell the computer to run the program. The computer will type the output from the program, Hello World!, on the next line.) You must be curious about all the other stuff in the above program. Part of it consists of comments. Comments in a program are entirely ignored by the computer; they are there for human readers only. This doesn't mean that they are unimportant. Programs are meant to be read he main() subroutine, and the statements that it contains are executed. These statements make up the script that tells the computer exactly what to do when the program is executed. The main() routine can call subroutines that ar called HelloWorld.java.tics of the language.",
  "page499": "Variables and the Primitive Types - Names are fundamental to programming. In programs, names are used to refer to many different sorts of things. In order to use those things, a programmer must\u00aeunderstand the rules for giving names to things and the rules for using the names to work with those things. That is, the programmer must understand the syntax and the semantics of names. According to the syntax rules of Java, a name is a s\u00aeequence of one or more characters. It must begin with a letter or underscore and must consist entirely of letters, digits, and underscores. (\"Underscore\" refers to the character ' '.) For example, here are some legal names: No spaces are allowed in identifiers; HelloWorld is a legal identifier, but \"Hello World\u00ae1d is not. Upper case and lower case letters are considered to be different, so that HelloWorld, helloworld, HELLOWORLD, and hElloWorLD are all distinct names. Certain names are reserved for special uses in Java, and cannot be used by the programmer for other purposes. These reserved words include: class, public, static, if, else, while, and several dozen other words. Java is actually pretty libe\u00aeral about what counts as a letter or a digit. Java uses the Unicode character set, which includes thousands of characters from many different languages and different alphabets, and many of these characters count as letters or digits. However, I will be sticking to what can be t\u00aeyped on a regular English keyboard. The pragmatics of naming includes style guidelines about how to choose names for things. For example, it is customary for names of classes to begin with upper case letters, while names of variables and of subroutines begin with lower case letters; you can avoid a lot of confusion by following the same convention in your own programs. Most Java programmers do not use underscores in names, although some do use them at the beginning of the names of imple or compound that can be used to refer to something in Java. (Note that the reserved words are not identifiers, since they can't be used as names for things.) Variables - Programs manipulate data that are stored in memay \"Hillary Clintoncuting a program, it was there before. Now, consider the following more complicated assignment statement, which might come later in the same program.",
  "page500": "Types and Literals- A variable in Java is designed to hold only one particular type of data; it can legally hold that type of data and no other. The compiler will consider it to be a syntax erro\u00aer if you try to violate this rule. We say that Java is a strongly typed language because it enforces this rule. There are eight so-called primitive types built into Java. The primitive types are named byte, short, int, long, float, double,\u00aechar, and boolean. The first four types hold integers (whole numbers such as 17, -38477, and 0). The four integer types are distinguished by the ranges of integers they can hold. The float and double types hold real numbers (such as 3.6 and -145.99). Again, the two real types are distinguished by their range and accuracy. A variable of typ\u00aee char holds a single character from the Unicode character set. And a variable of type boolean holds one of the two logical values true or false. Any data value stored in the computer's memory must be represented as a binary number, that is as a string of zeros and ones. A single zero or one is called a bit. A string of eight bits is called a byte. Memory is usually measured in terms of\u00aebytes. Not surprisingly, the byte data type refers to a single byte of memory. A variable of type byte holds a string of eight bits, which can represent any of the integers between -128 and 127, inclusive. (There are 256 integers in that range; eight bits can represent 256 two\u00aeraised to the power eight different values.) As for the other integer types, short corresponds to two bytes (16 bits). Variables of type short have values in the range -32768 to 32767. int corresponds to four bytes (32 bits). Variables of type int have values in the range -2147483648 to 2147483647. long corresponds to eight bytes (64 bits). Variables of type long have values in the range -9223372036854775808 to 9223372036854775807. You don't have to remember these numbers, but they do grogram, it must be surrounded by single quotes; for example: 'A', '*', or 'x'. Without the quotes, A would be an identifier and * would be a multiplication operator. The quotes are not part of the v. For example, \"1.2e for real numbers.) literals."
}

const dataB = {
  "page1": "When you begin a journey, it's a good idea to have a mental map of the terrain you'll be passing through. The same is true for an intellectual journey, such as learning to write computer programs. In this case, you'll need to know the basics of what computers are and how they work.\u00aeYou'll want to have some idea of what a computer program is and how one is created. Since you will be writing programs in the Java programming language, you'll want to know something about that language in particular and about the modern, networked computing environment for which Java is designed. As you read this chapter, don't worry if you can't understand everything in detail.(In fact, it would be impossible for you to learn all the details from the brief expositions in this chapter.)\u00aeConcentrate on learning enough about the big ideas to orient yourself, in preparation for the rest of the book. Most of what is covered in this chapter will be covered in much greater detail later in the book. A computer is a complex system consisting of many different components. But at the heart or the brain,\u00aeif you want of the computer is a single component that does the actual computing. This is the Central Processing Unit, or CPU. In a modern desktop computer, the CPU is a single \"chip\" on the order of one square inch in size. The job of the CPU is to execute programs. A program is simply a list of unambiguous\u00aeinstructions meant to be followed mechanically by a computer.",
  "page2": "To begin to get a handle on all of this complexity, let's look at the subroutine System.out.print as an example. As you have seen earlier in this chapter, this subroutine is used to display information to the user. For example, System.out.print(\\\"Hello World\\\") displays the\u00aemessage, Hello World. System is one of Java's standard classes. One of the static member variables in this class is named out. Since this variable is contained in the class System, its full name which you have to use to refer to it in your programs is System.out. The variable System.out refers to an object, and that object in turn contains a subroutine named print. The compound identifier System.out.print refers to the subroutine print in the object out in the class System. (As an a\u00aeside, I will note that the object referred to by System.out is an object of the class PrintStream. PrintStream is another class that is a standard part of Java.\u00aeAny object of type PrintStream is a destination to which information can be printed;any object of type PrintStream has a print subroutine that can be\u00aeused to send information to that destination. The object System.out is just one possible destination, and System.out.print is the subroutine that sends information to that particular destination. Other objects of type PrintStream might send information to other destinations such as files or across a network to other computers. ",
  "page3": "The function call Math.sqrt(x) represents a value of type double, and it can be used anyplace where a numeric literal of type double could be used. The Math class contains many static member functions. Here is a list of some of the more important of them: Math.abs(x), which compute\u00aes the absolute value of x. The usual trigonometric functions, Math.sin(x), Math.cos(x), and Math.tan(x). (For all the trigonometric functions, angles are measured in radians, not degrees.) The inverse trigonometric functions arcsin, arccos, and arctan, which are written as: Math.asin(x), Math.acos(x), and Math.atan(x).The return value is expressed in radians, not degrees. The exponential function Math.exp(x) for computing the number e raised to the power x, and the natural logarithm functi\u00aeon Math.log(x) for computing the logarithm of x in the base e. Math.pow(x,y) for computing x raised to the power y. Math.floor(x), which rounds x down to the nearest integer value that is less than or equal to x. Even though the return value is mathematically an integer, it is returned as a value of type double\u00ae, rather than of type int as you might expect. For example, Math.floor(3.76) is 3.0. The function Math.round(x) returns the integer that is closest to x. Math.random(), which returns a randomly chosen double in the range 0.0 <= Math.random() < 1.0. (The computer actually calculates so-called \"pseudorando\u00aem\" numbers, which are not truly random but are random enough for most purposes.) For these functions, the type of the parameter the x or y inside the parentheses can be any value of any numeric type. For most of the functions, the value returned by the function is of type double no matter what the type of the parameter.",
  "page4": "s1.substring(N,M), where N and M are integers, returns a value of type String. The returned value consists of the characters in s1 in positions N, N+1,. . . , M-1. Note that the character in position M is not included. The returned value is called a substring of s1. s1.index\u00aeOf(s2) returns an integer. If s2 occurs as a substring of s1, then the returned value is the starting position of that substring. Otherwise, the returned value is -1. You can also use s1.indexOf(ch) to search for a particular character, ch, in s1. To find the first occurrence of x at or after position N, you can use s1.indexOf(x,N). s1.compareTo(s2) is an integer-valued function that compares the two strings. If the strings are equal, the value returned is zero. If s1 is less than s2\u00ae, the value returned is a number less than zero, and if s1 is greater than s2, the value returned is some number greater than zero.(If both of the strings consist entirely of lower case letters, then \"less than\" and \"greater than\" refer to alphabetical order. Otherwise, the ordering is more\u00aecomplicated.) s1.toUpperCase() is a String-valued function that returns a new string that is equal to s1, except that any lower case letters in s1 have been converted to upper case. For example, \\\"Cat\\\".toUpperCase() is the string \\\"CAT\\\". There is also a function s1.toLowerCase(). s1.trim()\u00aeis a String-valued function that returns a new string that is equal to s1 except that any non-printing characters such as spaces and tabs have been trimmed from the beginning and from the end of the string. Thus, if s1 has the value \\\"fred \\\", then s1.trim() is the string \\\"fred\\\". For the functions s1.toUpperCase(), s1.toLowerCase(), and s1.trim(), note that the value of s1 is not modified.",
  "page5": "enum Season { SPRING, SUMMER, FALL, WINTER } By convention, enum values are given names that are made up of upper case letters, but that is a style guideline and not a syntax rule. Enum values are not variables. Each value is a constant that always has the same value. In fact, the\u00aepossible values of an enum type are usually referred to as enum constants. Note that the enum constants of type Season are considered to be \"contained in\" Season, which means following the convention that compound identifiers are used for things that are contained in other things the names that you actually use in your program to refer to them are Season.SPRING, Season.SUMMER, Season.FALL, and Season.WINTER. Once an enum type has been created, it can be used to declare variables\u00aein exactly the same ways that other types are used. For example, you can declare a variable named vacation of type Season with the statement: Season vacation; After declaring the variable, you can assign a value to it using an assignment statement.The value on the right-hand side of the assignment can be one of\u00aethe enum constants of type Season. Remember to use the full name of the constant, including \"Season\"! For example: vacation = Season.SUMMER; You can print out an enum value with an output statement such as System.out.print(vacation). The output value will be the name of the enum constant (without\u00aethe \"Season.\"). In this case, For some unfathomable reason, Java has never made it easy to read data typed in by the user of a program.You've already seen that output can be displayed to the user using the subroutine System.out.print. This subroutine is part of a pre-defined object called System.out. The purpose of this object is precisely to display output to the user. There is a corresponding object called System.in that exists to read data input by the user, but it provides only very primitive input facilities, and it requires some advanced Java programming skills to use it effectively.",
  "page6": "enum Season { SPRING, SUMMER, FALL, WINTER } By convention, enum values are given names that are made up of upper case letters, but that is a style guideline and not a syntax rule. Enum values are not variables. Each value is a constant that always has the same value. In fact, the\u00aepossible values of an enum type are usually referred to as enum constants.Note that the enum constants of type Season are considered to be \"contained in\" Season, which means following the convention that compound identifiers are used for things that are contained in other things the names that you actually use in your program to refer to them are Season.SPRING, Season.SUMMER, Season.FALL, and Season.WINTER.Once an enum type has been created, it can be used to declare variables in\u00aeexactly the same ways that other types are used. For example, you can declare a variable named vacation of type Season with the statement: Season vacation; After declaring the variable, you can assign a value to it using an assignment statement. The value on the right-hand side of the assignment can be one of\u00aethe enum constants of type Season. Remember to use the full name of the constant, including \"Season\"! For example: vacation = Season.SUMMER; You can print out an enum value with an output statement such as System.out.print(vacation). The output value will be the name of the enum constant (without the\u00ae\"Season.\"). In this case, the output would be \"SUMMER\". For some unfathomable reason, Java has never made it easy to read data typed in by the user of a program. You've already seen that output can be displayed to the user using the subroutine System.out.print. This subroutine is part of a pre-defined object called System.out. The purpose of this object is precisely to display output to the user. There is a corresponding object called System.",
  "page7": "are to be output. Here is a statement that will print a number in the proper format for a dollar amount, where amount is a variable of type double: System.out.printf( \\\"%1.2f\\\", amount ); TextIO can also do formatted output. The function TextIO.putf has the same functionality a\u00aes System.out.printf. Using TextIO, the above example would be: TextIO.printf(\\\"%1.2\\\",amount);and you could say TextIO.putln(\\\"%1.2f\\\",principal); instead of TextIO.putln(principal); in the Interest2 program to get the output in the right format. The output format of a value is specified by a format specifier. The format string (in the simple cases that I cover here) contains one format specifier for each of the values that is to be output. Some typical format specifiers are %d, %1\u00ae2d, %10s, %1.2f, %15.8e and %1.8g. Every format specifier begins with a percent sign (%) and ends with a letter, possibly with some extra formatting information in between. The letter specifies the type of output that is to be produced. For example, in %d and %12d, the \"d\" specifies that an integer is\u00aeto be written. The \"12\" in %12d specifies the minimum number of spaces that should be used for the output.If the integer that is being output takes up fewer than 12 spaces, extra blank spaces are added in front of the integer to bring the total up to 12. We say that the output is \"right-justified\u00aein a field of length 12.\" The value is not forced into 12 spaces; if the value has more than 12 digits, all the digits will be printed, with no extra spaces. The specifier %d means the same as %1d;that is an integer will be printed using just as many spaces as necessary. (The \"d,\" by the way, stands for \"decimal\" (base-10) numbers. You can use an \"x\" to output an integer value in hexadecimal form.) The letter \"s\" at the end of a format specifier can be used with any type of value.",
  "page8": "Introduction to File I/O System.out sends its output to the output destination known as \"standard output.\" But standard output is just one possible output destination. For example, data can be written to a file that is stored on the user's hard drive.The advantage to\u00aethis, of course, is that the data is saved in the file even after the program ends, and the user can print the file, email it to someone else, edit it with another program, and so on. TextIO has the ability to write data to files and to read data from files.When you write output using the put, putln, or putf method in TextIO, the output is sent to the current output destination. By default, the current output destination is standard output. However, TextIO has some subroutines that can be\u00aeused to change the current output destination. To write to a file named \"result.txt\", for example, you would use the statement: TextIO.writeFile(\\\"result.txt\\\"); After this statement is executed, any output from TextIO output statements will be sent to the file named \"result.txt\" instea\u00aed of to standard output. The file should be created in the same directory that contains the program. Note that if a file with the same name already exists, its previous contents will be erased! In many cases, you want to let the user select the file that will be used for output. The statement TextIO.writeUser\u00aeSelectedFile(); will open a typical graphical-user-interface file selection dialog where the user can specify the output file. If you want to go back to sending output to standard output, you can say TextIO.writeStandardOutput(); You can also specify the input source for TextIO's various \"get\" functions. The default input source is standard input. You can use the statement TextIO.readFile(\\\"data.txt\\\") to read from a file named \"data.txt\" instead, or you can let the user select the input file by saying TextIO.readUserSelectedFile(), and you can go back to reading from standard input with TextIO.readStandardInput(). When your program is reading from standard input, the user gets a chance to correct any errors in the input.",
  "page9": "Arithmetic Operators Arithmetic operators include addition, subtraction, multiplication, and division. They are indicated by +, -, *, and /. These operations can be used on values of any numeric type: byte, short, int, long, float, or double. When the computer actually calculates o\u00aene of these operations, the two values that it combines must be of the same type.If your program tells the computer to combine two values of different types, the computer will convert one of the values from one type to another. For example, to compute 37.4 + 10, the computer will convert the integer 10 to a real number 10.0 and will then compute 37.4 + 10.0. This is called a type conversion.Ordinarily, you don't have to worry about type conversion in expressions, because the computer\u00aedoes it automatically. When two numerical values are combined (after doing type conversion on one of them, if necessary), the answer will be of the same type. If you multiply two ints, you get an int; if you multiply two doubles, you get a double.This is what you would expect, but you have to be very careful wh\u00aeen you use the division operator /. When you divide two integers, the answer will always be an integer; if the quotient has a fractional part, it is discarded. For example, the value of 7/2 is 3, not 3.5. If N is an integer variable, then N/100 is an integer, and 1/N is equal to zero for any N greater than onel\u00aeThis fact is a common source of programming errors.You can force the computer to compute a real number as the answer by making one of the operands real: For example, when the computer evaluates 1.0/N, it first converts N to a real number in order to match the type of 1.0, so you get a real number as the answer. Java also has an operator for computing the remainder when one integer is divided by another. This operator is indicated by %. If A and B are integers, then A % B represents the remainder when A is divided by B. (However, for negative operands, % is not quite the same as the usual mathematical \"modulus\" operator, since if one of A or B is negative, then the value of A % B will be negative.)For example, 7 % 2 is 1, while 34577 % 100 is 77, and 50 % 8 is 2. A common use of % is to test whether a given integer is even or odd.",
  "page10": " and closing the persistence context H. References a and b are in detached state when the first persistence context is closed. You're dealing with instances that live outside of a guaranteed scope of object identity. You can see that a and c, loaded in a different persistence cont\u00aeext, aren't identical The test for equality with a.equals(c) is also false J. A test for database identity still returns true 1). This behavior can lead to problems if you treat entity instances as equal in detached state. For example, consider the following extension of the code, after the second unit of work has ended This example adds all three references to a Set. All are references to detached instances. Now, if you check the size of the collection the number of elements what result\u00aedo you expect? A Set doesn't allow duplicate elements. Duplicates are detected by the Set;whenever you add a reference, the Item#equals() method is called automatically against all other elements already in the collection. If equals() returns true for any element already in the collection, the addition do\u00aeesn't occur By default, all Java classes inherit the equals() method of java.lang.Object. This implementation uses a double-equals = comparison to check whether two references refer to the same in-memory instance on the Java heap.",
  "page11": " You may guess that the number of elements in the collection is two. After all, a and b are references to the same in-memory instance; they have been loaded in the same persistence context. You obtained reference c from another persistence context; it refers to a different instanc\u00aee on the heap. You have three references to two instances, but you know this only because you've seen the code that loaded the data. In a real application, you may not know that a and b are loaded in a different context than c. Furthermore, you obviously expect that the collection has exactly one element, because a, b, and c represent the same database row, the same Item. Whenever you work with instances in detached state and you test them for equality (usually inhash-based collection\u00aes), you need to supply your own implementation of the equals() and hashCode() methods for your mapped entity class. This is an important issue: if you don't work with entity instances in detached state, no action is needed, and the default equals() implementation of java.lang.Object is fine. You rely on Hi\u00aebernate's guaranteed scope of object identitywithin a persistence context. Even if you work with detached instances: if you never check if they're equal, you never put them in a Set or use them as keys in a Map, you don't have to worry. If all you do is render a detached Item on the screen, you\u00aearen't comparing it to anything.",
  "page12": " Many developers new to JPA think they always have to provide a custom equality routine for all entity classes, but this isn't the case. In section 18.3, we'll show you an application design with an extended persistence context strategy. This strategy will also extend th\u00aee scope of guaranteed object identity to span an entire conversation and several system transactions. Note that you still need the discipline not to compare detached instances obtained in two conversations! Let's assume that you want to use detached instances and that you have to test them for equality with your own method. You can implement equals() and hashCode() methods several ways. Keep in mind that when you override equals(), you always need to also override hashCode() so the tw\u00aeo methods are consistent. If two instances are equal, they must have the same hash value. A seemingly clever approach is to implement equals() to compare just the database identifier property, which is often a surrogate primary key value.Basically, if two Item instances have the same identifier returned by get\u00aeId(), they must be the same. If getId() returns null, it must be a transient Item that hasn't been saved. Unfortunately, this solution has one huge problem: identifier values aren't assigned by Hibernate until an instance becomes persistent. If a transient instance were added to a Set before being\u00aesaved, then when you save it, its hash value would change while it's contained by the Set. This is contrary to the contract of java.util.Set, breaking the collection.In particular, this problem makes cascading persistent state useless for mapped associations based on sets. We strongly discourage database identifier equality.",
  "page13": "To get to the solution that we recommend, you need to understand the notion of a business key. A business key is a property, or some combination of properties, that is unique for each instance with the same database identity. Essentially, it's the natural key that you would u\u00aese if you weren't using a surrogate primary key instead. Unlike a natural primary key, it isn't an absolute requirement that the business key never changes as long as it changes rarely, that's enough. We argue that essentially every entity class should have a business key, even if it includes all properties of the class (which would be appropriate for some immutable classes). If your user is looking at a list of items on screen, how do they differentiate between items A, B,\u00aeand C? The same property, or combination of properties, is your business key. The business key is what the user thinks of as uniquely identifying a particular record, whereas the surrogate key is what the application and database systems rely on. The business key property or properties are most likely constrai\u00aened UNIQUE in your database schema. Let's write custom equality methods for the User entity class; this is easier than comparing Item instances. For the User class, username is a great candidate business key. It's always required, it's unique with a database constraint, and it changes rarely,\u00aeif ever.",
  "page14": "You may have noticed that the equals() method code always accesses the properties of the \"other\" reference via getter methods. This is extremely important, because the reference passed as other may be a Hibernate proxy, not the actual instance that holds the persistent s\u00aetate. You can't access the username field of a User proxy directly. To initialize the proxy to get the property value, you need to access it with a getter method. This is one point where Hibernate isn't completely transparent, but it's good practice anyway to use getter methods instead of direct instance variable access. Check the type of the other reference with instanceof, not by comparing the values of getClass(). Again, the other reference may be a proxy, which is a run\u00aetimegenerated subclass of User, so this and other may not be exactly the same type but a valid super/subtype. You can find more about proxies in section 12.1.1. For some other entities, the business key may be more complex, consisting of a combination of properties. Here are some hints that should help you iden\u00aetify a business key in your domain model classes: Consider what attributes users of your application will refer to when they have to identify an object (in the real world). How do users tell the difference between one element and another if they're displayed on the screen? This is probably the business key you're looking for.",
  "page15": "Every immutable attribute is probably a good candidate for the business key. Mutable attributes may be good candidates, too, if they're updated rarely or if you can control the case when they're updated for example, by ensuring the instances aren't in a Set at the t\u00aeime. Every attribute that has a UNIQUE database constraint is a good candidate for the business key. Remember that the precision of the business key has to be good enough to avoid overlaps. Any date or time-based attribute, such as the creation timestamp of the record, is usually a good component of a business key, but the accuracy of System currentTimeMillis() depends on the virtual machine and operating system. Our recommended safety buffer is 50 milliseconds, which may not be accurate e\u00aenough if the time-based property is the single attribute of a business key. You can use database identifiers as part of the business key. This seems to contradict our previous statements, but we aren't talking about the database identifier value of the given entity. You may be able to use the database iden\u00aetifier of an associated entity instance.  For example, a candidate business key for the Bid class is the identifier of the Item it matches together with the bid amount. You may even have a unique constraint that represents this composite business key in the database schema. You can use the identifier value of the associated Item because it never changes during the life cycle of a Bid the Bid constructor can require an already-persistent Item.",
  "page16": "If you follow our advice, you shouldn't have much difficulty finding a good business key for all your business classes. If you encounter a difficult case, try to solve it without considering Hibernate. After all, it's purely an object-oriented problem. Notice that it\u00ae19s almost never correct to override equals() on a subclass and include another property in the comparison. It's a little tricky to satisfy the Object identity and equality requirements that equality be both symmetric and transitive in this case; and, more important, the business key may not correspond to any well-defined candidate natural key in the database (subclass properties may be mapped to a different table). For more information on customizing equality comparisons, see Effective J\u00aeava, 2nd edition, by Joshua Bloch (Bloch, 2008), a mandatory book for all Java programmers. The User class is now prepared for detached state; you can safely put instances loaded in different persistence contexts into a Set. Next, we'll look at some examples that involve detached state, and you see some o\u00aef the benefits of this concept. Sometimes you might want to detach an entity instance manually from the persistence context. You don't have to wait for the persistence context to close. You can evict entity instances manually: This example also demonstrates the EntityManager#contains() operation, which\u00aereturns true if the given instance is in managed persistent state in this persistence context. You can now work with the user reference in detached state. Many applications only read and render the data after the persistence context is closed. Modifying the loaded user after the persistence context is closedhas no effect on its persistent representation in the database. JPA allows you to merge any changes back into the database in a new persistence context, though.",
  "page17": "The goal is record the new username of the detached User. First, when you call merge(), Hibernate checks whether a persistent instance in the persistence context has the same database identifier as the detached instance you're merging. In this example, the persistence contex\u00aet is empty; nothing has been loaded from the database. Hibernate therefore loads an instance with this identifier from the data base. Then, merge() copies the detached entity instance onto this loaded persistent instance. In other words, the new username you have set on the detached User is also set on the persistent merged User, which merge() returns to you. Now discard the old reference to the stale and outdated detached state; the detachedUser no longer represents the current state. Yo\u00aeu can continue modifying the returned mergedUser; Hibernate will execute a single UPDATE when it flushes the persistence context during commit. If there is no persistent instance with the same identifier in the persistence context, and a lookup by identifier in the database is negative, Hibernate instantiates a\u00aefresh User. Hibernate then copies your detached instance onto this fresh instance, which it inserts into the database when you synchronize the persistence context with the database. If the instance you're giving tomerge() is not detached but rather is transient (it doesn't have an identifier value\u00ae), Hibernate instantiates a fresh User, copies the values of the transient User onto it, and then makes it persistent and returns it to you. In simpler terms, the merge() operation can handle detached and transient entity instances. Hibernate always returns the result to you as a persistent instance.",
  "page18": " An application architecture based on detachment and merging may not call the persist() operation. You can merge new and detached entity instances to store data. The important difference is the returned current state and how you handle this switch of references in your application\u00aecode. You have to discard the detachedUser and from now on reference the current mergedUser. Every other component in your application still holding on to detachedUser has to switch to mergedUser. If you want to delete a detached instance, you have to merge it first. Then call remove() on the persistent instance returned by merge(). We'll look at detached state and merging again in chapter 18 and implement a more complex conversation between a user and the system using this strategy\u00ae. In this chapter, we finally talk about transactions: how you create and control concurrent units of work in an application. A unit of work is an atomic group of operations. Transactions allow you to set unit of work boundaries and help you isolate one unit of work from another. In a multiuser application, yo\u00aeu may also be processing these units of work concurrently. To handle concurrency, we first focus on units of work at the lowest level: database and system transactions. You'll learn the APIs for transaction demarcation and how to define units of work in Java code. We'll talk about how to preserve i\u00aesolation and control concurrent access with pessimistic and optimistic strategies. Finally, we look at some special cases and JPA features, based on accessing the database without explicit transactions. Let's start with some background information.",
  "page19": "Application functionality requires that several things be done in one go. For example, when an auction finishes, the CaveatEmptor application must perform three different tasks: Find the winning bid (highest amount) for the auction item. Charge the seller of the item the cost of t\u00aehe auction. Notify the seller and successful bidder. What happens if you can't bill the auction costs because of a failure in the external credit-card system? The business requirements may state that either all listed actions must succeed or none must succeed. If so, you call these steps collectively a transaction or unit of work. If only a single step fails, the entire unit of work must fail. ACID stands for atomicity, consistency, isolation, durability. Atomicity is the notion that\u00aeall operations in a transaction execute as an atomic unit. Furthermore, transactions allow multiple usersto work concurrently with the same data withoutcompromising the consistency of the data (consistent with database integrity rules). A particular transaction should not be visible to other concurrently runnin\u00aeg transactions; they should run in isolation. Changes made in a transaction should be durable, even if the system fails after the transaction has completed successfully. In addition, you want correctness of a transaction. For example, the business rules dictate that the application charges the seller once, n\u00aeot twice. This is a reasonable assumption, but you may not be able to express it with database constraints. Hence, the correctness of a transaction is the responsibility of the application, whereas consistency is the responsibility of the database. Together, these transaction attributes define the ACID criteria.",
  "page20": "We've also mentioned system and database transactions. Consider the last example again: during the unit of work ending an auction, we might mark the winning bid in a database system. Then, in the same unit of work, wetalk to an external system to bill the seller's credit\u00aecard. This is a transaction spanning several (sub)systems, with coordinated subordinate transactions on possibly several resources such as a database connection and an external billing processor. Database transactions have to be short, because open transactions consume database resources and potentially prevent concurrent access due to exclusive locks on data. A single database transaction usually involves only a single batch of database operations. To execute all of your database opera\u00aetions inside a system transaction, you have to set the boundaries of that unit of work. You must start the transaction and, at some point, commit the changes. If an error occurs (either while executing database operations or when committing the transaction), you have toroll back the changes to leave the data in\u00aea consistent state. This process defines a transaction demarcation and, depending on the technique you use, involves a certain level of manual intervention. In general, transaction boundaries that begin and end a transaction can be set either programmatically in application code or declaratively.",
  "page21": "In a Java SE environment, you call the JDBC API to mark transaction boundaries. You begin a transaction with setAutoCommit(false) on a JDBC Connection and end it by calling commit(). You may, at any time while the transaction is in progress, force an immediate rollback with rollba\u00aeck(). In an application that manipulates data in several systems, a particular unit of work involves access to more than one transactional resource. In this case, you can't achieve atomicity with JDBC alone. You need a transaction manager that can handle several resources in one system transaction. JTA standardizes system transaction management and distributed transactions so you won't have to worry much about the lower-leveldetails. The main API in JTA is the UserTransaction in\u00aeterface with methods to begin() and commit() a system transaction. The most complicated bit of this code snippet seems to be the exception handling; we'll discuss this part in a moment. First, you haveto understand how the transaction management and the EntityManager work together. The EntityManager is la\u00aezy; we mentioned in the previous chapter that it doesn't consume any database connections until SQL statements have to be executed. The same is true for JTA: starting and committing an emptytransaction is cheap when you haven't accessed any transactional resources. For example, you could execute thi\u00aes empty unit of work on a server, for each client request, without consuming anyresources or holding any database locks.",
  "page22": "When you create an EntityManager, it looks for an ongoing JTA system transaction within the current thread of execution. If the EntityManager finds an ongoing transaction, it joins the transaction by listening to transaction events. This means you should always call UserTransactio\u00aen#begin() and EntityManagerFactory#createEntityManager() on the same thread if you want them to be joined. By default, and as explained in chapter 10, Hibernate automatically flushes the persistence context when the transaction commits. If the EntityManager can't find a started transaction in the same thread when it's created, it's in a special unsynchronized mode. In this mode, JPA won't automatically flush the persistence context. We talk more about this behavior lat\u00aeer in this chapter; it's a convenient feature of JPA when you design more complex conversations. The transaction manager will stop a transaction when it has been running for too long. Remember that youwant to keep database transactions as short as possible in a busy OLTP system. The default timeout depends\u00aeon the JTA provider Bitronix, for example, defaults to 60 seconds. You can override this selectively, before you begin the transaction, with UserTransaction#setTransactionTimeout(). We still need to discuss the exception handling of the previous code snippet. If any EntityManager call or flushing the persis\u00aetence context during a commit throws an exception, you must check the current state of the system transaction. When an exception occurs, Hibernate marks the transaction for rollback. This means the only possible outcome for this transaction is undoing all of its changes.",
  "page23": "Because you started the transaction, it's your job to check for STATUS_MARKED_ROLLBACK. The transaction might also still be STATUS_ACTIVE, if Hibernate wasn't able to mark it for rollback. In both cases, call UserTransaction#rollback() to abort any SQL statements that ha\u00aeve been sent to the database within this unit of work. All JPA operations, including flushing the persistence context, can throw a RuntimeException. But the methods UserTransaction#begin(), commit(), and even rollback() throw a checked Exception. The exception for rollback requires special treatment: you want to catch this exception and log it; otherwise, the original exception that led to the rollback is lost. Continue throwing the original exception after rollback. Typically, you have a\u00aenother layer of interceptors in your system that will finally deal with the exception, for example by rendering an error screen or contacting the operations team. An error during rollback is more difficult to handle properly; we suggest logging and escalation, because a failed rollback indicates a serious syste\u00aem problem. Hibernate throws typed exceptions, all subtypes of RuntimeException that help you identify errors: The most common, HibernateException, is a generic error. You have to either check the exception message or find out more about the cause by calling getCause() on the exception. A JDBCException is any\u00aeexception thrown by Hibernate's internal JDBC layer. This kind of exception is always caused by a particular SQL statement, and you can get the offending statement with getSQL(). The internal exception thrown by the JDBC connection (the JDBC driver) is available with getSQLException() or getCause(), and the database- and vendor-specific error code is available with getErrorCode().",
  "page24": "Hibernate includes subtypes of JDBCException and an internal converter that tries to translate the vendor-specific error code thrown by the database driver into something more meaningful. The built-in converter can produce JDBCConnectionException, SQLGrammarException, LockAcquisit\u00aeionException, DataException, and ConstraintViolationException for the most important database dialects supported by Hibernate. You can either manipulate or enhance the dialect for your database or plug in a SQLExceptionConverterFactory to customize this conversion. Some developers get excited when they see how many fine-grained exception types Hibernate can throw. This can lead you down the wrong path. For example, you may be tempted to catch a ConstraintViolationException for validation p\u00aeurposes. If you forget to set the Item#name property, and its mapped column is NOT NULL in the database schema, Hibernate will throw this exception when you flush the persistence context. Why not catch it, display a (customized depending on the error code and text) failure message to application users, and let\u00aethem correct the mistake? This strategy has two significant disadvantages. First, throwing unchecked values against the database to see what sticks isn't the right strategy for a scalable application. You want to implement at least some dataintegrity validation in the application layer. Second, exceptio\u00aens are fatal for your current unit of work. But this isn't how application users will interpret a validation error: they expect to still be inside a unit of work.",
  "page25": " Doing so helps you during development and also helps any customer-support engineer who has to decide quickly whether it's an application error (constraint violated, wrong SQL executed) or whether the database system is under load (locks couldn't be acquired). For valida\u00aetion, you have a unifying framework available with Bean Validation. From a single set of rules in annotations on entities, Hibernate can verify all domain and single-row constraints at the user interface layer and can automatically generate SQL DDL rules. You now know what exceptions you should catch and when to expect them. One question is probably on your mind: what should you do after you've caught an exception and rolled back the system transaction? Exceptions thrown by Hibernate\u00aeare fatal. This means you have to close the current persistence context. You aren't allowed to continue working with the EntityManager that threw an exception. Render an error screen and/or log the error, and then let the user restart the conversation with the system using a fresh transaction and persiste\u00aence context. As usual, this isn't the whole picture. Some standardized exceptions aren't fatal: javax.persistence.NoResultException Thrown when a Query or TypedQuery is executed with getSingleResult() and no result was returned from the database. You can wrap the query call with exception-handling\u00aecode and continue working with the persistence context. The current transaction won't be marked for rollback.",
  "page26": "javax.persistence.NonUniqueResultException Thrown when a Query or TypedQuery is executed with getSingleResult() and several results were returned from the database. You can wrap the query call with exception handling code and continue working with the persistence context. Hibernat\u00aee won't mark the current transaction for rollback. javax.persistence.QueryTimeoutException Thrown when a Query or TypedQuery takes too long to execute. Doesn't mark the transaction for rollback. You may want to repeat the query, if appropriate. javax.persistence.LockTimeoutException Thrown when a pessimistic lock couldn't be acquired. May occur during flushing or explicit locking (more on this topic later in this chapter). The transaction isn't marked for rollback, and\u00aeyou may want to repeat the operation. Keep in mind that endlessly hammering on a database system that is already struggling to keep up won't improve the situation. Notably absent from this list is javax.persistence.EntityNotFoundException. It can be thrown by the EntityManager#getReference() and refresh()\u00aemethods, as well as lock(), which you'll see later in this chapter. Hibernate may throw it when you try to access the reference/proxy of an entity instance and the database record is no longer available. It's a fatal exception: it marks the current transaction for rollback, and you have to close an\u00aed discard the persistence context. Programmatic transaction demarcation requires application code written against a transaction demarcation interface such as JTA's UserTransaction. Declarative transaction demarcation, on the other hand, doesn't require extra coding.",
  "page27": "In a Java EE application, you can declare when you wish to work inside a transaction. It's then the responsibilityof the runtime environment to handle this concern. Usually you set transaction boundaries with annotations on your managed components (EJBs, CDI beans, and so on)\u00ae. You can use the older annotation @javax.ejb.TransactionAttribute to demarcate transaction boundaries declaratively on EJB components. You can find examples in section18.2.1. You can apply the newer and more general @javax.transaction.Transactional on any Java EE managed component. You can find an example in section 19.3.1. All other examples in this chapter work in any Java SE environment, without a special runtime container. Hence, from now on, you'll only see programmatic trans\u00aeaction demarcation code until we focus on specific Java EE application examples. Next, we focus on the most complex aspect of ACID properties: how you isolate concurrently running units of work from each other. Databases (and other transactional systems) attempt to ensure transaction isolation, meaning that,\u00aefrom the point of view of each concurrent transaction, it appears that no other transactions are in progress. Traditionally, database systems have implemented isolation with locking. A transaction may place a lock on a particular item of data in the database, temporarily preventing read and/or write access to\u00aethat item by other transactions. Some modern database engines implement transaction isolation with multiversion concurrency control (MVCC), which vendors generally consider more scalable. We'll discuss isolation assuming a locking model, but most of our observations are also applicable to MVCC.",
  "page28": " How databases implement concurrency control is of the utmost importance in your Java Persistence application. Applications inherit the isolation guarantees provided by the database management system. For example, Hibernate never locks anything in memory. If you consider the many\u00aeyears of experience that database vendors have with implementing concurrency control, you'll see the advantage of this approach. Additionally, some features in Java Persistence, either because you explicitly use them or by design, can improve the isolation guarantee beyond what the database provides. We discuss concurrency control in several steps. First, we explore the lowest layer: the transaction isolation guarantees provided by the database. After that, you'll see the Java Pe\u00aersistence features for pessimistic and optimistic concurrency control at the application level, and what other isolation guarantees Hibernate can provide. If we're talking about isolation, you may assume that two things are either isolated or not; there is no grey area in the real world. When we talk about\u00aedatabase transactions, complete isolation comes at a high price. You can't stop the world to access data exclusively in a multiuser OLTP system. Therefore, several isolation levels are available, which, naturally, weaken full isolation but increase performance and scalability of the system First, let\u00aelook at several phenomena that may occur when you weaken full transaction isolation. The ANSI SQL standard defines the standard transaction isolation levels in terms of which of these phenomena are permissible.",
  "page29": " A lost update occurs if two transactions both update a data item and then the second transaction aborts, causing both changes to be lost. This occurs in systems that don't implement concurrency control, where concurrent transactions aren't isolated. This is shown in fig\u00aeure 11.1. A dirty read occurs if a transaction reads changes made by another transaction that hasn't yet been committed. This is dangerous because the changes made by the other transaction may later be rolled back, and invalid data may be written by the first transaction; see figure 11.2. An unrepeatable read occurs if a transaction reads a data item twice and reads different state each time. For example, another transaction may have written to the data item and committed between th\u00aee two reads, as shown in figure 11.3. A special case of an unrepeatable read is the last commit wins problem. Imagine that two concurrent transactions both read a data item, as shown in figure 11.4. One writes to it and commits, and then the second writes to it and commits. The changes made by the first writer\u00aeare lost. This issue is especially frustrating for users: user A's changes are overwritten without warning, and B has potentially made a decision based on outdated information. A phantom read is said to occur when a transaction executes a query twice, andthe second result includes data that wasn't\u00aevisible in the first result or less data because something was deleted. It need not necessarily be exactly the same query. Another transaction inserting or deleting data between the executions of the twoqueries causes this situation, as shown in figure 11.5.",
  "page30": "The standard isolation levels are defined by the ANSI SQL standard, but they aren't specific to SQL databases. JTA defines exactly the same isolation levels, and you'll use these levels to declare your desired transaction isolation. With increased levels of isolation com\u00aee higher cost and serious degradation of performance and scalability: Read uncommitted isolation A system that permits dirty reads but not lost updates operates in read uncommitted isolation. One transaction may not write to a row if another uncommitted transaction has already written to it. Any transaction may read any row, however. A DBMS may implement this isolation level with exclusive write locks. Read committed isolation A system that permits unrepeatable reads but not dirty reads i\u00aemplements read committed isolation. A DBMS may achieve this by using shared read locks and exclusive write locks. Reading transactions don't block other transactions from accessing a row, but an uncommitted writing transaction blocks all other transactions from accessing the row. Repeatable read isolation\u00aeA system operating in repeatable read isolation mode permits neither unrepeatable reads nor dirty reads. Phantom reads may occur. Reading transactions block writing transactions but not other reading transactions, and writing transactions block all other transactions. Serializable isolation The strictest iso\u00aelation, serializable, emulates serial execution, as if transactions were executed one after another, rather than concurrently. A DBMS may not implement serializable using onlyrow-level locks. A DBMS must instead provide some other mechanism that prevents a newly inserted row from becoming visible to a transaction that has already executed a query that would return the row. A crude mechanism is exclusively locking the entire database table after a write, so no phantom reads can occur.",
  "page31": "Developers (ourselves included) are often unsure what transaction isolation level to use in a production application. Too high an isolation level harms the scalability of a highly concurrent application. Insufficient isolation may cause subtle, difficult-toreproduce bugs in an app\u00aelication that you won'tdiscover until the system is working under heavy load. Note that we refer to optimistic locking (with versioning) in the following explanation, a concept explained later in this chapter. You may want to skip this section for now and come back to it later when it's time to pick an isolation level for your application. Choosing the correct isolation level is, after all, highly dependent on your particular scenario. Read the following discussion as recommenda\u00aetions, not dictums carved in stone. Hibernate tries hard to be as transparent as possible regarding transactional semantics of the database. Nevertheless, persistence context caching and versioning affect these semantics. What is a sensible database isolation level to choose in a JPA application? First, for a\u00aelmost all scenarios, eliminate the read uncommitted isolation level. It's extremely dangerous to use one transaction's uncommitted changes in a different transaction. The rollback or failure of one transaction will affect other concurrent transactions. Rollback of the first transaction could bring o\u00aether transactions down with it, or perhaps even cause them to leave the database in an incorrect state (the seller of an auction item might be charged twice consistent with database integrity rules but incorrect). It's possible that changes made by a transaction that ends up being rolled back could be committed anyway, because they could be read and then propagated by another transaction that is successful!",
  "page32": "Second, most applications don't need serializable isolation. Phantom reads aren't usually problematic, and this isolation level tends to scale poorly. Few existing applications use serializable isolation in production, but rather rely on selectively applied pessimistic l\u00aeocks that effectively force a serialized execution of operations in certain situations. Next, let's consider repeatable read. This level provides reproducibility for query result sets for the duration of a database transaction. This means you won't read committed updates from the database if you query it several times. But phantom reads are still possible: new rows might appear rows you thought existed might disappear if another transaction committed such changes concurrently. A\u00aelthough you may sometimes want repeatable reads, you typically don't need them in every transaction. The JPA specification assumes that read committed is the default isolation level. This means you have to deal with unrepeatable reads, phantom reads, and the last commit wins problem. Let's assume you\u00ae're enabling versioning of your domain model entities, something that Hibernate can do for you automatically. The combination of the (mandatory) persistence context cache and versioning already gives you most of the nice features of repeatable read isolation. The persistence context cache ensures that th\u00aee state of the entity instances loaded by one transaction is isolated from changes made by other transactions. If you retrieve the same entity instance twice in a unit of work, the second lookup will be resolved within the persistence context cache and not hit the database. Hence, your read is repeatable, and you won't see conflicting committed data. (You still get phantom reads, though, which are typically much easier to deal with.) Additionally, versioning switches to first commit wins. Hence, for almost all multiuser JPA applications, read committed isolation for all database transactions is acceptable with enabled entity versioning.",
  "page33": "nabled entity versioning. Hibernate retains the isolation level of your database connection; it doesn't change the level. Most products default to read committed isolation. There are several ways you can change either the default transaction isolation level or the settings o\u00aef the current transaction. First, you can check whether your DBMS has a global transaction isolation level setting in its proprietary configuration. If your DBMS supports the standard SQL statement SET SESSION CHARACTERISTICS, you can execute it to set the transaction settings of all transactions started in this particular database session (which means a particular connection to the database, not a Hibernate Session). SQL also standardizes the SET TRANSACTION syntax, which sets the isolat\u00aeion level of the current transaction. Finally, the JDBC Connection API offers the setTransactionIsolation() method, which (according to its documentation) \"attempts to change the transaction isolation level for this connection.\" In a Hibernate/JPA application, you can obtain a JDBC Connection from the\u00aenative Session API; see section 17.1. We recommend a different approach if you're using a JTA transaction manager or even a simple JDBC connection pool. JTA transaction management systems, such as Bitronix used for the examples of this book, allow you to set a default transaction isolation level for ev\u00aeery connection obtained from the pool. In Bitronix, you can set the default isolation level on startup withPoolingDataSource#setIsolationLevel(). Check the documentation of your data source provider, application server, or JDBC connection pool for more information.",
  "page34": "We assume from now on that your database connections are by default in read committed isolation level. From time totime, a particular unit of work in your application may require a different, usually stricter isolation level. Instead of changing the isolation level of the entire t\u00aeransaction, you should use the Java Persistence API to obtain additional locks on the relevant data. This fine-grained locking is more scalable in a highly concurrent application. JPA offers optimistic version checking and databaselevel pessimistic locking. Handling concurrency in an optimistic way is appropriate when concurrent modifications are rare and it's feasible to detect conflicts late in a unit of work. JPA offers automatic version checking as an optimistic conflict-detectio\u00aen procedure. First you'll enable versioning, because it's turned off by default that's why you get last commit wins if you don't do anything. Most multiuser applications, especially webapplications, should rely on versioning for any concurrently modified @Entity instances, enabling the more\u00aeuser-friendly first commit wins. The previous sections have been somewhat dry; it's time for code. After enabling automatic version checking, you'll see how manual version checking works and when you have to use it. You enable versioning with an @Version annotation on a special additional property\u00aeof your entity class, as shown next. In this example, each entity instance carries a numeric version. It's mapped to an additional column of the ITEM database table; as usual, the column name defaults to the property name, here VERSION. The actual name of the property and column doesn't matter you could rename it if VERSION is a reserved keyword in your DBMS.",
  "page35": "You could add a getVersion() method to the class, but you shouldn't have a setter method and the application shouldn't modify the value. Hibernate automatically changes the version value: it increments the version number whenever an Item instance has been found dirty dur\u00aeing flushing of the persistence context. The version is a simple counter without any useful semantic value beyond concurrency control. You can use an int, an Integer, a short, a Short, or a Long instead of a long; Hibernate wraps and starts from zero again if the version number reaches the limit of the data type. After incrementing the version number of a detected dirty Item during flushing, Hibernate compares versions when executing the UPDATE and DELETE SQL statements. For example, assu\u00aeme that in a unit of work, you load an Item and change its name, as follows. Retrieving an entity instance by identifier loads the current version from the database with a SELECT. The current version of the Item instance is 0. When the persistence context is flushed, Hibernate detects the dirty Item instance\u00aeand increments its version to 1. SQL UPDATE now performs the version check, storing the new version in the database, but only if the database version is still 0.",
  "page36": "Pay attention to the SQL statements, in particular the UPDATE and its WHERE clause. This update will be successful only if there is a row with VERSION 0 in the database. JDBC returns the number of updated rows to Hibernate; if that result is zero, it means the ITEM row is either g\u00aeone or doesn't have the version 0 anymore. Hibernate detects this conflict during flushing, and a javax.persistence.OptimisticLockException is thrown. Now imagine two users executing this unit of work atthe same time, as shown previously in figure 11.4. The first user to commit updates the name of the Item and flushes the incremented version 1 to the database. The second user's flush (and commit) will fail, because their UPDATE statement can't find the row in the database w\u00aeith version 0. The database version is 1. Hence, the first commit wins, and you can catch the OptimisticLockException and handle it specifically. For example, you could show the following message to the second user: \"The data you have been working with has been modified by someone else. Please start your u\u00aenit of work again with fresh data. Click the Restart button to proceed.\" What modifications trigger theincrement of an entity's version? Hibernateincrements the version whenever an entity instance is dirty. This includes all dirty valuetyped properties of the entity, no matter if they're single\u00ae-valued (like a String or int property), embedded (like an Address), or collections. The exceptions are @OneToMany and @ManyToMany association collections that have been made read-only with mappedBy. Adding or removing elements to these collections doesn't increment the version number of the owning entity instance. You should know that none of this is standardized in JPA don't rely on two JPA providers implementing the same rules when accessing a shared database.",
  "page37": "If you don't want to increment the version of the entity instance when a particular property's value has changed, annotate the property with @org.hibernate.annotations .OptimisticLock(excluded true). You may not like the additional VERSION column in your database schema.\u00aeAlternatively, you may already have a \"last updated\" timestamp property on your entity class and a matching database column. Hibernate can check versions with timestamps instead of the extra counter field If your database schema already contains a timestamp column such as LASTUPDATED or MODIFIED_ON, you can map it for automatic versionchecking instead of using a numeric counter. This example maps the column LASTUPDATED to a java.util.Date property; a Calendar type would also wor\u00aek with Hibernate. The JPA standard doesn't define these types for version properties; JPA only considers java.sql.Timestamp portable. This is less attractive, because you'd have to import that JDBC class in your domain model. You should try to keep implementation details such as JDBC out of the domain\u00aemodel classes so they can be tested, instantiated, cross-compiled (to JavaScript with GWT, for example), serialized, and deserialized in as many environments as possible. In theory, versioning with a timestamp is slightly less safe, because two concurrent transactions may both load and update the same Item\u00aein the same millisecond; this is exacerbated by the fact that a JVM usually doesn't have millisecond accuracy (you should check your JVM and operating system documentation for the guaranteed precision). Furthermore, retrieving the current time from the JVM isn' t necessarily safe in a clustered environment, where the system time of nodes may not be synchronized, or time synchronization isn't as accurate as you'd need for your transactional load.",
  "page38": " You can switch to retrieval of the current time from the database machine by placing an @org.hibernate.annotations.Type annotation on the version property. Hibernate now asks the database, with for example call current _timestamp() on H2, for the current time before updating. Thi\u00aes gives you a single source of time for synchronization. Not all Hibernate SQL dialects support this, so check the source of your configured dialect and whether it overrides the getCurrentTimestampSelectString() method. In addition, there is always the overhead of hitting the database for every increment. We recommend that new projects rely on versioning with a numeric counter, not timestamps. If you're working with a legacy database schema or existing Java classes, it may be impossi\u00aeble to introduce a version or timestamp property and column. If that's the case, Hibernate has an alternative strategy for you. If you don't have version or timestamp columns, Hibernate can still perform automatic versioning. This alternative implementation of versioning checks the current database st\u00aeate against the unmodified values of persistent properties at the time Hibernate retrieved the entity instance (or the last time the persistence context was flushed). You enable this functionality with the proprietary Hibernate annotation @org.hibernate.annotations.OptimisticLocking: Hibernate lists all colu\u00aemns and their last known values in the WHERE clause. If any concurrent transaction has modified any of these values or even deleted the row, this statement returns with zero updated rows. Hibernate then throws an exception at flush time. Alternatively, Hibernate includes only the modified properties in the restriction (only NAME, in this example) if you switch to OptimisticLockType.DIRTY. This means two units of work may modify the same Item concurrently, and Hibernate detects a conflict only if they both modify the same value-typed property (or a foreign key value).",
  "page39": "The WHERE clause of the last SQL excerpt would be reduced to where ID 123 and NAME = 'Old Name'. Someone else could concurrently modify the price, and Hibernate wouldn't detect any conflict. Only if the application modified the name concurrently would you get a javax.persiste\u00aence.OptimisticLockException. In most cases, checking only dirty properties isn't a good strategyfor business entities. It's probably not OK to change the price of an item if the description changes! This strategy also doesn't work with detached entities and merging: if you merge a detached entity into a new persistence context, the \"old\" values aren't known. The detached entity instance will have to carry a version number or timestamp for optimistic concurre\u00aency control. Automatic versioning in Java Persistence prevents lost updates when two concurrent transactions try to commit modifications on the same piece of data. Versioning can also help you to obtain additional isolation guarantees manually when you need them. Here's a scenario that requires repeatabl\u00aee database reads: imagine you have some categories in your auction system and that each Item is in a Category. This is a regular @ManyToOne mapping of an Item#category entity association. Let's say you want to sum up all item prices in several categories. This requires a query for all items in each cate\u00aegory, to add up the prices. The problem is, what happens if someone moves an Item from one Category to another Category while you're still querying and iterating through all the categories and items? With read-committed isolation, the same Item might show up twice while your procedure runs!",
  "page40": "For each Category, query all Item instances with an OPTIMISTIC lock mode. Hibernate now knows it has to check each Item at flush time. For each Item loaded earlier with the locking query, Hibernate executes a SELECT during flushing. It checks whether the database version of each\u00aeITEM row is still the same as when itwas loaded. If any ITEM row has a different version or the row no longer exists, an OptimisticLockException is thrown. Don't be confused by the locking terminology: The JPA specificationleaves open how exactly each LockModeType is implemented; for OPTIMISTIC, Hibernate performs version checking. There are no actual locks involved. You'll have to enable versioning on the Item entity class as explained earlier; otherwise, you can't use the\u00aeoptimistic LockModeTypes with Hibernate. Hibernate doesn't batch or otherwise optimize the SELECT statements for manual version checking: If you sum up 100 items, you get 100 additional queries at flush time. A pessimistic approach, as we show later in this chapter, may be a better solution for this parti\u00aecular case. As shown in the previous example, the Query interface accepts a LockModeType. Explicit lock modes are also supported by the TypedQuery and the NamedQuery interfaces, with the same setLockMode() method. An additional optimistic lock mode is available in JPA, forcing an increment of an entity'\u00aes version.",
  "page41": "What happens if two users place a bid for the same auction item at the same time? When a user makes a new bid, the application must do several things: Retrieve the currently highest Bid for the Item from the database. Compare the new Bid with the highest Bid; if the new Bid is hi\u00aegher, it must be stored in the database. There is the potential for a race condition in between these two steps. If, in between reading the highest Bid and placing the new Bid, another Bid is made, you won't see it. This conflict isn't visible; even enabling versioning of the Item doesn't help. The Item is never modified during the procedure. Forcing a version increment of the Item makes the conflict detectable find() accepts a LockModeType. The OPTIMISTIC_FORCE_INCREMENT mo\u00aede tells Hibernate that the version of the retrieved Item should be incremented after loading, even if it's never modified in the unit of work. The code persists a new Bid instance; this doesn't affect any values of the Item instance. A new row is inserted into the BID table. Hibernate wouldn't\u00aedetect concurrently made bids without a forced version increment of the Item. You use a checked exception to validate the new bid amount. It must be greater than the currently highest bid. When flushing the persistence context, Hibernate executes an INSERT for the new Bid and forces an UPDATE of the Item wi\u00aeth a version check. If someone modified the Item concurrently or placed a Bid concurrently with this procedure, Hibernate throws an exception.",
  "page42": "For the auction system, placing bids concurrently is certainly a frequent operation. Incrementing a version manually is useful in many situations whereyou insert or modify data and want the version of some root instance of an aggregate to be incremented. Note that if instead of a\u00aeBid#item entity association with @ManyToOne, you have an @ElementCollection of Item#bids, adding a Bid to the collection will increment the Item version. The forced increment then isn't necessary. You may want to review the discussion of parent/child ambiguity and how aggregates and composition work with ORM in section 7.3. So far, we've focused on optimistic concurrency control: we expect that concurrent modifications are rare, so we don't prevent concurrent accessand det\u00aeect conflicts late. Sometimes you know that conflicts will happen frequently, and you want to place an exclusive lock on some data. This calls for a pessimistic approach. Let's repeat the procedure shown in the section \"Manual version checking\" with a pessimistic lock instead of optimistic versio\u00aen checking. You again summarize the total price of all items in several categories. This is the same code as shown earlier in listing 11.5, with a different LockModeType. For each Category, query all Item instances in PESSIMISTIC_READ lock mode. Hibernate locks the rows in the database with the SQL query. If\u00aepossible, wait 5 seconds if another transaction holds a conflicting lock. If the lock can't be obtained, the query throws an exception. If the query returns successfully, you know that you hold an exclusive lock on the data and no other transaction can access it with an exclusive lock or modify it until this transaction commits. Your locks are released after commit, when the transaction completes.",
  "page43": "The JPA specification defines that the lock mode PESSIMISTIC_READ guarantees repeatable reads. JPA also standardizes the PESSIMISTIC_WRITE mode, with additional guarantees:in addition to repeatable reads, the JPA provider must serialize data access, and no phantom reads can occur.\u00ae It's up to the JPA provider to implement these requirements. For both modes, Hibernate appends a \"for update\" clause to the SQL query when loading data. This places a lock on the rows at the database level. What kind of lock Hibernate uses depends on the LockModeType and your Hibernate database dialect. On H2, for example, the query is SELECT * FROM ITEM ... FOR UPDATE. Because H2 supports only one type of exclusive lock, Hibernate generates the same SQL for all pessimist\u00aeic modes. PostgreSQL, on the other hand, supports shared read locks: the PESSIMISTIC_READ mode appends FOR SHARE to the SQL query. PESSIMISTIC_WRITE uses an exclusive write lock with FOR UPDATE. On MySQL, PESSIMISTIC_READ translates to LOCK IN SHARE MODE, and PESSIMISTIC_ WRITE to FOR UPDATE. Check your databa\u00aese dialect. This is configured with the getReadLockString() and getWriteLockString() methods. The duration of a pessimistic lock in JPA is a single database transaction. This means you can't use an exclusive lock to block concurrent access for longer than a single database transaction. When the database\u00aelock can't be obtained, an exception is thrown. Compare this with an optimistic approach, where Hibernate throws an exception at commit time, not when you query.",
  "page44": "With a pessimistic strategy, you know that you can read and write the data safely as soon as your locking query succeeds. With an optimistic approach, you hope for the best and may be surprised later, when you commit. You can configure how long the database will wait to obtain the\u00aelock and block the query in milliseconds with the javax.persistence.lock.timeout hint. As usual with hints, Hibernate might ignore it, depending on your database product. H2,for example, doesn't support lock timeouts per query, only a global lock timeout per connection (defaulting to 1 second). With some dialects, such as PostgreSQL and Oracle, a lock timeout of 0 appends the NOWAIT clause to the SQL string. We've shown the lock timeout hint applied to a Query. You can also set\u00aethe timeout hint for find() operations: When a lock can't be obtained, Hibernate throws either a javax.persistence.LockTimeoutException or a javax.persistence.PessimisticLockException. If Hibernate throws a PessimisticLockException, the transaction must be rolled back, and the unit of work ends. A timeout\u00aeexception, on the other hand, isn't fatal for the transaction, as explained in section 11.1.4. Which exception Hibernate throws again depends on the SQL dialect. For example, because H2 doesn't support per-statement lock timeouts, you always get a PessimisticLockException. You can use both the PES\u00aeSIMISTIC_READ and PESSIMISTIC_WRITE lock modes even if you haven't enabled entity versioning. They translate to SQL statements with database-level locks The special mode PESSIMISTIC_FORCE_INCREMENT requires versioned entities, however. In Hibernate, this mode executes a FOR UPDATE NOWAIT lock (or whatever your dialect supports; check its getForUpdateNowaitString() implementation). Then, immediately after the query returns, Hibernate increments the version and UPDATE (!) each returned entity instance.",
  "page45": "This indicates to any concurrent transaction that you have updated these rows, even if you haven't so far modified anydata. This mode is rarely useful, mostly for aggregate locking as explained in the section \"Forcing a version increment.\" If you enable pessimistic\u00aelocking, Hibernate locks only rows that correspond to entity instance state. In other words, if you lock an Item instance, Hibernate will lock its row in the ITEM table. If you have a joined inheritance mapping strategy, Hibernate will recognize this and lock the appropriate rows in super- and sub-tables. This also applies to any secondary table mappings of an entity. Because Hibernate locks entire rows, any relationship where the foreign key is in that row will also effectively be locked:\u00aeThe Item#seller association is locked if the SELLER_ID foreign key column is in the ITEM table. The actual Seller instance isn't locked! Neither are collections or other associations of the Item where the foreign key(s) are in other tables. With exclusive locking in the DBMS, you may experience transacti\u00aeon failures because you run into deadlock situations. Deadlocks can occur if your DBMS relies on exclusive locks to implement transaction isolation. Consider the following unit of work, updating two Itementity instances in a particular order: With a deadlock, both transactions are blocked and can't move\u00aeforward, each waiting for a lock to be released. The chance of a deadlock is usually small, but in highly concurrent applications, two Hibernate applications may execute this kind of interleaved update.",
  "page46": "Note that you may not see deadlocks during testing (unless you write the right kinds of tests). Deadlocks can suddenly appear when the application has to handle a high transaction load in production. Usually the DBMS terminates one of the deadlocked transactions after a timeout pe\u00aeriod and fails; the other transaction can then proceed. Alternatively, depending on the DBMS, the DBMS may detect a deadlock situation automatically and immediately abort one of the transactions. You should try to avoid transaction failures, because they're difficult to recover from in application code. One solution is to run the database connectionin serializable mode when updating a single row locks the entire table. The concurrent transaction has to wait until the first transaction\u00aecompletes its work. Alternatively, the first transaction can obtain an exclusive lock on all data when you SELECT the data, as shown in the previous section. Then any concurrent transaction also has to wait until these locks are released. An alternative pragmatic optimization that significantly reduces theprob\u00aeability of deadlocks is to order the UPDATE statements by primary key value: Hibernate should always update the row with primary key 1 before updating row 2, no matter in what order the data was loaded and modified by the application. You can enable this optimization for the entire persistence unit with the c\u00aeonfiguration property hibernate.order_updates. Hibernate then orders all UPDATE statements it executes in ascending order by primary key value of the modified entity instances and collection elements detected during flushing.",
  "page47": "(As mentioned earlier, make sure you fully understand the transactional and locking behavior of your DBMS product. Hibernate inherits most of its transaction guarantees from the DBMS; for example, your MVCC database product may avoid read locks but probably depends on exclusive lo\u00aecks for writer isolation, and you may see deadlocks.) We didn't have an opportunity to mention the EntityManager#lock() method. It accepts an already-loaded persistent entity instance and a lock mode. It performs the same locking you've seen with find() and a Query, except that it doesn't load the instance. Additionally, if a versioned entity is being locked pessimistically, the lock() method performs an immediate version check on the database and potentially throws an Opti\u00aemisticLockException. If the database representation is no longer present, Hibernate throws an EntityNotFoundException. Finally, the EntityManager#refresh() method also accepts a lock mode, with the same semantics. We've now covered concurrency control at the lowest level the database and the optimistic an\u00aed pessimistic locking features of JPA. We still have one more aspect of concurrency to discuss: accessing data outside of a transaction. A JDBC Connection is by default in auto-commit mode. This mode is useful for executing ad hoc SQL. Imagine that you connect to your database with an SQL console and that yo\u00aeu run a few queries, and maybe even update and delete rows. This interactive data access is ad hoc; most of the time you don't have a plan or a sequence of statements that you consider a unit of work. The default auto-commit mode on the database connection is perfect for this kind of data access after all, you don't want to typebegin transaction and end transaction for every SQL statement you write and execute.",
  "page48": "In autocommit mode, a (short) database transaction begins and ends for each SQL statement you send to the database. You're working effectively in nontransactional mode, because there are no atomicity or isolation guarantees for your session with the SQL console. (The only gua\u00aerantee is that a single SQL statement is atomic.) An application, by definition, always executes a planned sequence of statements. It seems reasonable that you therefore always create transaction boundaries to group your statements into units that are atomic and isolated from each other. In JPA, however, special behavior is associated with auto-commit mode, and you may need it to implement long-running conversations. You can access the database in auto-commit mode and read data. Consider\u00aethe following example, which loads an Item instance, changes its name, and then rolls back that change by refreshing No transaction is active when you create the EntityManager. The persistence context is now in a special unsynchronized mode; Hibernate won't flush automatically. You can access the database\u00aeto read data; this operation executes a SELECT, sent to the database in auto-commit mode. Usually Hibernate flushes the persistence context when you execute a Query. But because the context is unsynchronized, flushing doesn't occur, and the query returns the old, original database value. Queries with sc\u00aealar results aren't repeatable: you see whatever values are in the database and given to Hibernate inthe ResultSet. This also isn't a repeatable read if you're in synchronized mode.",
  "page49": "Retrieving a managed entity instance involves a lookup during JDBC result-set marshaling, in the current persistence context. The already-loaded Item instance with the changed name is returned from the persistence context; values from the database are ignored. This is a repeatable\u00aeread of an entity instance, even without a system transaction. If you try to flush the persistence context manually, to store the new Item#name, Hibernate throws a javax.persistence.TransactionRequiredException. You can't execute an UPDATE in unsynchronized mode, because you wouldn't be able to roll back the change. You can roll back the change you made with the refresh() method. It loads the current Item state from the database and overwrites the change you made in memory. Wi\u00aeth an unsynchronized persistence context, you read data in auto-commit mode with find(), getReference(), refresh(), or queries. You can load data on demand as well: proxies are initialized if you access them, and collections are loaded if you start iterating through their elements. But if you try to flush the p\u00aeersistence context or lock data with anything but LockModeType.NONE, a TransactionRequiredException will occur So far, auto-commit mode doesn't seem very useful. Indeed, many developers often rely on auto-commit for the wrong reasons: Manysmall per-statement database transactions (that's what auto-\u00aecommit means) won't improve the performance of your application. You won't improve the scalability of your application: a longer-running database transaction, instead of many small transactions for every SQL statement, may hold database locks for a longer time. This is a minor issue, because Hibernate writes to the database as late as possible within a transaction (flush at commit), so the database already holds write locks for a short time.",
  "page50": "You also have weaker isolation guarantees if the application modifies data concurrently. Repeatable reads based on read locks are impossible with auto-commit mode. (The persistence context cache helps here, naturally.) If your DBMS has MVCC (for example, Oracle, PostreSQL, Informi\u00aex, and Firebird), you likely want to use its capability for snapshot isolation to avoid unrepeatable and phantom reads. Each transaction gets its own personal snapshot of the data; you only see a (database-internal) version of the data as it was before your transaction started. With auto-commit mode, snapshot isolation makes no sense, because there is no transaction scope. Your code will be more difficult to understand. Any reader of your code now has to pay special attention to whether a\u00aepersistence context is joined with a transaction, or if it's in unsynchronized mode. If you always group operations within a system transaction, even if you only read data, everyone can follow this simple rule, and the likelihood of difficult-to-find concurrency issues is reduced. So, what are the benefits\u00aeof an unsynchronized persistence context? If flushing doesn't happenautomatically, you can prepare and queue modifications outside of a transaction. You can call persist() to save a transient entity instance with an unsynchronized persistence context. Hibernate only fetches a new identifier value, typic\u00aeally by calling a database sequence, and assigns it to the instance. The instance is now in persistent state in the context, but the SQL INSERT hasn't happened. Note that this is only possible with pre-insert identifier generators; see section 4.2.5.",
  "page51": " Hibernate provides the following ways to get data out of the database and into memory: Retrieving an entity instance by identifier is the most convenient method when the unique identifier value of an entity instance is known: for example, entityManager.find(Item.class, 123). You\u00aecan navigate the entity graph, starting from an already-loaded entity instance, by accessing the associated instances through property accessor methods such as someItem.getSeller().getAddress().getCity(), and so on. Elements ofmapped collections are also loaded on demand when you start iterating through a collection. Hibernate automatically loads nodes of the graph if the persistence context is still open. What and how data is loaded when you call accessors and iterate through collections\u00aeis the focus of this chapter. You can use the Java Persistence Query Language (JPQL), a full object-oriented query language based on strings such as select i from Item i where i.id?. The CriteriaQuery interface provides a type-safe and object-oriented way to perform queries without string manipulation. You can\u00aewrite native SQL queries, call stored procedures, and let Hibernate take care of mapping the JDBC result sets to instances of your domain model classes. In your JPA applications, you'll use a combination of these techniques, but we won't discuss each retrieval method in much detail in this chapter.\u00aeBy now you should be familiar with the basic Java Persistence API for retrieval by identifier. We keep our JPQL and CriteriaQuery examples as simple as possible, and you won't need the SQL query-mapping features. Because these query options are sophisticated, we'll explore them further in chapters 15 and 17.",
  "page52": "This chapter covers what happens behind the scenes when you navigate the graph of your domain model and Hibernate retrieves data on demand. In all the examples, we show you the SQL executed by Hibernate in a comment right immediately after the operation that triggered the SQL exec\u00aeution. What Hibernate loads depends on the fetch plan: you define the (sub)graph of the network of objects that should be loaded. Then you pick the right fetch strategy, defining how the data should be loaded. You can store your selection of plan and strategy as a fetch profile and reuse it. Defining fetch plans and what data should be loaded by Hibernate relies on two fundamental techniques: lazy and eager loading of nodes in the network of objects. At some point, you must decide what da\u00aeta should be loaded into memory from the database. When you execute entityManager.find(Item.class, 123), what is available in memory and loaded into the persistence context? What happens if you use EntityManager#getReference() instead? In your domain-model mapping, you define the global default fetch plan, with\u00aethe FetchType.LAZY and FetchType.EAGER options on associations and collections. This plan is the default setting for all operations involving your persistent domain model classes. It's always active when you load an entity instance by identifier and when you navigate the entity graph by following associ\u00aeations and iterating through persistent collections. Our recommended strategy is a lazy default fetch plan for all entities and collections.If you map all of your associations and collections with FetchType.LAZY, Hibernate will only load the data you're accessing at this time.",
  "page53": "Consider the getReference() method of the EntityManager API. In section 10.2.3, you had a first look at this operation and how it may return a proxy. Let's further explore this important feature and find out how proxies work: This code doesn't execute any SQL against the\u00aedatabase. All Hibernate does is create an Item proxy: it looks (and smells) like the real thing, but it's only a placeholder. In the persistence context, in memory, you now have this proxy available in persistent state, as shown in figure 12.1. The proxy is an instance of a runtime-generated subclass of Item, carrying the identifier value of the entity instance it represents. This is why Hibernate (in line with JPA) requires that entity classes have at least a public or protected no\u00ae-argument constructor (the class may have other constructors, too). The entity class and its methods must not be final; otherwise, Hibernate can't produce a proxy. Note that the JPA specification doesn't mention proxies; it's up to the JPA provider how lazy loading is implemented. If you call an\u00aey method on the proxy that isn't the \"identifier getter,\" you trigger initialization of the proxy and hit the database. If you call item.getName(), the SQL SELECT to load the Item will be executed. The previous example called item.getId() without triggering initialization because getId() is the\u00aeidentifier getter method in the given mapping; the getId() method was annotated with @Id. If @Id was on a field, then calling getId(), just like calling any other method, would initialize the proxy!",
  "page54": "The first two calls produce proxies of Item and User, respectively. Then the item and bidder association properties of the transient Bid are set with the proxies. The persist() call queues one SQL INSERT when the persistence context is flushed, and no SELECT is necessary to create\u00aethe new row in the BID table. All (foreign) key values areavailable as identifier values of the Item and User proxy. Runtime proxy generation as provided by Hibernate is an excellent choice for transparent lazy loading. Your domain model classes don't have to implement any special (super)type, as some older ORM solutions would require. No code generation or post-processing of bytecode is needed either, simplifying your build procedure. But you should be aware of some potentially neg\u00aeative aspects: Cases where runtime proxies aren't completely transparent are polymorphic associations that are tested with instanceof, a problem shown in section 6.8.1. With entity proxies, you have to be careful not to access fields directly when writing custom equals() and hashCode() methods, as discuss\u00aeed in section 10.3.2. Proxies can only be used to lazy-load entity associations. They can't be used to lazy load individual basic properties or embedded components, such as Item#description or User#homeAddress. If you set the @Basic(fetch FetchType.LAZY) hint on such a property, Hibernate ignores it; th\u00aee value is eagerly loaded when the owning entity instance is loaded. Although possible with bytecode instrumentation andinterception, we consider this kind of optimization to be rarely useful. Optimizing at the level of individual columns selected in SQL is unnecessary if you aren't working with (a) a significant number of optional/nullable columns or (b) columns containing large values that have to be retrieved on demand because of the physical limitations of your system.",
  "page55": "You map persistent collections with either @ElementCollection fora collection of elements of basic or embeddable type or with @OneToMany and @ManyToMany for manyvalued entity associations. These collections are, unlike @ManyToOne, lazy-loaded by default. You don't have to spe\u00aecify the FetchType.LAZY option on the mapping. When you load an Item, Hibernate doesn't load its lazy collection of images right away. The lazy bids one-to-many collection is also only loaded on demand, when accessed and needed: The find() operation loads the Item entity instance into the persistence context, as you can see in figure 12.2. The Item instance has a reference to an uninitialized User proxy: the seller. It also has a reference to an uninitialized Set of bids and an unini\u00aetialized List of images. Hibernate implements lazy loading (and dirty checking) of collections with its own special implementations called collection wrappers. Although the bids certainly look like a Set, Hibernate replaced the implementation with an org.hibernate.collection.internal PersistentSet while you we\u00aeren't looking It's not a HashSet, but it has the same behavior. That's why it's so important to program with interfaces in your domain model andonly rely on Set and not HashSet. Lists and maps work the same way. These special collections can detect when you access them and load their data\u00aeat that time. As soon as you start iterating through the bids, the collection and all bids made for the item are loaded.",
  "page56": "The fundamental problem with lazy loading is that the JPA provider must know when to load the seller of an Item or the collection of bids. Instead of runtime-generated proxies and smart collections, many other JPA providers rely exclusively on interception of method calls. For exa\u00aemple, when you call someItem.getSeller(), the JPA provider would intercept this call and load the User instance representing the seller. This approach requires special code in your Item class to implement the interception: the getSeller() method or the seller field must be wrapped. Because you don't want to write this code by hand, typically you run a bytecode enhancer (bundled with your JPA provider) after compiling your domain model classes. This enhancer injects the necessary inte\u00aerception code into your compiled classes, manipulating the fields and methods at the bytecode level. Let's discuss lazy loading based on interception with afew examples. First, you probably want to disable Hibernate's proxy generation: Instead, the proprietary LazyToOneOption.NO_PROXY setting tells H\u00aeibernate that the bytecode enhancer must add interception code for the seller property. Without this option, or if you don't run the bytecode enhancer, this association would be eagerly loaded and the field would be populated right away when the Item is loaded, because proxies for the User entity have be\u00aeen disabled. If you run the bytecode enhancer, Hibernate intercepts access of the seller field and triggers loading when you touch the field: This is less lazy than proxies. Remember that you could call User#getId() on a proxy without initializing the instance, as explained in the previous sections. With interception, any access of the seller field, and calling getSeller(), will trigger initialization.",
  "page57": " For lazy entity associations, proxies are usually a better choice than interception. A more common use case for interception is properties of basic type, such a String or byte[], with potentially large values. We might argue that LOBs (see \"Binary and large value types\"\u00aein chapter 5) should be preferred for large strings or binary data, but you might not want to have the java.sql.Blob or java.sql.Clob type in your domain model. With interceptionand bytecode enhancement, you can load a simple String or byte[] field on demand: The Item#description will be lazy loaded if you run the bytecode enhancer on the compiled class. If you don't run the bytecode enhancer for example, during development the String will be loaded together with the Item instance. I\u00aef you rely on interception, be aware that Hibernate will load all lazy fields of an entity or embeddable class, even if only one has to be loaded: When Hibernate loads the description of the Item, it loads the seller and any other intercepted field right away, too. There are no fetch groups in Hibernate at the\u00aetime of writing: it's all or nothing. The downside of interception is the cost of running a bytecode enhancer every time you build your domain model classes, and waiting for the instrumentation to complete. You may decide to skip the instrumentation during development, if the behavior of your applicatio\u00aen doesn't depend on an item's description load state. Then, when building the testing and production package, you can execute the enhancer.",
  "page58": "We leave it to you to decide whether you want interception for lazy loading in our experience, good use cases are rare. Note that we haven't talked about collection wrappers when discussing interception: although you could enable interception for collection fields, Hibernate\u00aewould still use its smart collection wrappers. The reason is that these collection wrappers are, unlike entity proxies, needed for other purposes besides lazy loading. For example, Hibernate relies on them to track additions and removals of collection elements when dirty checking. You can't disable the collection wrappers in your mappings; they're always on. (Of course, you never have to map a persistent collection; they're a feature, not a requirement. See our earlier discu\u00aession in section 7.1.) Persistent arrays, on the other hand, can only be lazily loaded with field interception they can't be wrapped like collections. You've now seen all available options for lazy loading in Hibernate. Next, we look at the opposite of on-demand loading: the eager fetching of data. We\u00ae've recommended a lazy default fetch plan, with FetchType.LAZY on all your association and collection mappings. Sometimes, although not often, you want the opposite: to specify that a particular entity association or collection should always be loaded. You want the guarantee that this data is available i\u00aen memory without an additional database hit. More important, you want a guarantee that, for example, you can access the seller of an Item once the Item instance is in detached state. When the persistence context is closed, lazy loading is no longer available. If seller were an uninitialized proxy, you'd get a LazyInitializationException when you accessed it in detached state.",
  "page59": "For data to be available in detached state, you need to either load it manually while the persistence context is still open or, if you always want it loaded, change your fetch plan to be eager instead of lazy. Let's assume that you always require loading of the seller and th\u00aee bids of an Item: Unlike FetchType.LAZY, which is a hint the JPA provider can ignore, a FetchType.EAGER is a hard requirement. The provider has to guarantee that the data is loaded and available in detached state; it can't ignore the setting. Consider the collection mapping: is it really a good idea to say, \"Whenever an item is loaded into memory, load the bids of the item right away, too\"? Even if you only want to display the item's name or find out when the auction\u00aeends, all bids will be loaded into memory. Always eager-loading collections, with FetchType.EAGER as the default fetch plan in the mapping, usually isn't a great strategy. You'll also see the Cartesian product problem appear if you eagerly load several collections, which we discuss later in this chapt\u00aeer. It's best if you leave collections as the default FetchType.LAZY. If you now find() an Item (or force the initialization of an Item proxy), both the seller and all the bids are loaded as persistent instances into your persistence context.",
  "page60": "Hibernate executes SQL SELECT statements to load data into memory. If you load an entity instance, one or more SELECT(s) are executed, depending on the number of tables involved and the fetching strategy you've applied. Your goal is to minimize the number of SQL statements an\u00aed to simplify the SQL statements so that querying can be as efficient as possible. Consider our recommended fetch plan from earlier in this chapter: everyassociation and collection should be loaded on demand, lazily. This default fetch plan will most likely result intoo many SQL statements, each loading only one small piece of data. This will lead to n 1 selects problems, and we discuss this issue first. The alternative fetch plan, using eager loading, will result in fewer SQL statements,\u00aebecause larger chunks of data are loaded into memory with each SQL query. You might then see the Cartesian product problem, as SQL result sets become too large. You need to find the middle ground between these two extremes: the ideal fetching strategy for each procedure and use case in your application. Like\u00aefetch plans, you can set a global fetching strategy in your mappings: the default settingthat is always active. Then, for a particular procedure, you might override the default fetching strategy with a custom JPQL, CriteriaQuery, or even SQL query. First, let's discuss the fundamental problems you see,\u00aestarting with the n 1 selects issue.",
  "page61": "This problem is easy to understand with some example code. Let's assume that you mapped a lazy fetch plan, so everything is loaded on demand. The following example code checks whether the seller of each Item has a username You see one SQL SELECT to load the Item entity instan\u00aeces. Then, while you iterate through all the items, retrieving each User requires an additional SELECT. This amounts to one query for the Item plus n queries depending on how many items you have and whether a particular User is selling more than one Item. Obviously, this is a very inefficient strategy if you know you'll access the seller of each Item. You can see the same issue with lazily loaded collections. The following example checks whether each Item has some bids: Again, if you\u00aeknow you'll access each bids collection, loading only one at a time is inefficient. If you have 100 items, you'll execute 101 SQL queries! With what you know so far, you might be tempted to change the default fetch plan in your mappings and put a FetchType.EAGER on your seller or bids associations.\u00aeBut doing so can lead to our next topic: the Cartesian product problem. If you look at your domain and data model and say, \"Every time I need an Item, Ialso need the seller of that Item,\" you can map the association with FetchType.EAGER instead of a lazy fetch plan. You want a guarantee that wheneve\u00aer an Item is loaded, the seller will be loaded right away youwant that data to be available when the Item is detached and the persistence context is closed.",
  "page62": "Eager fetching with the default JOIN strategy isn't problematic for @ManyToOne and @OneToOne associations. You can eagerly load, with one SQL query and JOINs, an Item, its seller, the User's Address, the City they live in, and so on. Even if you map all these association\u00aes with FetchType.EAGER, the result set will have only one row. Now, Hibernate has to stop following your FetchType.EAGER plan at some point. The number of tables joined depends on the global hibernate.max_fetch_depth configuration property. By default, no limit is set.Reasonable values are small, usually between 1 and 5. You may even disable JOIN fetching of @ManyToOne and @OneToOne associations by setting the property to 0. If Hibernate reaches the limit, it will still eagerly load the da\u00aeta according to your fetch plan, but with additional SELECT statements. (Note that some database dialects may preset this property: for example, MySQLDialect sets it to 2.) Eagerly loading collections with JOINs, on the other hand, can lead to serious performance issues. If you also switched to FetchType.EAGER\u00aefor the bids and images collections, you'd run into the Cartesian product problem. This issue appears when you eagerly load two collections with one SQL query and a JOIN operation. First, let's create such a fetch plan and then look at the SQL problem: It doesn't matter whether both collectio\u00aens are @OneToMany, @ManyToMany, or @ElementCollection. Eager fetching more than one collection at once with the SQL JOIN operator is the fundamental issue, no matter what the collection content is. If you load an Item, Hibernate executes the problematic SQL statement.",
  "page63": "As you can see, Hibernate obeyed your eager fetch plan, and you can access the bids and images collections in detached state. The problem is how they were loaded, with an SQL JOIN that results in a product. Look at the result set in figure 12.5. This result set contains many redu\u00aendant data items, and only the shaded cells are relevant for Hibernate. The Item has three bids and three images. The size of the product depends on the size of the collections you're retrieving: three times three is nine rows total. Now imagine that you have an Item with 50 bids and 5 images you'll see a result set with possibly 250 rows! You can create even larger SQL products when you write your own queries with JPQL or CriteriaQuery: imagine what happens if you load 500 items\u00aeand eager-fetch dozens of bids and images with JOINs. Considerable processing time and memory are required on the database server to create such results, which then must be transferred across the network. If you're hoping that the JDBC driver will compress the data on the wire somehow, you're probab\u00aely expecting too much from database vendors. Hibernate immediately removes all duplicates when it marshals the result set into persistent instances and collections; information in cells that aren't shaded in figure 12.5 will be ignored. Obviously, you can't remove these duplicates at the SQL level;\u00aethe SQL DISTINCT operator doesn't help here. Instead of one SQL query with an extremely large result, three separate queries would be faster to retrieve an entity instance and two collections at the same time. Next, we focus on this kind of optimization and how you find and implement the best fetch strategy.",
  "page64": "If Hibernate fetches every entity association and collection only on demand, many additional SQL SELECT statements may be necessary to complete a particular procedure. As before, consider a routine that checks whether the seller of each Item has a username. With lazy loading, this\u00aewould require one SELECT to get all Item instances and n more SELECTs to initialize the seller proxy of each Item. Hibernate offers algorithms that can prefetch data. The first algorithm we discuss is batch fetching, and it works as follows: if Hibernate must initialize one User proxy, go ahead and initialize several with the same SELECT. In other words, if you already know that there are several Item instances in the persistence context and that they all have a proxy applied to their se\u00aeller association, you may as well initialize several proxies instead of just one if you make the round trip to the database. Let's see how this works. First, enable batch fetching of User instances with a proprietary Hibernate annotation: This setting tells Hibernate that it may load up to 10 User proxies\u00aeif one has to be loaded, all with the same SELECT. Batch fetching is often called a blind-guess optimization, because you don't know how many uninitialized User proxies may be in a particular persistence context. You can't say for sure that 10 is an ideal value it's a guess. You know that inst\u00aeead of n 1 SQL queries, you'll now see n 1/10 queries, a significant reduction. Reasonable values are usually small, because you don't want to load too much data into memory either, especially if you aren't sure you'll need it. This is the optimized procedure, which checks the username of eachseller.",
  "page65": "Note the SQL query that Hibernate executes while you iterate through the items. When you call item.getSeller().getUserName() for the first time, Hibernate must initialize the first User proxy. Instead of only loading a single row from the USERS table, Hibernate retrieves several r\u00aeows, and up to 10 User instances are loaded. Once you access the eleventh seller, another 10 are loaded in one batch, and so on, until the persistence context contains no uninitialized User proxies. When you call item.getBids().size() for the first time while iterating, a whole batch of Bid collections are preloaded for the other Item instances. Batch fetching is a simple and often smart optimization that can significantly reduce the number of SQL statements that would otherwise be necess\u00aeary to initialize all your proxies and collections. Although you may prefetch data you won't need in the end and consume more memory, the reduction in database round trips can make a huge difference. Memory is cheap, but scaling database servers isn't. Another prefetching algorithm that isn't a\u00aeblind guess uses subselects to initialize many collections with a single statement. A potentially better strategy for loading all bids of several Item instances is prefetching with a subselect. To enable this optimization, add a Hibernateannotation to your collection mapping: Hibernate remembers the original\u00aequery used to load the items. It then embeds this initial query (slightly modified) in a subselect, retrieving the collection of bids for each Item.",
  "page66": "When you're trying to fetch several collections with one SQL query and JOINs, you run into the Cartesian product problem, as explained earlier. Instead of a JOIN operation, you can tell Hibernate to eagerly load data with additional SELECT queries and hence avoid large result\u00aes and SQL products with duplicates: Hibernate uses one SELECT to load a row from the ITEM table. It then immediately executes two more SELECTs: one loading a row from the USERS table (the seller) and the other loading several rows from the BID table (the bids). The additional SELECT queries aren't executed lazily; the find() method produces several SQL queries. You can see how Hibernate followed the eager fetch plan: all data is available in detached state. Still, all of these setti\u00aengs are global; they're always active. The danger is that adjusting one setting for one problematic case in your application might have negative side effects on some other procedure. Maintaining this balance can be difficult, so our recommendation is to map every entity association and collection as FetchT\u00aeype.LAZY, as mentioned before. A better approach is to dynamically use eager fetching and JOIN operations only when needed, for a particular procedure. As inthe previous sections, let's say you have to check the username of each Item#seller. With a lazy global fetch plan, load the data you need for this\u00aeprocedure and apply a dynamiceager fetch strategy in a query.",
  "page67": "The important keywords in this JPQL query are join fetch, telling Hibernate to use a SQL JOIN (an INNER JOIN, actually) to retrieve the seller of each Item in the same query. The same query can be expressed with the CriteriaQuery API instead of a JPQL string: Note that for collect\u00aeion fetching, a LEFT OUTER JOIN is necessary, because you also want rows from the ITEM table if there are no bids. We'll have much more to say about fetching with JPQL and CriteriaQuerylater in this book, in chapter 15. You'll see many more examplesthen of inner, outer, left, and right joins, so don't worry too much about these details now. Writing queries by hand isn't the only available option if you want to override the global fetch plan of your domain model dynamica\u00aelly. You can write fetch profiles declaratively Fetch profiles complement the fetching options in the query languages and APIs. They allow you to maintain your profile definitions in either XML or annotation metadata. Early Hibernate versions didn't have support for special fetch profiles, but today Hibern\u00aeate supports the following: Fetch profiles A proprietary API based on declaration of the profile with @org.hibernate.annotations.FetchProfile and execution with Session #enableFetchProfile(). This simple mechanism currently supports overriding lazy-mapped entity associations and collections selectively, enab\u00aeling a JOIN eager fetching strategy for a particular unit of work.",
  "page68": "Entity graphs Specified in JPA 2.1, you can declare a graph of entity attributes and associations with the @EntityGraph annotation. This fetch plan, or a combination of plans, can be enabled as a hint when executing EntityManager #find() or queries (JPQL, criteria). The provided g\u00aeraph controls what should be loaded; unfortunately it doesn't control how it should be loaded. It's fair to say that there is room for improvement here, and we expect future versions of Hibernate and JPA to offer a unified and more powerful API. Don't forget that you can externalize JPQL and SQL statements and move them to metadata (see section 14.4). A JPQL query is a declarative (named) fetch profile; what you're missing is the ability to overlay different plans easi\u00aely on the same base query. We've seen some creative solutions with string manipulation that are best avoided. With criteria queries, on the other hand, you already have the full power of Java available to organize your query-building code. Then the value of entity graphs is being able to reuse fetch plans\u00aeacross any kind of query. Let's talk about Hibernate fetch profiles first and how you can override a global lazy fetch plan for a particular unit of work.",
  "page69": "Hibernate fetch profiles are global metadata: they're declared for the entire persistence unit. Although you could place the @FetchProfile annotation on a class, we prefer it as package-level metadata in a package-info.java: Each profile has a name. This is a simple string is\u00aeolated in a constant. Each override in a profile names one entity association or collection. The only supported mode at the time of writing is JOIN. The profiles can now be enabled for a unit of work: The Item#seller is mapped lazy, so the default fetch plan only retrieves the Item instance. You need the Hibernate API to enable a profile. It's then active for any operation in that unit of work. The Item#seller is fetched with a join in the same SQL statement whenever an Item is loade\u00aed with this EntityManager. You can overlay another profile on the same unit of work. Now the Item#seller and the Item#bids collection are fetched with a join in the same SQL statement whenever an Item is loaded. Although basic, Hibernate fetch profiles can be an easy solution for fetching optimization in small\u00aeer or simpler applications. With JPA 2.1, the introduction of entity graphs enables similar functionality in a standard fashion An entity graph is a declaration of entity nodes and attributes, overriding or augmenting the default fetch plan when you execute an EntityManager#find() or with a hint on query oper\u00aeations. This is an example of a retrieval operation using an entity graph: The name of the entity graph you're using is Item, and the hint for the find() operation indicates it should be the load graph.",
  "page70": " This means attributes that are specified by attribute nodes of the entity graph are treated as FetchType.EAGER, and attributes that aren't specified are treated according to their specified or default FetchType in the mapping. This is the declaration of this graph and the d\u00aeefault fetch plan of the entity class: Entity graphs in metadata have names and are associated with an entity class; they're usually declared in annotations on top of an entity class. You can put them in XML if you like. If you don't give an entity graph a name, it gets the simple name of its owning entity class, which here is Item. If you don't specify any attribute nodes in the graph, like the empty entity graph in the last example, the defaults of the entity class are use\u00aed. In Item, all associations and collections are mapped lazy; this is the default fetch plan. Hence, what you've done so far makes little difference, and the find() operation without any hints will produce the same result: the Item instance is loaded, and the seller, bids, and images aren't. Alternat\u00aeively, you can build an entity graph with an API: This is again an empty entity graphwith no attribute nodes, given directly to a retrieval operation. Let's say you want to write an entity graph that changes the lazy default of Item#seller to eager fetching, when enabled.",
  "page71": " We explore the following data-filtering features and APIs: In section 13.1, you learn to react to state changes of an entity instance and cascade the state change to associated entities. For example, when a User is saved, Hibernate can transitively and automatically saveall relat\u00aeed BillingDetails. When an Item is deleted, Hibernate can delete all Bid instances associated with that Item. You can enable this standard JPA feature with special attributes in your entity association and collection mappings. The Java Persistence standard includes life cycle callbacks and event listeners. An event listener is a class you write with special methods, called by Hibernate when an entity instance changes state: for example, after Hibernates loads it or is about to delete it fr\u00aeom the database. These callback methods can also be on your entity classes and marked with special annotations. This gives you an opportunity to execute custom side effects when a transition occurs. Hibernate also has several proprietary extension points that allow interception of life cycle events at a lower l\u00aeevel within its engine, which we discuss in section 13.2. A common side effect is writing an audit log; such a log typically contains information about the data that was changed, when the change was made, and who made the modification. A moresophisticated auditing system might require storing several versions\u00aeof data and temporal views; you might want to ask Hibernate to load data \"as it was last week.\" This being a complex problem, we introduce Hibernate Envers in section 13.3, a subproject dedicated to versioning and auditing in JPA applications.",
  "page72": "In section 13.4, you see that data filters are also available as a proprietary Hibernate API. These filters add custom restrictions to SQL SELECT statements executed by Hibernate. Hence, you can effectively define a custom limited view of the data in the application tier. For exam\u00aeple, you could apply a filter that restricts loaded data by sales region, or any other authorization criteria. When an entity instance changes state for example, when it was transient and becomes persistent associated entity instances may also be included in this state transition. This cascading of state transitions isn't enabled by default; each entity instance has an independent life cycle. But for some associations between entities, you may want to implement fine-grained life cycle\u00aedependencies. For example, in section 7.3, you created an association between the Item and Bid entity classes. In this case, not only did you make the bids of an Item automatically persistent when they were added to an Item, but they were also automatically deleted when the owning Item was deleted. You effect\u00aeively made Bid an entity class that was dependent on another entity, Item. The cascading settings you enabled in this association mapping were CascadeType.PERSIST and CascadeType.REMOVE. We also talked about the special switch orphanRemoval and how cascading deletion at the database level (with the foreign k\u00aeey ON DELETE option) affects your application. You should review this association mapping and its cascading settings; we won't repeat it here. In this section, we look at some other, rarely used cascading options.",
  "page73": "If you're curious, you'll find more cascading options defined in the org.hibernate .annotations.CascadeType enumeration. Today, though, the only interesting option is REPLICATE and the Session#replicate() operation. All other Session operations have a standardized equiva\u00aelent or alternative on the EntityManager API, so you can ignore these settings. We've already covered the PERSIST and REMOVE options. Let's look at transitive detachment, merging, refreshing, and replication. Let's say you want to retrieve an Item and its bids from the database and work with this data in detached state. The Bid class maps this association with an @ManyToOne. It's bidirectional with this @OneToMany collection mapping in Item: Transitive detachment and m\u00aeerging is enabled with the DETACH and MERGE cascade types. Now you load the Item and initialize its bids collection: The EntityManager#detach() operation is cascaded: it evicts the Item instance from the persistence context as well as all bids in the collection. If the bids aren't loaded, they aren't\u00aedetached. (Of course, you could have closed the persistence context, effectively detaching all loaded entity instances.) In detached state, you change the Item#name, create a new Bid, and link it with the Item: Because you're working with detached entity state and collections, you have to pay extra atte\u00aention to identity and equality. As explained in section 10.3, you should override the equals() and hashCode() methods on the Bid entity class: Two Bid instances are equal when they have the same amount and are linked with the same Item. After you're done with your modifications in detached state, the next step is to store the changes. Using a new persistence context, merge the detached Item and let Hibernate detect the changes.",
  "page74": "Hibernate merges the detached item. First it checks whether the persistence context already contains an Item with the given identifiervalue. In this case, there isn't any, so the Item is loaded from the database. Hibernate is smart enough to know that it will also need the bi\u00aeds during merging, so it fetches them right away in the same SQL query. Hibernate then copies the detached item values onto the loaded instance, which it returns to you in persistent state. The same procedure is applied to every Bid, and Hibernate will detect that one of the bids is new. Hibernate made the new Bid persistent during merging. It now has an identifier value assigned. When you flush the persistence context, Hibernate detects that the name of the Item changed during merging. T\u00aehe new Bid will also be stored. Cascaded merging with collections is a powerful feature; consider how much code you would have to write without Hibernate to implement this functionality. The User entity class has a one-to-many relationship with BillingDetails: each user of the application may have several cred\u00aeit cards, bank accounts, and so on. If you aren't familiar with the BillingDetails class, review the mappings in chapter 6. You can map the relationship between User and BillingDetails as a unidirectional one-to-many entity association (there is no @ManyToOne): The cascading options enabled for this ass\u00aeociation are PERSIST and REFRESH. The PERSIST option simplifies storing billing details; they become persistent when you add an instance of BillingDetails to the collection of an already persistent User.",
  "page75": " In section 18.3, we'll discuss an architecture where the persistence context may be open for a long time, leading to managed entity instances in the context becoming stale. Therefore, in some long-running conversations, you'll want to reload them from the database. The\u00aeREFRESH cascading option ensures that when you reload the state of a User instance, Hibernate will also refresh the state of each BillingDetails instance linked to the User: An instance of User is loaded from the database. Its lazy billingDetails collection is initialized when you iterate through the elements or when you call size(). When you refresh() the managed User instance, Hibernate cascades the operation to the managed BillingDetails and refreshes each with an SQL SELECT. If none\u00aeof these instances remain in the database, Hibernate throws an EntityNotFoundException. Then, Hibernate refreshes the User instance and eagerly loads the entire billingDetails collection to discover any new BillingDetails. This is a case where Hibernate isn't as smart as it could be. First it executes an S\u00aeQL SELECT for each BillingDetails instance in the persistence context and referenced by the collection. Then it loads the entire collection again to find any added BillingDetails. Hibernate could obviously do this with one SELECT. The last cascading option is for the Hibernate-only replicate() operation.",
  "page76": "You first saw replication in section 10.2.7. This nonstandard operation is available on the Hibernate Session API. The main use case is copying data from one database into another. Consider this many-to-one entity association mapping between Item and User: When you call replicate\u00ae() on the detached Item, Hibernate executes SQL SELECT statements to find out whether the Item and its seller are already present in the database. Then, on commit, when the persistence context is flushed, Hibernate writes the values of the Item and the seller into the target database. In the previous example, these rows were already present, so you see an UPDATE of each, overwriting the values in the database. If the target database doesn't contain the Item or User, two INSERTs are ma\u00aede. The last cascading option we're going to discuss is a global setting, enabling transitive persistence for all entity associations. An object persistence layer is said to implement persistence by reachability if any instance becomes persistent whenever the application creates a reference to the instanc\u00aee from another instance that is already persistent. In the purest form of persistence by reachability, the database has some top-level or root object from which all persistent objects are reachable. Ideally, an instance should become transient and be deleted from the database if it isn't reachable via re\u00aeferences from the root persistent object.",
  "page77": "Neither Hibernate nor any other ORM solutions implement this. In fact, there is no analogue of the root persistent object in any SQL database, and no persistent garbage collector can detect unreferenced instances. Object-oriented (network) data stores may implement a garbage-colle\u00aection algorithm, similar to the one implemented for in-memory objects by the JVM; but this option isn't available in the ORM world, and scanning all tables for unreferenced rows won't perform acceptably. Still, there is some value in the concept of persistence by reachability. It helps you make transient instances persistent and propagate their state to the database without many calls to the persistence manager. You can enable cascaded persistence for all entity associations in\u00aeyour orm.xml mapping metadata, as a default setting of the persistence unit: Hibernate now considers all entity associations in the domain model mapped by this persistence unit as CascadeType.PERSIST. Whenever you create a reference from an already persistent entity instance to a transient entity instance, Hib\u00aeernate automatically makes that transient instance persistent. Cascading options are effectively predefined reactions to life cycle events in the persistence engine. If you need to implement a custom procedure when data is stored or loaded, you can implement your own event listeners and interceptors. In this\u00aesection, we discuss three different APIs for custom event listeners and persistence life cycle interceptors available in JPA and Hibernate. You can Use the standard JPA life cycle callback methods and event listeners. Write a proprietary org.hibernate.Interceptor and activate it on a Session. Use extension points of the Hibernate core engine with the org.hibernate.event SPI. Let's start with the standard JPA callbacks. They offer easy access to persist, load, and remove life cycle events.",
  "page78": "Let's say you wantto send a notification email to a system administrator whenever a new entity instance is stored. First, write a life cycle event listener with a callback method, annotated with @PostPersist, as shown in the following listing An entity listener class must hav\u00aee either no constructor or a public no-argument constructor. It doesn't have to implement anyspecial interfaces. An entity listener is stateless; the JPA engine automatically creates and destroys it. You may annotateany method of an entity listener class as a callback method for persistence life cycle events. The notifyAdmin() method is invoked after a new entity instance is stored in the database. Because event listener classes are stateless, it can be difficult to get more contextu\u00aeal information when you need it. Here, you want the currently logged-in user and access to the email system to send a notification. A primitive solution is to use thread-local variables and singletons; you can find the source for CurrentUser and Mail in the example code A callback method of an entity listener c\u00aelass has a single Object parameter: the entity instance involved in the state change. If you only enable the callback for a particular entity type, you may declare the argument as that specific type. The callback method may have any kind of access; it doesn't have to be public. It must not be static or f\u00aeinal and return nothing. If a callback method throws an unchecked RuntimeException, Hibernate will abort the operation and mark the current transaction for rollback. If a callback method declares and throws a checked Exception, Hibernate will wrap and treat it as a RuntimeException.",
  "page79": "Let's assume that you want to write an audit log of data modifications in a separate database table. For example, you may record information about creation and update events for each Item. The audit log includes the user, the date and time of the event, what type of event occ\u00aeurred, and the identifier of the Item that was changed. Audit logs are often handled using database triggers. On the other hand, it's sometimes better for the application to take responsibility, especially if portability between different databases is required. You need several elements to implement audit logging. First, you have to mark the entity classes for which you want to enable audit logging. Next, you define what information to log, such as the user, date, time, and type of\u00aemodification. Finally, you tie it all together with an org.hibernate.Interceptor that automatically creates the audit trail. First, create a marker interface, Auditable: This interface requires that a persistent entity class expose its identifier with a getter method; you need this property to log the audit tr\u00aeail. Enabling audit logging for a particular persistent class is then trivial. You add it to the class declaration, such as for Item: You want to store an instance of AuditLogRecord whenever Hibernate inserts or updates an Item in the database. A Hibernate interceptor can handle this automatically. Instead of\u00aeimplementing all methods in org.hibernate.Interceptor, extend the EmptyInterceptor and override only the methods you need, as shown next.",
  "page80": "You need to access the database to write the audit log, so this interceptor needs a Hibernate Session. You also want to store the identifier of the currently logged-in user in each audit log record. The inserts and updates instance variables are collections where this interceptor\u00aewill hold its internal state. This method is called when an entity instance is made persistent. This method is called when an entity instance is detected as dirty during flushing of the persistence context. The interceptor collects the modified Auditable instances in inserts and updates. Note that in onSave(), there may not be an identifier value assigned to the given entity instance. Hibernate guarantees to set entity identifiers during flushing, so the actual audit log trail is written\u00aein the postFlush() callback, which isn't shown in listing 13.2: This method is called after flushing of the persistence context is complete. Here, you write the audit log records for all insertions and updates you collected earlier. You can't access the original persistence context: the Session that\u00aeis currently executing this interceptor. The Session is in a fragile state during interceptor calls. Hibernate lets you create a new Session that inherits some information from the original Session with the sessionWithOptions() method. The new temporary Session works with the same transaction anddatabase con\u00aenection as the original Session. You store a new AuditLogRecord for each insertion and update using the temporary Session. You flush and close the temporary Session independently fromthe original Session You're now ready to enable this interceptor with a Hibernate property when creating an EntityManager.",
  "page81": "This EntityManager now has an enabled AuditLogInterceptor, but the interceptor must also be configured with the current Session and logged-in user identifier. This involves some typecasts to access the Hibernate API: The EntityManager is now ready foruse, and an audit trail will b\u00aee written whenever you store or modify an Item instance with it. Hibernate interceptors are flexible, and, unlike JPA event listeners and callback methods, you have access to much more contextual information when an event occurs. Having said that, Hibernate allows you to hook even deeper into its core with the extensible event system it's based on. The Hibernate core engine is based on a model of events and listeners. For example, if Hibernate needs to save an entity instance, it tr\u00aeiggers an event. Whoever listens to this kind of event can catch it and handle saving the data. Hibernate therefore implements all of its core functionality as a set of default listeners, which can handle all Hibernate events. Hibernate is open by design: you can write and enable your own listeners for Hiberna\u00aete events. You can either replace the existing default listeners or extend them and execute a side effect or additional procedure. Replacing the event listeners is rare; doing so implies that your own listener implementation can take care of a piece of Hibernate core functionality. Essentially, all the metho\u00aeds of the Session interface (and its narrower cousin, the EntityManager) correlate to an event. The find() and load() methods trigger a LoadEvent, and by default this event is processed with the DefaultLoadEventListener.",
  "page82": "A custom listener should implement the appropriate interface for the event it wants to process and/or extend one of the convenience base classes provided by Hibernate, or any of the default event listeners. Here's an example of a custom load event listener. This listener perf\u00aeorms custom authorizationcode. A listener should be considered effectively a singleton, meaning it's shared between persistence contexts and thus shouldn't save any transaction-related state as instance variables. For a list of all events and listener interfaces in native Hibernate, see the API Javadoc of the org.hibernate .event package. You enable listeners for each core event in your persistence.xml, in a <persistenceunit>: The property name of the configuration setting alway\u00aes startswith hibernate .ejb.event, followed by the type of event you want to listen to. You can find a list of all event types in org.hibernate.event.spi.EventType. The value of the property can be a comma-separated list of listener class names; Hibernate will call each listener in the specified order. You rar\u00aeely have to extend the Hibernate core event system with your own functionality. Most of the time, an org.hibernate.Interceptor is flexible enough. It helps to have more options and to be able to replace any piece of the Hibernate core engine in a modular fashion. The audit-logging implementation you saw in t\u00aehe previous section was very simple. If you need to log more information for auditing, such as the actual changed property values of an entity, consider Hibernate Envers.",
  "page83": "Envers is a project of the Hibernate suite dedicated to audit logging and keeping multiple versions of data in the database. This is similar to version control systems you may already be familiar with, such as Subversion and Git. With Envers enabled, a copy of your data is automa\u00aetically stored in separate database tables when you add, modify, or delete data in the main tables of the application. Envers internally uses the Hibernate event SPI you saw in the previous section. Envers listens to Hibernate events, and when Hibernate stores changes in the database, Envers creates a copy of the data and logs a revision in its own tables. Envers groups all data modifications in a unit of work that is, in a transaction as a change set with a revision number. You can writ\u00aee queries with the Envers API to retrieve historical data given a revision number or timestamp: for example, \"find all Item instances as they were last Friday.\" First you have to enable Envers in your application. Envers is available without further configuration as soon as you put its JAR file on you\u00aer classpath (or, as shown in the example code of this book, include it as a Maven dependency). You enable audit logging selectively for an entity class with the @org.hibernate .envers.Audited annotation. You've now enabled audit logging for Item instances and all properties of the entity. To disable audi\u00aet logging for a particular property,annotate it with @NotAudited. In this case, Envers ignores the bids but audits the seller. You also have to enable auditing with @Audited on the User class.",
  "page84": "Hibernate will now generate (or expect) additional database tables to hold historical data for each Item and User. Figure 13.1 shows the schema for these tables. The ITEM_AUD and USERS_AUD tables are where the modification history of Item and User instances is stored. When you mo\u00aedify data and commit a transaction, Hibernate inserts a new revision number with a timestamp into the REVINFO table. Then, for each modified and audited entity instance involved in the change set, a copy of its data is stored in the audit tables. Foreign keys on revision number columns link the change set together. The REVTYPE column holds the type of change: whether the entity instance was inserted, updated, or deleted in the transaction. Envers never automatically removes any revision in\u00aeformation or historical data; even after you remove() an Item instance, you still have its previous versions stored in ITEM_AUD. Let's run through some transactions to see how this works. In the following code examples, you see several transactions involving an Item and its seller, a User. You create and\u00aestore an Item and User, then modify both, and then finally delete the Item. You should already be familiar with this code. Envers automatically creates an audit trail when you workwith the EntityManager: Envers transparently writes the audit trail for this sequence of transactions by logging three change set\u00aes. To access this historical data, you first have to obtain the number of the revision, representing the change set you'd like to access.",
  "page85": "The main Envers API is AuditReader. It can be accessed with an EntityManager. Given a timestamp, you can find the revision number of a change set made before or on that timestamp. If you don't have a timestamp, you can get all revision numbers in which a particular audited\u00aeentity instance was involved. This operation finds all change sets where the given Item was created, modified, or deleted. In our example, we created, modified, and then deleted the Item. Hence, we have three revisions. If you have a revision number, you can get the timestamp when Envers logged the change set. We created and modified the User, so there are two revisions. In listing 13.5, we assumed that either you know the (approximate) timestamp for a transaction or you have the identif\u00aeier value of an entity so you can obtain its revisions. If you have neither, you may want to explore the audit log with queries. This is also useful if you have to show a list of all change sets in the user interface of your application The following code discovers all revisions of the Item entity class and loa\u00aeds each Item version and the audit log information for that change set: If you don't know modification timestamps or revision numbers, you can write a query with forRevisionsOfEntity() to obtain all audit trail details of a particular entity. This query returns the audit trail details as a List of Object\u00ae[]. Each result tuple contains the entity instance for a particular revision, the revision details (including revision number and timestamp), as well as the revision type. The revision type indicates why Envers created the revision, because the entity instance was inserted, modified, or deleted in the database.",
  "page86": "The find() method returns an audited entity instance version, given a revision. This operation loads the Item as it was after creation. This operation loads the Item after it was updated. Note how the modified seller of this change set is also retrieved automatically. In this re\u00aevision, the Item was deleted, so find() returns null. The example didn't modify the User in this revision, so Envers returns its closest historical revision. The AuditReader#find() operation retrieves only a single entity instance, like EntityManager#find(). But the returned entity instances are not in persistent state: the persistence context doesn't manage them. If you modify an older version of Item, Hibernate won't update the database. Consider the entity instances retu\u00aerned by the AuditReader API to be detached, or read-only. AuditReader also has an API for execution of arbitrary queries, similar to the native Hibernate Criteria API (see section 16.3). This query returns Item instances restricted to a particular revision and change set. You can add further restrictions to th\u00aee query; here the Item#name must start with \"Ba\". Restrictions can include entity associations: for example, you're looking for the revision of an Item sold by a particular User. You can order query results. You can paginate through large results. Envers supports projection. The following qu\u00aeery retrieves only the Item#name of a particular version: Finally, you may want to roll back an entity instance to an older version. This can be accomplished with the Session#replicate() operation and overwriting an existing row. The following example loads the User instance from the first change set and then overwrites the current User in the database with the older version.",
  "page87": "Envers will also track this change as an update in the audit log; it's just anothernew revision of the User instance. Temporal data is a complex subject, and we encourage you to read the Envers reference documentation for more information. Adding details to the audit log, su\u00aech as the user who made a change, isn't difficult. The documentation also shows how you can configure different tracking strategies and customize the database schema used by Envers. Next, imagine that you don't want to see all the data in your database. For example, the currently logged-in application user may not have the rights to see everything. Usually, you add a condition to your queries and restrict the result dynamically. This becomes difficult if you have to handle a con\u00aecern such as security, because you'd have to customize most of the queries in your application. You can centralize and isolate these restrictions with Hibernate's dynamic data filters. The first use case for dynamic data filtering relates to data security. A User in CaveatEmptor may have a ranking pr\u00aeoperty, which is a simple integer: Now assume that users can only bid on items that other users offer with an equal or lower rank. In business terms, you have several groups of users that are defined by an arbitrary rank (a number), and users can trade only with people who have the same or lower rank. To imp\u00aelement this requirement, you'd have to customize all queries that load Item instances from the database. You'd check whether the Item#seller you want to load has an equal or lower rank than the currently logged-inuser. Hibernate can do this work for you with a dynamic filter.",
  "page88": "First, you define your filter with a name and the dynamic runtime parameters it accepts. You can place the Hibernate annotation for this definition on any entity class of your domain model or in a package-info.javametadata file: This example names this filter limitByUserRank; note\u00aethat filter names must be unique in a persistence unit. It accepts one runtime argument of type int. If you have several filter definitions, declare them within @org.hibernate.annotations.FilterDefs. The filter is inactive now;nothing indicates that it's supposed to apply to Item instances. You must apply and implement the filter on the classes or collections you want to filter You want to apply the defined filter on the Item class so that no items are visible if the logged-in user d\u00aeoesn't have the necessary rank: The condition is an SQL expression that's passed through directly to the database system, so you can use any SQL operator or function. It must evaluate to true if a record should pass the filter. In this example, you use a subquery to obtain the rank of the seller of th\u00aee item.Unqualified columns, such as SELLER_ID, refer to the table mapped to the entity class. If the currently logged-in user's rank isn't greater than or equal to the rank returned by the subquery, the Item instance is filtered out. You can apply several filters by grouping them in an @org.hibernat\u00aee.annotations.Filters. A defined and applied filter, if enabled for a particular unit of work, filters out any Item instance that doesn't pass the condition. Let's enable it.",
  "page89": "You've defined a data filter and applied it to a persistent entity class. It's still not filtering anything it must be enabled and parameterized in the application for a particular unit of work, with the Session API: You enable the filter by name; the method returns a Fi\u00aelter on which you set the runtime arguments dynamically. You must set the parameters you've defined; here it's set to rank 0. This example then filters out Items sold by a User with a higher rank in this Session. Other useful methods of the Filter are getFilterDefinition() (which allows you to iterate through the parameter names and types) and validate() (which throws a HibernateException if you forget to set a parameter). You can also set a list of arguments with setParameterLis\u00aet(); this is mostly useful if your SQL restriction contains an expression with a quantifier operator (the IN operator, for example). Now, every JPQL or criteria query that you execute on the filtered persistence context restricts the returned Item instances: Note how Hibernate dynamically appends the SQL restr\u00aeiction conditions to the statement generated. When you first experiment with dynamic filters, you'll most likely run into an issue with retrieval by identifier. You might expect that em.find(Item.class, ITEM_ID) will be filtered as well. This is not the case, though: Hibernate doesn't apply filters\u00aeto retrieval by identifier operations. One of the reasons is that data-filter conditions are SQL fragments, and lookup by identifier may be resolved completely in memory, in the first-level persistence context cache. Similar reasoning applies to filtering of manyto-one or one-to-one associations.",
  "page90": " Common to all APIs, a query must be prepared in application code before execution. There are three distinct steps: Create the query, with any arbitrary selection, restriction, and projection of data that you want to retrieve. Prepare the query: bind runtime arguments to query p\u00aearameters, set hints, and set paging options. You can reuse the query with changing settings. Execute the prepared query against the database and retrieve the data. You can control how the query is executed and how data should be retrieved into memory (all at once or piecemeal, for example). Depending on the query options you use, your starting point for query creation is either the EntityManager or the native Session API. First up is creating the query. JPA represents a query with a java\u00aex.persistence.Query or javax.persistence .TypedQuery instance. You create queries with the EntityManager#createQuery() method and its variants. You can write the query in the Java Persistence Query Language (JPQL), construct it with the CriteriaBuilder and CriteriaQuery APIs, or use plain SQL. (There is also ja\u00aevax.persistence.StoredProcedureQuery, covered in section 17.4.) Hibernate has its own, older API to represent queries: org.hibernate.Query and org.hibernate.SQLQuery. We talk more about these in a moment. Let's start with the JPA standard interfaces and query languages. Say you want to retrieve all Item\u00aeentity instances from the database. With JPQL, this simple query string looks quite a bit like the SQL you know: The JPA provider returns a fresh Query; so far, Hibernate hasn't sent any SQL to the database. Remember that further preparation and execution of the query are separate steps.",
  "page91": "JPQL is compact and will be familiar to anyone with SQL experience. Instead of table and column names, JPQL relies on entity class and property names. Except for these class and property names, JPQL is case-insensitive, so it doesn't matter whether you write SeLEct or select.\u00aeJPQL (and SQL) query strings can be simple Java literals in your code, as you saw in the previous example. Alternatively, especially in larger applications, you can move the query strings out of your data-access code and into annotations or XML. A query is then accessed by name with EntityManager#createNamedQuery(). We discuss externalized queries separately later in this chapter; there are many options to consider. A significant disadvantage of JPQL surfaces as problems during refactori\u00aeng of the domain model: if you rename the Item class, your JPQL query will break. (Some IDEs can detectand refactor JPQL strings, though.) First you get a CriteriaBuilder from your EntityManager by calling getCriteriaBuilder(). If you don't have an EntityManager ready, perhaps because you want to create th\u00aee query independently from a particular persistence context, you may obtain the CriteriaBuilder from the usually globally shared EntityManagerFactory. You then use the builder to create any number of CriteriaQuery instances. Each CriteriaQuery has at least one root class specified with from(); in the last ex\u00aeample, that's Item.class. This is called selection; we'll talk more about it in the next chapter. The shown query returns all Item instances from the database.",
  "page92": "The CriteriaQuery API will appear seamless in your application, without string manipulation. It's the best choice when you can't fully specify the query at development time and the application must create it dynamically at runtime. Imagine that you have to implement a se\u00aearch mask in your application, with many check boxes, input fields, and switches the user can enable. You must dynamically create a database query from the user's chosen search options. With JPQL and string concatenation, such code would be difficult to write and maintain. You can write strongly typed CriteriaQuery calls, without strings, using the static JPA metamodel. This means your queries will be safe and included in refactoring operations, as already shown in the section \"\u00aeUsing a static metamodel\" in chapter 3. After execution of this SQL query, Hibernate reads the java.sql.ResultSet and creates a List of managed Item entity instances. Of course, all columns necessary to construct an Item must be available in the result, and an error is thrown if your SQL query doesn't\u00aereturn them properly. In practice, the majority of the queries in your application will be trivial easily expressed in JPQL or with a CriteriaQuery. Then, possibly during optimization, you'll find a handful of complex and performance-critical queries. You may have to use special and proprietary SQL key\u00aewords to control the optimizer of your DBMS product. Most developers then write SQL instead of JPQL and move such complex queries into an XML file, where, with the help of a DBA, you change them independently from Java code.Hibernate can still handle the query result for you; hence you integrate SQL into your JPA application.",
  "page93": "There is nothing wrong with using SQL in Hibernate; don't let some kind of ORM \"purity\" get in your way. When you have a special case, don't try to hide it, but rather expose and document it properly so the next engineer will understand what's going on. I\u00aen certain cases, it's useful to specify the type of data returned from aquery. A query has several aspects: it defines what data should be loaded from the database and the restrictions that apply, such as the identifier of an Item or the name of a User. When you write a query, you shouldn't code these arguments into the query string using string concatenation. You should use parameter placeholders instead and then bind the argument values before execution. This allows you to reus\u00aee the query with different argument values while keeping you're safe from SQL injection attacks. Depending on your user interface, you frequently also need paging. You limit the number of rows returned from the database by your query. For example, you may want to return only result rows 1 to 20 because yo\u00aeu can only show so much data on each screen, then a bit later you want rows 21 to 40, and so on. Let's start with parameter binding.",
  "page94": "You should never write code like this, because a malicious user could craft a search string to execute code on the database you didn't expect or want that is, by entering the value of searchString in a search dialog box as foo' and callSomeStoredProcedure() and 'bar''bar. As\u00aeyou cansee, the original searchString is no longer a simple search for a string but also executes a stored procedure in the database! The quote characters aren't escaped; hence the call to the stored procedure is another valid expression inthe query. If you write a query like this, you open a major security hole in your application by allowing the execution of arbitrary code on your database. This is an SQL injection attack. Never pass unchecked values from user input to the database\u00ae! Fortunately, a simple mechanism prevents this mistake. The JDBC API includes functionality for safely binding values to SQL parameters. It knows exactly what characters in the parameter value to escape so the previous vulnerability doesn't exist. For example, the databasedriver escapes the single-quote\u00aecharacters in the given searchString and no longer treats them as control characters but as a part of the search string value. Furthermore, when you use parameters, the database can efficiently cache precompiled prepared statements, improving performance significantly. There are two approaches to parameter b\u00aeinding: named and positional parameters. JPA support both options, but you can't use both at the same time for a particular query.",
  "page95": "A commonly usedtechnique to process large result sets is paging. Users may see the result of their search request (for example, for specific items) as a page. This page shows a limited subset (say, 10 items) at a time, and users can navigate to the next and previous pages manually\u00aeto view the rest of the result. The Query interface supports paging of thequery result. In this query, the requestedpage starts in the middle of the result set: Starting from the fortieth row, you retrieve the next 10 rows. The call to setFirstResults(40) starts the result set at row 40. The call to setMaxResults(10) limits the query result set to 10 rows returned by the database. Because there is no standard way to express paging in SQL, Hibernate knows the tricks to make this work effi\u00aeciently on your particular DBMS. It's crucially important to remember that paging operates at the SQL level, on result rows. Limiting a result to 10 rows isn't necessarily the same as limiting the result to 10 instances of Item! In section 15.4.5, you'll see some queries with dynamic fetching th\u00aeat can't be combined with row-based paging at the SQL level, and we'll discuss this issue again. You can even add this flexible paging option to an SQL query: Hibernate will rewrite your SQL query to include the necessary keywords and clauses for limiting the number of returned rows to the page you\u00aespecified. In practice, you frequently combine paging with a special count-query. If you show a page of items, you also let the user know the total count of items. In addition, you need this information to decide whether there are more pages to show and whether the user can click to the next page.",
  "page96": "Maintaining two almost identical queries is overhead you should avoid. A popular trick is to write only one query but execute it with a database cursor first to get the total result count: Unwrap the Hibernate API to use scrollable cursors. Execute the query with a database curs\u00aeor; this doesn't retrieve the result set into memory. Jump to the last row of the result in the database, and then get the row number. Because row numbers are zero-based, add 1 to get the total count of rows. You must close the database cursor. Execute the query again, and retrieve an arbitrary page of data. There is one significant problem with this convenient strategy: your JDBC driver and/or DBMS may not support database cursors. Even worse, cursors seem to work, but the data is\u00aesilently retrieved into application memory; the cursor isn't operating directly on the database. Oracle and MySQL drivers are known to be problematic, and we have more to say about scrolling and cursors in the next section. Later in this book, in section 19.2, we'll further discuss paging strategies\u00aein an application environment. Your query is now ready for execution. Once you've created and prepared a Query, you're ready to execute it and retrieve the result into memory. Retrieving the entire result set into memory in one go is the most common way to execute a query; we call this listing. Som\u00aee other options are available that we also discuss next, such as scrolling and iterating.",
  "page97": "Hibernate executes one or several SQL SELECT statements immediately, depending on your fetch plan. If you map any associations or collections with FetchType.EAGER, Hibernate must fetch them in addition to the data you want retrieved with your query. All data is loaded into memory,\u00aeand any entity instances that Hibernate retrieves are in persistent state and managed by the persistence context. Of course, the persistence context doesn't manage scalar projection results. The following query returns a List of Strings: With some queries, you know the result is only a single result for example, if you want only the highest Bid or only one Item. Plain JDBC provides a feature called scrollable result sets. This technique uses a cursor that the database management sys\u00aetem holds. The cursor points to a particular row in the result of a query, and the application can move the cursor forward and backward. You can even directly jump to a row with the cursor. One of the situations where you should scroll through the results of a query instead of loading them all into memory invo\u00aelves result sets that are too large to fit into memory. Usually you try to restrict the result further by tightening the conditions in the query. Sometimes this isn't possible, maybe because you need all the data but want to retrieve it in several steps. We'll show such a batch-processing routine in\u00aesection 20.1. JPA doesn't standardize scrolling through results with database cursors, so you need the org.hibernate.ScrollableResults interface available on the proprietary org.hibernate.Query.",
  "page98": "Start by creating an org.hibernate.Query and opening a cursor . You then ignore the first two result rows, jump to the third row , and get that row's first \"column\" value . There are no columns in JPQL, so this is the first projection element: here, i in the select clau\u00aese. More examples of projection are available in the next chapter. Always close the cursor before you end the database transaction! As mentioned earlier in this chapter, you can also unwrap() the Hibernate query API from a regular javax.persistence.Query you've constructed with CriteriaBuilder. A proprietary org.hibernate.Criteria query can also be executed with scrolling instead of list(); the returned ScrollableResults cursor works the same. The ScrollMode constants of the Hibern\u00aeate API are equivalent to the constants in plain JDBC. In the previous example, ScrollMode.SCROLL_INSENSITIVE means the cursor isn't sensitive to changes made in the database, effectively guaranteeing that no dirty reads, unrepeatable reads, or phantom reads can slip into your result set while you scroll.\u00aeOther available modes are SCROLL_SENSITIVE and FORWARD_ONLY. A sensitive cursor exposes you to committed modified data while the cursor is open; and with a forward-only cursor, you can't jump to an absolute position in the result. Note that the Hibernate persistence context cache still provides repeatabl\u00aee read for entity instances even with a sensitive cursor, so this setting can only affect modified scalar values you project in the result set. Be aware that some JDBC drivers don't support scrolling with database cursors properly, although it might seem to work. With MySQL drivers, for example, the drivers always retrieve the entire result set of a query into memory immediately; hence you only scroll through the result set in application memory.",
  "page99": "To get real row-by-row streaming of the result, you have to set the JDBC fetch size of the query to Integer .MIN_VALUE (as explained in section 14.5.4) and only use ScrollMode.FORWARD_ONLY. Check the behavior and documentation of your DBMS and JDBC driver before using cursors. An\u00aeimportant limitation of scrolling with a database cursor is that it can't be combined with dynamic fetching with the join fetch clause in JPQL. Join fetching works with potentially several rows at a time, so you can't retrieve data row by row. Hibernate will throw an exception if you try to scroll() a query with dynamic fetch clauses. Another alternative to retrieving all data at once is iteration. Let's say you know that most of the entity instances your query will retrie\u00aeve are already present in memory. They may be in the persistence context or in the second-level shared cache (see section 20.2). In such a case, it might make sense to iterate the query result with the proprietary org.hibernate.Query API: When you call query.iterate(), Hibernate executes your query and sends an\u00aeSQL SELECT to the database. But Hibernate slightly modifies the query and, instead of retrieving all columns from the ITEM table, only retrieves the identifier/primary key values. Then, every time you call next() on the Iterator, an additional SQL query is triggered and the rest of the ITEM row is loaded. O\u00aebviously, this will cause an n 1 selects problem unless Hibernate can avoid the additional queries on next(). This will be the case if Hibernate can find the item's data in either the persistence context cache or the second-level cache.",
  "page100": "The Iterator returned by iterate() must be closed. Hibernate closes it automatically when the EntityManager or Session is closed. If your iteration procedure exceeds the maximum number of open cursors in your database, you can close the Iterator manually with Hibernate.close(iter\u00aeator). Iteration is rarely useful, considering that in the example all auction items would have to be in the caches to make this routine perform well. Like scrolling with a cursor, you can't combine it with dynamic fetching and join fetch clauses; Hibernate will throw an exception if you try. So far, the code examples have all embedded query string literals in Java code. This isn't unreasonable for simple queries, but when you begin considering complex queries that must be spli\u00aet over multiple lines, it gets a bit unwieldy. Instead, you can give each query a name and move it into annotations or XML files. Externalizing query strings lets you store all queries related to a particular persistent class (or a set of classes) with the other metadata for that class. Alternatively, you can\u00aebundle your queries into an XML file, independent of any Java class. This technique is often preferred in larger applications; hundreds of queries are easier to maintain in a few well-known places rather than scattered throughout the code base in various classes accessing the database. You reference and acces\u00aes an externalized query by its name.",
  "page101": "Named queries are global that is, the name of a query is a unique identifier for a particular persistence unit or org.hibernate.SessionFactory. How and where they're defined, in XML files or annotations, is no concern of your application code. On startup, Hibernate loads nam\u00aeed JPQL queries from XML files and/or annotations and parses them to validate their syntax. (This is useful during development, but you may want to disable this validation in production, for a faster bootstrap, with the persistence unit configuration property hibernate.query.startup_check.)  Even the query language you use to write a named query doesn't matter. It can be JPQL or SQL. You can place a named query in any JPA <entity-mappings> element in your orm.xmlmetadata. In larger a\u00aepplications, we recommend isolating and separating all named queries into their own file. Alternatively, you may want the same XML mapping file to define the queries and a particular class. You should wrap the query text intoa CDATA instruction so any characters in your query string that may accidentally be con\u00aesidered XML (such as the less than operator) don't confuse the XML parser. We omit CDATA from most other examples for clarity. Named queries don't have to be written in JPQL. They may even be native SQL queries and your Java code doesn't need to know the different. This is useful if you think\u00aeyou may want to optimize your queries later by fine-tuning the SQL. It's also a good solution if you have to port a legacy application to JPA/Hibernate, where SQL code can be isolated from the hand-coded JDBC routines. With named queries, you can easily port the queries one by one to mapping files",
  "page102": "JPA supports named queries with the @NamedQuery and @NamedNativeQuery annotations. You can only place these annotations on a mapped class. Note that the query. name must again be globally unique in all cases; no class or package name is automatically prefixed to the query name. T\u00aehe class is annotated with and @NamedQueries containing an array of @NamedQuery. A single query can be declared directly; you don't need to wrap it in @NamedQueries. If you have an SQL instead of a JPQL query, use the @NamedNativeQuery annotation This registers your query with the persistence unit, the EntityManagerFactory, and make it reusable as a named query. The saved Query doesn't have to be a JPQL statement; you can also save a criteria or native SQL query. Typically, you r\u00aeegister your queries once, on startup of your application. We leave it to you to decide whether you want to use the named query feature. But we consider query strings in the application code (unless they're in annotations) to be the second choice; you should always externalize query strings if possible.In\u00aepractice, XML files are probably the most versatile option.  Finally, for some queries, you may need extra settings and hints. In this section, we introduce someadditional query options from the JPA standard and some proprietary Hibernate settings. As the name implies, you probably won't. need them rig\u00aeht away, so you can skip this section if you like and read it later as a reference. With Hibernate, this method has the same semantics and consequences as the setQueryTimeout() method on the JDBC Statement API. Note that a JDBC driver doesn't necessarily cancel the query precisely when the timeout occurs",
  "page103": "The JDBC specification says, \"Once the data source has had an opportunity to process the request to terminate the running command, a SQL Exception will. be thrown to the client \u2026.\" Hence, there is room for interpretation as to when exactly. the data source has an o\u00aepportunity to terminate the command. It might only be after the execution completes. You may want to test this with your DBMS product and driver. You can also specify this timeout hint as a global default property in persistence.xml as a property when creating the EntityManagerFactory or as a name query option. The Query#setHint() method then overrides this global default for a particular query Let's assume that you make modifications to persistent entity instances before execu\u00aeting a query. For example, you modify the name of managed Item instances. These modifications are only present in memory, so Hibernate by default flushes the persistence context and all changes to the database before executing your query. This guarantees that the query runs on current data and that\u00aeno conflict between the query. result and the in-memory instances can occur. This may be impractical at times, if you execute a sequence that consists of many query-modify-query-modify operations, and each query is retrieving a different data set than the one before. In other words, you sometimes know you do\u00aen't need to flush your modifications to the database before executing a query, because conflicting results aren't a problem. Note that the persistence context provides repeatable read for entity instances, so only scalar results of a query are a problem anyway. You can disable flushing of the persistence context before a query with the org.hibernate.flushMode hint on a Query and the value org.hibernate.Flush Mode.COMMIT ",
  "page104": "we talked about how you can reduce memory consumption and prevent long dirty-checking cycles. You can tell Hibernate that it should consider all entity instances returned by a query as read-only (although not detached) with a hint: All Item instances returned by this query are in\u00aepersistent state, but Hibernate doesn't enable snapshotfor automatic dirty checking in the persistence context. Hibernate doesn't persist any modifications automatically unless you disable read-only mode with session.setReadOnly(item, false). This hint may not result in any performance improvement if the driver doesn't implement this functionality. If it does, it can improve the communication between the JDBC client and the database by retrieving many rows in one bat\u00aech when the client (Hibernate) operates on a query result (that is, on a ResultSet). When you optimize an application, you often have to read complex SQL logs. We highly recommend that you enable the property hibernate.use_sql_comments in your persistence.xml configuration. Hibernate will then add an auto-gener\u00aeated comment to each SQL statement it writes to the logs. You can set a custom comment for a particular Query with a hint The hints you've been setting so far are all related to Hibernate or JDBC handling. Many developers (and DBAs) consider a query hint to be something completely different. In SQL, a q\u00aeuery hint is an instruction in the SQL statement for the optimizer of the DBMS. For example, if the developer or DBA thinks the execution plan selected by the database optimizerfor a particular SQL statement isn't the fastest, they use a hint. to force a different execution plan. Hibernate and Java Persistence don't support arbitrary SQL hints with an API; you'll have to fall back to native SQL and write your own SQL statement you can of course execute that statement with the provided APIs",
  "page105": " On the other hand, with some DBMS products, you can control the optimizer with an SQL comment at the beginning of an SQL statement. In that case, use the comment. hint as shown in the last example. In all previous examples, you've set the query hint directly on the Query i\u00aenstance. If you have externalized and named queries, you must set hints in annotations or XML Queries are the most interesting part of writing good data access code. A complex query may require a long time to get right, and its impact on the performance of the application can be tremendous. On the other hand, writing queries becomes. much easier with more experience, and what seemed difficult at first is only a matter of knowing the available query languages. This chapter covers the query\u00aelanguages available in JPA: JPQL and the criteria. query API. We always show the same query example with both languages/API, where the result of the queries is equivalent, We expect that you won't read this chapter just once but will rely on it as a reference to look up the correct syntax for a particular\u00aequery when coding your application. Hence, our writing style is less verbose, with many small code examples fordifferent. use cases. We also sometimes simplify parts of the CaveatEmptor application for better readability. For example, instead of referring to MonetaryAmount, we use a simple BigDecimal amount i\u00aen comparisons. Let's start with some query terminology. You apply selection to define where the data should be retrieved from, restriction to match records to a given criteria, and projection to select the data you want returned from a query. You'll find this chapter organized in this manner",
  "page106": "When we talk about queries in this chapter, we usually mean SELECT statements: operations that retrieve data from the database. JPA also supports UPDATE, DELETE, and even INSERT ... SELECT statements in JPQL, criteria, and SQL flavors, which we'll discuss in section 20.1. We\u00aewon't repeat those bulk operations here and will focus on SELECT statements. We start with some basic selection examples. First, when we say selection, we don't mean the SELECT clause of a query. We aren't talking about the SELECT statement as such, either. We are referring to selecting a relation. variable or, in SQL terms, the FROM clause. It declares where data for your query should come from: simplifying, which tables you \"select\" for a query. Alternatively, w\u00aeith classes instead of table names in JPQL: Hibernate understands queries with only a FROM clause or criterion. Unfortunately, the JPQL and criteria queries we've just shown aren't portable; they aren't JPA-compliant. The JPA specification requires that a JPQL query have a SELECT clause and\u00aethat. portable criteria queries call the select() method Usually, you don't want to retrieve all instances of a class from the database. You must be able to express constraints on the data returned by the query. We call this restriction. The WHERE clause declares restriction conditions in SQL and JPQL,\u00aeand the where() method is the equivalent in the criteria query API. You can include string literals in your statements and conditions, with single quotes. For date, time, and timestamp literals, use the JDBC escape syntax: ... where i.auctionEnd {d '2013-26-06'}. Note that your JDBC driver and DBMS define how to parse this literal and what other variations they support. Remember our advice from the previous chapter: don't concatenate unfiltered user input into your query string use parameter binding. ",
  "page107": "All expressions in the previous sections had only single-valued path expressions: user.username, item.buyNowPrice, and so on. You can also write path expressions that end in collections, and apply some operators and functions. For example, let's assume you want to restrict\u00aeyour query result to Category. instances that have an element in their items collection The citers path expression in the JPQL query terminates in a collection property: theitems of a Category. Note that it's always illegal to continue a path expression after a collection-valued property: you can't write c.items.buyNowPrice. For persistent maps, the special operators key(), value (), and entry() are available. Let's say you have a persistent map of Image embeddables for each\u00aeItem, as shown in section 7.2.4. The filename of each Image is the map key. The following query retrieves all Image instances with a .jpg suffix in the filename: The value() operator returns the values of the Map, and the key() operator returns the key set of the Map. If you want to return Map.Entry instances,\u00aeuse the entry() operator. An extremely powerful feature of the query languages is the ability to call functions in the WHERE clause. The following queries call the lower() function for case-insensitive searching Look at the summary of all available functions in table 15.2. For criteria queries, the equival\u00aeent methods are in CriteriaBuilder, with slightly different name formatting (using camelCase and no underscores)",
  "page108": "With Hibernate, any function call in the WHERE clause of a JPQL statement that isn't known to Hibernate is passed directly to the database as an SQL function call. For example, the following query returns all items with an auction period longer than one day Here you ca\u00aell the proprietary datediff() function of the H2 database system, it returns the difference in days between the creation date and the auction end date of an Item. This syntax only works in Hibernate though; in JPA, the standardized invocation syntax for calling arbitrary SQL functions The first argument of function() is the name of the SQL function you want to call in single quotes. Then, you append any additional operands for the actual function; you may have none or many. This is the sam\u00aee criteria query The Integer.class argument is the return type of the datediff() function and is irrelevant here because you aren't returning the result of the function call in a restriction. A function call in the SELECT clause would return the value to the Java layer; you can also invoke arbitrary SQL d\u00aeatabase functions in the SELECT clause. Before we talk about this clause and projection, let's see how results can be ordered. All query languages provide some mechanism for ordering query results. JPQL provides an ORDER BY clause, similar to SQL. The following query returns all users, ordered by\u00aeusername, ascending by default",
  "page109": "he JPA specification only allows properties/paths in the ORDER BY clause if the SELECT clause projects the same properties/paths. The following queries may be nonportable but work in Hibernate: select i.name from Item i order by i.buyNowPrice asc select i from Item i order by i.s\u00aeeller.username des Be careful with implicit inner joins in path expressions and ORDER BY: The last query returns only Item instances that have a seller. This may be unexpected, as the same query without the ORDER BY clause would retrieve all Item instances. (Ignoring for a moment that in our model the Item always has a seller, this issue is visible with optional references.) You'll find a more detailed discussion of inner joins and path. expressions later in this chapter. You now kno\u00aew how to write the FROM, WHERE, and ORDER BY clauses. You know how. to select the entities, you want to retrieve instances of and the necessary expressions. and operations to restrict and order the result. All you need now is the ability to project the data of this result to what you need in your application. I\u00aen simple terms, selection and restriction in a query is the process of declaring which. tables and rows you want to query. Projection is defining the \"columns\" you want returned to the application: the data you need. The SELECT clause in JPQL performs projections. As promised earlier,this criteria q\u00aeuery shows how you can add several Roots by calling the from() method several times. To add several elements to your projection, either call the tuple() methodof CriteriaBuilder, or the shortcut multiselect().",
  "page110": " Because this is a product, the result contains every possible combination of Item. and Bid rows found in the two underlying tables. Obviously, this query isn't useful, but you shouldn't be surprised to receive a collection of Object  as a query result. Hibernate manag\u00aees all Item and Bid entity instances in persistent state, in the persistence. context. Note how the HashSets filter out duplicate Item and Bid instances. Alternatively, with the Tuple API, in criteria queries you get typed access to the result list. Start by calling createTupleQuery() to create a CriteriaQuery<Tuple>. Then, refine the query definition by adding aliases for the entity classes The Object  returned by this query contain a Long at index 0, a String at index 1, and an Address\u00aeat index 2. The first two are scalar values; the third is an embedded class instance. None are managed entity instances! Therefore, these values aren't in any persistent state, like an entity instance would be. They aren't transactional and obviously aren't checked automatically for dirty state.\u00aeWe say that all of these values are transient. This is the kind of query you need to write for a simple reporting screen, showing all user names and their home addresses. You have now seen path expressions several times: using dot-notation, you can reference properties of an entity, such as User#username wi\u00aeth u.username. For a nested embedded property, for example, you can write the path u.homeAddress.city.zipcode.These are single-valued path expressions, because they don't terminate in a mapped collection property A more convenient alternative than Object[] or Tuple, especially for report queries, is dynamic instantiation in projections, which is next",
  "page111": "Let's say you have a reporting screen in your application where you need to show. some data in a list. You want to show all auction items and when each auction ends. You don't want to load managed Item entity instances, because no data will be modified: you only readdat\u00aea. First, write a class called ItemSummary with a constructor that takes a Long for the item's identifier, a String for the item's name, and a Date for the item's auction end timestamp: We sometimes call these kinds of classes data transfer objects (DTOs), because their main purpose is to shuttle data around in the application. The ItemSummary class isn't mapped to the database, and you can add arbitrary methods (getter, setter, printing of values) as needed by your re\u00aeporting user interface. Hibernate can directly return instances of Item Summary from a query with the new keyword in JPQL and the construct() method in criteria We sometimes call these kinds of classes data transfer objects (DTOs), because their main purpose is to shuttle data around in the application. The It\u00aeemSummary class isn't mapped to the database, and you can add arbitrary methods (getter, setter, printing of values) as needed by your reporting user interface. Hibernate can directly return instances of ItemSummary from a query with thenew keyword in JPQL and the construct() method in criteria ",
  "page112": "your DTO class doesn't have the right constructor, and you want to populate it from a query result through setter methods or fields, apply a ResultTransformer, as shown in in section 16.1.3. Later, we have more examples of aggregation and grouping. Next, we're going to\u00aelook at an issue with projection that is frequently confusing for many engineers: handling duplicates When you create a projection in a query, the elements of the result aren't guaranteed. to be unique. For example, item names aren't unique, so the following query may return the same name more than once: It's difficult to see how it could be meaningful to have two identical rows in a query result, so if you think duplicates are likely, you normally apply the DISTINCT keywor\u00aed or distinct() method This eliminates duplicates from the returned list of Item descriptions and translates. directly into the SQL DISTINCT operator. The filtering occurs at the database level. Later in this chapter, we show you that this isn't always the case. Earlier, you saw function calls in restrict\u00aeions, in the WHERE clause. You can also call functions in projections, to modify the returned data within the query If an Item doesn't have a buyNowPrice, a BigDecimal for the value zero is returned instead of null. Similar to coalesce() but more powerful are case/when expressions. The following query\u00aereturns the username of each User and an additional String with either \"Germany\", \"Switzerland For the built-in standard functions, refer to the tables in the previous section. Unlike function calls in restrictions, Hibernate won't pass on an unknown function call in a projection to the database as a plain direct SQL function call. Any function you'd like to call in a projection must be known to Hibernate and/or invoked with the special function() operation of JPQL.",
  "page113": " This projection returns the name of each auction Item and the number of days between item creation and auction end, calling the SQL datediff() function of the H2 database If instead you want to call a function directly, you give Hibernate the function's return type, so it c\u00aean parse the query. You add functions for invocation in projections by extending your configured org.hibernate.Dialect. The datediff() function is already registered for you in the H2 dialect. Then, you can either call it as shown with function(), which works in other JPA providers when accessing H2, or directly as datediff(), which most likely only works in Hibernate. Check the source code of the dialect for your database; you'll probably find many other proprietary SQL functions. al\u00aeready registered there. Furthermore, you can add SQL functions programmatically on boot to Hibernate by calling the method applySqlFunction() on a Hibernate MetadataBuilder See the Javadoc of SQLFunction and its subclasses for more information.  Next, we look at aggregation functions, which are the most usefu\u00ael functions in reporting queries. Reporting queries take advantage of the database's ability to perform efficient grouping and aggregation of data. For example, a typical report query would retrieve the highest initial item price in a given category. This calculation can occur in the database\u00ae, and you don't have to load many Item entity instances into memory. The aggregation functions standardized in JPA are count(), min(), max(), sum(), and avg(). This query returns a BigDecimal, because the amount property is of type BigDecimal. The sum() function also recognizes the BigInteger property type and returns Long for all other numeric property types",
  "page114": "When you call an aggregation function in the SELECT clause, without specifying any grouping in a GROUP BY clause, you collapse the results down to a single row, containing. the aggregated value(s). This means (in theabsence of a GROUP BY clause) any SELECT. clause that contains a\u00aen aggregation function must contain only aggregation function for more advanced statistics and for reporting, you need to be able to perform. grouping, which is up next JPA standardizes several features of SQL that are most commonly used for reporting although they're also used for other things. In reporting queries, you write the SELECT. clause for projection and the GROUP BY and HAVING clauses for aggregation. Just like in SQL, any property or alias that appears outside of an aggr\u00aeegate function. in the SELECT clause must also appear in the GROUP BY clause in this example, the u.lastname property isn't inside an aggregation function, so projected data has to be \"grouped by\" u.lastname. You also don't need to specify the property you want to count; the count(u)expressi\u00aeon is automatically translated into. count(u.id) When grouping, you may run into a Hibernate limitation. The following query is specification compliant but not properly handled in Hibernate The JPA specification allows grouping by an entity path expression, group by i. But Hibernate doesn't automatically\u00aeexpand the properties of Item in the generated SQL GROUP BY clause, which then doesn't match the SELECT clause. You have to expand the The JPA specification allows grouping by an entity path expression, group by i. But Hibernate doesn't automatically expand the properties of Item in the generated SQL GROUP BY clause, which then doesn't match the SELECT clause. You have to expand the fix",
  "page115": "Join operations combine data in two (or more) relations. Joining data in a query also enables you to fetch several associated instances and collections in a single query: for example, to load an Item and all its bids in one round trip to the database. We now show you how basic jo\u00aein operations work and how to use them to write such dynamic fetching strategies. Let's first look at how joins work in SQL queries, without JPA Let's start with the example we already mentioned: joining the data in the ITEM and BID tables, as shown in figure 15.1. The database contains three items: the first has three bids, the second has one bid, and the third has no bids. Note that we don't show. all columns; hence the dotted lines. What most people think of when they he\u00aear the word join in the context of SQL databases is an inner join. An inner join is the most important of several types of joins and the easiest to understand. Consider the SQL statement and result in figure 15.2. This SQL statement contains an ANSI-style inner join in the FROM clause. If you join the ITEM and\u00aeBID tables with an inner join, with the condition that the ID of an ITEM row must match the ITEM_ID value of a BID row, you get items combined with their bids in the result. Note thatthe result of this operation contains only items that have bids",
  "page116": "You can think of a join as working as follows: first you take a product of the two. tables, by taking all possible combinations of ITEM rows with BID rows. Second, you filter these combined rows with a join condition: the expression in the ON clause. (Any good database engine has\u00aemuch more sophisticated algorithms to evaluate a join; it usually doesn't build a memory-consuming product and then filter out rows.) The join condition is a Boolean expression that evaluates to true if the combined row is to be included in the result. It's crucial to understand that the join condition can be any expression that evaluates to true. You can join data in arbitrary ways; you aren't limited to comparisons of identifier values. For example, the join condit\u00aeion on i.ID b.ITEM_ID and amount > 100 would only include rows from the BID table that also have an AMOUNT greater than 100. that a BID has a reference to an ITEM row. This doesn't mean you can only join by comparing primary and foreign key columns. Key columns are of course the most common operands in a\u00aejoin condition, because you often want to retrieve related information together. If you want all items, not just the ones which have related bids, and NULL instead of bid data when there is no corresponding bid, then you write a (left) outer join",
  "page117": " In case of the left outer join, each row inthe (left) ITEM table that never satisfies the join condition is also included in the result, with NULL returned for all columns of BID. Right outer joins are rarely used; developers always think from left to right and put the \"dri\u00aeving\" table of a join operation first. In figure 15.4, you can see the same result with BID instead of ITEM as the driving table, and a right outer join. In SQL, you usually specify the join condition explicitly. Unfortunately, it isn't possible. to use the name of a foreign key constraint to specify how two tables are to be joined: select * from ITEM join BID on FK_BID_ITEM_ID doesn't work. You specify the join condition in the ON clause for an ANSI-style join or in the WH\u00aeERE. clause for a so-called theta-style join: select * from ITEM, BID b where i.ID b.ITEM_ID. This is an inner join; here you see that a product is created first in the FROM clause. We now discuss JPA join options. Remember thatHibernate eventually translates all queries into SQL, so even if the syntax is slig\u00aehtly different, you should always refer to the illustrations shown in this section and verify that you understand what the resulting SQL and result set looks like JPA provides four ways of expressing (inner and outer) joins in queries: An implicit association join with path expressions. An ordinary join in\u00aethe FROM clause with the join operator A fetch join in the FROM clause with the join operator and the fetch keyword for eager fetching A theta-style join in the WHERE clause. Let's start with implicit association joins. In JPA queries, you don't have to specify a join condition explicitly. Rather, you specify the name of a mapped Java class association. This is the same feature we'd prefer to have in SQL: a join condition expressed with a foreign key constraint name. Because you've mapped most, if not all, foreign key relationships of your database schema, you can use the names of these mapped associations in the query language. This is syntactical sugar, but it's convenient",
  "page118": " For example, the Bid entity class has a mapped many-to-one association named item, with the Item entity class. If you refer to this association in a query, Hibernate has enough information to deduce the join expression with a key column comparison. This helps make queries\u00aeless verbose and more readable. Earlier in this chapter, we showed you property path expressions, using dot-notation: single-valued path expressions such as user.homeAddress.zipcode and collectionvalued path expressions such as item.bids. You can create a path expression in an implicit inner join query The path b.item.name creates an implicit join on the many-to-one associations from Bid to Item the name of this association is item. Hibernate knows that you mapped this association w\u00aeith the ITEM_ID foreign key in the BID table and generates the SQL join condition accordingly. Implicit joins are always directed along many-to-one or one-to-one associations, never through a collection-valued association (you can't write item.bids.amount). This query joins rows from the BID, the ITEM, and\u00aethe USER tables. We frown on the use of this syntactic sugar for more complex queries. SQL joins are important, and especially when optimizing queries, you need to be able to see at a glance exactly how many of them there are How many joins are required to express such a query in SQL? Even if you get the an\u00aeswer right, it takes more than a few seconds to figure out. The answer is two. The generated SQL looks something like this: Alternatively, instead of joins with such complex path expressions, you can write ordinary joins explicitly in the FROM clause",
  "page119": "JPA differentiates between purposes you may have for joining. Suppose you're querying items; there are two possible reasons you may be interested in joining them with bids. You may want to limit the items returned by the query based on some criterion to apply to their bids.\u00aeFor example, you may want all items that have a bid of more than 100, which requires an inner join. Here, you aren't interested in items that have no bids. On the other hand, you may be primarily interested in the items but may want to execute an outer join just because you want to retrieve all bids for the queried items in a single SQL statement, something we called eager join fetching earlier. Remember that you prefer to map all associations lazily by default, so an eager fetch qu\u00aeery will override the default fetching strategy at runtime for a particular use case. Let's first write some queries that use joins for the purpose of restriction. If you want to retrieve Item instances and restrict the result to items that have bids with a certain amount, you have to assign an alias to a\u00aejoined association. Then yourefer to the alias in a WHERE clause to restrict the data you want This query assigns the alias b to the collection bids and limits the returned Item instances to those with Bid#amount greater than 100. So far, you've only written inner joins. Outer joins are mostly used for\u00aedynamic fetching, which we discuss soon. Sometimes, you want to write a simple query with an outer join without applying a dynamic fetching strategy. For example, the following query and retrieves items that have no bids, and items with bids of a minimum bid amount:",
  "page120": "This query returns ordered pairs of Item and Bid, in a List<Object[]>. The first thing that is new in this query is the LEFT keyword and JoinType.LEFT in the criteria query. Optionally you can write LEFT OUTER JOIN and RIGHT OUTER JOIN in JPQL, but we usually prefer the short fo\u00aerm. The second change is the additional join condition following the ON keyword. If instead you placethe amount > 100 expression into the WHERE clause, you restrict the result to Item instances that have bids. This isn't what you want here: you want to retrieve items and bids, and even items that don't have bids. If an item has bids, the bid amount must be greater than 100. By adding an additional join condition in the FROM clause, you can restrict the Bid instances and still re\u00aetrieve all Item instances, whether they have bids or not The SQL query will always contain the implied join condition of the mapped association, i.ID b.ITEM_ID. You can only append additional expressions to the join condition. JPA and Hibernate don't support arbitrary outer joins without a mapped en\u00aetity association or collection. Hibernate has a proprietary WITH keyword, it's the same as the ON keyword in JPQL. You may see it in older code examples, because JPA only recently standardized ON. You can write a query returning the same data with a right outer join, switching the driving table This ri\u00aeght outer join query is more important than you may think. Earlier in this book, we told you to avoid mapping a persistent collection whenever possible. If you don't have a one-to-many Item#bids collection, you need a right outer join to retrieve all Items and their Bid instances. You drive the query from the \"other\" side: the many-toone Bid#item. ",
  "page121": "All the queries you saw in the previous sections have one thing in common: the returned Item instances have a collection named bids. This @OneToMany collection, if mapped as FetchType.LAZY (the default for collections), isn't initialized, and an additional SQL statemen\u00aet is triggered as soon as you access it. The same is true for all single-valued associations, like the @ManyToOne association seller of each Item. By default, Hibernate generates a proxy and loads the associated User instance lazily and only on demand. What options do you have to change this behavior? First, you can change the fetch. plan in your mapping metadata and declare a collection or single-valued association. as FetchType.EAGER. Hibernate then executes the necessary SQL to g\u00aeuarantee that. the desired network of instances is loaded at all times. This also means a single JPA. query may result in several SQL operations! As an example, the simple query selects I from Item i may trigger additional SQL statements to load the bids of each Item, the seller of each Item, and so on. In cha\u00aepter 12, we made the case for a lazy global fetch plan in mapping metadata, where you shouldn't have FetchType.EAGER on association and collection mappings. Then, for a particular use case in your application, you dynamically override the lazy fetch plan and write a query that fetches the data you need a\u00aes efficiently as possible. For example, there is no reason you need several SQL statements to fetch all Item instances and to initialize their bids collections, or to retrieve the seller for each Item. You can do this at the same time, in a single SQL statement, with a join operation. Eager fetching of associated data is possible with the FETCH keyword in JPQL and the fetch() method in the criteria query API",
  "page122": "You've already seen the SQL query this produces and the result set in figure 15.3. This query returns a List<Item>; each Item instance has its bids collection fully initialized. This is different than the ordered pairs returned by the queries in the previous section! Be ca\u00aereful youmay not expect the duplicate results from the previous query: Make sure you understand why these duplicates appear in the result List. Verify the number of Item \"rows\" in the result set, as shown in figure 15.3. Hibernate preserves the rows as list elements; you may need the correct row count to make rendering a report table in the user interface easier. You can filter out duplicate Item instances by passing the result List through a LinkedHashSet, which doesn't al\u00aelow duplicate elements but preserves the order of elements. Alternatively, Hibernate can remove the duplicate elements with the DISTINCT operation and distinct() criteria method:Understand that in this case the DISTINCT operation does not execute in the database. There will be no DISTINCT keyword in the SQL sta\u00aetement. Conceptually, you can't remove the duplicate rows at the SQL ResultSet level. Hibernate performs deduplication in memory, just as you would manually with a LinkedHashSet This query returns a List<Item>, and each Item has its bids collection initialized. The seller of each Item is loaded as well.\u00aeFinally, the bidder of each Bid instance is loaded. You can do this in one SQL query by joining rows of the ITEM, BID, and USERS tables. If you write JOIN FETCH without LEFT, you get eager loading with an inner join (also if you use INNER JOIN FETCH) ",
  "page123": "An eager inner join fetch makes sense if there must be a fetched value: an Item must have a seller, and a Bid must have abidder. There are limits to how many associations you should eagerly load in one query and how much data you should fetch in one round trip. Consider the foll\u00aeowing query, which initializes the Item bids and Item images collections: This is a bad query, because it creates a Cartesian product of bids and images, with a potentially extremely large result set. We covered this issue in section 12.2.2. To summarize, eager dynamic fetching in queries has the following caveats: Never assign an alias to any fetch-joined association or collection for further restriction or projection. The query left join fetch i.bids b where b.amount ... is invalid. Yo\u00aeu can't say, \"Load the Item instances and initialize their bids collections, but only with Bid instances that have a certain amount.\" You can assign an alias to a fetch-joined association for further fetching: for example, retrieving the bidder of each Bid: left join fetch i.bids b join fet\u00aech b.bidder. You shouldn't fetch more than one collection; otherwise, you create a Cartesian product. You can fetch as many single-valued associations as you like without creating a product Queries ignore any fetching strategy you've defined in mapping metadata with @org.hibernate.annotations.Fetch.\u00aeFor example, mapping the bids collection with org.hibernate.annotations.FetchMode.JOIN has no effect on the queries you write. The dynamic fetching strategy of your query ignores the global fetching strategy. On the other hand, Hibernate doesn't ignore the mapped fetch plan: Hibernate always considers a FetchType.EAGER, and you may see several additional SQL statements when you execute your query.",
  "page124": "If you eager-fetch a collection, the List returned by Hibernate preserves the number of rows in the SQL result as duplicate references. You can filter out the duplicates in-memory either manually with a LinkedHashSet or with the special DISTINCT operation in the query. There is o\u00aene more issue to be aware of, and it deserves some special attention. You can't paginate a result set at the database level if you eagerly fetch a collection. For example, for the query select i from Item i fetch i.bids, how should Query#setFirstResult(21) and Query#setMaxResults(10) be handled? Clearly, you expect to get only 10 items, starting with item 21. But you also want to load all bids of each Item eagerly. Therefore, the database can't do the paging operatio\u00aen; you can't limit the SQL result to 10 arbitrary rows. Hibernate will execute paging in-memory if a collection is eagerly fetched in a query. This means all Item instances will be loaded into memory, each with the bids collection fully initialized. Hibernate then gives you the requested page of items: for\u00aeexample, only items 21 to 30. Not all items might fit into memory, and you probably expected the paging to occur in the database before it transmitted the result to the application! Therefore, Hibernate will log a warning message if your query contains fetch [collectionPath] and you call setFirstResult() or\u00aesetMaxResults(). We don't recommend the use of fetch [collectionPath] with setMaxResults() or setFirstResult() options. Usually there is an easier query you can write to get the data you want to render and we don't expect that you load data page by page to modify it. For example, if you want to showseveral pages of items and for each item the number of bids, writea report query ",
  "page125": "In traditional SQL, a theta-style join is a Cartesian product together with a join condition in the WHERE clause, which is appliedon the product to restrict the result. In JP queries, the theta-style syntax is useful when your join condition isn't a foreign key relationship\u00aemapped to a class association. For example, suppose you store the User's name in log records instead of mapping an association from LogRecord to User.The classes don't know anything about each other, because they aren't associated. You can then find all the Users and their Log Records with the following theta-style join The join condition here is a comparison of username, present as an attribute in both. classes. If both rows have the same username, they're joined (wit\u00aeh an inner join) in the result. The query result consists of ordered pairs You probably won't need to use the theta-style joins often. Note that it's currently not. possible in JPA to outer join two tables that don't have a mapped association thetastyle joins are inner joins. Another more\u00aecommon case for theta-style joins is comparisons of primary key or foreign key values to either query parameters or other primary or foreign key values in the WHERE clause: This query returns pairs of Item and Bid instances, where the bidder is also the seller. This is an important query in CaveatEmptor becau\u00aese it lets you detect people who bid on their own items. You probably should translate this query into a database constraint and not allow such a Bid instance to be stored.  The previous query also has an interesting comparison expression: i.seller b.bidder. This is an identifier comparison, our next topic",
  "page126": "In this query, i.seller refers to the SELLER_ID foreign key column of the ITEM table, referencing the USERS table. The alias u refers to the primary key of the USERS table. (on the ID column). Hence, this query has a theta-style join and is equivalent to the easier, readable alte\u00aernative A path expression ending with id is special in Hibernate: the id name always refers to the identifier property of an entity. It doesn't matter what the actual name of the property annotated with @Id is you can always reach it with entityAlias.id. That's why we recommend you always name the identifier property of yourentity classes id, to avoid confusion in queries. Note that this isn't a requirement or standardized in JPA; only Hibernate treats an id path element spe\u00aecially. You may also want to compare a key value to a query parameter, perhaps to find all Items for a given seller (a User) The first query pair uses an implicit table join; the second has no joins at all!  This completes our discussion of queries that involve joins. Our final topic is nesting selects within\u00aeselects: subselect Sub selects are an important and powerful feature of SQL. Asubselect is a select query embedded in another query, usually in the SELECT, FROM, or WHERE clause. JPA supports subqueries in the WHERE clause. Subselects in theFROM clause aren't supported because the query languages doesn\\u00aeu2019t have transitive closure. The result of a query may not be usable for further selection in a FROM clause. The query language also doesn't support subselects in the SELECT clause, but you map can subselects to derived properties with @org.hibernate.annotations.Formula, as shown in section 5.1.3. Subselects can be either correlated with the rest of the query or uncorrelated.",
  "page127": "The result of a subquery may contain either a single row or multiple rows. Typically, subqueries that return single rows perform aggregation. The following subquery returns the total number of items sold by a user; the outer query returns all users who have sold more than one ite\u00aem: The subquery in this example returns the maximum bid amount in the entire system; the outer query returns all bids whose amount is within one (U.S. dollar, Euro, and so on) of that amount. Note that in both cases, parentheses enclose the subquery in JPQL. This is always required Uncorrelated subqueries are harmless, and there is no reason not to use them when convenient. You can always rewrite them as two queries, because they don't. reference each other. You should think more care\u00aefully about the performance impact. of correlated subqueries. On a mature database, the performance cost of a simple correlated subquery is similar to the cost of a join. But it isn't necessarily possible to rewrite a correlated subquery using several separate queries.  If a subquery returns multiple rows\u00ae, you combine it with quantification The following quantifiers are standardized: ALL The expression evaluates to true if the comparison is true for all values in the result of the subquery. It evaluates to false if a single value of the subquery result fails the comparison test. ANY The expression evaluates\u00aeto true if the comparison is true for some (any) value in the result of the subquery. If the subquery result is empty or no value satisfies the comparison, it evaluates to false. The keyword SOME is a synonym for ANY. EXISTS Evaluates to true if the result of the subquery consists of one or more values",
  "page128": "This chapter explains query options that you may consider optional or advanced: transforming query results, filtering collections,and the Hibernate criteria query API. First, we discuss Hibernate's ResultTransformer API, with which you can apply a result transformer to a que\u00aery result to filter or marshal the result with your own code instead of Hibernate'sdefault behavior. In previous chapters, we always advised you to be careful when mapping collections, because it's rarely worth the effort. In this chapter, we introduce collection filters, a native Hibernate feature that makes persistent collections more valuable. Finally, we look at another proprietary Hibernate feature, the org.hibernate.Criteria query API, and some situations when you mi\u00aeght prefer it to the standard JPA query-by-criteria. Let's start with the transformation of query results. Transforming query results You can apply a result transformer to a query result so that you can filter or marshal the result with your own procedure instead of the Hibernate default behavior. Hibern\u00aeates default behavior provides a set of default transformers that you can replace and/or customize. The result you're going to transform is that of a simple query, but you need to access the native Hibernate API org.hibernate.Query through the Session, as shown in the following listing Each object array\u00aeis a \"row\" of the query result. Each element of that tuple can be accessed by index: here index 0 is a Long, index 1 a String, and index 2 a Date. The first result transformer we introduce instead returns a List of Lists",
  "page129": "In section 15.3.2, we showed how a query can return instances of a JavaBean dynamically by calling the ItemSummary constructor. In JPQL, you achieve this with the new operator. For criteria queries, you use the construct() method. The ItemSummary class must have a constructor tha\u00aet matches the projected query result. Alternatively, if your JavaBean doesn't have the right constructor, you can still instantiate and populate its values through setters and/or fields with the AliasToBeanResultTransformer.  You create the transformer with the JavaBean class you want to instantiate, here ItemSummary. Hibernate requires that this class either has no constructor or a public nonargument constructor. When transforming the query result, Hibernate looks for\u00aesetter methods and fields with the same names as the aliases in the query. The ItemSummary class must either have the fields itemId, name, unauctioned, or the setter methods setItemId(), setName(), and setAuctionEnd(). The fields or setter method parameters must be of the right type. If you have fields that map\u00aeto some query aliases and setter methods for the rest, that's fine too.  You should also know how to write your own ResultTransformer when none of the built-in ones suits you The built-in transformers in Hibernate aren't sophisticated; there isn't much difference between result tuples r\u00aeepresented as lists, maps, or object arrays.  Next, we show you how to implement a ResultTransformer. Let's assume that you want a List<ItemSummary> returned from the query shown in listing 16.1, but you can't let Hibernate create an instance of ItemSummary through reflection on a constructor. Maybe your ItemSummary class is predefined and doesn't have the right constructor, fields, and setter methods. Instead, you have an ItemSummaryFactory to produce instances of ItemSummary",
  "page130": "For each result \"row,\" an Object[] tuple must be transformed into the desired result value for that row. Here you access each projection element by index in the tuple array and then call the ItemSummaryFactory to produce the query result value. Hibernate passes the meth\u00aeod the aliases found in the query, for each tuple element. You don't need the aliases in this transformer, though. C You can wrap or modify the result list after transforming the tuples. Here you make. the returned List unmodifiable: ideal for a reporting screen where nothing should change the data. As you can see in the example, you transform query results in two steps: first you customize how to convert each \"row\" or tuple of the query result to whatever value you desire.\u00aeThen you work on the entire List of these values, wrapping or converting again. Next, we discuss another convenient Hibernate feature (where JPA doesn't have an equivalent): collection filters. In chapter 7, you saw reasons you should (or rather, shouldn't) map a collection in your Java domain model\u00ae. The main benefit of a collection mapping is easier access to data: you can call item.getImages() or item.getBids() to access all images and bids associated with an Item. You don't have to write a JPQL or criteria query; Hibernate will execute the query for you when you start iterating through the colle\u00aection elements. The most obvious problem with this automatic data access is that Hibernate will always write the same query, retrieving all images or bids for an Item. You can customize the order of collection elements, but even that is a static mapping. What would you do to render two lists of bids for an Item, in ascending and descending order by creation date? ",
  "page131": " Instead, you can use a Hibernate proprietary feature, collection filters, that makes writing these queries easier, using the mapped collection. Let's say you have a persistent Item instance in memory, probably loaded with the EntityManager API. You want to list all bids mad\u00aee for this Item but further restrict the result to bids made by a particular User. You also want the list sorted in descending order by Bid#amount.The session.createFilter() method accepts a persistent collection and a JPQL query fragment. This query fragment doesn't require a select or from clause; here it only has a restriction with the where clause and an order by clause. The alias this always refers to elements of the collection, here Bid instances. The filter created is an ordina\u00aery org.hibernate.Query, prepared with a bound parameter and executed with list(), as usual. Hibernate doesn't execute collection filters in memory. The Item bids collection may be uninitialized when you call the filter and, and if so, remains uninitialized. Furthermore, filters don't apply to transie\u00aent collections or query results. You may only apply them to a mapped persistent collection currently referenced by an entity instance managed by the persistence context. The term filter is somewhat misleading, because the result of filtering is a completely new and different collection; the original collectio\u00aen isn't touched. To the great surprise of everyone, including the designer of this feature, even trivial filters turn out to be useful. For example, you can use an empty query to paginate collection elements:",
  "page132": "Here, Hibernate executes the query, loading the collection elements and limiting the returned rows to two, starting with row zero of the result. Usually, you'd use an order by with paginated queries. You don't need a from clause in a collection filter, but you can have\u00aeone if that's your style. A collection filter doesn't even need to return elements of the collection being filtered. This next filter returns any Item sold by any of the bidders All this is a lot of fun, but the most important reason for the existence of collection filters is to allow your application to retrieve collection elements without initializing the entire collection. For large collections, this is important to achieve acceptable performance. The following query r\u00aeetrieves all bids made for the Item with an amount greater or equal to 100: Again, this doesn't initialize the Item#bids collection but returns a new collection. Before JPA 2, query-by-criteria was only available as a proprietary Hibernate API. Today, the standardized JPA interfaces are equally as powerfu\u00ael as the old org.hibernate.Criteria API, so you'll rarely need it. But several features are still only available in the Hibernate API, such as query-by-example and embedding of arbitrary SQL fragments. In the following section, you find a short overview of the org.hibernate .Criteria API and\u00aesome of its unique options Using the org.hibernate.Criteria and org.hibernate.Example interfaces, you can build queries programmatically by creating and combining org.hibernate.criterion.* instances. You see how to use these APIs and how to express selection, restriction, joins, and projection. We assume that you've read the previous chapter and that you know how these operations are translated into SQL.",
  "page133": ". All query examples shown here have an equivalent JPQL or JPA criteria example in the previous chapter, so you can easily flip back andforth if you need to compare all three APIs. Let's start with some basic selection examples. When you're ready to execute the query,\u00ae\"attach\" it to a Session with getExecutableCriteria(). Note that this is a unique feature of the Hibernate criteria API. With JPA, you always need at least an EntityManagerFactory to get a CriteriaBuilder. You can declare the order of the results, equivalent to an order by clause in JPQL. The following query loads all User instances sorted in ascending order by first and last name: In this example, the code is written in the fluent style (using method chaining); method's\u00aesuch as add Order() return the original org.hibernate.Criteria.  Next, we look at restricting the selected records The Restrictions interface is the factory for individual Criterion you can add to the Criteria. Attributes are addressed with simple strings, here Item#name with \"name\". You can also match subs\u00aetrings, similar to the like operator in JPQL. The following query loads all User instances with username starting with \"j\" or \"J\" A unique feature of the Hibernate Criteria API is the ability to write plain SQL fragments in restrictions. This query loads all User instances with a use\u00aername shorter than eight characters Hibernate sends the SQL fragment to the database as is. You need the {alias} placeholder to prefix any table alias in the final SQL; it always refers to the table the root entity is mapped to (USERS, in this case). You also apply a position parameter (named parameters aren't supported by this API) and specify its type as StandardBasicTypes.INTEGER",
  "page134": "The result of this query is a List of Object[], one array for each tuple. Each array contains a Long (or whatever the type of the user's identifier is), a String, and an Address. Just as with restrictions, you can add arbitrary SQL expressions and function calls to projecti\u00aeons This query returns a List of Strings, where strings have the form \"[Item name]:[Auction end date]\". The second parameter for the projection is the name of the alias(es) you used in the query: Hibernate needs this to read the value of the ResultSet. The type of each projected element/alias is also needed: here, StandardBasicTypes.STRING. Hibernate supports grouping and aggregation. This query counts users' last names This query returns all Bid instances of an\u00aey Item sold by User \"johndoe\" that doesn't have a buyNowPrice. The first inner join of the Bid#item association is made with createCriteria(\"item\") on the root Criteria of the Bid. This nested Criteria now represents the association path, on which anotherinner join is made with createCrit\u00aeeria(\"seller\"). Further restrictions are placed on each join Criteria; they will be combined with logical and in the where clause of the final SQL query. This query returns all Item instances, loads the Item#bids collection with an outer join, and loads Bid#bidder with an inner join. The Item#seller is also\u00aeloaded: because it can't be null, it doesn't matter whether an inner or outer join is used. As always, don't fetch several collections in one query, or you'll create a Cartesian products (see section 15.4.5).  Next, you see that subqueries with criteria also work with nested Criteria instances.",
  "page135": "The DetachedCriteria is a query that returns the number of items sold restricted by a given User. The restriction relies on the alias u, so this is a correlated subquery. The \"outer\" query then embeds the DetachedCriteria and provides the alias u. Note that the subquery\u00aeis theright operand of the lt() (less than) operation, which translates into 1 < ([Result of count query]) in SQL. Again, the position of the operands dictates that the comparison is based on geAll() (greater or equal than all) to find the bids with \"less or equal than 10\" amount. So far, there are a few good reasons to use the old org.hibernate.Criteria API. You really should use the standardized JPA query languages in new applications, though. The most interesting features of\u00aethe old proprietary API we've shown are embedded SQL expressions in restrictions and projections. Another Hibernate-only feature you may find interesting is query-by-example The idea behind example queries is that you provide an example entity instance, and Hibernate loads all entity instances that \"\u00aelook like the example.\" This can be convenient if you have a complex search screen in your user interface, because you don't have to write extra classes to hold the enteredsearch terms. Let's say you have a search form in your application where you can search for User instances by last name. Y\u00aeou can bind the form field for \"last name\" directly to the User#lastname property and then tell Hibernate to load \"similar\" User instances",
  "page136": "Create an \"empty\" instance of User as a template for your search, and set the property values you're looking for: people with the last name \"Doe\" Create an instance of Example with the template. This APIallows you to fine-tune the search. You want the cas\u00aee of the last name to be ignored, and a substring search, so \"Doe\", \"Dex\", or \"Doe Y\" will match. D The User class has a Boolean property called activated. As a primitive, it can't be null, and its default value is false, so Hibernate would include it in the search and only return users that aren't activated. You want all users, so tell Hibernate to ignore that property. E The Example is added to a Criteria as a restriction. Because you've writ\u00aeten the User entity class following JavaBean rules, binding it to a UI form should be trivial. It has regular getter and setter methods, and you can create an \"empty\" instance with the public no-argument constructor (remember our discussion of constructor design in section 3.2.3.) One obvious d\u00aeisadvantage of the Example API is that any string-matching options, such as ignoreCase() and enableLike(), apply to all string-valued properties of the template. If you searched for both last name and first name, both would be case insensitive substring matches. nsensitive substring matches. By default, all\u00aenon-null valued properties of the given entity template are added to the restriction of the example query. As shown in the last code snippet, you can manually exclude properties of the entity template by name with excludeProperty",
  "page137": "Other exclusion options are exclusion of zero-valued properties (such as int or long) with excludeZeroes() and disabling exclusion altogether with excludeNone(). If no properties are excluded, any null property of the template is added to the restriction in the SQL query with an\u00aeis null check. If you need more control over exclusion and inclusion of properties, you can extend Example and write your own PropertySelector: After adding an Example restriction to a Criteria, you can add further restrictions to the query. Alternatively, you can add multiple example restrictions to a single query. The following query returns all Item instances with names starting with \"B\" or \"b\" and a seller matching a User example: You used the ResultTransformer AP\u00aeI to write custom code to process a query result, returning a list of lists and a list of maps, and mapping aliases to bean properties. We covered Hibernate's collection-filtering interfaces as well as making better use of mapped persistent collections. You explored the older Hibernate Criteria query faci\u00aelity and when you might use it instead of the standardized criteria queries in JPA. We covered all the relational and Hibernate goodies using this API: selection and ordering, restriction, projection and aggregation, joins, subselects, and example queries.",
  "page138": "In this chapter, we cover customizing and embedding SQL in a Hibernate application. SQL was created in the 1970s, but ANSI didn't standardized it until 1986. Although each update of the SQL standard has seen new (and many controversial) features, every DBMS product that supp\u00aeorts SQL does so in its own unique way. The burden of portability is again on the database application developers. This is where Hibernate helps: its built-in query languages produce SQL that depends on the configured database dialect. Dialects also help produce all other automatically generated SQL (for example, when Hibernate has to retrieve a collection on demand). With a simple dialect switch, you can run your application on a different DBMS. Hibernate generates all SQL statements for\u00aeyou, for all create, read, update, and delete (CRUD) operations. Sometimes, though, you need more control than Hibernate and theJava Persistence API provide: you need to work at a lower level of abstraction. With Hibernate, you can write your own SQL statements: Fall back to the JDBC API, and work directly wit\u00aehthe Connection, PreparedStatement, and ResultSet interfaces. Hibernate provides the Connection, so you don't have to maintain a separate connection pool, and your SQL statements execute within the same (current) transaction. Write plain SQL SELECT statements, and either embed them wit\u00aehin your Java code or externalize them (in XML files or annotations) as named queries. You execute these SQLqueries with the Java Persistence API, just like a regular JPQL query. Hibernate can then transform the query result according to your mapping. This also works with stored procedure calls.",
  "page139": "Replace SQL statements generated by Hibernate with your own hand-written SQL. For example, when Hibernate loads an entity instance with em.find() or loads a collection on-demand, your own SQL query can perform the load. You can also write your own Data Manipulation Language (DML)\u00aestatements, such as UPDATE, INSERT, and DELETE. You might even call a stored procedure to preform a CRUD operation. You can replace all SQL statements automatically generated by Hibernate with custom statements. We start with JDBC fallback usage and then discuss Hibernate's automatic result-mapping capabilities. Then, we show you how to override queries and DML statements in Hibernate. Last, we discuss integration with stored database procedures. Sometimes you want Hibernate to\u00aeget out of the way and directly access the database through the JDBC API. To do so, you need a java.sql.Connection interface to write and execute your own PreparedStatement and direct access to your statement ResultSet. Because Hibernate already knows how to obtain and close database connections, it can\u00aeprovide your application with a Connection and release it when you're done. This functionality is available with the org.hibernate.jdbc.Work API, acallbackstyle interface. You encapsulate your JDBC \"work\" by implementing this interface; Hibernate calls your implementation providing a Con\u00aenection. The following example executes an SQL SELECT and iterates through the ResultSet For this \"work,\" an item identifier is needed, enforced with the final field and the constructor paramet",
  "page140": "The execute() method is called by Hibernate with a JDBC Connection. You don't have to close the connection when you're done. D You have to close and release other resources you've obtained, though, such as the PreparedStatement and ResultSet In this case, Hibernate\u00aehas already enlisted the JDBC Connection it provides with the current system transaction. Your statements are committed when the system transaction is committed, and all operations, whether executed with the EntityManager or Session API, are part of the same unit of work. Alternatively, if you want to return a value from your JDBC \"work\" to the application, implement the interface org.hibernate.jdbc.ReturningWork. There are no limits on the JDBC operations you can perform\u00aein a Work implementation. Instead of a PreparedStatement, you may use a CallableStatement and execute a stored procedure in the database; you have full access to the JDBC API. For simple queries and working with a ResultSet, such as the one in the previous example, a more convenient alternative is available.\u00aeWhen you execute an SQL SELECT query with the JDBC API or execute a stored procedures that returns a ResultSet, you iterate through each row of this result set and retrieve the data you need. This is a labor-intensive task, and you end up duplicating the same lines of code repeatedly. When you execute an SQL\u00aeSELECT query with the JDBC API or execute a stored procedure that returns a ResultSet, you iterate through each row of this result set and retrieve the data you need. This is a labor-intensive task, and you end up duplicating the same lines of code repeatedly.",
  "page141": "The returned Item instances are in persistent state, managed by the current persistence context. The result is therefore the same as with the JPQL query select i from Item i. For this transformation, Hibernate reads the result set of the SQL query and tries to discover the colum\u00aen names andtypes as defined in your entity mapping metadata. If the column AUCTIONEND is returned, and it's mapped to the Item#auctionEnd property, Hibernate knows how to populate that property and returns fully loaded entity instances. Note that Hibernate expects the query to return all columns required to create an instance of Item, including all properties, embedded components, and foreign keycolumns. If Hibernate can't find a mapped column (by name) in the result set, an exc\u00aeeption is thrown. You may have to use aliases in SQL to return the same column names as defined in your entity mapping metadata. The interfaces javax.persistence.Query and org.hibernate.SQLQuery both support parameter binding. The following query returns only a single Itementity instance Although available in\u00aeHibernate for both APIs, the JPA specification doesn't consider named parameter binding for native queries portable. Therefore, some JPA providers may not support named parameters for native queries.  If your SQL query doesn't return the columns as mapped in your Java entity class, and you\u00aerewrite the query with aliases to rename columns in the result, you must create a result-set mapping",
  "page142": "The following query returns a List of managed Item entity instances. All columns of the ITEM table are included in the SQL projection, as required for the construction of an Item instance. But the query renames the NAME column to EXTENDED_NAME with an alias in the projection Hibe\u00aernate can no longer automatically match the result set fields to Item properties: the NAME column is missing from the result set. You therefore specify a \"result mapping\" with You map all fields of the result set to properties of the entity class. Even if only one field/column doesn't match the already mapped column name (here EXTENDED_NAME), all other columns and properties have to be mapped as well SQL result mappings in annotations are difficult to read and as usua\u00ael with JPAannotations, they only work when declared ona class, not in a package-info.java metadata file. We prefer externalizing such mappings into XML files. The following provides the same mapping: If both result-set mappings have the same name, the mapping declared in XML overrides the one defined with\u00aeannotations. You can also externalize the actual SQL query with @NamedNativeQuery or <namednative-query>, as shown in section 14.4. In all following examples, we keep the SQL statement embedded in the Java code, because this will make it easier for you to understand what the code does. But most of the\u00aetime, you'll see result-set mappings in the more succinct XML syntax.",
  "page143": "With the Hibernate API, you can perform the result-set mapping directly within the query through alias placeholders. When calling addEntity(), you provide an alias, here i. In the SQL string, you then let Hibernate generate the actual aliases in the projection with placeholders s\u00aeuch as {i.name} and {i.auctionEnd}, which refer to properties of the Item entity. No additional result-set mapping declaration is necessary; Hibernate generatesthe aliases in the SQL string and knows how to read the property values from the query ResultSet. This is much more convenient than the JPA result-set mapping option. Or, if you can't or don't want to modify the SQL statement, use add Root() and add Property() on the org.hibernate.SQLQuery to perform the mapping This is e\u00aeffectively an eager fetch of the association Item#seller. Hibernate knows that each row contains the fields for an Itemand a User entity instance, linked by the SELLER_ID. The duplicate columns in the result set would be i.ID and u.ID, which both have the same name. You've renamed them with an alias to IT\u00aeEM_ID and USER_ID, so you have to map how the result set is to be transformed As before, you have to map all fields of each entity result to column names, even if only two have different names as the original entity mapping. This query is much easier to map with the Hibernate API:",
  "page144": "Hibernate will add auto-generated unique aliases to the SQL statement for the {i.*} and {u.*} placeholders, so the query won't return duplicate column names.  You may have noticed the dot syntax in the previous JPA result mapping for the home Address embedded component in a\u00aeUser. Let's look at this special case again We've shown this dot syntax several times before when discussing embedded components: you reference the street property of home Address with homeAddress.street. For nested embedded components, you can write homeAddress.city.name if City isn't just a string but another embeddable class. Hibernate's SQL query API also supports the dot syntax in alias placeholders for component properties. Here are the same query and result-set\u00aemapping: The query (outer) joins the ITEM and BID tables. The projection returns all columns required to constructItem and Bid instances. The query renames duplicate columns such as ID with aliases, so field names are unique in the result. C Because of the renamed fields, you have to map each column to its res\u00aepective entity property. D Add a Fetch Return for the bids collection with the alias of the owning entity and map the key and element special properties to the foreign key column BID_ITEM_ID and the identifier of the Bid. Then the code maps each property of Bid to a field of the result set. Some fields are m\u00aeapped twice, as required by Hibernate for construction of the collection The number of rows in the result set is a product: one item has three bids, one item has one bid, and the last item has no bids, for a total of five rows in the result. F The first element of the result tuple is the Item instance; Hibernate initialized the bids collection",
  "page145": "The second element of the result tuple is each Bid. Alternatively, if you don't have to manually map the result because the field names returned by your SQL query match the already-mapped columns of the entities, you can let Hibernate insert aliases into your SQL statement w\u00aeith placeholders: Eager fetching of collections with dynamic SQL result mappings is only available with the Hibernate API; it's not standardized in JPA So far, you've seen SQL queries returning managed entity instances. You can also return transient instances of any class with the right constructor. The returned column types have to match the constructor parameter types; Hibernate would default to BigInteger for the ID column, so you map it to a Long with the class attribute. T\u00aehe Hibernate API gives you a choice. You can either use an existing result mapping for the query by name, or apply a result transformer, as you saw for JPQL queries in section 16.1: You can use an existing result mapping. C Alternatively, you can map the fields returned by the query as scalar values. With\u00aeout a result transformer, you'd get an Object[] for each result row. D Apply a built-in result transformer to turn the Object[] into instances of ItemSummary. As explained in section 15.3.2, Hibernate can use any class constructor with such a mapping. Instead of ItemSummary, you can construct Item instan\u00aeces. They will be in either transient or detached state, depending on whether you return and map an identifier value in your query.  You can also mix different kinds of result mappings or return scalar values directly.",
  "page146": "Hibernate usually generates CRUD SQL statements at startup. It caches the SQL statements internally for future use, thus avoiding any runtime costs of SQL generation forthe most common operations. You've seen how you can override the R in CRUD, sonow, let's do the same\u00aefor CUD. For each entity, you can define custom CUD SQL statements with the Hibernate annotations @SQLInsert, @SQLUpdate, and @SQLDelete,respectivelyYou need to be careful with the binding of arguments for SQL statements and parameter placeholders (the ? in the statements). For CUD customization, Hibernate onlysupports positional parameters. What is the right order for the parameters? There's an internal order to howHibernate binds arguments to SQL parameters for CUD statements. The e\u00aeasiest way tofigure out the right SQL statement and parameter order is to let Hibernate generatean example for you. Without any customSQL statements, enable DEBUG logging forthe org.hibernate.persister.entity category, and search the Hibernate startupoutput for lines like the following:These automatically gener\u00aeated SQL statements show the right parameter order, andHibernate always binds the values in that order. Copy the SQL statements you want tocustomize into annotations, and make the necessary changes. A special case is properties of an entity class mapped to another table with@SecondaryTable. The CUD customizat\u00aeion statements we've shown so far only refer tothe columns of the main entity table. Hibernate still executes automatically generatedSQL statements for insertion, updates, and deletion of rows in the secondary table(s).",
  "page147": "statements in an XML file. This also simplifies ad hoc testing, because you can copyand paste SQL statements between an XML file and your SQL database console. You've probably noticed that all the SQL examples in the previous sections weretrivial. In fact, none of the exampl\u00aees required a query written in SQL we could haveused JPQL in each case. To make the next example moreinteresting, we write a query that can't be expressed inJPQL, only in SQL. This is the mapping of the association a regular@ManyToOne of the PARENT_ID foreign key column:Categories form a tree. The root of the tree is a Category node without a parent. Thedatabase data for the example tree is in figure 17.2. You can also represent this data as a tree diagram, as shown in figure 17.3. Al\u00aeternatively, you can use a sequence of paths and the level of each node:Now, consider how your application loads Category instances. You may wantto findthe root Category of the tree. This is a trivial JPQL query:You can easily query for the categories in a particular level of the tree, such as all children of t\u00aehe root:This querywill only return direct children of the root: here, categories Two and Three. How can you load the entire tree (or a subtree) in one query? This isn't possible withJPQL, because it would require recursion: \"Load categories at this level, then all the children on thenext level, then\u00aeall the children of those, and so on.\" In SQL, you can writesuch a query, using a common table expression (CTE), a feature alsoknown as subquery factoring.",
  "page148": "It's a complex query, and we won't spend too much time on it here. To understand it,read the last SELECT, querying the CATEGORY_LINK view. Each row in that view represents a node in the tree. The view is declared here in the WITH() AS operation. TheCATEGORY_LINK view is\u00aea combined (union) result of two other SELECTs. You add additional information to the view during recursion, such as the PATH and the LEVEL ofeach node.The XML maps the ID, CAT_NAME, and PARENT_ID fields to properties of the Categoryentity. The mapping returns the PATH and LEVEL as additional scalar values. To execute the named SQL query and access the result, write the following:Each tuple contains a managed, persistent Category instance; its path in the tree as astring (such as /One, /O\u00aene/Two, and so on); and the tree level of the node. Alternatively, you can declare and map an SQL query in a Hibernate XML metadata file:We left out the SQL query in this snippet; it's the same as the SQL statement shownearlier in the JPA example. As mentioned in section 14.4, with regard to the execution\u00aein Java code, it doesn'tmatter which syntax you declare your named queries in: XML file or annotations. Eventhe language doesn't matter it can be JPQL or SQL. Both Hibernate and JPA queryinterfaces have methods to \"get a named query\" and execute it independently fromhow you defined it. Thi\u00aes concludes our discussion of SQL result mapping for queries. The next subjectis customization of SQL statements for CRUD operations, replacing the SQL automatically generated by Hibernate for creating, reading, updating, and deleting data in thedatabase.",
  "page149": "The first custom SQL you write loads an entity instance of the User class. All the following code examples show the same SQL that Hibernate executes automatically bydefault, without much customization this helps you understand the mapping technique more quickly. You can customize\u00aeretrieval of an entity instance with a loaderHibernate has two requirements when you override an SQL query to load an entityinstance: Write a named query that retrieves the entity instance. We show an example inSQL, but as always, you can also write named queries in JPQL. For an SQL query,you may need a custom result mapping, as shown earlier in this chapter.Activate the query on an entity class with @org.hibernate.annotationsLoader. This enables the query as the replacement forthe Hibern\u00aeate-generated query.Let's override how Hibernate loads an instance of the User entity, as shown in the following listing.Annotations declare the query to load an instance of User; you can also declare it inan XML file (JPA or Hibernate metadata). You can call this named query directly inyour data-accesscod\u00aee when needed. The query must have exactly one parameter placeholder, which Hibernate sets as theidentifier value of the instance to load. Here it's a positional parameter, but a namedparameter would also work. For this trivial query, you don't need a custom result-set mapping. The User class mapsal\u00ael fields returned by the query. Hibernate can automatically transform the result.",
  "page150": "Setting the loader for an entity class to a named query enables the query for all operations that retrieve an instance of User from the database. There's no indication of thequery language or where you declared it; this is independent of the loader declaration.In a named loa\u00aeder query for an entity, you have to SELECT (that is, perform a projection for) the following properties of the entity class: The value of the identifier property or properties, if a composite primary key isused. All scalar properties of basic type. All properties of embedded components. An entity identifier value for each @JoinColumn of each mapped entity association such as @ManyToOne owned by the loaded entity class. All scalar properties, embedded component properties, and association\u00aejoinreferences that are inside a @SecondaryTable annotation. If you enable lazy loading for some properties, through interception and bytecodeinstrumentation, you don't need to load the lazy properties (see section 12.1.3).Hibernate always calls the enabled loader query when a User has to be retrieved from\u00aethe database by identifier. When you call em.find(User.class, USER_ID), your custom query will execute. When you call someItem.getSeller().getUsername(), andthe Item#seller proxy has to be initialized, your custom query will load the data. You may also want to customize how Hibernate creates, updates, and del\u00aeetes aninstance of User in the database.",
  "page151": "Hibernate usually generates CRUD SQL statements at startup. It caches the SQL statements internally for future use, thus avoiding any runtime costs of SQL generation forthe most common operations. You've seen how you can override the R in CRUD, sonow, let's do the same\u00aefor CUD. For each entity, you can define custom CUD SQL statements with the Hibernate annotations @SQLInsert, @SQLUpdate, and @SQLDelete,respectivelyYou need to be careful with the binding of arguments for SQL statements and parameter placeholders (the ? in the statements). For CUD customization, Hibernate onlysupports positional parameters. What is the right order for the parameters? There's an internal order to howHibernate binds arguments to SQL parameters for CUD statements. The e\u00aeasiest way tofigure out the right SQL statement and parameter order is to let Hibernate generatean example for you. Without any custom SQL statements, enable DEBUG logging forthe org.hibernate.persister.entity category, and search the Hibernate startupoutput for lines like the following:These automatically gene\u00aerated SQL statements show the right parameter order, andHibernate always binds the values in that order. Copy the SQL statements youwant tocustomize into annotations, and make the necessary changes. A special case is properties of an entity class mapped to another table with@SecondaryTable. The CUD customizat\u00aeion statements we've shown so far only refer tothe columns of the main entity table. Hibernate still executes automatically generatedSQL statements for insertion, updates, and deletion of rows in the secondary table(s).",
  "page152": "You can customize this SQL by adding the @org.hibernate.annotations.Table annotation to your entity class and setting its sqlInsert, sqlUpdate, and sqlDelete attributes. If you prefer to have your CUD SQL statementsin XML, your only choice is to mapthe entire entity in a Hibernat\u00aee XML metadata file. The elements in this proprietarymapping format for custom CUD statements are <sql-insert>, <sql-update>, and<sql-delete>. Fortunately, CUD statements are usually much more trivial than queries, so annotations are fine in most applications. You've now added custom SQL statements for CRUD operations of an entityinstance. Next, we show how to override SQL statements loading and modifying acollection.Let's override the SQL stafortement Hibernate uses when loadin\u00aeg the Item#images collection. This is a collection of embeddable components mapped with @ElementCollection; the procedure is the same for collections of basic types or many-valuedentity associations (@OneToMany or @ManyToMany)As before, you declare that a named query will load the collection. This time, however\u00ae,you must declare and map the result of the query in a Hibernate XML metadata file,which is the only facility that supports mapping of query results to collection properties:The query has to have one (positional or named) parameter. Hibernate sets its valueto the entity identifier that owns the collection. Wh\u00aeenever Hibernate need to initializethe Item#images collection, Hibernate now executes your custom SQL query.",
  "page153": "Sometimes you don't have to override the entire SQL statement for loading a collection: for example, if you only want to add a restriction to the generated SQL statement. Let's say the Category entity has a collection of Item references, and the Itemhas an activation fl\u00aeag. If the property Item#active is false, you don't want to load itwhen accessing the Category#items collection. You can append this restriction to theSQL statement with the Hibernate @Where annotation on the collection mapping, as aplain SQL fragment:To find the right parameter order, enable DEBUG logging for the org.hibernate.persister.collection category and search the Hibernate startup output for the generated SQL statements for this collection, before you add your custom SQL anno\u00aetations. A new annotation here is @SQLDeleteAll, which only applies to collections ofbasic or embeddable types. Hibernate executes this SQL statement when the entirecollection has to be removed from the database: for example, when you call someItem.getImages().clear() or someItem.setImages(new HashSet()). No @S\u00aeQLUpdate statement is necessary for this collection, because Hibernatedoesn't update rows for this collection of embeddable type. When an Image propertyvalue changes, Hibernate detects this as a new Image in the collection (recall thatimages are compared \"by value\" of all their properties). Hib\u00aeernate will DELETE the oldrow and INSERT a new row to persist this change. Instead of lazily loading collection elements, you can eagerly fetch them when theowning entity is loaded. You can also override this query with a custom SQL statement.",
  "page154": "Let's consider theItem#bids collection and how it's loaded. Hibernate enables lazyloading by default because you mapped with @OneToMany, so it's only when you beginiterating through the collection's elements that Hibernate will execute a query andretrieve the\u00aedata. Hence, when loading the Item entity, you don't have to load any collection data. If instead you want to fetch the Item#bids collection eagerly when the Item isloaded, first enable a custom loader query on the Item class:As in the previous section, you must declare this named query in a Hibernate XMLmetadata file; no annotations are available to fetch collections with named queries.Here is the SQL statement to load an Item and its bids collection in a single OUTER JOIN:You saw th\u00aeis query and result mapping in Java code earlier in this chapter in the section \"Eager-fetching collections.\" Here, you apply an additional restriction to only onerow of ITE,with the given primary key value. You can also eagerly load single-valued entity associations such as a @ManyToOnewith a custom\u00aeSQL statement. Let's say you want to eagerly load the bidder when a Bidentity is retrieved from the database. First, enable a named query as the entity loader:Unlike custom queries used to load collections, you can declare this named querywith standard annotations (of course, you can also have it in an X\u00aeML metadata file, ineither the JPA or Hibernate syntax).",
  "page155": "An INNER JOIN is appropriate forthis SQL query, because a Bid always has a bidderand the BIDDER_ID foreign key column is never NULL. You rename duplicate ID columns in the query, and because you rename them to BID_ID and USER_ID, a customresult mapping is necessaryHibernate execu\u00aetes this custom SQL query and maps the result when loading aninstance of the Bid class, either through em.find(Bid.class, BID_ID) or when it hasto initialize a Bid proxy. Hibernate loads the Bid#bidder right away and overrides theFetchType.LAZY setting on the association. You've now customized Hibernate operations with your own SQL statements. Let'scontinue with stored procedures and explore the options for integrating them intoyour Hibernate application.Stored procedures are com\u00aemonin database application development. Moving codecloser to the data and executing it inside the database has distinct advantages. Youend up not duplicating functionality and logic in each program that accesses the data.A different point of view is that a lot of business logic shouldn't be duplicated, so\u00aeitcan be applied all the time. This includes procedures that guarantee the integrity ofthe data: for example, constraints that are too complex to implement declaratively.You'll usually also find triggers in a database that contain procedural integrity rules. Stored procedures have advantages for all proc\u00aeessing on large amounts of data,such as reporting and statistical analysis. You should always try to avoid moving largedata sets on your network and between your database and application servers, so astored procedure is the natural choice for mass data operations.",
  "page156": "There are of course (legacy) systems that implement even the most basic CRUDoperations with stored procedures. In a variation on this theme, some systems don'tallow any directuse of SQL INSERT, UPDATE, or DELETE, but only stored procedure calls;these systems also had (and so\u00aemetimes still have) their place. In some DBMSs, you can declare user-defined functions, in addition to, or insteadof, stored procedures. The summary in table 17.1 shows some of the differencesbetween procedures and functionsIt's difficultto generalize and compare procedures and functions beyond these obvious differences. This is one area where DBMS support differs widely; some DBMSsdon't support stored procedures or user-defined functions, whereas others roll bothinto one (for ex\u00aeample, PostgreSQL has only user-defined functions). Programminglanguages for stored procedures are usually proprietary. Some databases even supportstored procedures written in Java. Standardizing Java stored procedures was part ofthe SQLJ effort, which unfortunately hasn't been successful. In this section,\u00aewe show you how to integrate Hibernate with MySQL stored procedures and PostgreSQL user-defined functions. First, we look at defining and callingstored procedures with the standardized Java Persistence API and the native HibernateAPI. Then, we customize and replace Hibernate CRUD operations with procedurecal\u00aels. It's important that you read the previous sections before this one, because theintegration of stored procedures relies on the same mapping options as other SQLcustomization in Hibernate.",
  "page157": " As before in this chapter, the actual SQL stored procedures we cover in the examples are trivial so you can focus on the more important parts how to call the procedures and use the API in your application. When calling a stored procedure, you typically want to provide input and\u00aereceivethe output of the procedure. You can distinguish between procedures that Return a result set Return multiple result sets Update data and return the count of updated rows Accept input and/or output parameters Return a cursor, referencing a result in the databaseLet's start with the simplest case: a stored procedure that doesn't have any parametersand only returns data in a result set.As you've previously seen in this chapter, Hibernate automatically maps the columnsret\u00aeurned in the result set to properties of the Item class. The Item instances returnedby this query will be managed and in persistent state. To customize the mapping ofreturned columns, provide the name of a result-set mapping instead of the Item.class parameter.The Hibernate getCurrent() method already indicates\u00aethat a procedure may returnmore than a single ResultSet. A procedure may return multiple result sets and evenreturn update counts if it modified data.The following MySQL procedure returns all rows of the ITEM table that weren'tapproved and all rows that were already approved, and also sets the APPROVED\u00aeflag forthese rows.",
  "page158": "In the application, you get two result sets and an update count. Accessing and processing the results of a procedure call is a bit more involved, but JPA is closely alignedto plain JDBC, so this kind of code should be familiar if you've worked with storedprocedures:Execute t\u00aehe procedure call with execute(). This method returns true if the firstresult of the call isa result set and false if the first result is an update count.Process all results of thecall in a loop. Stop looping when no more results are available, which is always indicated by hasMoreResults() returning false and getUpdateCount() returning -1.If the current result is a result set, read and process it. Hibernate maps the columns ineach result set to managed instances of the Item class. Alternat\u00aeively, provide a resultset mapping name applicable to all result sets returned by the call.If the current result is an update count, getUpdateCount() returns a value greaterthan -1.hasMoreResults()advances to the next result and indicates the type of that result.The alternative procedure execution with the Hibe\u00aernate API may seem morestraightforward. It hides some of the complexity of testing the type of each result andwhether there is more procedure output to process:As long as getCurrent() doesn't return null, there are more outputs to process. An output may be a result set: test and cast it. If an output isn\u00ae't a result set, it's an update count. Proceed with the next output, if any.Next, we consider stored procedures with input and output parameters.",
  "page159": "The following MySQL procedure returns a row for the given identifier from the ITEMtable, as well as the total number of items:This next procedure returns a result setwith the data of the ITEM row. Additionally,the output parameter PARAM_TOTAL is set. To call this procedure in JPA\u00ae, you must firstregister all parameters:Register all parameters by position (starting at 1) and their type. Bind values to the input parameters.Retrieve the result set returned by the procedure. After you've retrieved the result sets, you can access the output parameter values.You can also register and use named parameters, but you can't mix named and positional parameters in a particular call. Also, note that any parameter names you choosein your Java code don't have to mat\u00aech the names of the parameters in your stored procedure declaration. Ultimately, you must register the parameters in the same order asdeclared in the signature of the procedure. Register all parameters; you can bind input values directly. Output parameter registrations can be reused later to read the output val\u00aeue. Process all returned result sets before you access any output parameters. Access the output parameter value throughthe registration.The following MySQL procedure uses input parameters to update a row in the ITEMtable with a new item name:In this example, you can also see how named parameters work and that\u00aenames in theJava code don't have to match names in the stored procedure declaration. The orderof parameter registrations is still important, though; PARAM_ITEM_ID must be first, andPARAM_ITEM_NAME must be second.",
  "page160": "A shortcut for calling procedures that don't return a result set but modify datais executeUpdate(): its return value is your update count. Alternatively, you canexecute() the procedure and call getUpdateCount(). This is the same procedure execution using the Hibernate API:Be\u00aecause you know there is no result set returned by this procedure, you can directlycast the first (current) output to UpdateCountOutputNext, instead of returning a result set, we see procedures that return a cursor reference.MySQL doesn't support returning cursors from stored procedures. The followingexample only works on PostgreSQL. This stored procedure (or, because this is thesame in PostgreSQL, this user-defined function) returns a cursor to all rows of theITEM table:The type of th\u00aee parameter is void, because its only purpose is to prepare the call internally for reading data with the cursor. When you call getResultList(), Hibernateknows how to get the desired output.Supporting database cursors across DBMS dialects is difficult, and Hibernate has somelimitations. For example, with Postgr\u00aeeSQL, a cursor parameter must always be the firstregistered parameter, and (because it's a function) only one cursor should bereturned by the database. With the PostgreSQL dialect, Hibernate doesn't supportnamed parameter binding if a cursor return is involved: you must use positionalparameter bindi\u00aeng. Consult your Hibernate SQL dialect for more information; the relevant methods are Dialect#getResultSet(CallableStatement) and so on. This completes our discussion of APIs for direct stored procedure calls. Next, youcan also use stored procedures to override the statements generated by Hibernatewhen it loads or stores data.",
  "page161": "The first customized CRUD operation you write loads an entity instance of the Userclass. Previously in this chapter, you used a native SQL query with a loader to implement this requirement. If you have to call a stored procedure to load an instance ofUser, the process is equally\u00aestraightforward.First, write a named query that calls a stored procedure for example, in annotationson the User class:Compare this with the previous customization in section 17.3.1: the declaration of theloader is still the same, and it relies on a defined named query in any supported language. You've only changed the named query, which you can also move into an XMLmetadata file to further isolate and separate this concern.JPA doesn't standardize what goes into a @NamedNativeQuer\u00aey, you're free to writeany SQL statement.With the JDBC escape syntax in curly braces, you're saying, \"Letthe JDBC driver figure out what to do here.\" If your JDBC driver and DBMS understandstored procedures, you can invoke a procedure with {call PROCEDURE}. Hibernateexpects that the procedur\u00aee will return a result set, and the first row in the resultset isexpected to have the columns necessary to construct a User instance. We listed therequired columns and properties earlier, in section 17.3.1. Also remember that youcan always apply a result-set mapping if the column (names) returned by your proc\u00aeedure aren't quite right or you can't change the procedure code. The stored procedure must have a signature matching the call with a single argument. Hibernate sets this identifier argument when loading an instance of User.Here's an example of a stored procedure on MySQL with such a signature.",
  "page162": "You use the Hibernate annotations @SQLInsert, @SQLUpdate, and @SQLDeleteto customize how Hibernate creates, updates, and deletes an entity instance in the database.Instead of a custom SQL statement, you can call a stored procedure to perform theoperation:You have to indicate that\u00aeHibernate must execute an operation with a JDBC CallableStatement instead of a PreparedStatement; hence set the callable true option. As explained in section 17.3.2, argument binding for procedure calls is only possible with positional parameters, and you must declare them in the order Hibernateexpects. Your stored procedures must have a matching signature. Here are some procedure examples for MySQL that insert, update, and delete a row in the USERS table:When a stored procedure inserts,\u00aeupdates, or deletes an instance of User, Hibernatehas to know whether the call was successful. Usually, for dynamically generated SQL,Hibernate looks at the number of updated rows returned from an operation. If youenabled versioning (see section 11.2.2), and the operation didn't or couldn't updateany\u00aerows, an optimistic locking failure occurs.If you write your own SQL, you can customize this behavior as well. It's up to the stored procedure to perform the versioncheck against the database state when updating or deleting rows. With the checkoption in your annotations, you can let Hibernate know how th\u00aee procedure will implement this requirement.",
  "page163": "The default is ResultCheckStyle.NONE, and the following settings are available: NONE The procedure will throw an exception if the operation fails. Hibernatedoesn't perform any explicit checks but instead relies on the procedure code todo the right thing. If you enable versio\u00aening, your procedure must compare/increment versions and throw an exception if it detects a version mismatch. COUNT The procedure will perform any required version increments andchecks and return the number of updated rows to Hibernate as an updatecount. Hibernate uses CallableStatement#getUpdateCount() to access theresult. PARAM The procedure will perform any required version increments andchecks and return the number of updated rows to Hibernate in its first outputparameter. For this che\u00aeck style, you need to add an additional question mark toyour call and, in your stored procedure, return the row count of your DMLoperation in this (first) output parameter. Hibernate automatically registers theparameter and reads its value when the call completes.Finally, remember that stored procedures and fun\u00aections sometimes can't be mappedin Hibernate. In such cases, you have to fall back to plain JDBC. Sometimes you canwrap a legacy stored procedure with another stored procedure that has the parameterinterface Hibernate expects.You saw how to fall back to the JDBC API when necessary. Even for custom SQLque\u00aeries, Hibernate can do the heavy lifting and transform the ResultSet intoinstances of your domain model classes, with flexible mapping options, including customizing result mapping. You can also externalize native queries for acleaner setup. We discussed how to override and provide your own SQL statements for regularcreate, read, update, and delete (CRUD) operations, as well as for collectionoperations.",
  "page164": "Most JPA developers build client/server applications with a Java-based server accessing the database tier through Hibernate. Knowing how the EntityManager and system transactions work, you could probably come up with your own serverarchitecture. You'd have to figure out wher\u00aee to create the EntityManager, whenand how to close it, and how to set transaction boundaries. You may be wondering what the relationship is between requests and responsesfrom and to your client, and the persistence context and transactions on the server.Should a single system transaction handle each client request? Can several consecutive requests hold a persistence context open? How does detached entity state fitinto this picture? Can you and should you serialize entity data between clie\u00aent andserver? How will these decisions affectyour client design?Before we start answering these questions, we have to mention that we won't talkabout any specific frameworks besides JPA and EJB in this chapter. There are severalreasons the code examples use EJBs in addition to JPA: Our goal is to focus on\u00aeclient/server design patterns with JPA. Many cross-cutting concerns, such as data serialization between client and server, are standardized in EJB, so we don't have to solve every problem immediately. We know youprobably won't write an EJB client application. With the example EJB clientcode in this\u00aechapter, though, you'll have the foundation to make informed decisions when choosing and working with a different framework. We'll discuss custom serialization procedures in the next chapter and explain how to exchangeyour JPA-managed data with any client.",
  "page165": "We can't possibly cover every combination of client/server frameworks in theJava space. Note that we haven't even narrowed our scope to web server applications. Of course, web applications are important, so we'll dedicate the nextchapter to JPA with JSF and JAX-RS.\u00aeIn this chapter, we're concerned with any client/server system relying on JPA for persistence, and abstractions like the DAOpattern, which are useful no matter what frameworks you use. EJBs are effective even if you only use them on the server side. They offer transaction management, and you can bind the persistence context to stateful session beans. We'll discuss these details as well, so if your application architecturecalls for EJBs on the server side, you'll know how to\u00aebuild them.Throughout this chapter, you implement two simple use cases withstraightforwardworkflows as an actual working application: editing an auction item, and placing bidsfor an item. First we look at the persistence layer and how you can encapsulate JPAoperations into reusable components: in particular, u\u00aesing the DAO pattern. This willgive you a solid foundation to build more application functionality. Then you implement the use cases as conversations: units of work from the perspective of your application users. You see the code for stateless and stateful server-side components and the impact this has on cli\u00aeent design and overall application architecture.This affects not only the behavior of your application but also its scalability and robustness. We repeat all the examples with both strategies and highlight the differences. Let's start with fleshing out a persistence layer and the DAO pattern.",
  "page166": "In section 3.1.1, we introduced the encapsulation of persistence code in a separatelayer. Although JPA already provides a certain level of abstraction, there are severalreasons you should consider hiding JPA calls behind a facade: A custom persistence layer can provide a higher l\u00aeevel of abstraction for dataaccess operations. Instead of basic CRUD and query operations as exposed bythe EntityManager, you can expose higher-level operations, such as getMaximumBid(Item i) and findItems(User soldBy) methods. This abstractionis the primary reason to create a persistence layer in larger applications: to support reuse of the same data-access operations. The persistence layer can have a generic interface without exposing implementation details. In other words, you can hide\u00aethe fact that you're using Hibernate(or Java Persistence) to implement the data-access operations from any client ofthe persistence layer. We consider persistence-layer portability an unimportantconcern because full object/relational mapping solutions like Hibernatealready provide database portability. It\\u00aeu2019s highly unlikely that you'll rewrite yourpersistence layer with different software in the future and still not want tochange any client code. Furthermore, Java Persistence is a standardized andfully portable API; there is little harm in occasionally exposing it to clients of thepersistence layer.Th\u00aee persistence layer can unify data-access operations. This concern relates to portability, but from a slightly different angle. Imagine that you have to deal with mixeddata-access code, such as JPA and JDBC operations. By unifying the facade that clientssee and use, you can hide this implementation detail from the client. If you have todeal with different types of data stores, this is a valid reason to write a persistence layer.",
  "page167": "The persistence layer can unify data-access operations. This concern relates to portability, but from a slightly different angle. Imagine that you have to deal with mixeddata-access code, such as JPA and JDBC operations. By unifying the facade that clientssee and use, you can hid\u00aee this implementation detail from the client. If you have todeal with different types of data stores, this is a valid reason to write a persistence layer. If you consider portability and unification to be side effects of creating a persistence layer, your primary motivation is achieving a higher level of abstraction andimprove the maintainability and reuse of data-access code. These are good reasons,and we encourage you to create a persistence layer with a generic facade in all but thesimp\u00aelest applications. But always first consider using JPA directly without any additional layering. Keep it as simple as possible, and create a lean persistence layer on topof JPA when you realize you're duplicating the same query and persistence operations. Many tools available claim to simplify creating a p\u00aeersistence layer for JPA or Hibernate. We recommend that you try to work without such tools first, and only buy into aproduct when you need a particular feature. Be especially wary of code and query generators: the frequently heard claims of a holistic solution to every problem, in the longterm, can become a\u00aesignificant restriction and maintenance burden. There can alsobe a huge impact on productivity if the development process depends on running acode-generation tool. This is of course also true for Hibernate's own tools: for example, if you have to generate the entity class source from an SQL schema every time youmake a change.",
  "page168": "The persistence layer is an important part of your application, andyou must be aware of the commitment you're making by introducing additionaldependencies. You see in this chapter and the next how to avoid the repetitive codeoften associated with persistence-layer components\u00aewithout using any additional tools. There is more than one way to design a persistence layer facade some smallapplications have a single DataAccess class; others mix data-access operations intodomain classes (the Active Record pattern, not discussed in this book) but we preferthe DAO pattern.The DAO design pattern originated in Sun's Java Blueprints more than 15 years ago;it's had a long history. A DAO class defines an interface to persistence operations relating to a particular\u00aeentity; it advises you to group together code that relates to the persistence of that entity. Givenits age, there are many variations of the DAO pattern. Thebasic structure of our recommended design is shown in figure 18.1.We designed the persistence layer with two parallel hierarchies: interfaces on oneside,\u00aeimplementations on the other. The basic instance-storage and -retrieval operations are grouped in a generic super-interface and a superclass that implements theseoperations with a particular persistence solution (using Hibernate, of course). Thegeneric interface is extended by interfaces for particular entiti\u00aees that require additional business-related data-access operations. Again, you may have one or severalimplementations of an entity DAO interface.",
  "page169": "Let's quickly look at some of the interfaces and methods shown in this illustration.There are a bunch of finder methods. These typically return managed (in persistentstate) entity instances, but they may also return arbitrary data-transfer objects such asItemBidSummary. Find\u00aeer methods are your biggest code duplication issue; you mayend up with dozens if you don't plan carefully. The first step is to try to make them asgeneric as possible and move them up in the hierarchy, ideally into the top-level interface. Consider the findByName() method in the ItemDAO: you'll probably have to addmore options for item searches soon, or you may want the result presorted by the database, or you may implement some kind of paging feature. We'll elaborate on thi\u00aesagain later and show you a generic solution for sorting and paging in section 19.2. The methods offered by the DAO API indicate clearly that this is a state-managingpersistence layer. Methods such as makePersistent() and makeTransient() changean entity instance's state (or the state of many instances at o\u00aence, with cascadingenabled). A client can expect that updates are executed automatically (flushed) bythepersistence engine when an entity instance is modified (there is no performUpdate() method). You'd write a completely different DAO interface if your persistence layer were statement-oriented: for exam\u00aeple, if you weren't using Hibernate toimplement it, but rather only plain JDBC.",
  "page170": "The persistence layer facade we introduce here doesn't expose any Hibernate orJava Persistence interface to the client, so theoretically you can implement it with anysoftware without making changes to the client code. You may not want or need persistence-layer portability, a\u00aes explained earlier. In that case, you should consider exposingHibernate or Java Persistence interfaces for example, you could allow clients toaccess the JPA CriteriaBuilder and then have a generic findBy(CriteriaQuery)method. This decision is up to you; you may decide that exposing Java Persistenceinterfaces is a safer choice than exposing Hibernate interfaces. You should know, however, that although it's possible to change the implementation of the persistence layerfrom one JPA prov\u00aeider to another, it's almost impossible to rewrite a persistence layerthat is state-oriented with plain JDBC statements. Next, you implement the DAO interfaces.This generic implementation needs two things to work: an EntityManager and anentity class. A subclass must provide the entity class asa constructor\u00aeargument. TheEntityManager, however, can be provided either by a runtime container that understands the @PersistenceContext injection annotation (for example, any standardJava EE container) or through setEntityManager().You can see how the code uses the entity class to perform the query operations. We'v\u00aeewritten some simple criteria queries, but you could use JPQL or SQL. Finally, here are the state-management operations.",
  "page171": "An important decision is how you implement the makePersistent() method. Herewe've chosen EntityManager#merge() because it's the most versatile. If the givenargument is a transient entity instance, merging will return a persistent instance. If theargument is a detached e\u00aentity instance, merging will also return a persistent instance.This provides clients with a consistent API without worrying about the state of an entityinstance before calling makePersistent(). But the client needs to be aware that thereturned value of makePersistent() is always the current instance and that the argument it has given must now be thrown away You've now completed building the basic machinery of the persistence layer andthe generic interface it exposes to the upper laye\u00aer of the system. In the next step, youcreate entity-related DAO interfaces and implementations by extending the genericinterface and implementation.Everything you've created so far is abstract and generic you can't even instantiateGenericDAOImpl. You now implement the ItemDAO interface by extending Ge\u00aenericDAOImpl with a concrete class. First you must make choices about how callers will access the DAOs. You also needto think about the life cycle of a DAO instance. With the current design, the DAOclasses are stateless except for the EntityManager member. Caller threads can share a DAO instance. In a multith\u00aereaded Java EE environment,for example, the automatically injected EntityManager is effectively thread-safe,because internally it's often implemented as a proxy that delegates to some thread- ortransaction-bound persistence context. Of course, if you call setEntityManager() ona DAO, that instance can't be shared and should only be used by one (for example,integration/unit test) thread.",
  "page172": "You shouldn't have any problem writing these queries after reading the previous chapters; they're straightforward: Either use the criteria query APIs or call externalizedJPQL queries by name. You should consider the static metamodel for criteria queries,as explained in\u00aethe section \"Using a static metamodel\" in chapter 3. With the ItemDAO finished, you can move on to BidDAO:As you can see, this is an empty DAO implementation that only inherits generic methods. In the next section, we discuss some operations you could potentially move intothis DAO class. We also haven't shown any UserDAO or CategoryDAO code and assumethat you'll write these DAO interfaces and implementations as needed. Our next topic is testing this persistence layer: S\u00aehould you, and if so,how?We've pulled almost all the examples in this book so far directly from actualtest code.We continue to do so in all future examples, but we have to ask: should you write testsfor the persistence layer to validate its functionality? In our experience, it doesn't usually make sen\u00aese to test the persistence layer separately. You could instantiate your domain DAO classes and provide a mock EntityManager. Such a unit test would be of limited value and quite a lot of work to write.Instead, we recommend that you create integration tests, which test a larger part of theapplication stack and\u00aeinvolve the database system. All the rest of the examples in thischapter are from such integration tests; they simulate a client calling the server application, with an actual database back end.",
  "page173": "Hence, you're testing what's important: thecorrect behavior of your services, the business logic of the domain model they rely on,and database access through your DAOs, all together. The problem then is preparing such integration tests. You want to test in a real JavaEE\u00aeenvironment, in the actual runtime container. For this, we use Arquillian(http://arquillian.org), a tool that integrates with TestNG. With Arquillian, youprepare a virtual archive in your test code and then execute it on a real applicationserver. Look at the examples to see how this works. A more interesting problem is preparing test data for integration tests. Most meaningful tests require that some data exists in the database. You want to load that test datainto the database before your\u00aetest runs, and each test should work with a clean andwell-defined data set so you can write reliable assertions. Based on our experience, here are three common techniques to import test data:Your test fixture executes a method before every test to obtain an EntityManager. You manually instantiate your test dat\u00aea entities and persist them withthe EntityManager API. The major advantage of this strategy is that you testquite a few of your mappings as a side effect. Another advantage is easy programmatic access to test data.",
  "page174": "For example, if you need the identifier value of aparticular test Item in your test code, it's already there in Java because you canpass it back from your data-import method. The disadvantage is that test datacan be hard to m aintain,because Java code isn't a great data\u00aeformat. You canclear test data from the database by dropping and re-creating the schema afterevery test using Hibernate's schema-export feature. All integration tests in thisbook so far have used this approach; you can find the test data-import procedure next to each test in the example code. Arquillian can execute a DbUnit (http://dbunit.sourceforge.net) data-setimport before every test run. DbUnit offers several formats for writing data sets,including the commonly used flat XML syn\u00aetax. This isn't the most compact format but is easy to read and maintain. The examples in this chapter use thisapproach. You can find Arquillian's @UsingDataSet on the test classes with apath to the XML file to import. Hibernate generates and drops the SQL schema,and Arquillian, with the help of DbUni\u00aet, loads the test data into the database. Ifyou like to keep your test data independent of tests, this may be the rightapproach for you. If you don't use Arquillian, manually importing a data set ispretty easy with DbUnit see the SampleDataImporter in this chapter's examples. We deploy this importer\u00aewhen running the example applications duringdevelopment, to have the same data available for interactive use as in automated tests.",
  "page175": "In section 9.1.1, you saw how toexecute custom SQL scripts when Hibernatestarts. The load script executes after Hibernate generatesthe schema; this is agreat utility for importing test data with plain INSERT SQL statements. Theexamples in the next chapter use this approach. The m\u00aeajor advantage is thatyou can copy/paste the INSERT statements from an SQL console intoyour testfixture and vice versa. Furthermore, if your database supports the SQL rowvalue constructor syntax, you can write compact multirow insertion statementslike insert into MY_TABLE (MY_COLUMN) values (1), (2), (3), ....We leave it up to you to pick astrategy. This is frequently a matter of taste and howmuch test data you have to maintain. Note that we're talking about test data forintegration t\u00aeests, not performance or scalability tests. If you need large amounts of(mostly random) test data for load testing, consider data-generation tools such as Benerator (http://databene.org/databene-benerator.html). This completes the first iteration of the persistence layer. You can now obtainItemDAO instances and\u00aework with a higher level of abstraction when accessing the database. Let's write a client that calls this persistence layer and implement the rest of theapplication.The application will be a stateless server application, which means no applicationstate will be managed on the server between client reques\u00aets. The application will besimple, in that it supports only two use cases: editing an auction item and placing abid for an item.",
  "page176": "Consider these workflows to be conversations: units of work from the perspective ofthe application user. The point of view of application users isn't necessarily the samethat we as developers have on the system; developers usually consider one systemtransaction to be a unit\u00aeof work. We now focus on this mismatch and how a user's perspective influences the design of server and client code. We start with the first conversation: editing an item.The client is a trivial text-based EJB console application. Look at the \"edit an auctionitem\" user conversation with this client in figure 18.2. The client presents the user witha list of auction items; the user picks one. Then theclient asks which operation the user would like to perform. Finally, after en\u00aetering a newname, the client shows a success confirmation message. The system is now ready for thenext conversation. The example client starts again and presents a list of auction items.The sequence of calls for this conversation is shown in figure 18.3. This is your roadmap for the rest of the section. Let\u00ae19s have a closer look at this in code; you can refer to the bullet items in the illustration to keep track of where we are. The code you see next is from a test case simulating the client, followed by code from the server-side components handling theseclient calls. The client retrieves a list of Item instances f\u00aerom the server to start the conversation and also requests with true that the Item#bids collection be eagerly fetched.Because the server doesn't hold the conversation state, the client must do this job.",
  "page177": "(You can ignore the interfaces declared here; they're trivial but necessary for remotecalls and local testing of an EJB.) Because no transaction is active when the client callsgetItems(), a new transaction is started. The transaction is committed automaticallywhen the method\u00aereturns. The @TransactionAttribute annotation is optional in thiscase; the default behavior requires a transaction on EJB method calls. The getItems() EJB method calls the ItemDAO to retrieve a List of Item instancesThe Java EE container automatically looks up and injects the ItemDAO, and theEntityManager is set on the DAO. Because no EntityManager or persistence context isassociated with the current transaction, a new persistence context is started andjoined with the transaction. The per\u00aesistence context is flushed and closed when thetransaction commits. This is a convenient feature of stateless EJBs; you don't have todo much to use JPA in a transaction. A List of Item instances in detached state (after the persistence context is closed)is returned to the client . You don't have to wo\u00aerry about serialization right now; aslong as List and Item and all other reachable types are Serializable, the EJB framework takescare of it. Next, the clientsets the new name of a selected Item and asks the server to storethat change by sending the detached and modifiedItem.",
  "page178": "The updated state the result of the merge is returned to the client. Theconversation is complete, and the client may ignore the returned updatedItem. But the client knows that this return value is the latest state and that any previous state it was holding during the conversation\u00ae, such as the List of Item instances, isoutdated and should probably be discarded. A subsequent conversation should beginwith fresh state: using the latest returned Item, or by obtaining a fresh list. You've now seen how to implement a single conversation the entire unit of work,from the user's perspective with two system transactions on the server. Because youonly loaded data in the first system transaction and deferred writing changes to thelast transaction, the conversation wa\u00aes atomic: changes aren't permanent until the laststep completes successfully. Let's expand on this with the second use case: placing abid for an item.In the console client, a user's \"placing a bid\" conversation looks like figure 18.4. The client presents the user with a list of auction\u00aeitems again and asks the user to pick one. Theuser can place a bid and receives a success-confirmation message if the bid was storedsuccessfully. The sequence of calls and the code road map are shown in figure 18.5.",
  "page179": "We again step through the test client and server-side code. First , the client gets a listof Item instances and eagerly fetches the Item#bids collection. You saw the code forthis in the previous section. Then, the client creates a new Bid instance after receiving user input for t\u00aeheamount , linking the new transient Bid with the detached selected Item. The clienthas to store the new Bid and send it to the server. If you don't document your serviceAPI properly, a client may attempt to send the detached Item:Here, the client assumes that the server knows it added the new Bid to the Item#bidscollection and that it must be stored. Your server could implement this functionality,maybe with merge cascading enabled on the @OneToMany mapping of that collection.Then, th\u00aee storeItem() method of the service would work as in the previous section,taking the Item and calling the ItemDAO to make it (and its transitive dependencies)persistent. This isn't the case in this application: the service offers a separate placeBid()method. You have to perform additional validation before\u00aea bid is stored in the database, such as checking whether it was higher than the last bid. You also want to forcean increment of the Item version, to prevent concurrent bids. Hence, you documentthe cascading behavior of your domain model entity associations: Item#bids isn'ttransitive, and new Bid instan\u00aeces must be stored through the service's placeBid()method exclusively.",
  "page180": "Two interesting things are happening here. First, a transaction is started and spans theplaceBid() method call. The nested EJB method calls to ItemDAO and BidDAO are inthat same transaction context and inherit the transaction. The same is true for thepersistence context: it has t\u00aehe same scope as the transaction . Both DAO classesdeclare that theyneed the current @PersistenceContext injected; the runtime container provides the persistence context bound to the current transaction. Transactionand persistence-context creation and propagation with stateless EJBs is straightforward, always \"along with the call.\" Second, the validation of the new Bid is business logic encapsulated in the domainmodel classes. The service calls Item#isValid(Bid) and delegates the\u00aeresponsibilityfor validation to the Item domain model class. Here's how you implement this in theItem class:The isValid() method performs several checks to find out whether the Bid is higherthan the last bid. If your auction system has to support a \"lowest bid wins\" strategy atsome point, all yo\u00aeu have to do is change the Item domain-model implementation; theservices and DAOs using that class won't know the difference. (Obviously, you'd need adifferent message for the InvalidBidException.) What is debatable is theefficiency of the getHighestBid() method. It loads theentire bids collection i\u00aento memory, sorts it there, and then takes just one Bid. Anoptimized variation could look like this.",
  "page181": "The service (or controller, if you like) is still completely unaware of any businesslogic it doesn't need to know whether a new bid must be higher or lower than thelast one. The service implementation must provide the currentHighestBid andcurrentLowestBid when calling the It\u00aeem#isValid() method. This is what we hintedat earlier: that you may want to add operations to the BidDAO. You could write database queries to find those bids in the most efficient way possible without loading allthe item bids into memory and sorting them there.The application is now complete. It supports the two use cases you set out to implement. Let's take a step back and analyze the resultYou've implemented code to support conversations: units of work from the perspective of t\u00aehe users. The users expect to perform a series of steps in a workflow and thateach step will be only temporary until they finalize the conversation with the last step.That last step is usually a final request from the client to the server, ending the conversation. This sounds a lot like transactions, but you ma\u00aey have to perform several systemtransactions on the server to complete a particular conversation. The question is howto provide atomicity across several requests and system transactions.",
  "page182": " Conversations by users can be of arbitrary complexity and duration. More thanone client request in a conversation's flow may load detached data. Because you're incontrol of the detached instances on the client, you can easily make a conversationatomic if you don't\u00aemerge, persist, or remove any entity instances on the server untilthe final request in your conversation workflow. It's up to you to somehow queue modifications and manage detached data where the list of items is held during user thinktime. Just don't call any service operation from the client that makes permanentchanges on the server until you're sure you want to \"commit\" the conversation. One issue you have to keep an eye on is equality ofdetached references: fo\u00aer example, if you load several Item instances and put them in a Set or use them as keys in a Map.Because you're then comparing instances outside the guaranteed scope of object identity the persistence context you must override the equals() and hashCode() methods on the Item entity class as explained in sec\u00aetion 10.3.1. In the trivial conversationswith only one detached list of Item instances, this wasn't necessary. You never comparedthem in a Set, used them as keys in a HashMap, or tested them explicitly for equality. You should enable versioning of the Item entity for multiuser applications, asexplained i\u00aen the section \"Enabling versioning\" in chapter 11. When entity modifications are merged in AuctionService#storeItem(), Hibernate increments the Item'sversion (it doesn't if the Item wasn't modified, though).",
  "page183": " Hence, if another user haschanged the name of an Item concurrently, Hibernate will throw an exception whenthe system transaction is committed and the persistence context is flushed. The firstuser to commit their conversation always wins with this optimistic strategy. The secondu\u00aeser should be shown the usual error message: \"Sorry, someone else modified thesame data; please restart your conversation.\" What you've created is a system with a rich client or thick client; the client isn't a dumbinput/output terminal but an application with an internal state independent of theserver (recall that the server doesn't hold any application state). One of the advantagesof such a stateless server is that any server can handle any client request. If a s\u00aeerver fails,you can route the next request to a different server, and the conversation process continues. The servers in a cluster share nothing; you can easily scale up your system horizontally by adding more servers. Obviously, all application servers still share thedatabase system, but at least you only have\u00aeto worry about scaling up one tier of servers.The downside is that you need to write rich-client applications, and you have to dealwith network communication and data-serialization issues. Complexity shifts from theserver side to the client side, and you have to optimize communication between theclient and server.",
  "page184": " If, instead of on an EJB client, your (JavaScript) client has to work on several webbrowsers or even as a native application on different (mobile) operating systems, thiscan certainly be a challenge. We recommend this architecture if your rich client runsin popular web browsers,\u00aewhere users download the latest version of the client application every time they visit your website. Rolling out native clients on several platforms,and maintaining and upgrading the installations, can be a significant burden even inmedium-sized intranets where you control the user's environment. Without an EJB environment, you have to customize serialization and transmissionof detached entity state between the client and the server. Can you serialize and deserialize an Item instanc\u00aee? What happens if you didn't write your client in Java? We'lllook at this issue in section 19.4. Next, you implement the same use cases again, but with avery different strategy.The server will now hold the conversational state of the application, and the client willonly be a dumb input/output device.\u00aeThis is an architecture with a thin client and astateful server.The application you'll write is still simple. It supports the same two uses cases as before:editing an auction item and placing a bid for an item. No difference is visible to theusers of the application; the EJB console client still looks like figures 18.2 and 18.4.",
  "page185": " With a thin client, it's the server's job to transform data for output into a display format understood by the thin client for example, into HTML pages rendered by a webbrowser. The client transmits user input operations directly to the server for example, as simple HT\u00aeMLform submissions. The server is responsible for decoding andtransforming the input into higher-level domain model operations. We keep this partsimple for now, though, and use only remote method invocation with an EJB client. The server must then also hold conversational data, usually stored in some kind ofserver-side session associated with a particular client. Note that a client's session has alarger scope than a single conversation; a user may perform several conversations during\u00aea session. If the user walks away from the client and doesn't complete a conversation, temporary conversation data must be removed on the server at some point. Theserver typically handles this situation with timeouts; for example, the server may discard a client's session and all the data it contains\u00aeafter a certain period of inactivity.This sounds like a job for EJB stateful session beans, and, indeed, they're ideal for thiskind of architecture if you're in need of a standardized solution. Keeping those fundamental issues in mind, let's implement the first use case: editing an auction item.",
  "page186": "The client presents the user again with a list of auction items, and the user picks one.This part of the application is trivial, and you don't have to hold any conversationalstate on the server. Have a look at the sequence of calls and contexts in figure 18.6.Even if you hav\u00aee a stateful server architecture, there will be many short conversationsin your application that don'trequire any state to be held on the server. This is bothnormal and important: holding state on the server consumes resources. If you implemented the getSummaries() operation with a stateful session bean, you'd wasteresources. You'd only use the stateful bean for a single operation, and then it wouldconsume memory until the container expired it. Stateful server architecture d\u00aeoesn'tmean you can only use stateful server-side components. Next, the client renders the ItemBidSummary list, which only contains the identifier of each auction item, its description, and the current highest bid. This is exactlywhat the user sees on the screen, as shown in figure 18.2. The user then enter\u00aes an itemidentifier and starts a conversation that works with this item. You can see theroad mapfor this conversation in figure 18.7.The service called here isn't the pooled stateless AuctionService from the last section. This new ItemService is a stateful component; the server will create an instanceand\u00aeassign it to this client exclusively. You implement this service with a stateful session bean.",
  "page187": "There are many annotations on this class, defining how the container will handle thisstateful component. With a 10-minute timeout, the server removes and destroys aninstance of this component if the client hasn't called it in the last 10 minutes. Thishandles dangling convers\u00aeations: for example, when a user walks away from the client. You also disable passivation for this EJB: an EJB container may serialize and storethe stateful component to disk, topreserve memory or to transmit it to another nodein a cluster when session-failover is needed. This passivation won't work because ofone member field: the EntityManager. You're attaching a persistence context to thisstateful component with the EXTENDED switch, and the EntityManager isn'tjava.io.Seria\u00aelizable.You use @PersistenceContext to declare that this stateful bean needs an EntityManager and that the container should extend the persistence context to span the sameduration as the life cycle of the stateful session bean. This extended mode is an optionexclusively for stateful EJBs. Without it, the contai\u00aener will create and close a persistence context whenever a transaction commits. Here, you want the persistence context to stay open beyond any transaction boundaries and remain attached to thestateful session bean instance.Furthermore, you don't want the persistence context to be flushed automaticallywhe\u00aen a transaction commits, so you configure it to be UNSYNCHRONIZED. Hibernate willonly flush the persistence context after you manually join it with a transaction. NowHibernate won't automatically write to the database changes you make to loaded persistent entity instances; instead, you queue them until you're readyto write everything.",
  "page188": "At the start of the conversation, the server loads an Item instance and holds it asconversational state in a member field (figure 18.7). The ItemDAO also needs anEntityManager; remember that it has a @PersistenceContext annotation without anyoptions. The rules for persistence co\u00aentext propagation in EJB calls you've seen beforestill apply: Hibernate propagates the persistence context along with the transactioncontext. The persistence context rides along into the ItemDAO with the transactionthat was started for the startConversation() method call. When startConversation() returns, the transaction is committed, but the persistence context is neitherflushed nor closed. The ItemServiceImpl instance waits on the server for the nextcall from the client. The next ca\u00aell from the client instructs the server to change the item's nameOn the server, a transaction is started for the setItemName() method. But because notransactional resources are involved (no DAO calls, no EntityManager calls), nothinghappens but a change to the Item you hold in conversational state:Note tha\u00aet the Item is still be in persistent state the persistence context is still open!But because it isn't synchronized, it won't detect the change you made to the Item,because it won't be flushed when the transaction commits. Finally,the client ends the conversation, giving the OK to store all chan\u00aeges on theserver On the server, you can flush changes to the database and discard the conversationalstate.",
  "page189": "You've now completed the implementation of the first use case. We'll skip the implementation of the second use case (placing a bid) and refer you to the example codefor details. The code for the second case is almost the same as the first, and youshouldn't have any\u00aeproblems understanding it. The important aspect you must understand is how persistence context and transaction handling work in EJBs.As in our analysis of the stateless application, the first question is how the unit of workis implemented from the perspective of the user. In particular, you need to ask howatomicity of the conversation works and how you can make all steps in the workflowappear as a single unit. At some point, usually when the last request in a conversation occurs, you comm\u00aeitthe conversation and write the changes to the database. The conversation is atomic ifyou don't join the extended EntityManager with a transaction until the last event inthe conversation. No dirty checking and flushing will occur if you only read data inunsynchronized mode. While you keep the persistence\u00aecontext open, you can keep loading data lazily byaccessing proxies and unloaded collections; this is obviously convenient. The loadedItem and other data become stale, however, if the user needs a long time to trigger thenext request.",
  "page190": "You may want to refresh() some managed entity instances during theconversation if you need updates from the database, as explained in section 10.2.6.Alternatively, you can refresh to undo an operation during a conversation. For example, if the user changes the Item#name in a dial\u00aeog but then decides to undo this, youcan refresh() the persistent Item instance to retrieve the \"old\" name from the database. This is a nice feature of an extended persistence context and allows the Item tobe always available in managed persistent state.The stateful server architecture may be more difficult to scale horizontally. If a serverfails, the state of the current conversation and indeed the entire session is lost. Replicating sessions on several servers is a costly opera\u00aetion, because any modification of session data on one server involves network communication to (potentially all) otherservers. With stateful EJBs and a member-extended EntityManager, serialization of thisextended persistence context isn't possible. If you use stateful EJBs and an extendedpersistence contex\u00aet in a cluster, consider enabling sticky sessions, causing a particularclient's requests to always route to the same server. This allows you to handle moreload with additional servers easily, but your users must accept losing session state whena server fails. On the other hand, stateful servers can act a\u00aes a first line of caches with theirextended persistence contexts in user sessions. Once an Item has been loaded for aparticular user conversation, that Item won't be loaded again from the database in thesame conversation. This can be a great tool to reduce the load on your database servers (the most expensive tier to scale).",
  "page191": " An extended persistence-context strategy requires more memory on the serverthan holding only detached instances: the persistence context in Hibernate contains asnapshot copy of all managed instances. You may want to manually detach() managedinstances to control what is held in t\u00aehe persistence context, or disable dirty checkingand snapshots (while still being able to lazy load) as explained in section 10.2.8. There are alternative implementations of thin clients and stateful servers, ofcourse. You can use regular request-scoped persistence contexts and manage detached(not persistent) entity instances on the server manually. This is certainly possible withdetaching and merging but can be much more work. One of the main advantages ofthe extended persistence context,\u00aetransparent lazy loading even across requests, wouldno longer be available either. In the next chapter, we'll show you such a stateful serviceimplementation with request-scoped persistence contexts in CDI and JSF, and you cancompare it with the extended persistence context feature of EJBs you've seen\u00aein thischapter. Thin client systems typically produce more load on servers than rich clients do.Every time the user interacts with the application, a client event results in a networkrequest. This can even happen for every mouse click in a web application. Only theserver knows the state of the current conver\u00aesation and has to prepare and render allinformation the user is viewing. A rich client, on the other hand, can load raw dataneeded for a conversation in one request, transform it, and bind it locally to the userinterface as needed. A dialog in a rich client can queue modifications on the clientside and fire a network request only when it has to make changes persistent at the endof a conversation.",
  "page192": "An additional challenge with thin clients is parallel conversations by one user: whathappens if a user is editing two items at the same time for example, in two webbrowser tabs? This means the user has two parallel conversations with the server. Theserver must separate data in th\u00aee user session by conversation. Client requests during aconversation must therefore contain some sort of conversationidentifier so you canselect the correct conversation state from the user's session for each request. This happens automatically with EJB clients and servers but probably isn't built into your favorite web application framework (unless it's JSF and CDI, as you'll see in the nextchapter). One significant benefit of a stateful server is less reliance on the\u00aeclient platform; ifthe client is a simple input/output terminal with few moving parts, there is less chancefor things to go wrong. The only place you have to implement data validation andsecurity checks is the server. There are no deployment issues to deal with; you can rollout application upgrades on servers w\u00aeithout touching clients. Today, there are few advantages to thin client systems, and stateful server installations are declining. This is especially true in the web application sector, where easyscalability is frequently a major concern.You implemented simple conversations units of work, from the perspective\u00aeofyour application user. You saw two server and client designs, with stateless and stateful servers, andlearned how Hibernate fits into both these architectures. You can work with either detached entity state or an extended conversationscoped persistence context.",
  "page193": "There are dozens of web application frameworks for Java, so we apologize ifwe don't cover your favorite combination here. We discuss JPA in the standard JavaEnterprise Edition environment, in particular combined with standards Contextsand Dependency Injection (CDI), JavaServ\u00aeer Faces (JSF), and Java API for RESTfulweb services (JAX-RS). As always, we show patterns you can apply in other proprietary environments. First we revisit the persistence layer and introduce CDI management for theDAO classes. Then we extend these classes with a generic solution for sorting andpaging data. This solution is useful whenever you have to display data in tables, nomatter what framework you choose. Next, you write a fully functional JSF application on top of the persistence lay\u00aeerand look at the Java EE conversation scope, where CDI and JSF work together toprovide a simple stateful model for server-side components. If you didn't like the stateful EJBs with the extended persistence context in the last chapter, maybe these conversation examples with detached entity state on the ser\u00aever are what you're looking for. Finally, if you prefer writing web applications with rich clients, a stateless server,and frameworks such as JAX-RS, GWT, or AngularJS, we show you how to customizeserialization of JPA entity instances into XML and JSON formats. We start with migrating the persistence lay\u00aeer from EJB components to CDI.",
  "page194": "The CDI standard offers a type-safe dependency injection and component life-cyclemanagement system in a Java EE runtime environment. You saw the Inject annotation in the previous chapter and used it to wire the ItemDAO and BidDAO componentstogether with the service EJB classes. T\u00aehe JPA @PersistenceContext annotation you used inside the DAO classes is justanother special injection case: you tell the runtime container to provide and automatically handle an EntityManager instance. This is a container-managed EntityManager.There are some strings attached, though,such as the persistence-context propagationand transaction rules we discussed in the previous chapter. Such rules are convenientwhen all of your service and DAO classes are EJBs, but if you don't employ E\u00aeJBs, you maynot want to follow these rules. With an application-managed EntityManager, you can create your own persistence context management, propagation, and injection rules. You now rewrite the DAO classes as simple CDI managed beans, which are just likeEJBs: plain Java classes with extra annotations. You wa\u00aent to Inject an EntityManagerand drop the PersistenceContext annotation, and thus have full control over thepersistence context. Before you can inject your own EntityManager, you must produce it.A producer in CDI parlance is a factory used to customize creation of an instance andtell the runtime container to\u00aecall a custom routine whenever the application needs aninstance based on the declared scope. For example, the container will create an application-scoped instance only once during the life cycle ofthe application.",
  "page195": "The container creates a request-scoped instance once for every request handled by a serverand a session-scoped instance once for every session a user has with a server. The CDI specification maps the abstract notions of request and session to servletrequests and sessions. Remembe\u00aer that both JSF and JAX-RS build on top of servlets, soCDI works well with those frameworks. In other words, don't worry much about this: ina Java EE environment, all the integration work has already been done for you. Let's create a producer of request-scoped EntityManager instances:This CDI annotation declares that only one producer is needed in the entire application: there will only ever be one instance of EntityManagerProducer. The Java EE runtime gives you the persistence u\u00aenit configured in persistence.xml,which is also an application-scoped component. (If you use CDI standalone and outsidea Java EE environment, you can instead use the static Persistence.createEntityManagerFactory() bootstrap.) Whenever an EntityManager is needed, create() is called. The container reuses thesame\u00aeEntityManager during a request handled by your server. (If you forget@RequestScoped on the method, the EntityManager will be application-scoped likethe producer class!)E When a request is over and the request context is being destroyed, the CDI containercalls this method to get rid of an EntityManager instanc\u00aee. You created this applicationmanaged persistence context (see section 10.1.2), so it's your job to close it.",
  "page196": "A common issue with CDI-annotated classes is mistaken imports of annotations. InJava EE 7, there are two annotations called @Produces; the other one is in javax.ws.rs (JAX-RS). This isn't the same as the CDI producer annotation and you can spendhours looking for this error i\u00aen your code if you pick the wrong one. Another duplicateis @RequestScoped, also in javax.faces.bean (JSF). Like all other outdated JSF beanmanagement annotations in the javax.faces.bean package, don't use them whenyou have the more modern CDI available. We hope that futureJava EE specificationversions will resolve these ambiguities. You now have a producer of application-managed EntityManagers and requestscoped persistence contexts. Next, you must find a way to let the EntityManager k\u00aenowabout your system transactions.When your server must handle a servlet request, the container creates the EntityManager automatically when it first needs it for injection. Remember that an EntityManager you manually create will only join a system transaction automatically if thetransaction is already in progr\u00aeess. Otherwise, it will be unsynchronized: you'll read datain auto-commit mode, and Hibernate won't flush the persistence context. It's not always obvious when the container will call the EntityManager producer orexactly when during a request the first EntityManager injection takes place. Ofcou\u00aerse, if you process a request without a system transaction, the EntityManager youget is always unsynchronized. Hence, you must ensure that the EntityManager knowsyou have a system transaction.",
  "page197": "You should call this method on any DAO before storing data, when you're sure you'rein a transaction. If you forget, Hibernate will throw a TransactionRequiredExceptionwhen you try to write data, indicating that the EntityManager has been created beforea transaction was\u00aestarted and that nobody told it about the transaction. If you want toexercise your CDI skills, you could try to implement this aspect with a CDI decorator orinterceptor. Let's implement this GenericDAO interface method and wire the new EntityManager to the DAO classes.The old GenericDAOImpl relies on the @PersistenceContext annotation for injectionof an EntityManager in a field, or someone calling setEntityManager() before theDAO is used. With CDI, you can use the safer constructor-in\u00aejection technique:Anyone who wants to instantiate a DAO must provide an EntityManager. This declaration of an invariant of the class is a much stronger guarantee; hence, although we frequently use field injection in our examples, you should always first think aboutconstructor injection. (We don't do it in\u00aesome examples because they would be evenlonger, and this book is already quite big.) In the concrete (entity DAO) subclasses, declare the necessary injection on theconstructor:When your application needs an ItemDAO, the CDI runtime will call your EntityManagerProducer and then call the ItemDAOImpl constructor\u00ae. The container will reusethe same EntityManager for any injection in any DAO during a particular request. What scope then is ItemDAO? Because you don't declare a scope for the implementation class, it's dependent.",
  "page198": "An ItemDAO is created whenever someone needs it, and theItemDAO instance is then in the same context and scope as its caller and belongs to thatcalling object. This is a good choice for the persistence layer API, because you delegatescoping decisions to the upper layer with the s\u00aeervices calling the persistence layer. You're now ready to @Inject an ItemDAO field in a service class. Before you use yourCDI-enabled persistence layer, let's add some functionality for paging and sorting data.A very common task is loading data from the database with a query and then displaying that data on a web page in a table. Frequently you must also implement dynamicpaging and sorting of the data: Because the query returns much more information than can be displayed on asin\u00aegle page, you only show a subset of the data. You only render a certain number of rows in a data table and give users the options to go to the next, previous,first, or last page of rows. Users also expect the application to preserve sortingwhen they switch pages. Users want to be able to click a column header i\u00aen the table and sort the rows ofthe table by the values of this column. Typically, you can sort in either ascendingor descending order; this can be switched by subsequent clicks the columnheader.You now implement a generic solution for browsing through pages of data, based onthe metamodel of persistent classe\u00aes provided by JPA. Page browsing can be implemented in two variations: using the offset or the seektechnique. Let's look first at the differences and what you want to implement.",
  "page199": "Figure 19.1 shows an example of a browsing UI with an offset-based paging mechanism. You see that there's a handful of auction items and that a small page size ofthree records is used. Here, you're on the first page; the application renders links tothe other pages dynam\u00aeically. The current sort order is by ascending item name. Youcan click the column header and change the sorting to descending (or ascending) byname, auction end date, or highest bid amount. Clicking the item name in each tablerow opens the item detail view, where you can make bids for an item. We take on thisuse case later in this chapter.Behind this page are database queries with an offset and a limit condition, based onrow numbers. The Java Persistence API for this are Query#setFirstResu\u00aelt() andQuery#setMaxResults(), which we discussed in section 14.2.4. You write a query andthen let Hibernate wrap the offset and limit clauses around it, depending on yourdatabase SQL dialect. Now consider the alternative: paging with a seek method, as shown in figure 19.2.Here you don't offer users the op\u00aetion to jump to any page by offset; you only allowthem to seek forward, to thenext page. This may seem restrictive, but you've probablyseen or even implemented such a paging routine when you needed infinite scrolling.You can, for example, automatically load and display the next page of data when theuser\u00aereaches the bottom of the table/screen.",
  "page200": "The seek method relies on a special additional restriction in the query retrieving thedata. When the next page has to be loaded, you query for all items with a name\"greater than [Coffee Machine]\". You seek forward not by offset of result rows withsetFirstResult(), but b\u00aey restricting the result based on the ordered values of somekey. If you're unfamiliar with seek paging (sometimes called keyset paging), we're sureyou won't find this difficult once you see the queries later in this section. Let's compare the advantages and disadvantages of both techniques. You can ofcourse implement an endless-scrolling feature with offset paging or direct-page navigation with the seek technique; but they each have their strength and weaknesses: The of\u00aefset method is great when users want to jump to pages directly. For example, many search engines offer the option to jump directly to page 42 of a queryresult or directly to the last page. Because you can easily calculate the offset andlimit of a range of rows based on the desired page number, you can implement\u00aethis with little effort.With the seek method, providing such a page-jumping UIis more difficult; you must know the value to seek. You don't know what itemname the client displayed just before page 42, so you can't seek forward toitems with names \"greater than X.\" Seeking is best suited for\u00aea UI where usersonly move forward or backward page by page through a data list or table, and ifyou can easily remember the last value shown to the user. A great use case for seeking is paging based on anchor values that you don'thave to remember. For example, all customers whose names start with C could.",
  "page201": "Introduction to Threads- Like people, computers can multitask. That is, they can be working on several different tasks at the same time. A computer that has just a single central processing unit can't literally do two things at the same time, any more than a person can, but\u00aeit can still switch its attention back and forth among several tasks. Furthermore, it is increasingly common for computers to have more than one processing unit, and such computers can literally work on several tasks simultaneously. It is likely that from now on, most of the increase in computing power will come from adding additional processors to computers rather than from increasing the speed of individual processors. To use the full power of these multiprocessing computers, a programme\u00aer must do parallel programming, which means writing a program as a set of several tasks that can be executed simultaneously. Even on a single-processor computer, parallel programming techniques can be useful, since some problems can be tackled most naturally by breaking the solution into a set of simultaneous t\u00aeasks that cooperate to solve the problem. In Java, a single task is called a thread. The term \"thread\" refers to a \"thread of control\" or \"thread of execution,\" meaning a sequence of instructions that are executed one after another the thread extends through time, connecting each\u00aeinstruction to the next. In a multithreaded program, there can be many threads of control, weaving through time in parallel and forming the complete fabric of the program. (Ok, enough with the metaphor, already!) Every Java program has at least one thread; when the Java virtual machine runs your program, it creates a thread that is responsible for executing the main routine of the program. This main thread can in turn create other threads that can continue even after the main thread has terminated. In a GUI program, there is at least one additional thread, which isresponsible for handling events and drawing components on the screen. This GUI thread is created when the first window is opened. So in fact, you have already done parallel programming! When a main routine opens a window, both the main thread and the GUI thread can continue to run in parallel. Of course, parallel programming can be used in much more interesting ways.",
  "page202": "Unfortunately, parallel programming is even more difficult than ordinary, single-threaded programming. When several threads are working together on a problem, a whole new category of errors is possible. This just means that techniques for writing correct and robust programs are e\u00aeven more important for parallel programming than they are for normal programming. (That's one excuse for having this section in this chapter another is that we will need threads at several points in future chapters, and I didn't have another place in the book where the topic fits more naturally.) Since threads are a difficult topic, you will probably not fully understand everything in this section the first time through the material. Your understanding should improve as you encou\u00aenter more examples of threads in future sections. Creating and Running Threads- In Java, a thread is represented by an object belonging to the class java.lang.Thread (or to a subclass of this class). The purpose of a Thread object is to execute a single method. The method is executed in its own thread of contro\u00ael, which can run in parallel with other threads. When the execution of the method is finished, either because the method terminates normally or because of an uncaught exception, the thread stops running. Once this happens, there is no way to restart the thread or to use the same Thread object to start another\u00aethread. Operations on Threads- The Thread class includes several useful methods in addition to the start() method that was discussed above. I will mention just a few of them. If thrd is an object of type Thread, then the boolean-valued function thrd.isAlive() can be used to test whether or not the thread is alive. A thread is \"alive\" between the time it is started and the time when it terminates. After the thread has terminated it is said to be \"dead\". (The rather gruesome metaphor is also used when we refer to \"killing\" or \"aborting\" a thread.)",
  "page203": "The static method Thread.sleep(milliseconds) causes the thread that executes this method to \"sleep\" for the specified number of milliseconds. A sleeping thread is still alive, but it is not running. While a thread is sleeping, the computer will work on any other runnabl\u00aee threads (or on other programs). Thread.sleep() can be used to insert a pause in the execution of a thread. The sleep method can throw an exception of type InterruptedException, which is an exception class that requires mandatory exception handling. In practice, this means that the sleep method is usually used in a trycatch statement that catches the potential InterruptedException: try { Thread.sleep(lengthOfPause); } catch (InterruptedException e) { } One thread can interrupt another thr\u00aeead to wake it up when it is sleeping or paused for some other reason. A Thread, thrd, can be interrupted by calling its method thrd.interrupt(), but you are not likely to do this until you start writing rather advanced applications, and you are not likely to need to do anything in response to an InterruptedExc\u00aeeption (except to catch it). It's unfortunate that you have to worry about it at all, but that's the way that mandatory exception handling works. Mutual Exclusion with \"synchronized\" Programming several threads to carry out independent tasks is easy. The real difficulty arises when threads\u00aehave to interact in some way. One way that threads interact is by sharing resources. When two threads need access to the same resource, such as a variable or a window on the screen, some care must be taken that they don't try to use the same resource at the same time. Otherwise, the situation could be something like this: Imagine several cooks sharing the use of just one measuring cup, and imagine that Cook A fills the measuring cup with milk, only to have Cook B grab the cup before Cook A has a chance to empty the milk into his bowl. There has to be some way for Cook A to claim exclusive rights to the cup while he performs the two operations: Add-Milk-To-Cup and Empty-Cup-Into-Bowl.",
  "page204": "Wait and Notify- Threads can interact with each other in other ways besides sharing resources. For example, one thread might produce some sort of result that is needed by another thread. This imposes some restriction on the order in which the threads can do their computations. If\u00aethe second thread gets to the point where it needs the result from the first thread, it might have to stop and wait for the result to be produced. Since the second thread can't continue, it might as well go to sleep. But then there has to be some way to notify the second thread when the result is ready, so that it can wake up and continue its computation. Java, of course, has a way to do this kind of waiting and notification: It has wait() and notify() methods that are defined as ins\u00aetance methods in class Object and so can be used with any object. The reason why wait() and notify() should be associated with objects is not obvious, so don't worry about it at this point. It does, at least, make it possible to direct different notifications to a different recipients, depending on which o\u00aebject's notify() method is called. Volatile Variables- And a final note on communication among threads: In general, threads communicate by sharing variables and accessing those variables in synchronized methods or synchronized statements. However, synchronization is fairly expensive computationally, and\u00aeexcessive use of it should be avoided. So in some cases, it can make sense for threads to refer to shared variables without synchronizing their access to those variables. However, a subtle problem arises when the value of a shared variable is set is one thread and used in another. Because of the way that threads are implemented in Java, the second thread might not see the changed value of the variable immediately. That is, it is possible that a thread will continue to see the old value of the shared variable for some time after the value of the variable has been changed by another thread. This is because threads are allowed to cache shared data. That is, each thread can keep its own local copy of the shared data.",
  "page205": "It is still possible to use a shared variable outside of synchronized code, but in that case, the variable must be declared to be volatile. The volatile keyword is a modifier that can be added to a variable declaration, as in private volatile int count; If a variable is declared\u00aeto be volatile, no thread will keep a local copy of that variable in its cache. Instead, the thread will always use the official, main copy of the variable. This means that any change made to the variable will immediately be available to all threads. This makes it safe for threads to refer to volatile shared variables even outside of synchronized code. (Remember, though, that synchronization is still the only way to prevent race conditions.) When the volatile modifier is applied to an obje\u00aect variable, only the variable itself is declared to be volatile, not the contents of the object that the variable points to. For this reason,volatile is generally only used for variables of simple types such as primitive types and enumerated types.Analysis of Algorithms- This chapter has concentrated mostly on\u00aecorrectness of programs. In practice, another issue is also important: efficiency. When analyzing a program in terms of efficiency, we want to look at questions such as, \"How long does it take for the program to run?\" and \"Is there another approach that will get the answer more quickly?\"\u00aeEfficiency willalways be less important than correctness; if you don't care whether a program works correctly, you can make it run very quickly indeed, but no one will think it's much of an achievement! On the other hand, a program that gives a correct answer after ten thousand years isn't very useful either, so efficiency is often an important issue.The term \"efficiency\" can refer to efficient use of almost any resource, including time,computer memory, disk space, or network bandwidth. In this section, however, we will deal exclusively with time efficiency, and the major question that we want to ask about a program is, how long does it take to perform its task?",
  "page206": "it really makes little sense to classify an individual program as being \"efficient\" or \"inefficient.\" It makes more sense to compare two (correct) programs that perform the same task and ask which one of the two is \"more efficient,\" that is, which on\u00aee performs the task more quickly. However, even here there are difficulties. The running time of a program is not well-defined. The run time can be different depending on the number and speed of the processors in the computer on which it is run and, in the case of Java, on the design of the Java Virtual Machine which is used to interpret the program. It can depend on details of the compiler which is used to translate the program from high-level language to machine language. Furthermore, th\u00aee run time of a program depends on the size of the problem which the program has to solve. It takes a sorting program longer to sort 10000 items than it takes it to sort 100 items. When the run times of two programs are compared, it often happens that Program A solves small problems faster than Program B, while\u00aeProgram B solves large problems faster than Program A, so that it is simply not the case that one program is faster than the other in all cases. the efficiency of programs. The field is known as Analysis of Algorithms. The focus is on algorithms, rather than on programs as such, to avoid having to deal with\u00aemultiple implementations of the same algorithm written in different languages, compiled with different compilers, and running on different computers. Analysis of Algorithms is a mathematical field that abstracts away from these down-and-dirty details. Still, even though it is a theoretical field, every working programmer should be aware of some of its techniques and results. This section is a very brief introduction to some of those techniques and results. Because this is not a mathematics book, the treatment will be rather informal.",
  "page207": "One of the main techniques of analysis of algorithms is asymptotic analysis. The term \"asymptotic\" here means basically \"the tendency in the long run.\" An asymptotic analysis of an algorithm's run time looks at the question of how the run time depends on\u00aethe size of the problem. The analysis is asymptotic because it only considers what happens to the run time as the size of the problem increases without limit; it is not concerned with what happens for problems of small size or, in fact, for problems of any fixed finite size. Only what happens in the long run, as the problem size increases without limit, is important. Showing that Algorithm A is asymptotically faster than Algorithm B doesn't necessarily mean that Algorithm A will run f\u00aeaster than Algorithm B for problems of size 10 or size 1000 or even size 1000000 it only means that if you keep increasing the problem size, you will eventually come to a point where Algorithm A is faster than Algorithm B. An asymptotic analysis is only a first approximation, but in practice it often gives impo\u00aertant and useful information. Central to asymptotic analysis is Big-Oh notation. Using this notation, we might say, for example, that an algorithm has a running time that is O(n2 ) or O(n) or O(log(n)). These notations are read \"Big-Oh of n squared,\" \"Big-Oh of n,\" and \"Big-Oh of log\u00aen\" (where log is a logarithm function). More generally, we can refer to O(f(n)) (\"Big-Oh of f of n\"), where f(n) is some function that assigns a positive real number to every positive integer n. The \"n\" in this notation refers to the size of the problem. Before you can even begin an asymptotic analysis, you need some way to measure problem size. Usually, this is not a big issue. For example, if the problem is to sort a list of items, then the problem size can be taken to be the number of items in the list. When the input to an algorithm is an integer, as in the case of algorithm that checks whether a given positive integer is prime, the usual measure of the size of a problem is the number of bits in the input integer rather than the integer itself. More generally, the number of bits in the input to a problem is often a good measure of the size of the problem.",
  "page208": "To say that the running time of an algorithm is O(f(n)) means that for large values of the problem size, n, the running time of the algorithm is no bigger than some constant times f(n). (More rigorously, there is a number C and a positive integer M such that whenever n is greater\u00aethan M, the run time is less than or equal to C*f(n).) The constant takes into account details such as the speed of the computer on which the algorithm is run; if you use a slower computer, you might have to use a bigger constant in the formula, but changing the constant won't change the basic fact that the run time is O(f(n)). The constant also makes it unnecessary to say whether we are measuring time in seconds, years, CPU cycles, or any other unit of measure; a change from one uni\u00aet of measure to another is just multiplication by a constant. Note also that O(f(n)) doesn't depend at all on what happens for small problem sizes, only on what happens in the long run as the problem size increases without limit. To look at a simple example, consider the problem of adding up all the number\u00aes in an array. The problem size, n, is the length of the array. Using A as the name of the array, the algorithm can be expressed in Java as: total = 0; for (int i = 0; i < n; i++) total = total + A[i]; This algorithm performs the same operation, total = total + A[i], n times. The total time spent on this oper\u00aeation is a*n,where a is the time it takes to perform the operation once. time spent on this operation is a*n, where a is the time it takes to perform the operation once. Now, this is not the only thing that is done in the algorithm. The value of i is incremented and is compared to n each time through the loop. This adds an additional time of b*n to the run time, for some constant b. Furthermore, i and total both have to be initialized to zero; this adds some constant amount c to the running time. The exact running time would then be (a+b)*n+c, where the constants a, b, and c depend on factors such as how the code is compiled and what computer it is run on.",
  "page209": "Using the fact that c is less than or equal to c*n for any positive integer n, we can say that the run time is less than or equal to (a+b+c)*n. That is, the run time is less than or equal to a constant times n. By definition, this means that the run time for this algorithm is O(n\u00ae). If this explanation is too mathematical for you, we can just note that for large values of n, the c in the formula (a+b)*n+c is insignificant compared to the other term, (a+b)*n. We say that c is a \"lower order term.\" When doing asymptotic analysis, lower order terms can be discarded. A rough, but correct, asymptotic analysis of the algorithm would go something like this: Each iteration of the for loop takes a certain constant amount of time. There are n iterations of the loop\u00ae, so the total run time is a constant times n, plus lower order terms (to account for the initialization). Disregarding lower order terms, we see that the run time is O(n). Note that to say that an algorithm has run time O(f(n)) is to say that its run time is no bigger than some constant times n (for large valu\u00aees of n). O(f(n)) puts an upper limit on the run time. However, the run time could be smaller, even much smaller. For example, if the run time is O(n), it would also be correct to say that the run time is O(n2 ) or even O(n10). If the run time is less than a constant times n, then it is certainly less than th\u00aee same constant times n2 or n10 Of course, sometimes it's useful to have a lower limit on the run time. That is, we want to be able to say that the run time is greater than or equal to some constant times f(n) (for large values of n). The notation for this is Ohm(f(n)), read \"Omega of f of n.\" \"Omega\" is the name of a letter in the Greek alphabet, and Ohm is the upper case version of that letter. (To be technical, saying that the run time of an algorithm is Ohm(f(n)) means that there is a positive number C and a positive integer M such that whenever n is greater than M, the run time is greater than or equal to C*f(n).) O(f(n)) tells you something about the maximum amount of time that you might have to wait for an algorithm to finish; Ohm(f(n)) tells you something about the minimum time.",
  "page210": "The algorithm for adding up the numbers in an array has a run time that is Ohm(n) as well as O(n). When an algorithm has a run time that is both Ohm(f(n)) and O(f(n)), its run time is said to be Theta(f(n)), read \"Theta of f of n.\" (Thetais another letter from th\u00aee Greek alphabet.) To say that the run time of an algorithm is Theta(f(n)) means that for large values of n, the run time is between a*f(n) and b*f(n), where a and b are constants (with b greater than a, and both greater than 0).So far, my analysis has ignored an important detail. We have looked at how run time depends on the problem size, but in fact the run time usually depends not just on the size of the problem but on the specific data that has to be processed. For example, the run ti\u00aeme of a sorting algorithm can depend on the initial order of the items that are to be sorted, and not just on the number of items. To account for this dependency, we can consider either the worst case run time analysis or the average case run time analysis of an algorithm. For a worst case run time analysis, we\u00aeconsider all possible problems of size n and look at the longest possible run time for all such problems. For an average case analysis, we consider all possible problems of size n and look at the average of the run times for all such problems. Usually, the average case analysis assumes that all problems of s\u00aeize n are equally likelyto be encountered, although this is not always realistic or even possible in the case where there is an infinite number of different problems of a given size. In many cases, the average and the worst case run times are the same to within a constant multiple. This means that as far as asymptotic analysis is concerned, they are the same. That is, if the average case run time is O(f(n)) or Theta(f(n)), then so is the worst case. However, later in the book, we will encounter a few cases where the averageand worst case asymptotic analyses differ.",
  "page211": "So, what do you really have to know about analysis of algorithms to read the rest of this book? We will not do any rigorous mathematical analysis, but you should be able to follow informal discussion of simple cases such as the examples that we have looked at in this section. Mos\u00aet important, though, you should have a feeling for exactly what it means to say that the running time of an algorithmis O(f(n)) or Theta(f(n)) for some common functions f(n). The main point is that these notations do not tell you anything about the actual numerical value of the running time of the algorithm for any particular case. They do not tell you anything at all about the running time for small values of n. What they do tell you is something about the rate of growth of the running t\u00aeime as the size of the problem increases. Suppose you compare two algorithm that solve the same problem. The run time of one algorithm is Theta(n2), while the run time of the second algorithm is Theta(n3). What does this tell you? If you want to know which algorithm will be faster for some particular problem\u00aeof size, say, 100, nothing is certain. As far as you can tell just from the asymptotic analysis, either algorithm could be faster for that particular case or in any particular case. But what you can say for sure is that if you look at larger and larger problems, you will come to a point where the Theta(n2) a\u00aelgorithm is faster than the Theta(n3) algorithm. Furthermore, as you continue to increase the problem size, the relative advantage of the Theta(n2) algorithm will continue to grow. There will be values of n for which the Theta(n2) algorithm is a thousand times faster, a million times faster, a billion times faster, and so on. This is because for any positive constants a and b, the function a*n3 grows faster than the function b*n2 as n gets larger. (Mathematically, the limit of the ratio of a*n3 to b*n2 is infinite as n approaches infinity.) This means that for \"large\" problems, a Theta(n2) algorithm will definitely be faster than a Theta(n3) algorithm. You just don't know based on the asymptotic analysis alone exactly how large \"large\" has to be. In practice, in fact, it is likely that the Theta(n2) algorithm will be faster even for fairly small values of n, and absent other information you would generally prefer a Theta(n2) algorithm to a Theta(n3) algorithm.",
  "page212": "Recursion- At one time or another, you've probably been told that you can't define something in terms of itself. Nevertheless, if it's done right, defining something at least partially in terms of itself can be a very powerful technique. A recursive definition is o\u00aene that uses the concept or thing that is being defined as part of the definition. For example: An \"ancestor\" is either a parent or an ancestor of a parent. A \"sentence\" can be, among other things, two sentences joined by a conjunction such as \"and.\" A \"directory\" is a part of a disk drive that can hold files and directories. In mathematics, a \"set\" is a collection of elements, which can themselves be sets. A \"statement\" in Java c\u00aean be a while statement, which is made up ofthe word \"while\", a boolean-valued condition, and a statement. Recursive definitions can describe very complex situations with just a few words. A definition of the term \"ancestor\" without using recursion might go something like \"a parent, or\u00ae\"and so on\" is not very rigorous. (I've often thought that recursion is really just a rigorous way of saying \"and so on.\") You run into the same problem if you try to define a \"directory\" as \"a file that is a list of files, where some of the files can be lists of files,\u00aewhere some of those files can be lists of files, and so on.\" Trying to describe what a Java statement can look like, without using recursion in the definition, would be difficult and probably pretty comical. Recursion can be used as a programming technique. A recursive subroutine is one that calls itself, either directly or indirectly. To say that a subroutine calls itself directly means that its definition contains a subroutine call statement that calls the subroutine that is being defined. To say that a subroutine calls itself indirectly means that it calls a second subroutine which in turn calls the first subroutine (either directly or indirectly). A recursive subroutine can define a complex task in just a few lines of code. In the rest of this section, we'll look at a variety of examples, and we'll see other examples in the rest of the book.",
  "page213": "Recursive Binary Search- Binary search is used to find a specified value in a sorted list of items (or, if it does not occur in the list, to determine that fact). The idea is to test the element in the middle of the list. If that element is equal to the specified value, you are d\u00aeone. If the specified value is less than the middle element of the list, then you should search for the value in the first half of the list. Otherwise, you should search for the value in the second half of the list. The method used to search for the value in the first or second half of the list is binary search. That is, you look at the middle element in the half of the list that is still under consideration, and either you've found the value you are looking for, or you have to apply\u00aebinary search to one half of the remaining elements. And so on! This is a recursive description, and we can write a recursive subroutine to implement it. Before we can do that, though, there are two considerations that we need to take into account. Each of these illustrates an important general fact about recur\u00aesive subroutines. First of all, the binary search algorithm begins by looking at the \"middle element of the list.\" But what if the list is empty? If there are no elements in the list, then it is impossible to look at the middle element. In the terminology of Subsection 8.2.1, having a non-empty list\u00aeis a \"precondition\" for looking at the middle element, and this is a clue that we have to modify the algorithm to take this precondition into account. What should we do if we find ourselves searching for a specified value in an empty list? The answer is easy: If the list is empty, we can be sure that the value does not occur in the list, so we can give the answer without any further work. An empty list is a base case for the binary search algorithm. A base case for a recursive algorithm is a case that is handled directly, rather than by applying the algorithm recursively. The binary search algorithm actually has another type of base case: If we find the element we are looking for in the middle of the list, we are done. There is no need for further recursion.",
  "page214": "Linked Data Structures- Every useful object contains instance variables. When the type of an instance variable is given by a class or interface name, the variable can hold a reference to another object. Such a reference is also called a pointer, and we say that the variable point\u00aes to the object. (Of course, any variable that can contain a reference to an object can also contain the special value null, which points to nowhere.) When one object contains an instance variable that points to another object, we think of the objects as being \"linked\" by the pointer. Data structures of great complexity can be constructed by linking objects together. Recursive Linking- Something interesting happens when an object contains an instance variable that can refer to an\u00aeother object of the same type. In that case, the definition of the object's class is recursive. Such recursion arises naturally in many cases. For example, consider a class designed to represent employees at a company. Suppose that every employee except the boss has a supervisor, who is another employee of\u00aethe company. As the while loop is executed, runner points in turn to the original employee, emp, then variable is incremented each time runner \"visits\" a new employee. The loop ends when runner.supervisor is null, which indicates that runner has reached the boss. At that point, count has counted th\u00aee number of steps between emp and the boss. In this example, the supervisor variable is quite natural and useful. In fact, data structures that are built by linking objects together are so useful that they are a major topic of study in computer science. We'll be looking at a few typical examples. In this section and the next, we'll be looking at linked lists. A linked list consists of a chain of objects of the same type, linked together by pointers from one object to the next. This is much like the chain of supervisors between emp and the boss in the above example. It's also possible to have more complex situations, in which one object can contain links to several other objects.",
  "page215": "Linked Lists- For most of the examples in the rest of this section, linked lists will be constructed out of objects belonging to the class Node which is defined as follows: class Node { String item; Node next; } The term node is often used to refer to one of the objects in a link\u00aeed data structure. Objects of type Node can be chained together as shown in the top part of the above picture. Each node holds a String and a pointer to the next node in the list (if any). The last node in such a list can always be identified by the fact that the instance variable next in the last node holds the value null instead of a pointer to another node. The purpose of the chain of nodes is to represent a list of strings. The first string in the list is stored in the first node, the\u00aesecond string is stored in the second node, and so on. The pointers and the node objects are used to build the structure, but the data that we are interested in representing is the list of strings. Of course, we could just as easily represent a list of integers or a list of JButtons or a list of any other type\u00aeof data by changing the type of the item that is stored in each node. Although the Nodes in this example are very simple, we can use them to illustrate the common operations on linked lists. Typical operations include deleting nodes from the list, inserting new nodes into the list, and searching for a specifi\u00aeed String among the items in the list. We will look at subroutines to perform all of these operations, among others. For a linked list to be used in a program, that program needs a variable that refers to the first node in the list. It only needs a pointer to the first node since all the other nodes in the list can be accessed by starting at the first node and following links along the list from one node to the next. In my examples, I will always use a variable named head, of type Node, that points to the first node in the linked list. When the list is empty, the value of head is null.",
  "page216": "Stacks, Queues, and ADTs- A linked list is a particular type of data structure, made up of objects linked together by pointers. In the previous section, we used a linked list to store an ordered list of Strings, and we implemented insert, delete, and find operations on that list.\u00aeHowever, we could easily have stored the list of Strings in an array or ArrayList, instead of in a linked list. We could still have implemented the same operations on the list. The implementations of these operations would have been different, but their interfaces and logical behavior would still be the same. The term abstract data type, or ADT, refers to a set of possible values and a set of operations on those values, without any specification of how the values are to be represented or\u00aehow the operations are to be implemented. An \"ordered list of strings\" can be defined as an abstract data type. Any sequence of Strings that is arranged in increasing order is a possible value of this data type. The operations on the data type include inserting a new string, deleting a string, and fin\u00aeding a string in the list. There are often several different ways to implement the same abstract data type. For example, the \"ordered list of strings\" ADT can be implemented as a linked list or as an array. A program that only depends on the abstract definition of the ADT can use either implementati\u00aeon, interchangeably. In particular, the implementation of the ADT can be changed without affecting the program as a whole. This can make the program easier to debug and maintain, so ADTs are an important tool in software engineering. In this section, we'll look at two common abstract data types, stacks and queues. Both stacks and queues are often implemented as linked lists, but that is not the only possibleimplementation. You should think of the rest of this section partly as a discussion of stacks and queues and partly as a case study in ADTs.",
  "page217": "Stacks- A stack consists of a sequence of items, which should be thought of as piled one on top of the other like a physical stack of boxes or cafeteria trays. Only the top item on the stack is accessible at any given time. It can be removed from the stack with an operation calle\u00aed pop. An item lower down on the stack can only be removed after all the items on top of it have been popped off the stack. A new item can be added to the top of the stack with an operation called push. We can make a stack of any type of items. If, for example, the items are values of type int, then the push and pop operations can be implemented as instance methods void push (int newItem) Add newItem to top of stack. int pop() Remove the top int from the stack and return it. It is an error\u00aeto try to pop an item from an empty stack, so it is important to be able to tell whether a stack is empty. We need another stack operation to do the test, implemented as an instance method boolean isEmpty() Returns true if the stack is empty. To get a better handle on the difference between stacks and queues,\u00aeconsider the sample program DepthBreadth.java. I suggest that you run the program or try the applet version that can be found in the on-line version of this section. The program shows a grid of squares. Initially, all the squares are white. When you click on a white square, the program will gradually mark all\u00aethe squares in the grid, starting from the one where you click. To understand how the program does this, think of yourself in the place of the program. When the user clicks a square, you are handed an index card. The location of the square its row and column is written on the card. You put the card in a pile, which then contains just that one card. Then, you repeat the following: If the pile is empty, youare done. Otherwise, take an index card from the pile. The index card specifies a square. Look at each horizontal and vertical neighbor of that square. If the neighbor has not already been encountered, write its location on a new index card and put the card in the pile.",
  "page218": "While a square is in the pile, waiting to be processed, it is colored red; that is, red squares have been encountered but not yet processed. When a square is taken from the pile and processed, its color changes to gray. Once a square has been colored gray, its color won't ch\u00aeange again. Eventually, all the squares have been processed, and the procedure ends. In the index card analogy, the pile of cards has been emptied. The program can use your choice of three methods: Stack, Queue, and Random. In each case, the same general procedure is used. The only difference is how the \"pileof index cards\" is managed. For a stack, cards are added and removed at the top of the pile. For a queue, cards are added to the bottom of the pile and removed from the top.\u00aeIn the random case, the card to be processed is picked at random from among all the cards in the pile. The order of processing is very different in these three cases. You should experiment with the program to see how it all works. Try to understand how stacks and queues are being used. Try starting from one of\u00aethe corner squares. While the process is going on, you can click on other white squares, and they will be added to the pile. When you do this with a stack, you should notice that the square you click is processed immediately, and all the red squares that were already waiting for processing have to wait. On th\u00aee other hand, if you do this with a queue, the square that you click will wait its turn until all the squares that were already in the pile have been processed. Queues seem very natural because they occur so often in real life, but there are times when stacks are appropriate and even essential. For example, consider what happens when a routine calls a subroutine. The first routine is suspended while the subroutine is executed, and it will continue only when the subroutine returns. Now, suppose that the subroutine calls a second subroutine, and the second subroutine calls a third, and so on. Each subroutine is suspended while the subsequent subroutines are executed. The computer has to keep track of all the subroutines that are suspended. It does this with a stack.",
  "page219": "When a subroutine is called, an activation record is created for that subroutine. The activation record contains information relevant to the execution of the subroutine, such as its local variables and parameters. The activation record for the subroutine is placedon a stack. It w\u00aeill be removed from the stack and destroyed when the subroutine returns. If the subroutine calls another subroutine, the activation record of the second subroutine is pushed onto the stack, on top of the activation record of the first subroutine. The stack can continue to grow as more subroutines are called, and it shrinks as those subroutines return. Postfix Expressions As another example, stacks can be used to evaluate postfix expressions. An ordinary mathematical expression such as 2+(1\u00ae5-12)*17 is called an infix expression. In an infix expression, an operator comes in between its two operands, as in \"2 + 2\". In a postfix expression, an operator comes after its two operands, as in \"2 2 +\". The infix expression \"2+(15-12)*17\" would be written in postfix form as\u00ae201c2 15 12 - 17 * +\". The \"-\" operator in this expression applies to the two operands that precede it, namely \"15\" and \"12\". The \"*\" operator applies to the two operands that precede it, namely \"15 12 -\" and \"17\". And the \"+\" operator ap\u00aeplies to \"2\" and \"15 12 - 17 *\". These are the same computations that are done in the original infix expression. Now, suppose that we want to process the expression \"2 15 12 - 17 * +\", from left to right and find its value. The first item we encounter is the 2, but what can we do with it? At this point, we don't know what operator, if any, will be applied to the 2 or what the other operand might be. We have to remember the 2 for later processing. We do this by pushing it onto a stack. Moving on to the next item, we see a 15, which is pushed onto the stack on top of the 2. Then the 12 is added to the stack. Now, we come to the operator, \"-\". This operation applies to the two operands that preceded it in the expression.",
  "page220": "These numbers are multiplied, and the result, 51 is pushed onto the stack. The next item in the expression is a \"+\" operator, which is processed by popping 51 and 2 from the stack, adding them, and pushing the result, 53, onto the stack. Finally, we've come to the\u00aeend of the expression. The number on the stack is the value of the entire expression, so all we have to do is pop the answer from the stack, and we are done! The value of the expression is 53. Tree Traversal- Consider any node in a binary tree. Look at that node together with all its descendents (that is, its children, the children of its children, and so on). This set of nodes forms a binary tree, which is called a subtree of the original tree. For example, in the picture, nodes 2, 4, and\u00ae5 form a subtree. This subtree is called the left subtree of the root. Similarly, nodes 3 and 6 make up the right subtree of the root. We can consider any non-empty binary tree to be made up of a root node, a left subtree, and a right subtree. Either or both of the subtrees can be empty. This is a recursive de\u00aefinition, matching the recursive definition ofthe TreeNode class. So it should not be a surprise that recursive subroutines are often used to process trees. Consider the problem of counting the nodes in a binary tree. (As an exercise, you might try to come upwith a non-recursive algorithm to do the counting,\u00aebut you shouldn't expect to find one.) The heart of problem is keeping track of which nodes remain to be counted. It's not so easy to do this, and in fact it's not even possible without an auxiliary data structure such as a stack or queue. With recursion, however, the algorithm is almost trivial. Either the tree is empty or it consists of a root and two subtrees. If the tree is empty, the number of nodes is zero. (This is the base case of the recursion.) Otherwise, use recursion to count the nodes in each subtree. Add the results from the subtrees together, and add one to count the root.",
  "page221": "Binary Sort Trees- One of the examples in Section 9.2 was a linked list of strings, in which the strings were kept in increasing order. While a linked list works well for a small number of strings, it becomes inefficient for a large number of items. When inserting an item into th\u00aee list, searching for that item's position requires looking at, on average, half the items in the list. Finding an item in the list requires a similar amount of time. If the strings are stored in a sorted array instead of in a linked list, then searching becomes more efficient because binary search can be used. However, inserting a new item into the array is still inefficient since it means moving, on average, half of the items in the array to make a space for the new item. A binary t\u00aeree can be used tostore an ordered list of strings, or other items, in a way that makes both searching and insertion efficient. A binary tree used in this way is called a binary sort tree. A binary sort tree is a binary tree with the following property: For every node in the tree, the item in that node is great\u00aeer than every item in the left subtree of that node, and it is less than or equal to all the items in the right subtree of that node. Here for example is a binary sort tree containing items of type String. (In this picture, I haven't bothered to draw all the pointer variables. Non-null pointers are shown\u00aeas arrows.)",
  "page222": "Binary sort trees have this useful property: An inorder traversal of the tree will process the items in increasing order. In fact, this is really just another way of expressing the definition. For example, if an inorder traversal is used to print the items in the tree shown above\u00ae, then the items will be in alphabetical order. The definition of an inorder traversal guarantees that all the items in the left subtree of \"judy\" are printed before \"judy\", and all the items in the right subtree of \"judy\" are printed after \"judy\". But the binary sort tree property guarantees that the items in the left subtree of \"judy\" are precisely those that precede \"judy\" in alphabetical order, and all the items in the right s\u00aeubtree follow \"judy\" in alphabetical order. So, we know that \"judy\" is output in its proper alphabetical position. But the same argument applies to the subtrees. \"Bill\" will be output after \"alice\" and before \"fred\" and its descendents. \"Fred\" will be\u00aeoutput after \"dave\" and before \"jane\" and \"joe\". And so on. Suppose that we want to search for a given item in a binary search tree. Compare that item to the root item of the tree. If they are equal, we're done. If the item we are looking for is less than the root item, then\u00aewe need tosearch the left subtree of the root the right subtree can be eliminated because it only contains items that are greater than or equal to the root. Similarly, if the item we are looking for is greater than the item in the root, then we only need to look in the right subtree. In either case, the same procedure can then be applied to search the subtree. Inserting a new item is similar: Start by searching the tree for the position where the new item belongs. When that position is found, create a new node and attach it to the tree at that position.",
  "page223": "Searching and inserting are efficient operations on a binary search tree, provided that the tree is close to being balanced. A binary tree is balanced if for each node, the left subtree of that node contains approximately the same number of nodes as the right subtree. In a perfec\u00aetly balanced tree, the two numbers differ by at most one. Not all binary trees are balanced, but if the tree is created by inserting items in a random order, there is a high probability that the tree is approximately balanced. (If the order of insertion is not random, however, it's quite possible for the tree to be very unbalanced.) During a search of any binary sort tree, every comparison eliminates one of two subtrees from further consideration. If the tree is balanced, that means c\u00aeutting the number of items still under consideration in half. This is exactly the same as the binary search algorithm, and the result, is a similarly efficient algorithm. In terms of asymptotic analysis, searching, inserting, and deleting in a binary search tree have average case run time Theta(log(n)). The pr\u00aeoblem size, n, is the number of items in the tree, and the average is taken over all the different orders in which the items could have been inserted into the tree. As long the actual insertion order is random, the actual run time can be expected to be close to the average. However, the worst case run time fo\u00aer binary search tree operations is Theta(n), which is much worse than Theta(log(n)). The worst case occurs for certain particular insertion orders. For example, if the items are inserted into the tree in order of increasing size, then every item that is inserted moves always to the right as it moves down the tree. The result is a \"tree\" that looks more like a linked list, since it consists of a linear string of nodes strung together by their right child pointers. Operations on such a tree have the same performance as operations on a linked list. Now, there are data structures that are similar to simple binary sort trees, except that insertion and deletion of nodes are implemented in a way that will always keep the tree balanced, or almost balanced. For these data structures, searching, inserting, and deleting have both average case and worst case run times that are Theta(log(n)).",
  "page224": "Backus-Naur Form- Natural and artificial languages are similar in that they have a structure known as grammar or syntax. Syntax can be expressed by a set of rules that describe what it means to be a legal sentence or program. For programming languages, syntax rules are often expr\u00aeessed in BNF (Backus-Naur Form), a system that was developed by computer scientists John Backus and Peter Naur in the late 1950s. Interestingly, an equivalent system was developed independently at about the same time by linguist Noam Chomsky to describe the grammar of natural language. BNF cannot express all possible syntax rules. For example, it can't express the fact that a variable must be defined before it is used. Furthermore, it says nothing about the meaning or semantics of the\u00aelangauge. The problem of specifying the semantics of a language even of an artificial programming langauge is one that is still far from being completely solved. However, BNF does express the basic structure of the language, and it plays a central role in the design of translation programs. Files- The data and\u00aeprograms in a computer's main memory survive only as long as the power is on. For more permanent storage, computers use files, which are collections of data stored on a hard disk, on a USB memory stick, on a CD-ROM, or on some other type of storage device. Files are organized into directories (sometimes\u00aecalled folders). A directory can hold other directories, as well as files. Both directories and files have names that are used to identify them. Programs can read data from existing files. They can create new files and can write data to files. In Java, such input and output can be done using streams. Human-readable character data is read from a file using an object belonging to the class FileReader, which is a subclass of Reader. Similarly, data is written to a file in human-readable format through an object of type FileWriter, a subclass of Writer. For files that store data in machine format, the appropriate I/O classes are FileInputStream and FileOutputStream. In this section, I will only discuss character oriented file I/O using the FileReader and FileWriter classes. However, FileInputStream and FileOutputStream are used in an exactly parallel fashion. All these classes are defined in the java.io package.",
  "page225": "Word Counting- The final example in this section also deals with storing information about words. The problem here is to make a list of all the words that occur in a file, along with the number of times that each word occurs. The file will be selected by the user. The output of t\u00aehe program will consist of two lists. Each list contains all the words from the file, along with the number of times that the word occurred. One list is sorted alphabetically, and the other is sorted according to the number of occurrences, with the most common words at the top and the least common at the bottom. The problem here is a generalization, which asked you to make an alphabetical list of all the words in a file, without counting the number of occurrences. Symbol Tables- We begin w\u00aeith a straightforward but important application of maps. When a compiler reads the source code of a program, it encounters definitions of variables, subroutines, and classes. The names of these things can be used later in the program. The compiler has to remember the name is encountered later in the program. Th\u00aeis is a natural application for a Map. The name can be used as a keyin the map. The value associated to the key is the definition of the name, encoded somehow as an object. A map that is used in this way is called a symbol table. In a compiler, the values in a symbol table can be quite complicated, since the\u00aecompiler has to deal with names for various sorts of things, and it needs a different type of information for each different type of name. We will keep things simple by looking at a symbol table in another context. Suppose that we want a program that can evaluate expressions entered by the user, and suppose that the expressions can contain variables, in addition to operators, numbers, and parentheses. For this to make sense, we need some way of assigning values to variables. When a variable is used in an expression, we need to retrieve the variable's value. A symbol table can be used to store the data that we need. The keys for the symbol table are variable names. The value associated with a key is the value of that variable, which is of type double.",
  "page226": "Almost all applications require persistent data. Persistence is one of the fundamental concepts in application development. If an information system didn't preserve data when it was powered off, the system would be of little practical use. Object persistence means individual\u00aeobjects can outlive the application process; they can be saved to a data store and be re-created at a later point in time. When we talk about persistence in Java, we're normally talking about mapping and storing object instances in a database using SQL. We start by taking a brief look at the technology and how it's used in Java. Armed with this information, we then continue our discussion of persistence and how it's implemented in object-oriented applications. You, like mo\u00aest other software engineers, have probably worked with SQL and relational databases; many of us handle such systems every day. Relational database management systems have SQL-basedapplication programming interfaces; hence, we call today's relational database products SQL database management systems (DBMS)\u00aeor, when we're talking about particular systems, SQL databases. Relational technology is a known quantity, and this alone is sufficient reason for many organizations to choose it. But to say only this is to pay less respect than is due. Relational databases are entrenched because they're an incredi\u00aebly flexible and robust approach to data management. Due to the well-researched theoretical foundation of the relational data model, relational databases can guarantee and protect the integrity of the stored data, among other desirable characteristics. You may be familiar with E.F. Codd's four-decades-old introduction of the relational model, A Relational Model of Data for Large Shared Data Banks (Codd, 1970). A more recent compendium worth reading, with a focus on SQL, is C. J. Date's SQL and Relational Theory (Date, 2009).  Relational DBMSs aren't specific to Java, nor is an SQL database specific to a particular application. This important principle is known as data independence. In other words, and we can't stress this important fact enough, data lives longer than any application does.",
  "page227": "Before we go into more detail about the practical aspects of SQL databases, we have to mention an important issue: although marketed as relational, a database system providing only an SQL data language interface isn't really relational and in many ways isn't even close\u00aeto the original concept. Naturally, this has led to confusion. SQL practitioners blame the relational data model for shortcomings in the SQL language, and relational data management experts blame the SQL standard for being a weak implementation of the relational model and ideals. Application engineers are stuck somewhere in the middle, with the burden of delivering something that works. We highlight some important and significant aspects of this issue throughout this book, but generally we\u00aefocus on the practical aspects. If you're interested in more background material, we highly recommend Practical Issues in Database Management: A Reference for the Thinking Practitioner by Fabian Pascal (Pascal, 2000) and An Introduction to Database Systems by Chris Date (Date, 2003) for the theory, concep\u00aets, and ideals of (relational) database systems. The latter book is an excellent reference (it's big) for all questions you may possibly have about databases and data management. To use Hibernate effectively, you must start with a solid understanding of the relational model and SQL. You need to understa\u00aend the relational model and topics such as normalization to guarantee the integrity of your data, and you'll need to use your knowledge of SQL to tune the performance of your Hibernate application. Hibernate automates many repetitive coding tasks, but your knowledge of persistence technology must extend beyond Hibernate itself if you want to take advantage of the full power of modern SQL databases. To dig deeper, consult the bibliography at the end of this book.",
  "page228": "You've probably used SQL for many years and are familiar with the basic operations and statements written in this language. Still, we know from our own experience that SQL is sometimes hard to remember, and some terms vary in usage.  Let's review some of the SQL terms\u00aeused in this book. You use SQL as a data definition language (DDL) when creating, altering, and dropping artifacts such as tables and constraints in the catalog of the DBMS. When this schema is ready, you use SQL as a data manipulation language (DML) to perform operations on data, including insertions, updates, and deletions. You retrieve data by executing queries with restrictions, projections, and Cartesian products. For efficient reporting, you use SQL to join, aggregate, and group data\u00aeas necessary. You can even nest SQL statements inside each other a technique that uses subselects. When your business requirements change, you'll have to modify the database schema again with DDL statements after data has been stored; this is known as schema evolution.  If you're an SQL veteran and\u00aeyou want to know more about optimization and how SQL is executed, get a copy of the excellent book SQL Tuning, by Dan Tow (Tow, 2003). For a look at the practical side of SQL through the lens of how not to use SQL, SQL Antipatterns: Avoiding the Pitfalls of Database Programming (Karwin, 2010) is a good resour\u00aece.  Although the SQL database is one part of ORM, the other part, of course, consists of the data in your Java application that needs to be persisted to and loaded from the database.",
  "page229": "Using SQL in Java- When you work with an SQL database in a Java application, you issue SQL statements to the database via the Java Database Connectivity (JDBC) API. Whether the SQL was written by hand and embedded in the Java code or generated on the fly by Java code, you use the\u00aeJDBC API to bind arguments when preparing query parameters, executing the query, scrolling through the query result, retrieving values from the result set, and so on. These are low-level data access tasks; as application engineers, we're more interested in the business problem that requires this data access. What we'd really like to write is code that saves and retrieves instances of our classes, relieving us of this lowlevel drudgery. Because these data access tasks are often\u00aeso tedious, we have to ask, are the relational data model and (especially) SQL the right choices for persistence in objectoriented applications? We answer this question unequivocally: yes! There are many reasons why SQL databases dominate the computing industry relational database management systems are the onl\u00aeyproven generic data management technology, and they're almost always a requirement in Java projects.  Note that we aren't claiming that relational technology is always the best solution. There are many data management requirements that warrant a completely different approach. For example, internet\u00ae-scale distributed systems (web search engines, content distribution networks, peer-to-peer sharing, instant messaging) have to deal with exceptional transaction volumes. Many of these systems don't require that after a data update completes, all processes see the same updated data (strong transactional consistency). Users might be happy with weak consistency; after an update, there might be a window of inconsistency before all processes see the updated data. Some scientific applications work with enormous but very specialized datasets. Such systems and their unique challenges typically require equally unique and often custom-made persistence solutions. Generic data management tools such as ACID-compliant transactional SQL databases, JDBC, andHibernate would play only a minor role.",
  "page230": "ORM and JPA- In a nutshell, object/relational mapping is the automated (and transparent) persistence of objects in a Java application to the tables in an SQL database, using metadata that describes the mapping between the classes of the application and the schema of the SQL datab\u00aease. In essence, ORM works by transforming (reversibly) data from one representation to another. Before we move on, you need to understand what Hibernate can't do for you. A supposed advantage of ORM is that it shields developers from messy SQL. This view holds that object-oriented developers can't be expected to understand SQL or relational databases well and that they find SQL somehow offensive. On the contrary, we believe that Java developers must have a sufficient level of f\u00aeamiliarity with and appreciation of relational modeling and SQL in order to work with Hibernate. ORM is an advanced technique used by developers who have already done it the hard way. To use Hibernate effectively, you must be able to view and interpret the SQL statements it issues and understand their performan\u00aece implications Let's look at some of the benefits of Hibernate: Productivity Hibernate eliminates much of the grunt work (more than you'd expect) and lets you concentrate on the business problem. No matter which application-development strategy you prefer top-down, starting with a domain model, or\u00aebottom-up, starting with an existing database schema Hibernate, used together with the appropriate tools, will significantly reduce development time. Maintainability Automated ORM with Hibernate reduceslines of code (LOC), making the system more understandable and easier to refactor. Hibernate provides a buffer between the domain model and the SQL schema, insulating each model from minor changes to the other. Performance Although hand-coded persistence might be faster in the same sense that assembly code can be faster than Java code, automated solutions like Hibernate allow the use of many optimizations at all times. One example of this is efficient and easily tunable caching in the application tier. This means developers can spend more energy hand-optimizing the few remaining real bottlenecks instead of prematurely optimizing everything.",
  "page231": "The Hibernate approach to persistence was well received by Java developers, and the standard Java Persistence API was designed along similar lines. JPA became a key part of the simplifications introduced in recent EJB and Java EE specifications. We should be clear up front that\u00aeneither Java Persistence nor Hibernate are limited to the Java EE environment; they're general-purpose solutions to the persistence problem that any type of Java (or Groovy, or Scala) application can use.  The JPA specification defines the following: A facility for specifying mapping metadata how persistent classes and their properties relate to the database schema. JPA relies heavily on Java annotations in domain model classes, but you can also write mappings in XML files. APIs for\u00aeperforming basic CRUD operations on instances of persistent classes, most prominently javax.persistence.EntityManager to store and load data. A language and APIs for specifying queries that refer to classes and properties of classes. This language is the Java Persistence Query Language (JPQL) and looks similar\u00aeto SQL. The standardized API allows for programmatic creation of criteria queries without string manipulation. How the persistence engine interacts with transactional instances to perform dirty checking, association fetching, and other optimization functions. The latest JPA specification covers some basic ca\u00aeching strategies. Hibernate implements JPA and supports all the standardized mappings, queries, and programming interfaces.",
  "page232": "With object persistence, individual objects can outlive theirapplication process, be saved to a data store, and be re-created later. The object/relational mismatch comes into play when the data store is an SQL-based relational database management system. For instance, a network o\u00aef objects can't be saved to a database table; it must be disassembled and persisted to columns of portable SQL data types. A good solution for this problem is object/relational mapping (ORM). ORM isn't a silver bullet for all persistence tasks; its job is to relieve the developer of 95% of object persistence work, such as writing complex SQL statements with many table joins and copying values from JDBC result sets to objects or graphs of objects. A full-featured ORM middleware\u00aesolution may provide database portability, certain optimization techniques like caching, and other viable functions that aren't easy to hand-code in a limited time with SQL and JDBC. Better solutions than ORM might exist someday. We(and many others) may have to rethink everything we know about data manage\u00aement systems and their languages, persistence API standards, and application integration. But the evolution of today's systems into true relational database systems with seamless object-oriented integration remains pure speculation. We can't wait, and there is no sign that any of these issues will i\u00aemprove soon (a multibillion-dollar industry isn't very agile). ORM is the best solution currently available, and it's a timesaver for developers facing the object/relational mismatch every day.",
  "page233": "Hibernate is an ambitious project that aims to provide a complete solution to the problem of managing persistent data in Java. Today, Hibernate is not onlyan ORM service, but also a collection of data management tools extending well beyond ORM. The Hibernate project suite include\u00aes the following: Hibernate ORM Hibernate ORM consists of a core, a base service for persistence with SQL databases, and a native proprietary API. Hibernate ORM is the foundation for several of the other projects and is the oldest Hibernate project. You can use Hibernate ORM on its own, independent of any framework or any particular runtime environment with all JDKs. It works in every Java EE/J2EE application server, in Swing applications, in a simple servlet container, and soon. As long a\u00aes you can configure a data source for Hibernate, it works. Hibernate EntityManager This is Hibernate's implementation of the standard Java Persistence APIs, an optional module you can stack on top of Hibernate ORM. You can fall back to Hibernate when a plain Hibernate interface or even a JDBC Connection is\u00aeneeded. Hibernate's native features are a superset of the JPA persistence features in every respect. Hibernate Validator Hibernate provides the reference implementation of the BeanValidation (JSR 303) specification. Independent of other Hibernate projects, it provides declarative validation for\u00aeyour domain model (or any other) classes. Hibernate Envers Envers is dedicated to audit logging and keeping multiple versions of data in your SQL database. This helps you add data history and audit trails to your application, similar to version control systems you might already be familiar with such as Subversion and Git. Hibernate Search Hibernate Search keeps an index of your domain model data up to date in an Apache Lucene database. It lets you query this database with a powerful and naturally integrated API. Many projects use Hibernate Search in addition to Hibernate ORM, adding full-text search capabilities. If you have a free text search form in your application's user interface, and you want happy users, work with Hibernate Search. Hibernate Search isn't covered in this book; you can find more information in Hibernate Search in Action by Emmanuel Bernard (Bernard, 2008).",
  "page234": "The \"Hello World\" example in the previous chapter introduced you to Hibernate; certainly, it isn't useful for understanding the requirements of real-world applications with complex data models. For the rest of the book, we use a much more sophisticated example appl\u00aeication CaveatEmptor, an online auction system to demonstrate Hibernate and Java Persistence. (Caveat emptor means \"Let the buyer beware\".) We'll start our discussion of the application by introducing a layered application architecture. Then, you'll learn how to identify the business entities of a problem domain. You'll create a conceptual model of these entities and their attributes, called a domain model, and you'll implement it in Java by creating persisten\u00aet classes. We'll spend some time exploring exactly what these Java classes should look like and where they fit within a typical layered application architecture. We'll also look at the persistence capabilitiesof the classes and how this aspect influences the design and implementation. We'll add B\u00aeean Validation, which helps to automatically verify the integrity of the domain model data not only for persistent information but all business logic.  We'll then explore mapping metadata options the ways you tell Hibernate how your persistent classes and their properties relate to database tables and\u00aecolumns. This can be as simple as adding annotations directly in the Java source code of the classes or writing XML documents that you eventually deploy along with the compiled Java classes that Hibernate accesses at runtime. After reading this chapter, you'll know how to design the persistent parts of your domain model in complex real-world projects, and what mapping metadata option you'll primarily prefer and use. Let's start with the example application.",
  "page235": "The example CaveatEmptor application- The CaveatEmptor example is an online auction application that demonstrates ORM techniques and Hibernate functionality. You can download the source code for the application from www.jpwh.org. We won't pay much attention to the user inter\u00aeface in this book (it could be web based or a rich client); we'll concentrate instead on the data access code. When a design decision about data access code that has consequences for the user interface has to be made, we'll naturally consider both.  In order to understand the design issues involved in ORM, let's pretend the CaveatEmptor application doesn't yet exist and that you're building it from scratch. Let's start by looking at the architecture. A layere\u00aed architecture- With any nontrivial application, it usually makes sense to organize classes by concern. Persistence is one concern; others include presentation, workflow, and business logic. A typical object-oriented architecture includes layers of code that represent the concerns. A layered architecture define\u00aes interfaces between code that implements the various concerns, allowing changes to be made to the way one concern is implemented without significant disruption to code in the other layers. Layering determines the kinds of inter-layer dependencies that occur. The rules are as follows: Layers communicate from\u00aetop to bottom. A layer is dependent only on the interface of the layer directly below it. Each layer is unaware of any other layers except for the layer just below it.",
  "page236": "The CaveatEmptor domain model The CaveatEmptor site auctions many different kinds of items, from electronic equipment to airline tickets. Auctions proceed according to the English auction strategy: users continue to place bids on an item until the bid period for that item expires\u00ae, and the highest bidder wins.  In any store, goods are categorized by type and grouped with similar goods into sections and onto shelves. The auction catalog requires some kind of hierarchy of item categories so that a buyer can browse these categories or arbitrarily search by category and item attributes. Lists of items appear in the category browser and search result screens. Selecting an item from a list takes the buyer to an item-detail view where an item may have images attached to\u00aeit.  An auction consists of a sequence of bids, and one is the winning bid. User details include name, address, and billing information.  The result of this analysis, the high-level overview of the domain model, is shown in figure 3.3. Let's briefly discuss some interesting features of this model.  Each\u00aeitem can be auctioned only once, so you don't need to make Item distinct from any auction entities. Instead, you have a single auction item entity named Item. Thus, Bid is associated directly with Item. You model the Address information of a User as a separate class, a User may have three addresses, for\u00aehome, billing, and shipping. You do allow the user to have many BillingDetails. Subclasses of an abstract class represent the various billing strategies (allowing future extension).  The application may nest a Category inside another Category, and so on. A recursive association, from the Category entity to itself, expresses this relationship. Note that a single Category may have multiple child categories but at most one parent. Each Item belongs to at least one Category.  This representation isn't the complete domain model but only classes for which you need persistence capabilities. You'd like to store and load instances of Category, Item, User, and so on. We have simplified this high-level overview a little; we may introduce additional classes later or make minor modifications to them when needed for more complex examples.",
  "page237": "Implementing the domain model- You'll start with an issue that any implementation must deal with: the separation of concerns. The domain model implementation is usually a central, organizing component; it's reused heavily whenever you implement new application functiona\u00aelity. For this reason, you should be prepared to go to some lengths to ensure that concerns other than business aspects don't leak into the domain model implementation. When concerns such as persistence, transaction management,or authorization start to appear in the domain model classes, this is an example of leakage of concerns. The domain model implementation is such an important piece of code that it shouldn't depend on orthogonal Java APIs. For example, code in the domain mod\u00aeel shouldn't perform JNDI lookups or call the database via the JDBC API, not directly and not through an intermediate abstraction. This allows you to reuse the domain model classes virtually anywhere: The presentation layer can access instances and attributes of domain model entities when rendering views.\u00aeThe controller components in the business layer can also access the state of domain model entities and call methods of the entities to execute business logic. The persistence layer can load and store instances of domain model entities from and to the database, preserving their state.",
  "page238": "Most important, preventing leakage of concerns makes it easy to unit-test the domain model without the need for a particular runtime environment or container, or the need for mocking any service dependencies. You can write unit tests that verify the correct behavior of your domai\u00aen model classes without any special test harness. (We aren't talking about testing \"load from the database\" and \"store in the database\" aspects, but \"calculate the shipping cost and tax\" behavior.) The Java EE standard solves the problem of leaky concerns with metadata, as annotations within your code or externalized as XML descriptors. This approach allows the runtime container to implement some predefined cross-cutting concerns security, concurrency, p\u00aeersistence, transactions, and remoteness in a generic way, by intercepting calls to application components.  Hibernate isn't a Java EE runtime environment, and it's not an application server. It's an implementation of just one specification under the Java EE umbrella JPA and a solution for just\u00aeone of these concerns: persistence. JPA defines the entity class as the primary programming artifact. This programming model enables transparent persistence, and a JPA provider such as Hibernate also offers automated persistence. Transparent and automated persistence- We use transparent to mean a complete\u00aeseparation of concerns between the persistent classes of the domain model and the persistence layer. The persistent classes are unaware of and have no dependency on the persistence mechanism. We use automatic to refer to a persistence solution (your annotated domain, the layer, and mechanism) that relieves you of handling low-level mechanical details, such as writing most SQL statements and working with the JDBC API.",
  "page239": "The Item class of the CaveatEmptor domain model, for example, shouldn't have any runtime dependency on any Java Persistence or Hibernate API. Furthermore: JPA doesn't require that any special superclasses or interfaces be inherited or implemented by persistent classes.\u00aeNor are any special classes used to implement attributes and associations. (Of course, the option to use both techniques is always there.) You can reuse persistent classes outside thecontext of persistence, in unit tests or in the presentation layer, for example. You can create instances in any runtime environment with the regular Java new operator, preserving testability and reusability. In a system with transparent persistence, instances of entities aren't aware of the underlying d\u00aeata store; they need not even be aware that they're being persisted or retrieved. JPA externalizes persistenceconcerns to a generic persistence manager API. Hence, most of your code, and certainly your complex business logic, doesn't have to concern itself with the current state of a domain model enti\u00aety instance in a single thread of execution.",
  "page240": "We regard transparency as a requirement because it makes an application easier to build and maintain. Transparent persistence should be one of the primary goals of any ORM solution. Clearly, no automated persistence solution is completely transparent: Every automated persistence\u00aelayer, including JPA and Hibernate, imposes some requirements on the persistent classes. For example, JPA requires that collectionvalued attributes be typed to an interface such as java.util.Set or java.util.List and not to an actual implementation such as java.util.HashSet (this is a good practice anyway). Or, a JPA entity class has to have a special attribute, called the database identifier (which is also less of arestriction but usually convenient).  You now know why the persistence me\u00aechanism should have minimal impact on how you implement a domain model, and that transparent and automated persistence are required. Our preferred programming model to archive this is POJO.",
  "page241": "Around 10 years ago, many developers started talking about POJO, a back-to-basics approach that essentially revives JavaBeans, a component model for UI development, and reapplies it to the other layers of a system. Several revisions of the EJB and JPA specifications brought us ne\u00aew lightweight entities, and it would be appropriate to call them persistence-capable JavaBeans. Java engineers often use all these terms as synonyms for the same basic design approach.  You shouldn't be too concerned about what terms we use in this book; the ultimate goal is to apply the persistence aspect as transparently as possible to Java classes. Almost any Java class can be persistence-capableif you follow some simple practices. Let's see how this looks in code.  Writing\u00aepersistence-capable classes Working with fine-grained and rich domain models is a major Hibernate objective. This is a reason we work with POJOs. In general, using fine-grained objects means more classes than tables.  A persistence-capable plain-old Java class declares attributes, which represent state, and bu\u00aesiness methods, which define behavior. Some attributes represent associations to other persistence-capable classes.",
  "page242": "JPA doesn't require that persistent classes implement java.io.Serializable. But when instances are stored in an HttpSession or passed by value using RMI, serialization is necessary. Although this might not occur in your application, the class will be serializable without any\u00aeadditional work, and there are no downsides to declaring that. (We aren't going to declare it on every example, assuming that you know when it will be necessary.)  The class can be abstract and, if needed, extend a non-persistent class or implement an interface. It must be a top-level class, not nested within another class. The persistence-capable class and any of its methods can't be final (a requirement of the JPA specification).  Unlike the JavaBeans specification, which re\u00aequires no specific constructor, Hibernate (and JPA) require a constructor with no arguments for every persistent class. Alternatively, you might not write a constructor at all; Hibernate will then use the Java default constructor. Hibernate calls classes using the Java reflection API on such a no argument const\u00aeructor tocreate instances. The constructor may not be public, but it has to be at least package-visible if Hibernate will use runtime-generated proxies for performance optimization. Also, consider the requirements of other specifications: the EJB standard requires public visibility on session bean constructors\u00aejust like the JavaServer Faces (JSF) specification requires for its managed beans. There are other situations when you'd want a public constructor to create an \"empty\" state: for example, query-by-example building",
  "page243": "The properties of the POJO implement the attributes of the business entitiesfor example, the username of User. You usually implement properties as private or protected member fields, together with public or protected property accessor methods: for each field a method for retrievi\u00aeng its value and a method for setting the value. These methods are known as the getter and setter, respectively. The example POJO in listing 3.1 declares getter and setter methods for the username property.  The JavaBean specification defines the guidelines for naming accessor methods; this allows generic tools like Hibernate to easily discover and manipulate property values. A getter methodname begins with get, followed by the name of the property (the first letter in uppercase); a sette\u00aer method name begins with set and similarly is followed by the name of the property. You may begin getter methods for Boolean properties with is instead of get. Hibernate doesn't require accessor methods. You can choose how the state of an instanceof your persistent classes should be persisted. Hibernate\u00aewill either directly access fields or call accessor methods. Your class design isn't disturbed much by these considerations. You can make some accessor methods non-public or completely remove them then configure Hibernate to rely on field access for these properties.",
  "page244": "Should property fields and accessor methods be private, protected, or package visible? Typically, you want to discourage direct access to the internal state of your class, so you don't make attribute fields public. If you make fields or methods private, you're effectiv\u00aeely declaring that nobody should ever access them; only you're allowed to do that (or a service like Hibernate). This is a definitive statement. There are often good reasons for someone to access your \"private\" internals usually to fix one of your bugs and you only make people angry if they have to fall back to reflection access in an emergency. Instead, you might assume or know that the engineer who comes after you has access to your code and knows what they're doing.\u00aeThe protected visibility then is a more reasonable default. You're forbidding direct public access, indicating that this particular member detail is internal, but allowing access by subclasses if need be. You trust the engineer who creates the subclass. Package visibility is rude: you're forcing some\u00aeone to create code in the same package to access member fields and methods; this is extra work for no good reason. Most important, these recommendations for visibility are relevant for environments without security policies and a runtime SecurityManager. If you have to keep your internal code private, make it private.",
  "page245": "Implementing POJO associations- You'll now see how to associate and create different kinds of relationships between objects: one-to-many, many-to-one, and bidirectional relationships. We'll look at the scaffolding code needed to create these associations, how to simplif\u00aey relationship management, and how to enforce the integrity of these relationships. Shouldn't bids on an item be stored in a list? The first reaction is often to preserve the order of elements as they're entered by users, because this may also be the order in which you will show them later. Certainly, in an auction application there has to be some defined order in which the user sees bids for an item for example, highest bid first or newest bid last. You might even work with a j\u00aeava.util.List in your user interface code to sort and display bids of an item. That doesn't mean this display order should be durable; data integrity isn't affected by the order in which bids are displayed. You need to store the amount of each bid, so you can find the highest bid, and you need to stor\u00aee a timestamp for each bid when it's created, so you can find the newest bid. When indoubt, keep your system flexible and sort the data when it's retrieved from the datastore (in a query) and/or shown to the user (in Java code), not when it's stored.",
  "page246": "The addBid() method not only reduces the lines of code when dealing with Item and Bid instances, but also enforces the cardinality of the association. You avoid errors that arise from leaving out one of the two required actions. You should always provide this kind of grouping of\u00aeoperations for associations, if possible. If you compare this with the relational model of foreign keys in an SQL database, you can easily see how a network and pointer model complicates a simple operation: instead of a declarative constraint, you need procedural code to guarantee data integrity.  Because you want addBid() to be the only externally visible mutator method for the bids of an item (possibly in addition to a removeBid() method), you can make the Item#setBids() method private\u00aeor drop it and configure Hibernate to directly access fields for persistence. Consider making the Bid#setItem() method package-visible, for the same reason.  The Item#getBids() getter method still returns a modifiable collection, so clients can use it to make changes that aren't reflected on the inverse s\u00aeide. Bids added directly to the collection wouldn't have a reference to an item an inconsistent state, according to your database constraints. To prevent this, you can wrap the internal collection before returning it from the getter method, with Collections.unmodifiableCollection(c) and Collections.unmodifiableSet(s). The client then gets an exception if it tries to modify the collection; you therefore force every modification to go through the relationship management method that guarantees integrity. Note that in this case you'll have to configure Hibernate for field access, because the collection",
  "page247": "There are several problems with this approach. First, Hibernate can't call this constructor. You need to add a no-argument constructor for Hibernate, and it needs to be at least package-visible. Furthermore, because there is no setItem() method, Hibernate would have to be co\u00aenfigured to access the item field directly. This means the field can't be final, so the class isn't guaranteed to be immutable.  In the examples in this book, we'll sometimes write scaffolding methods such as the Item#addBid() shown earlier, or we may have additional constructors for required values. It's up to you how many convenience methods and layers you want to wrap around the persistent association properties and/or fields, but we recommend being consistent and a\u00aepplying the same strategy to all your domain model classes. For the sake of readability, we won't always show convenience methods, special constructors, and other such scaffolding in future code samples and assume you'll add them according to your own taste and requirements.  You now have seen domain\u00aemodel classes, how to represent their attributes, and the relationships between them. Next, we'll increase the level of abstraction, adding metadata to the domain model implementation and declaring aspects such as validation and persistence rules.",
  "page248": "Domain model metadata- Metadata is data about data, so domain model metadata is information about your domain model. For example, when you use the Java reflection API to discover the names of classes of your domain model or the names of their attributes, you're accessing dom\u00aeain model metadata. ORM tools also require metadata, to specify the mapping between classes and tables, properties and columns, associations and foreign keys, Java types and SQL types, and so on. This object/relational mapping metadata governs the transformation between the different type systems and relationship representations in objectoriented and SQL systems. JPA has a metadata API, which you can call to obtain details about the persistence aspects of your domain model, such as the na\u00aemes of persistent entities and attributes. First, it's your job as an engineer to create and maintain this information. JPA standardizes two metadata options: annotations in Java code and externalized XML descriptor files. Hibernate has some extensions for native functionality, also available as annotatio\u00aens and/or XMLdescriptors. Usually we prefer either annotations or XML files as the primary source of mapping metadata. After reading this section, you'll have the background information to make an educated decision for your own project.",
  "page249": "We'll also discuss Bean Validation (JSR 303) and how it provides declarative validation for your domain model (or any other) classes. The reference implementation of this specification is the Hibernate Validator project. Most engineers today prefer Java annotations as the pr\u00aeimary mechanism for declaring metadata. Applying Bean Validation rules Most applications contain a multitude of data-integrity checks. You've seen what happens when you violate one of the simplest data-integrity constraints: you get a NullPointerException when you expect a value to be available. Other examples are a string-valued property that shouldn't be empty (remember, an empty string isn'tnull), a string that has to match a particular regular expression pattern, and a n\u00aeumber or date value that must be within a certain range.  These business rules affect every layer of an application: The user interface code has to display detailed and localized error messages. The business and persistence layers must check input values received from the client before passing them to the data\u00aestore. The SQL database has to be the final validator, ultimately guaranteeing the integrity of durable data.",
  "page250": "The idea behind Bean Validation is that declaring rules such as \"This property can't be null\" or \"This number has to be in the given range\" is much easier and less error-prone than writing if-then-else procedures repeatedly. Furthermore, declaring these r\u00aeules on the central component of your application, the domain model implementation, enables integrity checks in every layer of the system. The rules are then available to the presentation and persistence layers. And if you consider how dataintegrity constraints affect not only your Java application code but also your SQL database schema which is a collection of integrity rules you might think of Bean Validation constraints as additional ORM metadata You add two more attributes the name of\u00aean item and the auctionEnd date when an auction concludes. Both are typical candidates for additional constraints: you want to guarantee that the name is always present and human readable (one-character item names don't make much sense), but it shouldn't be too long your SQL database will be most effi\u00aecient with variable-length strings up to 255 characters, and your user interface also has some constraints on visible label space. The ending time of an auction obviously should be in the future. If you don't provide an error message, a default message will be used. Messages can be keys to external properties files, for internationalization.",
  "page251": "The validation engine will access the fields directly if you annotate the fields. If you prefer calls through accessor methods, annotate the getter method with validation constraints, not the setter. Then constraints are part of the class's API and included in its Javadoc, m\u00aeaking the domain model implementation easier to understand. Note that this is independent from access by the JPA provider; that is, Hibernate Validator may call accessor methods,whereas Hibernate ORM may call fieldsdirectly.  Bean Validation isn't limited to the built-in annotations; you can create your own constraints and annotations. With a custom constraint, you can even use class-level annotations and validate several attribute values at the same time on an instance of the class.\u00aeThe following test code shows how you can manually check the integrity of an Item instance. We're not going to explain this code in detail but offer it for you to explore. You'll rarely write this kind of validation code; most of the time, this aspect is automatically handled by your user interface a\u00aend persistence framework. It's therefore important to look for Bean Validation integration when selecting a UI framework. JSF version 2 and newer automatically integrates with Bean Validation, for example.  Hibernate, as required from any JPA provider, also automatically integrates with Hibernate Validator if the libraries are available on the classpath and offers the following features:",
  "page252": "You don't have to manually validate instances before passing them to Hibernate for storage. Hibernate recognizes constraints on persistent domain model classes and triggers validation before database insert or update operations. When validation fails, Hibernate throws\u00aea ConstraintViolationException, containing the failure details, to the code calling persistence-management operations. The Hibernate toolset for automatic SQL schema generation understands many constraints and generates SQL DDL-equivalent constraints for you. For example, an @NotNull annotation translates into an SQL NOT NULL constraint, and an @Size(n) rule defines the number of characters in a VARCHAR(n)-typed column. You can control this behavior of Hibernate with the <validatio\u00aen-mode> element in your persistence.xml configuration file. The default mode is AUTO, so Hibernate will only validate if it findsa Bean Validation provider (such as Hibernate Validator) on the classpath of the running application. With mode CALLBACK, validation will always occur, and you'll get a deploymen\u00aet error if you forget to bundle a Bean Validation provider. The NONE mode disables automatic validation by the JPA provider. You'll see Bean Validation annotations again later in this book; you'll also find them in the example code bundles. At this point we could write much more about Hibernate Vali\u00aedator, but we'd only repeat what is already available in the project's excellent reference guide. Have a look, and find out more about features such as validation groups and the metadata API for discovery of constraints.  The Java Persistence and Bean Validation standards embrace annotations aggressively. The expert groups have been aware of the advantages of XML deployment descriptors in certain situations, especially for configuration metadata that changes with each deployment.",
  "page253": "This part is all about actual ORM, from classes and properties to tables and columns. Chapter 4 starts with regular class and property mappings and explains how you can map fine-grained Java domain models. Next, in chapter 5, you'll see how to map basic properties and embedd\u00aeable components, and how to control mapping between Java and SQL types. In chapter 6, you'll map inheritance hierarchies of entities to the database using four basic inheritance-mapping strategies; you'll also map polymorphic associations. Chapter 7 is all about mapping collections and entity associations: you map persistent collections, collections of basic and embeddable types, and simple many-to-one and one-to-many entity associations. Chapter 8 dives deeper with advanced enti\u00aety association mappings like mapping one-to-one entity associations, one-to-many mapping options, and many-to-many and ternary entity relationships. Finally, you'll find chapter 9 most interesting if you need to introduce Hibernate in an existing application, or if you have to work with legacy database sch\u00aeemas and handwritten SQL. We'll also talk about customized SQL DDL for schema generation in this chapter. After reading this part of the book, you'll be ready to create even the most complex mappings quickly and with the right strategy. You'll understand how the problem of inheritance mapping\u00aecan be solved and how to map collections and associations. You'll also be able to tune and customize Hibernate for integration with any existing database schema or application.",
  "page254": "Fine-grained domain models- A major objective of Hibernate is support for fine-grained and rich domain models. It's one reason we work with POJOs. In crude terms, fine-grained means more classes than tables.  For example, a user may have a home address in your domain model.\u00aeIn the database, you may have a single USERS table with the columns HOME_STREET, HOME_CITY, and HOME_ZIPCODE. (Remember the problem of SQL types we discussed in section 1.2.1?)  In the domain model, you could use the same approach, representing the address as three string-valued properties of the User class. But it's much better to model this using an Address class, where User has a homeAddress property. This domain model achieves improved cohesion and greater code reuse, and it\u00ae19s more understandable than SQL with inflexible type systems. JPA emphasizes the usefulness of fine-grained classes for implementing type safety and behavior. For example, many people model an email address as astring-valued property of User. A more sophisticated approach is to define an EmailAddress class, which\u00aeadds higher-level semantics and behavior it may provide a prepareMail() method (it shouldn't have a sendMail() method, because you don't want your domain model classes to depend on the mail subsystem).  This granularity problem leads us to a distinction of central importance in ORM. In Java, all c\u00aelasses are of equal standing all instances havetheir own identity and life cycle. When you introduce persistence, some instances may not have their own identity and life cycle but depend on others. Let's walk through an example",
  "page255": "Distinguishing entities and value types- You may find it helpful to add stereotype (a UML extensibility mechanism) information to your UML class diagrams so you can immediately recognize entities and value types. This practice also forces you to think about this distinction for a\u00aell your classes, which is a first step to an optimal mapping and well-performing persistence layer. The Item and User classes are obvious entities. They each have their own identity, their instances have references from many other instances (shared references), and they have independent lifespans.  Marking the Address as a value type is also easy: a single User instance references a particular Address instance. You know this because the association has been created as a composition, wher\u00aee the User instance has been made fully responsible for the life cycle of the referenced Address instance. Therefore, Address instances can't be referenced by anyone else and don't need their own identity.  The Bid class could be a problem. In object-oriented modeling, this is marked as a composition\u00ae(the association between Item and Bid with the diamond). Thus, an Item is the owner of its Bid instances and holds a collection of references. At first, this seems reasonable, because bids in an auction system are useless when the item they were made for is gone.  But what if a future extension of the domai\u00aen model requires a User#bids collection,containing all bids made by a particular User? Right now, the association between Bid and User is unidirectional; a Bid has a bidder reference. What if this was bidirectional?",
  "page256": "Mapping entities with identity- Mapping entities with identity requires you to understand Java identity and equality before we can walk through an entity class example and its mapping. After that, we'll be able to dig in deeper and select a primary key, configure key generat\u00aeors, and finally go through identifier generator strategies. First, it's vital to understand the difference between Java object identity and object equality before we discuss terms like database identity and the way JPA manages identity. Understanding Java identity and equality- Java developers understand the difference between Java object identity and equality. Object identity (=) is a notion defined by the Java virtual machine. Two references are identical if they point to the same\u00aememory location. On the other hand, object equality is a notion defined by a class's equals() method, sometimes also referred to as equivalence. Equivalence means two different (non-identical) instances have the same value the same state. Two different instances of String are equal if they represent the\u00aesame sequence of characters, even though each has its own location in the memory space of the virtual machine. (If you're a Java guru, we acknowledge that String is a special case. Assume we used a different class to make the same point.) Persistence complicates this picture. With object/relational persi\u00aestence, a persistent instance is an in-memory representation of a particular row (or rows) of a database table (or tables). Along with Java identity and equality, we define database identity",
  "page257": "Objects are identical if they occupy the same memory location in the JVM. This can be checked with the a = b operator. This concept is known as object identity. Objects are equal if they have the same state, as defined by the a.equals(Object b) method. Classes that do\u00aeexplicitlyoverride this method inherit the implementation defined by java.lang.Object, which compares object identity with ==. This concept is known as object equality. Objects stored in a relational database are identical if they share the same table and primary key value. This concept, mapped into the Java space, is known as database identity. We now need to look at how database identity relates to object identity and how to express database identity in the mapping metadata. As an exam\u00aeple, you'll map an entity of a domain model. Every entity class has to have an @Id property; it's how JPA exposes database identity to the application. We don't show the identifier property in our diagrams; we assume that each entity class has one. In our examples, we always name the identifierp\u00aeroperty id. This is a good practice for your own project; use the same identifier property name for all your domain model entity classes. If you specify nothing else, this property maps to a primary key columnnamed ID of the ITEM table in your database schema.",
  "page258": "Hibernate will use the field to access the identifier property value when loading and storing items, not getter or setter methods. Because @Id is on a field, Hibernate will now enable every field of the class as a persistent property by default. The rule in JPA is this: if @Id is\u00aeon a field, the JPA provider will access fields of the class directly and consider all fields part of the persistent state by default. You'll see how to override this later in this chapter in our experience, field access is often the best choice, because it gives you more freedom for accessor method design.  Should you have a (public) getter method for the identifier property? Well, the application often uses database identifiers as a convenient handle to a particular instance, even\u00aeoutside the persistence layer. For example, it's common for web applications to display the results of a search screen to the user as a list of summaries. When the user selects a particular element, the application may need to retrieve the selected item, and it's common to use a lookup by identifier\u00aefor this purpose you've probably already used identifiers this way, even in applications that rely on JDBC. Should you have a setter method? Primary key values never change, so you shouldn't allow modification of the identifier property value. Hibernate won't update a primary key column, and yo\u00aeu shouldn't expose a public identifier setter method on an entity.  The Java type of the identifier property, java.lang.Long in the previous example, depends on the primary key column type of the ITEM table and how key values are produced. This brings us to the @GeneratedValue annotation and primary keys in general.",
  "page259": "Selecting a primary key-Must primary keys be immutable? The relational model defines that a candidate key must be unique and irreducible (no subset of the key attributes has the uniqueness property). Beyond that, picking a candidate key as the primary key is a matter of taste. Bu\u00aet Hibernate expects a candidate key to be immutable when used as the primary key. Hibernate doesn't support updating primary key values with an API; if you try to work around this requirement, you'll run into problems with Hibernate's caching and dirty-checking engine. If your database schema relies on updatable primary keys (and maybe uses ON UPDATE CASCADE foreign key constraints), you must change the schema before it will work with Hibernate. The database identifier of an\u00aeentity is mapped to some table primary key, so let's first get some background on primary keys without worrying about mappings. Take a step back and think about how you identify entities.  A candidate key is a column or set of columns that you could use to identify a particular row in a table. To become\u00aethe primary key, a candidate key must satisfy the following requirements: The value of any candidate key column is never null. You can't identify something with data that is unknown, and there are no nulls in the relational model. Some SQL products allow defining (composite) primary keys with nullable c\u00aeolumns, so you must be careful. The value of the candidate key column(s) is a unique value for any row. The value of the candidate key column(s) never changes; it's immutable.",
  "page260": "If a table has only one identifying attribute, it becomes, by definition, the primary key. But several columns or combinations of columns may satisfy these properties for a particular table; you choose between candidate keys to decide the best primary key for the table. You shoul\u00aed declare candidate keys not chosen as the primary key as unique keys in the database if their value is indeed unique (but maybe not immutable).  Many legacy SQL data models use natural primary keys. A natural key is a key with business meaning: an attribute or combination of attributes that is unique by virtue of its business semantics. Examples of natural keys are the US Social Security Number and Australian Tax File Number. Distinguishing natural keys is simple: if a candidate key attr\u00aeibute has meaning outside the database context, it's a natural key, regardless of whether it's automatically generated. Think about the application users: if they refer to a key attribute when talking about and working with the application, it's a natural key: \"Can you send me the pictures o\u00aef item #123-abc?\" Experience has shown that natural primary keys usually cause problems in the end. A good primary key must be unique, immutable, and never null. Few entity attributes satisfy these requirements, and some that do can't be efficiently indexed by SQL databases (although this is an imp\u00aelementation detail and shouldn't be the deciding factor for or against a particular key). In addition, you should make certain that a candidate key definition never changes throughout the lifetime of the database. Changing the value (or even definition) of a primary key,and all foreign keys that refer to it, is a frustrating task. Expect your database schema to survive decades, even if your application won't.",
  "page261": "Furthermore, you can often only find natural candidate keys by combining several columns in a composite natural key. These composite keys, although certainly appropriate for some schema artifacts (like a link table in a many-to-many relationship), potentially make maintenance, ad\u00aehoc queries, and schema evolution much more difficult. We talk about composite keys later in the book, For these reasons, we strongly recommend that you add synthetic identifiers, also called surrogate keys. Surrogate keys have no business meaning they have unique values generated by the database or application. Application users ideally don't see or refer to these key values; they're part of the system internals. Introducing a surrogate key column is also appropriate in the com\u00aemon situation when there are no candidate keys. In other words, (almost) every table in your schema should have a dedicated surrogate primary key column with only this purpose. There are a number of well-known approaches to generating surrogate key values. The aforementioned @GeneratedValue annotation is how y\u00aeou configure this. Configuring key generators- The @Id annotation is required to mark the identifier property of an entity class. Without the @GeneratedValue next to it, the JPA provider assumes that you'll take care of creating and assigning an identifier value before you save an instance. We call this\u00aean application-assigned identifier. Assigning an entity identifier manually is necessary when you're dealing with a legacy database and/or natural primary keys. We have more to say about this kind of mapping in a dedicated section,",
  "page262": "Usually you want the system to generate a primary key value when you save an entity instance, so you write theGeneratedValue annotation next to @Id. JPA standardizes several value-generation strategies with the javax.persistence.GenerationType enum, which you select with @Generat\u00aeedValue(strategy): GenerationType.AUTO Hibernate picks an appropriate strategy, asking the SQL dialect of your configured database what is best. This is equivalent to @GeneratedValue() without any settings. GenerationType.SEQUENCE Hibernate expects (and creates, if you use the tools) a sequence named HIBERNATE_SEQUENCE in your database. The sequence will becalled separately before every INSERT, producing sequential numeric values. GenerationType.IDENTITY Hibernate expects (and creates in\u00aetable DDL) a special auto-incremented primary key column that automatically generates a numeric value on INSERT, in the database. GenerationType.TABLE Hibernate will use an extra table in your database schema that holds the next numeric primary key value, one row for each entity class. This table will be read a\u00aend updated accordingly, before INSERTs. The default table name isHIBERNATE_SEQUENCES with columns SEQUENCE_NAME and SEQUENCE_NEXT_HI_VALUE. (The internal implementation uses a more complex but efficient hi/lo generation algorithm; more on this later.)",
  "page263": "JPA has two built-in annotations you can use to configure named generators: @javax persistence.SequenceGenerator and @javax.persistence.TableGenerator. With these annotations, you can create a named generator with your own sequence and table names. As usual with JPA annotations,\u00aeyou can unfortunately only use them at the top of a (maybe otherwise empty) class, and not in a package-info.java file. For this reason, and because the JPA annotations don't give us access to the full Hibernate feature set, we prefer an alternative: the native @org.hibernate.annotations .GenericGenerator annotation. It supports all Hibernate identifier generator strategies and their configuration details. Unlike the rather limited JPA annotations, you can use the Hiberna\u00aete annotation in a package-info.java file, typically in the same package as your domain model classes. The next listing shows a recommended configuration. This Hibernate-specific generator configuration hasthe following advantages: The enhanced-sequence B strategy produces sequential numeric values. If your SQ\u00aeL dialect supports sequences, Hibernate will use an actual database sequence. If your DBMS doesn't support native sequences, Hibernate will manage and use an extra \"sequence table,\" simulating the behavior of a sequence. This gives you real portability: the generator can always be called\u00aebefore performing an SQL INSERT, unlike, for example, auto-increment identity columns, which produce a value on INSERT that has to be returned to the application afterward.",
  "page264": "You can configure the sequence_name C. Hibernate will either use an existing sequence or create it when you generate the SQL schema automatically. If your DBMS doesn't support sequences, this will be the special \"sequence table\" name. You can start with an initial_\u00aevalue D that gives you room for test data. For example, when your integration test runs, Hibernate will make any new data insertions from test code with identifier values greater than 1000. Any test data you want to import before the test can use numbers 1 to 999, and you can refer to the stable identifier values in your tests: \"Load item with id 123 and run some tests on it.\" This is applied when Hibernate generates the SQL schema and sequence; it'sa DDL option. You can sha\u00aere the same database sequence among all your domain model classes. There is no harm in specifying @GeneratedValue(generator \"ID_GENERATOR\") in all your entity classes. It doesn't matter if primary key values aren't contiguous for a particular entity, as long as they're unique within one table.\u00aeIf you're worried about contention, because the sequence has to be called prior to every INSERT, we discuss a variation of this generator configuration later,",
  "page265": "Finally, you use java.lang.Long as the type of the identifier property in the entity class, which maps perfectly to a numeric database sequence generator.You could also use a long primitive. The main difference is what someItem.getId() returns on a new item that hasn't been\u00aestored in the database: either null or 0. If you want to test whether an item is new, a null check is probably easier to understand for someone else reading your code. You shouldn't use another integral type such as int or short for identifiers. Although they will work for a while (perhaps even years), as your database size grows, you may be limited by their range. An Integer would work for almost two months if you generated a new identifier each millisecond with no gaps, and a\u00aeLong would last for about 300 million years. Although recommended for most applications, the enhanced-sequence strategy as shown in listing is just one of the strategies built into Hibernate. Identifier generator strategies Following is a list of all available Hibernate identifier generator strategies, their o\u00aeptions, and our usage recommendations. If you don't want to read the whole list now, enable GenerationType.AUTO and check what Hibernate defaults to for your database dialect. It's most likely sequence or identity a good but maybe not the most efficient or portable choice. If you require consistent\u00aeportable behavior, and identifier values to be available before INSERTs, use enhanced-sequence, as shown in the previous section. This is a portable, flexible, and modern strategy, also offering various optimizers for large datasets.",
  "page266": "We also show the relationship between each standard JPA strategy and its native Hibernate equivalent. Hibernate has been growing organically, so there are now two sets of mappings between standard and native strategies; we call them Old and New in the list. You can switch this ma\u00aepping with the hibernate.id.new_generator_mappings setting in your persistence.xml file. The default is true; hence the New mapping. Software doesn't age quite as well as wine: native Automatically selects other strategies, such as sequence or identity, depending on the configured SQL dialect. You have to look at the Javadoc (or even the source) of the SQL dialect you configured in persistence.xml. Equivalent to JPA GenerationType.AUTO with the Old mapping. sequence Uses a\u00aenative database sequence named HIBERNATE_SEQUENCE. The sequence is called before each INSERT of a new row. You can customize the sequence name and provide additional DDL settings; see the Javadoc for the class org.hibernate.id.SequenceGenerator and its parent. enhanced-sequence Uses a native database sequence\u00aewhen supported; otherwise falls back to an extra database table with a single column and row, emulating a sequence. Defaults to name HIBERNATE_SEQUENCE. Always calls the database \"sequence\" before an INSERT, providing the same behavior independently of whether the DBMS supports real sequences.\u00aeSupports an org.hibernate .id.enhanced.Optimizer to avoid hitting the database before each INSERT; defaults to no optimization and fetching a new value for each INSERT. You can find more examples in chapter20. For all parameters, see the Javadoc for the class org.hibernate.id.enhanced.SequenceStyleGenerator. Equivalent to JPA GenerationType.SEQUENCE and GenerationType.AUTO with the New mapping enabled, most likely your best option of the built-in strategies.",
  "page267": "seqhilo Uses a native database sequence named HIBERNATE_SEQUENCE, optimizing calls before INSERT by combining hi/lo values. If the hi value retrieved from the sequence is 1, the next 9 insertions will be made with key values 11, 12, 13, \u2026, 19. Then the sequence is call\u00aeed again to obtain the next hi value (2 or higher), and the procedure repeats with 21, 22, 23, and so on. You can configure the maximum lo value (9 is the default) with the max_lo parameter. Unfortunately, due to a quirk inHibernate's code, you can not configure this strategy in @GenericGenerator. The only way to use it is with JPA GenerationType.SEQUENCE and the Old mapping. You can configure it with the standard JPA @SequenceGenerator annotation on a (maybe otherwi\u00aese empty) class. See the Javadoc for the class org.hibernate.id.SequenceHiLoGenerator and its parent for more information. Consider using enhanced-sequence instead, with an optimizer hilo Uses an extra table named HIBERNATE_UNIQUE_KEY with the same algorithm as the seqhilo strategy. The table has a single\u00aecolumn and row, holding the next value of the sequence. The default maximum lo value is 32767, so you most likely want to configure it with the max_lo parameter. See the Javadoc for the class org.hibernate.id.TableHiLoGenerator for more information. We don't recommend this legacy strategy; use enhanced-\u00aesequence instead with an optimizer.",
  "page268": "enhanced-table Uses an extra table named HIBERNATE_SEQUENCES, with one row by default representing the sequence, storing the next value. This value is selected and updated when an identifier value has to be generated. You can configure this generator to use multiple rows instead:\u00aeone for each generator; see the Javadoc for org.hibernate.id.enhanced.TableGenerator. Equivalent to JPA GenerationType.TABLE with the New mapping enabled. Replaces the outdated but similar org.hibernate.id.MultipleHiLoPerTableGenerator, which is the Old mapping for JPA GenerationType.TABLE. identity Supports IDENTITY and auto-increment columns in DB2, MySQL, MS SQL Server, and Sybase. The identifier value for the primary key column will be generated on INSERT of a row. Has no optio\u00aens. Unfortunately, due to a quirk in Hibernate's code, you can not configure this strategy in @GenericGenerator. The only way to use it is with JPA GenerationType.IDENTITY and the Old or New mapping, making it the default for GenerationType.IDENTITY. increment At Hibernate startup, reads the maximum\u00ae(numeric) primary key column value of each entity's table and increments the value by one each time a new row is inserted. Especially efficient if a non-clustered Hibernate application has exclusive access to the database; but don't use it in any other scenario.",
  "page269": "select Hibernate won't generate a key value or include the primary key column in an INSERT statement. Hibernate expects the DBMS to assign a (default in schemaor by trigger) value to the column on insertion. Hibernate then retrieves the primary key column with a SELECT\u00aequery after insertion. Required parameter is key, naming the database identifier property (such as id) for the SELECT. This strategy isn't very efficient and should only be used with old JDBC drivers that can't return generated keys directly. uuid2 Produces a unique 128-bit UUID in the application layer. Useful when you need globally unique identifiers across databases (say, you merge data from several distinct production databases in batch runs every night into an archive). The\u00aeUUID can be encoded either as a java.lang.String, a byte[16], or a java .util.UUID property in your entity class. Replaces the legacy uuid and uuid .hex strategies. You configure it with an org.hibernate.id.UUIDGenerationStrategy; see the Javadoc for the class org.hibernate.id.UUIDGenerator for more deta\u00aeils. guid Uses a globally unique identifier produced by the database, with an SQL function available on Oracle, Ingres, MS SQL Server, and MySQL. Hibernate calls the database function before an INSERT. Maps to a java.lang.String identifier property. If you need full control over identifier generation, config\\u00aeu0002ure the strategy of @GenericGenerator with the fully qualified name of a class that implements the org.hibernate.id.IdentityGenerator interface.",
  "page270": "QUOTING SQL IDENTIFIERS- From time to time, especially in legacy databases, you'll encounter identifiers with strange characters or whitespace, or wish to force case sensitivity. Or, as in the previous example, the automatic mapping of a class or property would require a tab\u00aele or column name that is a reserved keyword.  Hibernate 5 knows the reserved keywords of your DBMS through the configureddatabase dialect. Hibernate 5 can automatically put quotes around such strings when generating SQL. You can enable this automatic quoting with hibernate.auto_quote _keyword true in your persistence unit configuration. If you're using an older version of Hibernate, or you find that the dialect's information is incomplete, you must still apply quotes on n\u00aeames manually in your mappings if there is a conflict with a keyword.  If you quote a table or column name in your mapping with backticks, Hibernate always quotes this identifier in the generated SQL. This still works in latest versions of Hibernate, but JPA 2.0 standardized this functionality as delimited ide\u00aentifiers with double quotes. If you have to quote all SQL identifiers, create an orm.xml file and add the setting <delimited-identifiers/> to its <persistence-unit-defaults> section, as shown in listing 3.8. Hibernate then enforces quoted identifiers everywhere.  You should consider renaming tables or\u00aecolumns with reserved keyword names whenever possible. Ad hoc SQL queries are difficult to write in an SQL console if you have to quote and escape everything properly by hand.  Next, you'll see how Hibernate can help when you encounter organizations with strict conventions for database table and column names.To summarize, our recommendations on identifier generator strategies are as follows: In general, we prefer pre-insert generation strategies that produce identifier values independently before INSERT. Use enhanced-sequence, which uses a native database sequence when supported and otherwise falls back to an extra database table with a single column and row, emulating a sequence.",
  "page271": "Entities are the coarser-grained classes of your system. Their instances have an independent life cycle and their own identity, and many other instances can reference them. Value types, on the other hand, are dependent on a particular entity class. A value type instance is\u00aebound to its owning entity instance, and only one entity instance can reference it it has no individual identity. We looked at Java identity, object equality, and database identity, and at what makes good primary keys. You learned which generators for primary key values Hibernate provides out of the box, and how to use and extend this identifier system. We discussed some useful class mapping options, such as naming strategies and dynamic SQL generation. After spending the previous chapt\u00aeer almost exclusively on entities and the respective class- and identity-mapping options, we now focus on value types in their various forms. We split value types into two categories: basic value-typed classes that come with the JDK, such as String, Date, primitives, and their wrappers; and developer\u00ae2defined value-typed classes, such as Address and MonetaryAmount in CaveatEmptor. In this chapter, we first map persistent properties with JDK types and learn the basic mapping annotations. You see how to work with various aspects of properties: overriding defaults, customizing access, and generated values. You a\u00aelso see how SQL is used with derived properties and transformed column values. We wrap up basic properties with temporal properties and mapping enumerations. We then discuss custom value-typed classes and map them as embeddable components. You learn how classes relate to the database schema and make your classes embeddable, while allowing for overriding embedded attributes. We complete embeddable components by mapping nested components. Finally, we discuss how to customize loading and storing of property values at a lower level with flexible JPA converters, a standardized extension point of every JPA provider. Dynamic SQL generation- By default, Hibernate creates SQL statements for each persistenC level QL statements will be large for even the simplest operations (say, only one column needs updating),",
  "page272": "This configuration by exception approach means you don't have to annotate a property to make it persistent; you only have to configure the mapping in an exceptional case. Several annotations are available in JPA to customize and control basic property map. Overriding b\u00aeasic property defaults- You might not want all properties of an entity class to be persistent. For example, although it makes sense to have a persistent Item#initialPrice property, an Item#totalPriceIncludingTax property shouldn't be persistent if you only compute and use its value at runtime, and hence shouldn't be stored in the database. To exclude a property, mark the field or the getter method of the property with the @javax.persistence.Transient annotation or use the J\u00aeava transient keyword. The transient keyword usually only excludes fields for Java serialization, but it's also recognized by JPA providers.  We'll come back to the placement of the annotation on fields or getter methods in a moment. Let's assume as we have before that Hibernate will acces\u00aes fields directly because @Id has been placed on a field. Therefore, all other JPA and Hibernate mapping annotations are also on fields. We have to admit that this annotation isn't very useful. It only has two parameters: the one shown here, optional, marks the property as not optional at the Java\u00aeobject level. By default, all persistent properties are nullable and optional; an Item may have an unknown initialPrice. Mapping the initialPrice property as non-optional makes sense if you have a NOT NULL constraint on the INITIALPRICE column in your SQL schema. If Hibernate is generating the SQL schema, it will include a NOT NULL constraint automatically for non-optional properties. Mapping basic properties- When you map a persistent class, whether it's an entity or an embeddable type (more about these later, in section 5.2), all of its properties are consideredpersistent by default. The default JPA rules for properties of persistent classes are these: you should always map Java classes instead of storing a heap of bytes in the database. Imagine maintaining a database with this binary information when the application is gone in a few years. Otherwise, Hibernate will throw an exception on startup, complaining that it doesn't understand the type of the property",
  "page273": "The Column annotation has a few other parameters, most of which control SQL-level details such as catalog and schema names. They're rarely needed, and we only show them throughout this book when necessary.  Property annotations aren't always on fields, and you may not\u00aewant Hibernate to access fields directly. Customizing property access- The persistence engine accesses the properties of a class either directly through fields or indirectly through getter and setter methods. An annotated entity inherits the default from the position of the mandatory @Id annotation. For example, if you've declared @Id on a field, not a getter method, all other mapping annotations for that entity are expected on fields. Annotations are never on the setter methods.  T\u00aehe default access strategy isn't only applicable to a single entity class. Any @Embedded class inherits the default or explicitly declared access strategy of its owning root entity class. We cover embedded components later in this chapter. Furthermore, Hibernate accesses any @MappedSuperclass p\u00aeroperties with the default or explicitly declared access strategy of the mapped entity class. Inheritance is the topic of chapter 6.  The JPA specification offers the @Access annotation for overriding the default behavior, with the parameters AccessType.FIELD and AccessType.PROPERTY. If you set @Access on th\u00aee class/entity level, Hibernate accesses all properties of the class according to the selected strategy. You then set any other mapping annotations, including the @Id, on either fields or getter methods, respectively. Now, when you store an Item and forget to set a value on the initialPrice field, Hibernate will complain with an exception before hitting the database with an SQL statement. Hibernate knows that a value is required to perform an INSERT or UPDATE.If you don't mark the property as optional and try to save a NULL, the database will reject the SQL statement, and Hibernate will throw a constraint-violation exception. There isn't much difference in the end result, but it's cleannotatioautomatically.  The @Column annotation can also override the mapping of the property name to the database column:",
  "page274": "Hibernate reads your Java domain model classes and mapping metadata and generates schema DDL statements. You can export them into a text file or execute them directly on your database whenever you run integration tests. Because schema languages are mostly vendor-specific, every o\u00aeption you put in your mapping metadata has the potential to bind the metadata to a particular database product keep this in mind when using schema features. Hibernate creates the basic schema for your tables and constraints automatically; it even creates sequences, depending on the identifier generator you select. But there are some schema artifacts Hibernate can't and won't create automatically. These include all kinds of highly vendor-specific performance options and any other\u00aeartifacts that are relevant only for the physical storage of data (tablespaces, for example). Besides these physical concerns, your DBA will often supply custom additional schema statements to improve the generated schema. DBAs should get involved early and verify the automatically generated schema from Hibern\u00aeate. Never go into production with an unchecked automatically generated schema. If your development process allows, changes made by the DBA can flow back into your Java systems, for you to add to mapping metadata. In many projects, the mapping metadata can contain all the necessary schema changes from a DBA.\u00aeThen Hibernate can generate the final production schema during the regular build by including all comments, constraints, indexes, and so on. In the following sections, we show you how to customize the generated schema and how to add auxiliary database schema artifacts (we call them objects sometimes; we don't mean Java objects here). We discuss custom data types, additional integrity rules, indexes, and how you can replace some of the (sometimes ugly) auto-generated artifact names produced by Hibernate.If your application can auction an item only once in the real world, your database schema should guarantee that. If an auction always has a starting price, yourdatabase model should include an aps you caed scripts and write an improved and final schema for production deployment. The first part of this chapter shows you how to improve the schema from within JPA and Hibernate, to make your DBA happy. At the other end of the spectrum are systems with existing, possibly complex schemas, with years' worth of data.",
  "page275": "This property defines when the create and drop scripts should be executed. Your custom SQL scripts will contain CREATE DOMAIN statements, which must be executed before the tables using these domains are created. With these settings, the schema generator runs the create script fir\u00aest before reading your ORM metadata (annotations, XML files) and creating the tables. The drop script executes after Hibernate drops the tables, giving you a chance to clean up anything you created. Other options are metadata (ignore custom script sources) and script (only use a custom script source; ignore ORM metadata in annotations and XML files). D This is the location of the custom SQL script for creation of the schema. The path is (a) the location of the script resource on the classp\u00aeath; (b) the location of the script as a file:// URL; or, if neither (a) nor (b) matches, (c) the absolute or relative file path on the local file system. This example uses (a). E This is the custom SQL script for dropping the schema. F This load script runs after the tables have been created. We've mentio\u00aened that DDL is usually highly vendor-specific. If your application has to support several database dialects, you may need several sets of create/drop/load scripts to customize the schema for each database dialect. You can solve this with several persistence unit configurations in the persistence.xml file. A\u00aelternatively, Hibernate hasits own proprietary configuration for schema customization in an hbm.xml mapping file, You can hook the following three types of custom SQL scriptsinto Hibernates schema-generation process: The create script executes when the schema is generated. A custom create script can run before, after, or instead of Hibernates automatically generated scripts. In other words, you can write an SQL script that runs before or after Hibernate generates tables, constraints, and so on from your mapping metadata. The drop script executes when Hibernate removes schema artifacts. Just like the create script, a drop script can run before, after, or instead of Hibernates automatication pro",
  "page276": "Table constraints An integrity rule that applies to several columns or several rows is a table constraint. A typical declarative table constraint is UNIQUE: all rows are checked for duplicate values (for example, each user must have a distinct email address). A rule affecting onl\u00aey a single row but multiple columns is \"the auction end time has to be after the auction start time.\" Database constraints If a rule applies to more than one table, it has database scope. You should already be familiar with the most common database constraint, the foreign key. This rule guarantees the integrity of references between rows, usually in separate tables, but not always (self-referencing foreign key constraints aren't uncommon). Other database constraints involvin\u00aeg several tables aren't uncommon: for example, a bid can only be stored if the auction end time of the referenced item hasn't been reached. Most (if not all) SQL database management systems support these kinds of constraints and the most important options of each. In addition to simple keywords such a\u00aes NOT NULL and UNIQUE, you can usually also declare more complex rules with the CHECK constraint that applies an arbitrary SQL expression. Still, integrity constraints are one of the weak areas in the SQL standard, and solutions from vendors can differ significantly. Furthermore, non-declarative and procedur\u00aeal constraints are possible with database triggers that intercept data-modification operations. A trigger can then implement the constraint procedure directly or call an existing stored procedure. Integrity constraints can be checked immediately when a data-modification statement is executed, or the check can be deferred until the end of a transaction. The violation response in SQL databases is usually rejection without any possibility of customization. Foreign keys are special because you can typically decide what should happen with ON DELETE or ON UPDATE for referenced rows. Systems that ensure data integrity only in application code are prone to data corruption and often degrade. The adCHAR data type can hold character strings: for example, all characters defined in ASCII or some other encoding. Because we mostly use data types built-in the DBMS, we rely on the domain constraints as defined by the vendor.",
  "page277": "This constraint restricts valid username values to a maximum length of 15 characters, and the string can't begin with admin to avoid confusion. You can call any SQL functions supported by your DBMS; the column Definition is always passed through into the exported schema. No\u00aete that you have a choice: creating and using a domain or adding a single column constraint has the same effect. Domains are usually easier to maintain and avoid duplicating. At the time of writing, Hibernate doesn't support its proprietary annotation @org.hibernate.annotations.Check on individual properties; you use it for table level constraints. Hibernate appends table constraints to the generated CREATE TABLE statement, which can contain arbitrary SQL expressions. You can implem\u00aeent multirow table constraints with expressions that are more complex. You may need a sub select in the expression to do this, which may not be supported by your DBMS. But there are some common multirow table constraints, like UNIQUE, that you can add directly in the mappings. You've already seen the @Colu\u00aemn(unique true | false) option in the previous section. Now all pairs of USERNAME and EMAIL must be unique, for all rows in the USERS table. If you don't provide a name for the constraint here, UNQ_USERNAME_EMAIL an automatically generated and probably ugly name is used. The last kinds of constraints we\u00aediscuss are database-wide rules that span several tables. Hibernate passes on database constraint violations in error exceptions; check whether the exception in your transaction has a cause, somewhere in the exception chain, of type org.hibernate.exception.ConstraintViolationException. This exception can provide more information about the error, such as the name of the failed database constraint. The SQL standard includes domains, which unfortunately are rather limited and often not supported by the DBMS. If your system supports SQL domains, you can use them to add constraints to data types. In your custom SQL create script, define an EMAIL_ADDRESS domain based on the VARCHAR data type: The additional concond is tly, so be careful with database-specific SQL",
  "page278": "This statement declares the foreign key constraint for the ITEM_ID column in the BID table, referencing the primary key column of the ITEM table. You can customize the name of the constraint with the foreign Key option in an @JoinColumn mapping A foreign Key attribute is also sup\u00aeported in @PrimaryKeyJoinColumn, @MapKeyJoinColumn, @JoinTable, @CollectionTable, and @AssociationOverride mappings. The @ForeignKey annotation has some rarely needed options we haven't shown: You can write your own foreign Key Definition, an SQL fragment such as FOREIGN KEY ([column]) REFERENCES [table]([column]) ON UPDATE [action]. Hibernate will use this SQL fragment instead of the provider-generated fragment, it can be in the SQL dialect supported by your DBMS. The Constraint Mo\u00aede setting is useful if you want to disable foreign key generation completely, with the value NO_CONSTRAINT. You can then write the foreign key constraint yourself with an ALTER TABLE statement, probably in a load script as we've shown. Naming constraints properly is not only good practice, but also helps\u00aesignificantly when you have to read exception messages. This completes our discussion of database integrity rules. Next, we look at some optimization you might want to include in your schema for performance reasons. Indexes are a key feature when optimizing the performance of a database application. The que\u00aery optimizer in a DBMS can use indexes to avoid excessive scans of the data tables. Because they're relevant only in the physical implementation of a database, indexes aren't part of the SQL standard, and the DDL and available indexing options are product specific. You can, however, embed the most common schema artifacts for typical indexes in mapping metadata A user can only make bids until an auction ends. Your database should guarantee that invalid bids can't be stored so that whenever a row is inserted into the BID table, the CREATEDON timestamp of the bid is checked against the auction ending time. This kind of constraint involves two tables: BID and ITEM. You can create a rule integrity rules. They're widely known as foreign keys, which are a combination of two things: a key value copy from a related row and a constraint that guarantees that the referenced value exists. Hibernate creates foreign key constraints automatically for all foreign key columns in association mappings.",
  "page279": "If you encounter a USERS table in a legacy schema, it's likely that USERNAME is the primary key. In this case, you have no surrogate identifier that Hibernate generates automatically. Instead, your application has to assign the identifier value when saving an instance of the\u00aeUser class Hibernate calls the protected no-argument constructor when it loads a User from the database and then assigns the username field value directly. When you instantiate a User, call the public constructor with a username. If you don't declare an identifier generator on the @Id property, Hibernate expects the application to take care of the primary key value assignment. Composite (natural) primary keys require a bit more work Suppose the primary key of the USERS table is a co\u00aemposite of the two columns USERNAME and DEPARTMENTNR. You write a separate composite identifier class that declares just the key properties and call this class User Id This class has to be @Embeddable and Serializable any type used as an identifier type in JPA must be Serializable. C You don't have to mark\u00aethe properties of the composite key as @NotNull; their database columns are automatically NOT NULL when embedded as the primary key of an entity. D The JPA specification requires a public no-argument constructor for an embeddable identifier class. Hibernate accepts protected visibility. E The only public con\u00aestructor should havethe key values as arguments. F You have to override the equals() and hashCode() methods with the same semantics the composite key has in your database. In this case, this is a straightforward comparison of the username and department values. Many queries in Caveat Emptor will probably involve the username of a User entity. You can speed up these queries by creating an index for the column of this property. Another candidate for an index is the combination of USERNAME and EMAIL columns, which you also use frequently in queries. You can declare single or multicolumn indexes on the entity class with the @Table annotation and its indexes attribute If you don't provide a name for the ining the mpact performance. Unfortunately, many legacy schemas use (natural) composite keys heavily; and for the reason we discourage the use of composite keys, it may be difficult to change the legacy schema to use non-composite natural or surrogate keys. Therefore, JPA supports natural and composite primary and foreign keys.",
  "page280": "We've already shown the @SecondaryTable annotation in an inheritance mapping in section 6.5. It helped to break out properties of a particular subclass into a separate table. This generic functionality has more uses but be aware that a properly designed system should have, s\u00aeimplified, more classes than tables. Suppose that in a legacy schema, you aren't keeping a user's billing address information with the other user details in the USERS main entity table, but in a separate table. Figure 9.5 shows this schema. The user's home address is stored in the columns STREET, ZIPCODE, and CITY of the USERS table. The user's billing address is stored in the BILLING_ADDRESS table, which has the primary key column USER_ID, which is also a foreign key\u00aeconstraint referencing the ID primary key of the USERS table The User class has two properties of embedded type: home Address and billing Address. The first is a regular embedded mapping, and the Address class is @Embeddable. Just as in section 5.2.3, you can use the @AttributeOverrides annotation to override\u00aethe mapping of embedded properties. Then, @Column maps the individual properties to the BILLING_ADDRESS table, with its table option. Remember that an @AttributeOverride replaces all mapping information for a property: any annotations on the Address fields are ignored if you override. Therefore, you have to\u00aespecify nullability and length again in the @Column override. We've shown you a secondary table mapping example with an embeddable property. Of course, you could also break out simple basic properties like the username string in a secondary table. Keep in mind that reading and maintaining these mappings can be a problem, though; you should only map legacy unchangeable schemas with secondary tables. A foreign key constraint on the SELLER column in the ITEM table ensures that the seller of the item exists by requiring the same seller value to be present onsome column in some row on some table. There are no other rules; the target column doesn't need a primary key constraint or even a unique mple rulma by refactoring foreign keys to reference primary keys if you can make changes to the database that don't disturb other applications sharing the data.",
  "page281": "strategies for runtime data management. These strategies are crucial to the performance and correct behavior of your applications. In this chapter, we discuss the life cycle of entity instances how an instance becomes persistent, and how it stops being considered persistent and\u00aethe method calls and management operations that trigger these transitions. The JPA Entity Manager is your primary interface for accessing data. Before we look at the API, let's start with entity instances, their life cycle, and the events that trigger a change of state. Although some of the material may be formal, a solid understanding of the persistence life cycle is essential. Because JPA is a transparent persistence mechanism classes are unaware of their own persistence capability\u00aeit's possible to write application logic that's unaware whether the data it operates on represents persistent state or temporary state that exists only in memory. The application shouldn't necessarily need to care that an instance ispersistent when invoking its methods. You can, for example, inv\u00aeoke the Item #calculateTotalPrice() business method without having to consider persistence at all (for example, in a unit test). Any application with persistent state must interact with the persistence service whenever it needs to propagate state held in memory to the database (or vice versa). In other words\u00ae, you have to call the Java Persistenceinterfaces to store and load data. When interacting with the persistence mechanism that way, the application must concern itself with the state and life cycle of an entity instance with respect to persistence. We refer to this as the persistence life cycle: the states an entity instance goes through during its life. We also use the term unit of work: a set of (possibly) state changing operations considered one (usually atomic) group. Another piece of the puzzle is the persistence context provided by the persistence service. In part 3, you'll load and store data with Hibernate and Java Persistence. You'll introduce the programming interfa most up new approaches for application design. You'll be ready to optimize any object-modification scenario and apply the best fetching and caching strategy to increase performance and scalability You now understand how Hibernate and ORM solve the static aspects of the object/relational mismatch.",
  "page282": "A persistent entity instance has a representation in the database. It's stored in the database or it will be stored when the unit of work completes. It's an instance with a database identity, as defined in section 4.2; its database identifier is set to the primary key v\u00aealue of the database representation. The application may have created instances and then made them persistent by calling Entity Manager #persist(). There may be instances that became persistent when the application created a reference to the object from another persistent instance that the JPA provider already manages. A persistent entity instance may be an instance retrieved from the database by execution of a query, by an identifier lookup, or by navigating the object graph starting fr\u00aeom another persistent instance. Persistent instances are always associated with a persistence context. Yousee more about this in a moment You can delete a persistent entity instance from the database in several ways: For example, you can remove it with Entity Manager #remove(). It may also become available for\u00aedeletion if you remove a reference to it from a mapped collection with orphan removal enabled. An entity instance is then in the removed state: the provider will delete it at the end of a unit of work. You should discard any references you may hold to it in the application after you finish working with it f\u00aeor example, after you've rendered the removal confirmation screen your users see Think of the persistence context as a service that remembers all the modifications and state changes you made to data in a particular unit of work (this is somewhat simplified, but it's a good starting point). We now dissect all these terms: entity states, persistence contexts, and managed scope. You're probably more accustomed to thinking about what SQL statements you have to manage to get stuff in and out of the database; but one of the key factors of your success with Java Persistence is your understanding of state management, so stick with us through this section. Different ORM solutions use differents to thecreation of a reference from an already-persistent instance and enabled cascading of ",
  "page283": "The persistence context acts as a first-level cache; it remembers all entity instances you've handled in a particular unit of work. For example, if you ask Hibernate to load an entity instance using a primary key value (a lookup by identifier), Hibernate can first check the\u00aecurrent unit of work in the persistence context. If Hibernate finds the entity instance in the persistence context, no database hit occurs this is a repeatable read for an application. Consecutive em.find(Item. Class, ITEM_ID) calls with the same persistence context will yield the same result. This cache also affects results of arbitrary queries, executed for example with the javax.persistence.Query API. Hibernate reads the SQL result set of a query and transforms it into entity instances\u00ae. This process first tries to resolve every entity instance in the persistence context by identifier lookup. Only if an instance with the same identifier value can't be found in the current persistence context does Hibernate read the rest of the data from the result-set row. Hibernate ignores any potential\u00aely newer data in the result set, due to read-committed transaction isolation at the database level, if the entity instance is already present in the persistence context. The persistence context cache is always on it can't be turned off. It ensures the following The persistence layer isn't vulnerabl\u00aee to stack overflows in the case of circular references in an object graph. There can never be conflicting representations of the same database row at the end of a unit of work. The provider can safely write all changes made to an entity instance to the database. Likewise, changes made in a particular persistence context are always immediately visible to all other code executed inside that unit of work and its persistence context. JPA guarantees repeatable entity-instance reads To understand detached entity instances, consider loading an instance. You call Entity Manager #find() to retrieve an entity instance by its (known) identifier. Then you end your unit of work and close the persistence context. The a and merxample, Hibernate may synchronize with the database before execution of a query. This ensures that queries are aware of changes made earlier during the unit of work.",
  "page284": "Hibernate joins the in-progress system transaction when an Entity Manager is created and waits for the transaction to commit. When Hibernate is notified (by the JTA engine) of the commit, it performs dirty checking of the persistence context and synchronizes with the database. Yo\u00aeu can also force dirty checking synchronization manually by calling EntityManager#flush() at any time during a transaction. You decide the scope of the persistence context by choosing when to close() the Entity Manager. You have to close the persistence context at some point, so always place the close() call in a finally block. How long should the persistence context be open? Let's assume for the following examples that you're writing a server, and each client request will be p\u00aerocessed with one persistence context and system transaction in a multithreaded environment. If you're familiar with servlets, imagine the code in listing 10.1 embedded in a servlet's service() method. Within this unit of work, you access the Entity Manager to load and store data. A new transient Item\u00aeis instantiated as usual. Of course, you may also instantiate it before creating the EntityManager. A call to persist() makes the transient instance of Item persistent. It's now managed by and associated with the current persistence context. To store the Item instance in the database, Hibernate has to\u00aeexecute an SQL INSERT statement. When the transaction of this unit of work commits, Hibernate flushes the persistence context, and the INSERT occurs at that time. Hibernate may even batch the INSERT at the JDBC level with other statements. When you call persist(), only the identifier value of the Item is assigned. Alternatively, if your identifier generator isn't pre-insert, the INSERT statement will be executed immediately when persist() is called. You may want to review section In Java SE and some EE architectures (if you only have plain servlets, for example), you get an Entity Manager by calling Entity Manager Factory #createEntityManager(). Your application code shares the Entityommit() ty Manager without hitting the database.",
  "page285": "You can modify the Item instance, and the persistence context will detect these changes and record them in the database automatically. When Hibernate flushes the persistence context during commit, it executes the necessary SQL DML statements to synchronize the changes with the da\u00aetabase. Hibernate propagates state changes to the database as late as possible, toward the end of the transaction. DML statements usually create locks in the database that are held until the transaction completes, so Hibernate keeps the lock duration in the database as short as possible. Hibernate writes the new Item#name to the database with an SQL UPDATE. By default, Hibernate includes all columns of the mapped ITEM table in the SQL UPDATE statement, updating unchanged columns to their\u00aeold values. Hence, Hibernate can generate these basic SQL statements at startup, not at runtime. If you want to include only modified (or non-nullable for INSERT) columns in SQL statements, you can enable dynamic SQL generation as discussed in section 4.3.2. Hibernate detects the changed name by comparing the\u00aeItem with a snapshot copy it took before, when the Item was loaded from the database. If your Item is different from the snapshot, an UPDATE is necessary. This snapshot in the persistence context consumes memory. Dirty checking with snapshots can also be time consuming, because Hibernate has to compare all in\u00aestances in the persistence context with their snapshot during flushing. It's better (but not required) to fully initialize the Item instance before managing it with a persistence context. The SQL INSERT statement contains the values that were held by the instance at the point when persist() was called. If you don't set the name of the Item before making it persistent, a NOT NULL constraint may be violated. You can modify the Item after calling persist(), and your changes will be propagated to the database with an additional SQL UPDATE statement. If one of the INSERT or UPDATE statements made when flushing fails, Hibernate causes a rollback of changes made to persistent instances",
  "page286": "As soon as you call any method such as Item#getName() on the proxy, a SELECT is executed to fully initialize the placeholder. The exception to this rule is a mapped database identifier getter method, such as getId(). A proxy may look like the real thing, but it's only a plac\u00aeeholder carrying the identifier value of the entity instance it represents. If the database record no longer exists when the proxy is initialized, an EntityNotFoundException is thrown. Note that the exception can be thrown when Item#getName() is called. E Hibernate has a convenient static initialize() method that loads the proxy's data. F After the persistence context is closed, item is in detached state. If you don't initialize the proxy while the persistence context is still op\u00aeen, you get a LazyInitializationException if you access the proxy. You can't load data on demand once the persistence context is closed. The solution is simple: load the data before you close the persistence context. We'll have much more to say about proxies, lazy loading, and on-demand fetching in c\u00aehapter 12. If you want to remove the state of an entity instance from the database, you have to make it transient. To make an entity instance transient and delete its database representation, call the remove() method on the EntityManager If you call find(), Hibernate executes a SELECT to load the Item. If yo\u00aeu call getReference(), Hibernate attempts to avoid the SELECT and returns a proxy. C Calling remove() queues the entity instance for deletion when the unit of work completes; it's now in removed state. If remove() is called on a proxy, Hibernate executes a SELECT to load the data. You may want to customize how Hibernate detects dirty state, using an extension point. Set the property hibernate.entity_dirtiness_strategy in your persistence.xml configuration file to a class name that implements org.hibernate.CustomEntityDirtinessStrategy. See the Javadoc of this interface for more information. org.hibernate.Interceptor is another extension point used to customize dirty checking, by implementing its findDi is retuu2019re working with an uninitialized proxy.",
  "page287": "Java Persistence also offers bulk operations that translate into direct SQL DELETE statements without life cycle interceptors in the application. We'll discuss these operations in section 20.1. Let's say you load an entity instance from the database and work with the d\u00aeata. For some reason, you know that another application or maybe another thread of your application has updated the underlying row in the database. Next, we'll see how to refresh the data held in memory. After you load the entity instance, you realize (how isn't important) that someone else changed the data in the database. Calling refresh() causes Hibernate to execute a SELECT to read and marshal a whole result set, overwriting changes you already made to the persistent instance\u00aein application memory. If the database row no longer exists (someone deleted it), Hibernate throws an EntityNotFoundException on refresh(). Most applications don't have to manually refresh in-memory state; concurrent modifications are typically resolved at transaction commit time. The best use case for\u00aerefreshing is with an extended persistence context, which might span several request/response cycles and/or system transactions. While you wait for user input with an open persistence context, data gets stale, and selective refreshing may be required depending on the duration of the conversation and the dialo\u00aegue between the user and the system. Refreshing can be useful to undo changes made in memory during a conversation, if the user cancels the dialogue. We'll have more to say about refreshing in a conversation in section. An entity instance must be fully initialized during life cycle transitions. You may have life cycle callback methods or an entity listener enabled (see section 13.2), and the instance must pass through these interceptors to complete its full life cycle. D An entity in removed state is no longer in persistent state. You can check this with the contains() operation. E You can make the removed instance persistent again, cancelling the deletion. F When the transaction commalue. Solue after removal of an entity instance. In the previous code example, the identifier value is reset to the default value of null (it's a Long). The Item is now the same as intransient state, and you can save it again in a new persistence context.",
  "page288": "The persistence context is a cache of persistent instances. Every entity instance in persistent state is associated with the persistence context. Many Hibernate users who ignore this simple fact run into an OutOfMemoryException. This is typically the case when you load thousands\u00aeof entity instances in a unit of work but never intend to modify them. Hibernate still has to create a snapshot of each instance in the persistence context cache, which can lead to memory exhaustion. (Obviously, you should execute a bulk data operation if you modify thousands of rows we'll get back to this kind of unit of work in section 20.1.) The persistence context cache never shrinks automatically. Keep the size of your persistence context to the necessary minimum. Often, many p\u00aeersistent instances in your context are there by accident for example, because you needed only a few items but queried for many. Extremely large graphs can have a serious performance impact and require significant memory for state snapshots. Check that your queries return only data you need, and consider the fo\u00aellowing ways to control Hibernate's caching behavior. You can call EntityManager#detach(i) to evict a persistent instance manually from the persistence context. You can call EntityManager#clear() to detach all persistent entity instances, leaving you with an empty persistence context. The native Sessio\u00aen API has some extra operations you might find useful. You can set the entire persistence context to read-only mode. This disables state snapshots and dirty checking, and Hibernate won't write modifications to the database. Replication is useful, for example, when you need to retrieve data from one database and store it in another. Replication takes detached instances loaded in one persistence context and makes them persistent in another persistence context. You usually open these contexts from two different EntityManagerFactory configurations, enabling two logical databases. You have to map the entity in both configurations. The replicate() operation is only available on the Hibernate Session API. Heame idennt databases. An example case is a product upgrade: if the new version of your application requires a new database (schema), you may want to migrate and replicate the existing data once.",
  "page289": "Next, we look at the detached entity state. We already mentioned some issues you'll see when entity instances aren't associated with a persistence context anymore, such as disabled lazy initialization. Let's explore the detached state with some examples, so you kno\u00aew what to expect when you work with data outside of a persistence context. If a reference leaves the scope of guaranteed identity, we call it a reference to a detached entity instance. When the persistence context is closed, it no longer provides an identity-mapping service. You'll run into aliasing problems when you work with detached entity instances, so make sure you understand how to handle the identity of detached instances. If you look up data using the same database identifier\u00aevalue in the same persistence context, the result is two references to the same in-memory instance on the JVM heap. Consider the two units of work shown next. In the first unit of work at begin() B, you start by creating a persistence context C and loading some entity instances. Because references a and b are\u00aeobtained from the same persistence context, they have the same Java identity D. They're equal from the same persistence context, they have the same Java identity D. They're equal E because by default equals() relies on Java identity comparison. They obviously have the same database identity F. They reference\u00aethe same Item instance, in persistent state, managed by the persistence context for that unit of work. The first part of this example finishes by committing the transaction. By default, Hibernate flushes the persistence context of an EntityManager and synchronizes changes with the database whenever the joined transaction is committed. All the previous code examples, except some in the last section, have used that strategy. JPA allows implementations to synchronize the persistence context at other times, if they wish. Hibernate, as a JPA implementation, synchronizes at the following times: When a joined JTA system transaction is committed Before a query is executed we do quertabase by calling EntityManager#flush(). This concludes our discussion of the transient, persistent, and removed entity states, and the basic usage of the EntityManager API. Mastering these state transitions and API methods is essential; every JPA application is built with these operations.",
  "page290": "When you begin a journey, it's a good idea to have a mental map ofthe terrain you'll be passing through. The same is true for an intellectual journey, such as learning to write computer programs. In this case, you'll need to know the basics of what computers are an\u00aed how they work. You'll want to have some idea of what a computer program is and how one is created. Since you will be writing programs in the Java programming language, you'll want to know something about that language in particular and about the modern, networked computing environment for which Java is designed. As you read this chapter, don't worry if you can't understand everything in detail. (In fact, it would be impossible for you to learn all the details from the\u00aebrief expositions in this chapter.) Concentrate on learning enough about the big ideas to orient yourself, in preparation for the rest of the book. Most of what is covered in this chapter will be covered in much greater detail later in the book. A computer is a complex system consisting of many different compo\u00aenents. But at the heart or the brain, if you want of the computer is a single component that does the actual computing. This is the Central Processing Unit, or CPU. In a modern desktop computer, the CPU is a single \"chip\" on the order of one square inch in size. The job of the CPU is to execute prog\u00aerams. A program is simply a list of unambiguous instructions meant to be followed mechanically by a computer. A computer is built to carry out instructions that are written in a very simple type of language called machine language. Each type of computer has its own machine language, and the computer can directly execute a program only if the program is expressed in that language. (It can execute programs written in other languages if they are first translated into machine language.) When the CPU executes a program, that program is stored in the computer's main memory (also called the RAM or random access memory). In addition to the program, memory can also hold data that is being used or processtion as chine language instructions in main memory. It does this by repeatedly reading, or fetching, an instruction from memory and then carrying out, or executing, that instruction. This process fetch an instruction, execute it,",
  "page291": "Machine language instructions are expressed as binary numbers. A binary number is made up of just two possible digits, zero and one. So, a machine language instruction is just a sequence of zeros and ones. Each particular sequence encodes some particular instruction. The data tha\u00aet the computer manipulates is also encoded as binary numbers. A computer can work directly with binary numbers because switches can readily represent such numbers: Turn the switch on to represent a one; turn it off to represent a zero. Machine language instructions are stored in memory as patterns of switches turned on or off, When a machine language instruction is loaded into CPU, all that happens is that certain switches are turned on or off in the pattern that encodes that particular in\u00aestruction. The CPU is built to respond to this pattern by executing the instruction it encodes; it does this simply because of the way all the other switches in the CPU are wired together. So, you should understand this much about how computers work: Main memory holds machine language programs and data. These a\u00aere encoded as binary numbers. The CPU fetches machine language instructions from memory one after another and executes them. It does this mechanically, without thinking about or understanding what it does-and therefore the program it executes must be perfect, complete in all details, and unambiguous because t\u00aehe CPU can do nothing but execute it exactly as written. Here is a schematic view of this first-stage understanding of the computer Asynchronous Events: Polling Loops and Interrupts The CPU spends almost all of its time fetching instructions from memory and executing them. However, the CPU and main memory are only two out of many components in areal computer system, A complete system contains other devices such as: A hard disk for storing programs and data files.(Note that main memory holds only a comparatively small amount of information, and holds it only as long as the power is turned on. A hard disk is necessary for permanent storage of larger amounts of information, but programs have to be loaded from disk iimages iere is a device driver, which consists of software that the CPU executes when it has to deal with the device. Installing a new device on a system generally has two steps: plugging the device physically into the computer",
  "page292": "Again, you should understand that this is a purely mechanical process: A device signals an interrupt simply by turning on a wire. The CPU is built so that when that wire is turned on, the CPU saves enough information about what it is currently doing so that it can return to the s\u00aeame state later. This information consists of the contents of important internal registers such as the program counter. Then the CPU jumps to some predetermined memory location and begins executing the instructions stored there. Those instructions make up an interrupt handler that does the processing necessary to respond to the interrupt. (This interrupt handler is part of the device driver software for the device that signalled the interrupt.) At the end of the interrupt handler is an ins\u00aetruction that tells the CPU to jump back to whatit was doing; it does that by restoring its previously saved state. Interrupts allow the CPU to deal with asynchronous events. In the regular fetch-and execute cycle, things happen in a predetermined order; everything that happens is \"synchronized\" with\u00aeeverything else. Interrupts make it possible for the CPU to deal efficiently with events that happen \"asynchronously,\" that is, at unpredictable times. As another example of how interrupts are used, consider what happens when the CPU need to access data that is stored on the hard disk. The CPU can a\u00aeccess data directly only if it is in main memory. Data on the disk has to be copied into memory before it can be accessed. Unfortunately, on the scale of speed at which the CPU operates, the disk drive is extremely slow. When the CPU needs data from the disk, it sends a signal to the disk drive telling it to locate the data and get it ready. (This signal is sent synchronously, under the control of a regular program.) Then, instead of just waiting the long and unpredictalble amount of time that the disk drive will take to do this, the CPU goes on with some other task. When the disk drive has the data ready, it sends an interrupt signal to the CPU. The interrupt handler can then read the requested data. Now,ly switcalled a thread. (Or a process; there are technical differences between threads and processes, but they are not important here.) At any given time, only one thread can actually be executed by a CPU.",
  "page293": "The Java Virtual Machine Machine language consists of very simple instructions that can be executed directly by the CPU of a computer. Almost all programs, though, are written in high-level programming languages such as Java, Pascal, or C++. A program written in a high-level lang\u00aeuage cannot be run directly on any computer. First, it has to be translated into machine language. This translation can be done by a program called a compiler. A compiler takes a high-level-language program and translates it into an executable machine-language program. Once the translation is done, the machine-language program can be run any number of times, but of course it can only be run on one type of computer (since each type of computer has its own individual machine language). If th\u00aee program is to run on another type of computer it has to be re-translated, using a different compiler, into the appropriate machine language. There is an alternative to compiling a high-level language program. Instead of using a compiler, which translates the program all at once, you can use an interpreter, wh\u00aeich translates it instruction-by-instruction, as necessary. An interpreter is a program that acts much like a CPU, with a kind of fetch-and-execute cycle. In order to execute a program, the interpreter runs in a loop in which it repeatedly reads one instruction from the program, decides what is necessary to c\u00aearry out that instruction, and then performs the appropriate machine-language commands to do so. one use of interpreters is to execute high-level language programs. For example, the programming language Lisp is usually executed by an interpreter rather than a compiler. However, interpreters have another purpose: they can let you use a machine-language program meant for one type of computer on a completely different type of computer. For example, there is a program called \"Virtual PC\" that runs on Macintosh computers. Virtual PC is an interpreter that executes machine-language programs written for IBM-PC-clone computers. If you run Virtual PC on your Macintosh, you can run any PC program, including progrcompilatputer. However, one of the main selling points of Java is that it can actually be used on any computer. All that the computer needs is an interpreter for Java bytecode. Such an interpreter simulates the Java virtual machct-oriented",
  "page294": "Programs must be designed. No one can just sit down at the computer and compose a program of any complexity. The discipline called software engineering is concerned with the construction of correct, working, well-written programs. The software engineer tends to use accepted and p\u00aeroven methods for analyzing the problem to be solved and for designing a program to solve that problem. During the 1970s and into the 80s, the primary software engineering methodology was structured programming. The structured programming approach to program design was based on the following advice: To solve a large problem, break the problem into several pieces and work on each piece separately; to solve each piece, treat it as a new problem which can itself be broken down into smaller pr\u00aeoblems; eventually, you will work your way down to problems that can be solved directly, without further decomposition. This approach is called top-down programming. There is nothing wrong with top-down programming. It is a valuable and often-used approach to problem-solving. However, it is incomplete. For one\u00aething, it deals almost entirely with producing the instructions necessary to solve a problem. But as time went on, people realized that the design of the data structures for a program was as least as important as the design of subroutines and control structures. Top-down programming doesn't give adequate\u00aeconsideration to the data that the program manipulates. Another problem with strict top-down programming is that it makes it difficult to reuse work done for other projects. By starting with a particular problem and subdividing it into convenient pieces, top-down programming tends to produce a design that is unique to that problem. It is unlikely that you will be able to take a large chunk of programming from another program and fit it into your project, at least not without extensive modification. Producing high-quality programs is difficult and expensive, so programmers and the people who employ them are always eager to reuse past work. So, in practice, top-down design is often combined with bottom-up design. nteractstware modules is to contain some data, along with some subroutines for manipulating that data. For example, a mailing-list module might contain a list of names and addresses along with a subroutine for adding a new name,d to be better messages.",
  "page295": "For example, consider a drawing program that lets the user draw lines, rectangles, ovals, polygons, and curves on the screen. In the program, each visible object on the screen could be represented by a software object in the program. There would be five classes of objects in the\u00aeprogram, one for each type of visible object that can be drawn. All the lines would belong to one class, all the rectangles to another class, and so on. These classes are obviously related; all of them represent \"drawable objects.\" They would, for example, all presumably be able to respond to a \"draw yourself\" message. Another level of grouping, based on the data to represent each type of object, is less obvious, but would be very useful in a program: We can group poly\u00aegons and curves together as \"multipoint objects,\" while lines, rectangles, and ovals are \"two-point objects.\" (A line is determined by its endpoints, a rectangle by two of its corners, and an oval by two corners of the rectangle that contains it.) We could diagram these relationships as foll\u00aeows DrawableObject, MultipointObject, and TwoPointObject would be classes in the program. MultipointObject and TwoPointObject would be subclasses of DrawableObject. The class Line would be a subclass of TwoPointObject and (indirectly) of DrawableObject. A subclass of a class is said to inherit the properties\u00aeof that class. The subclass can add to its inheritance and it can even \"override\" part of that inheritance (by defining a different response to some method). Nevertheless, lines, rectangles, and so on are drawable objects, and the class DrawableObject expresses this relationship. Inheritance is a powerful means for organizing a program. It is also related to the problem of reusing software components. A class is the ultimate reusable component. Not only can it be reused directly if it fits exactly into a program you are trying to write, but if it just almost fits, you can still reuse it by defining a subclass and making only the small changes necessary to adapt it exactly to your needs. So, OOP is meantdants wh the computer types back itsresponse. Early personal computers also used typed commands and responses, except that there was only one person involved at a time. This type of interaction between a user and a computer, and is usedwser tries to load the applet.",
  "page296": "The use of the term \"message\" here is deliberate. Messages, as you saw in the previous section, are sent to objects. In fact, Java GUI components are implemented as objects. Java includes many predefined classes that represent various types of GUI components. Some of th\u00aeese classes are subclasses of others. Here is a diagram showing some of Swing's GUI classes and their relationships. Don't worry about the details for now, but try to get some feel about how object-oriented programming and inheritance are used here. Note that all the GUI classes are subclasses, directly or indirectly, of a class called JComponent, which represents general properties that are shared by all Swing components. Two of the direct subclasses of JComponent themselves hav\u00aee subclasses. The classes JTextArea and JTextField, which have certain behaviors in common, are grouped together as subclasses of JTextComponent. Similarly JButton and JToggleButton are subclasses of JAbstractButton, which represents properties common to both buttons and checkboxes. (JComboBox, by the way, is t\u00aehe Swing class that represents pop-up menus.) Just from this brief discussion, perhaps you can see how GUI programming can make effective use of object-oriented design. In fact, GUI's, with their \"visible objects,\" are probably a major factor contributing to the popularity of OOP. The Internet\u00aeand the World-Wide Web - Computers can be connected together on networks. A computer on a network can communicate with other computers on the same network by exchanging data and files or by sending and receiving messages.Computers on a network can even work together on a large computation. Today, millions of computers throughout the world are connected to a single huge network called the Internet. New computers are being connected to the Internet every day. Computers can join the Internet by using a modem to establish a connection through telephone lines. Broadband connections to the Internet, such as DSL and cable modems, are increasingly common. They allow faster data transmission than is possible throug to anot some data being sent from one computer to another, along with addressing information that indicates where on the Internet that data is supposed to go. Think of a packet as an envelope with an address on the outside and AP, are used to by the programs that the person uses to send and receive email messages.",
  "page297": "On a basic level (the level of machine language), a computer can perform only very simple operations. A computer performs complex tasks by stringing together large numbers of such operations. Such tasks must be \"scripted\" in complete and perfect detail by programs. Crea\u00aeting complex programs will never be really easy, but the difficulty can be handled to some extent by giving the program a clear overall structure. The design of the overall structure of a program is what I call \"programming in the large.\" Programming in the small, which is sometimes called coding, would then refer to filling in the details of that design. The details are the explicit, step-by-step instructions for performing fairly small-scale tasks. When you do coding, you are w\u00aeorking fairly \"close to the machine,\" with some of the same concepts that you might use in machine language: memory locations, arithmetic operations, loops and branches. In a high-level language such as Java, you get to work with these concepts on a level several steps above machine language. However,\u00aeyou still have to worry about getting all the details exactly right. This chapter and the next examine the facilities for programming in the small in the Java programming language. Don't be misled by the term \"programming in the small\" into thinking that this material is easy or unimportant. T\u00aehis material is an essential foundation for all types of programming. If you don't understand it, you can't write programs, no matter how good you get at designing their large-scale structure. The Basic Java Application - A program is a sequence of instructions that a computer can execute to perform some task. A simple enough idea, but for the computer to make any use of the instructions, they must be written in a form that the computer can use. This means that programs have to be written in programming languages. Programming languages differ from ordinary human languages in being completely unambiguous and very strict about what is and is not allowed in a program. The rules that determine what isu fix thect program is one that does what you want it to. Furthermore, a program can be syntactically and semantically correct but still be a pretty bad program. Using the language correctly is not the same as using it well. For details here oming environments.",
  "page298": "Here is a Java program to display the message \"Hello World!\". Don't expect to understand what's going on here just yet some of it you won't really understand until a few chapters from now: // A program to display the message // \\\"Hello World!\\\" on sta\u00aendard output public class HelloWorld { public static void main(String[] args) { System.out.println(\\\"Hello World!\\\"); } } // end of class HelloWorld The command that actually displays the message is: System.out.println(\\\"Hello World!\\\"); This command is an example of a subroutine call statement. It uses a \"built-in subroutine\" named System.out.println to do the actual work. Recall that a subroutine consists of the instructions for performing some task, chunked together an\u00aed given a name. That name can be used to \"call\" the subroutine whenever that task needs to be performed. A built-in subroutine is one that is already defined as part of the language and therefore automatically available for use in any program. When you run this program, the message \"Hello World!\\u00aeu201d (without the quotes) will be displayed on standard output. Unfortunately, I can't say exactly what that means! Java is meant to run on many different platforms, and standard output will mean different things on different platforms. However, you can expect the message to show up in some convenient p\u00aelace. (If you use a command-line interface, like that in Sun Microsystem's Java Development Kit, you type in a command to tell the computer to run the program. The computer will type the output from the program, Hello World!, on the next line.) You must be curious about all the other stuff in the above program. Part of it consists of comments. Comments in a program are entirely ignored by the computer; they are there for human readers only. This doesn't mean that they are unimportant. Programs are meant to be read by people as well as by computers, and without comments, a program can be very difficult to understand. Java has two types of comments. The first type, used in the above program, bbove prohe main() subroutine, and the statements that it contains are executed. These statements make up the script that tells the computer exactly what to do when the program is executed. The main() routine can call subroutinestics of the lan for layout that",
  "page299": "Variables and the Primitive Types - Names are fundamental to programming. In programs, names are used to refer to many different sorts of things. In order to use those things, a programmer must understand the rules for giving names to things and the rules for using the names to w\u00aeork with those things. That is, the programmer must understand the syntax and the semantics of names. According to the syntax rules of Java, a name is a sequence of one or more characters. It must begin with a letter or underscore and must consist entirely of letters, digits, and underscores. (\"Underscore\" refers to the character ' '.) For example, here are some legal names: No spaces are allowed in identifiers; HelloWorld is a legal identifier, but \"Hello World\u00ae01d is not. Upper case and lower case letters are considered to be different, so that HelloWorld, helloworld, HELLOWORLD, and hElloWorLD are all distinct names. Certain names are reserved for special uses in Java, and cannot be used by the programmer for other purposes. These reserved words include: class, public,\u00aestatic, if, else, while, and several dozen other words. Java is actually pretty liberal about what counts as a letter or a digit. Java uses the Unicode character set, which includes thousands of characters from many different languages and different alphabets, and many of these characters count as letters or\u00aedigits. However, I will be sticking to what can be typed on a regular English keyboard. The pragmatics of naming includes style guidelines about how to choose names for things. For example, it is customary for names of classes to begin with upper case letters, while names of variables and of subroutines begin with lower case letters; you can avoid a lot of confusion by following the same convention in your own programs. Most Java programmers do not use underscores in names, although some do use them at the beginning of the names of certain kinds of variables. When a name is made up of several words, such as HelloWorld or interestRate, it is customary to capitalize each word, except possibly the first; this ist.printlimple or compound that can be used to refer to something in Java. (Note that the reserved words are not identifiers, since they can't be used as names for things.) Variables - Programs manipulate data that are storecuting a prograion is the number tever was there before.",
  "page300": "Types and Literals- A variable in Java is designed to hold only one particular type of data; it can legally hold that type of data and no other. The compiler will consider it to be a syntax error if you try to violate this rule. We say that Java is a strongly typed language becau\u00aese it enforces this rule. There are eight so-called primitive types built into Java. The primitive types are named byte, short, int, long, float, double, char, and boolean. The first four types hold integers (whole numbers such as 17, -38477, and 0). The four integer types are distinguished by the ranges of integers they can hold. The float and double types hold real numbers (such as 3.6 and -145.99). Again, the two real types are distinguished by their range and accuracy. A variable of ty\u00aepe char holds a single character from the Unicode character set. And a variable of type boolean holds one of the two logical values true or false. Any data value stored in the computer's memory must be represented as a binary number, that is as a string of zeros and ones. A single zero or one is called a b\u00aeit. A string of eight bits is called a byte. Memory is usually measured in terms of bytes. Not surprisingly, the byte data type refers to a single byte of memory. A variable of type byte holds a string of eight bits, which can represent any of the integers between -128 and 127, inclusive. (There are 256 integ\u00aeers in that range; eight bits can represent 256 two raised to the power eight different values.) As for the other integer types, short corresponds to two bytes (16 bits). Variables of type short have values in the range -32768 to 32767. int corresponds to four bytes (32 bits). Variables of type int have values in the range -2147483648 to 2147483647. long corresponds to eight bytes (64 bits). Variables of type long have values in the range -9223372036854775808 to 9223372036854775807. You don't have to remember these numbers, but they do give you some idea of the size of integers that you can work with. Usually, you should just stick to the int data type, which is good enough for most purposes. The float data type i the power 308, and has about 15 significant digits. Ordinarily, you should stick to the double type for real values. A variable of type char occupies two bytes in memory. The value of a char variable is a single character such s for 1.2 consi ridiculous-seemin say \"x = 1.2F;.",
  "page301": "All software problems can be termed as bugs. A software bug usually occurs when the software does not do what it is intended to do or does something that it is not intended to do. Flaws in specifications, design, code or other reasons can cause these bugs. Identifying and fixing\u00aebugs in the early stages of the software is very important as the cost of fixing bugs grows over time. So, the goal of a software tester is to find bugs and find them as early as possible and make sure they are fixed. Testing is context-based and risk-driven. It requires a methodical and disciplined approach to finding bugs. A good software tester needs to build credibility and possess the attitude to be explorative, troubleshooting, relentless, creative, diplomatic and persuasive. As agai\u00aenst the perception that testing starts only after the completion of coding phase, it actually begins even before the first line of code can be written. In the life cycle of the conventional software product, testing begins at the stage when the specifications are written, i.e. from testing the product specifica\u00aetions or product spec. Finding bugs at this stage can save huge amounts of time and money. Once the specifications are well understood, you are required to design and execute the test cases. Selecting the appropriate technique that reduces the number of tests that cover a feature is one of the most important\u00aethings that you need to take into consideration while designing these test cases. Test cases need to be designed to cover all aspects of the software, i.e. security, database, functionality (critical and general) and the user interface. Bugs originate when the test cases are executed. As a tester you might have to perform testing under different circumstances, i.e. the application could be in the initial stages or undergoing rapid changes, you have less than enough timeto test, the product might be developed using a life cycle model that does not support much of formal testing or retesting. Further, testing using different operating systems, browsers and the configurations are, keeping pace with the latest developments in the field will augment your career as a software test engineer.",
  "page302": "Software is a series of instructions for the computer that perform a particular task,called a program; the two major categories of software are system software andapplication software. System software is made up of control programs. Applicationsoftware is any program that process\u00aees data for the user (spreadsheet, wordprocessor, payroll, etc.).A software product should only be released after it has gone through a properprocess of development, testing and bug fixing. Testing looks at areas such asperformance, stability and error handling by setting up test scenarios undercontrolled conditions and assessing the results. This is why exactly any software hasto be tested. It is important to note that software is mainly tested to see that it meetsthe customers' need\u00aes and that it conforms to the standards. It is a usual norm thatsoftware is considered of good quality if it meets the user requirements.Quality can briefly be defined as \"a degree of excellence\". High quality softwareusually conforms to the user requirements. A customer's idea of quality may cov\u00aeer abreadth of features - conformance to specifications, good performance onplatform(s)/configurations, completely meets operationalrequirements (even if notspecified!), compatibility to all the end-user equipment, no negative impact onexisting end-user base at introduction time.Quality software saves good am\u00aeount of time and money. Because software will havefewer defects, this saves time during testing and maintenance phases. Greaterreliability contributes to an immeasurable increase in customer satisfaction as well aslower maintenance costs. Because maintenance represents a large portion of allsoftware costs, the overall cost of the project will most likely be lower than similarprojects.Following are two cases that demonstrate the importance of software quality:Apart from exposing faults (\"bugs\") in a software product confirming that theprogram meets the program specification, as a test engineer you need to create testcases, procedures, scripts and generate data. You execute test procedures angn flaws at an early stage beforefailures occur in production, or in the field Promote continual improvement",
  "page303": "As software engineering is now being considered as a technical engineeringprofession, it is important that the software test engineer's posses certain traits witha relentless attitude to make them stand out. Here are a few.Know the technology. Knowledge of the technology in\u00aewhich the application isdeveloped is an added advantage to any tester. It helps design better and powerfultest cases basing on the weakness or flaws of the technology. Good testers knowwhat it supports and what it doesn't, so concentrating on these lines will help thembreak the application quickly. Perfectionist and a realist. Being a perfectionist will help testers spot the problemand being a realist helps know at the end of the day which problems are reallyimportant problems. You wi\u00aell know which ones require a fix and which ones don't.Tactful, diplomatic and persuasive. Good software testers are tactful and knowhow to break the news to the developers. They are diplomatic while convincing thedevelopers of the bugs and persuade them when necessary and have their bug(s)fixed. It is impo\u00aertant to be critical of the issue and not let the person who developedthe application be taken aback of the findings. An explorer. A bit of creativity and an attitude to take risk helps the testersventure into unknown situations and find bugs that otherwise will be looked over.Troubleshoot. Troubleshooting an\u00aed figuring out why something doesn't workhelps testers be confident and clear in communicating the defects to the developers. Posses people skills and tenacity. Testers can face a lot of resistance fromprogrammers. Being socially smart and diplomatic doesn't mean being indecisive. Thebest testers are both-socially adept and tenacious where it matters.Organized. Best testers very well realize that they too can make mistakes anddon't take chances. They are very well organized and have checklists, use files, factsand figures to support their findings that can be used as an evidence and doublecheck their findings.Objective and accurate. They are very objective and know what they report and soconvey imat a later stage. Defects can cause serious problems if not managedproperly. Learning from defects helps - prevention of future problems, trackimprovements, improve prediction and estimation.",
  "page304": "Testing can't show that bugs don't exist. An important reason for testing is toprevent defects. You can perform your tests, find and report bugs, but at no point canyou guarantee that there are no bugs. It is impossible to test a program completely. Unfortunately this i\u00aes not possibleeven with the simplest program because - the number of inputs is very large, numberof outputs is very large, number of paths through the software is very large, and thespecification is subjective to frequent changes. You can't guarantee quality. As a software tester, you cannot test everything andare not responsible for the quality of the product. The main way that a tester can failis to fail to report accurately a defect you have observed. It is important to rememb\u00aeerthat we seldom have little control over quality. Target environment and intended end user. Anticipating and testing theapplication in the environment user is expected to use is one of the major factors thatshould be considered. Also, considering if the application is a single user system ormulti user system i\u00aes important for demonstrating the ability for immediate readinesswhen necessary. The error case of Disney's Lion King illustrates this. Disney Companyreleased its first multimedia CD-ROM game for children, The Lion King AnimatedStorybook. It was highly promoted and the sales were huge. Soon there were re\u00aeportsthat buyers were unable to get the software to work. It worked on a few systems -likelythe ones that the Disney programmers used to create the game - but not on themost common systems that the general public used.No application is 100% bug free. It is more reasonable to recognize there arepriorities, which may leave some less critical problems unsolved or unidentified.Simple case is the Intel Pentium bug. Enter the following equation into your PC'scalculator: (4195835 / 3145727) * 3145727 - 4195835. If the answer is zero, yourcomputer is just fine. If you get anything else, you have an old Intel Pentium CPU witha floating-point division bug.Be the customer. Try to",
  "page305": " Build your credibility. Credibility is like quality that includes reliability, knowledge,consistency, reputation, trust, attitude and attention to detail. It is not instant butshould be built over time and gives voice to the testers in the organization. Your keysto build credibi\u00aelity - identify your strengths and weaknesses, build good relations,demonstrate competency, and be willing to admit mistakes, re-assess and adjust. Test what you observe. It is very important that you test what you can observeand have access to. Writing creative test cases can help only when you have the opportunity to observe the results. So, assume nothing.Not all bugs you find will be fixed. Deciding which bugs will be fixed and whichwon't is a risk-based decision. Several rea\u00aesons why your bug might not be fixed iswhen there is no enough time, the bug is dismissed for a new feature, fixing it mightbe very risky or it may not be worth it because it occurs infrequently or has a workaround where the user can prevent or avoid the bug. Making a wrong decision can bedisastrous.Review comp\u00aeetitive products. Gaining a good insight into various products of thesame kind and getting to know their functionality and general behavior will help youdesign different test cases and to understand the strengths and weaknesses of yourapplication. This will also enable you to add value and suggest new feature\u00aes andenhancements to your product.Follow standards and processes. As a tester, your need to conform to thestandards and guidelines set by the organization. These standards pertain toreporting hierarchy, coding, documentation, testing, reporting bugs, using automatedtools etc. The software life cycle typically includes the following: requirements analysis, design,coding, testing, installation and maintenance. In between, there can be a requirementto provide Operations and support activities for the product.Requirements Analysis. Software organizations provide solutions to customerrequirements by developing appropriate software that best suits theirspecifications. Thus, the life of software starts with Select or Develop Algorithms (IfApplicable), Perform Detailed Design.",
  "page306": "Coding. The development process tends to run iteratively through these phasesrather than linearly; several models (spiral, waterfall etc.) have been proposed todescribe this process.Activities in this phase - Create Test Data, Create Source, Generate Object Code,Create Operating\u00aeDocumentation,Plan Integration, Perform IntegrationTesting. The process of using the developed system with the intent to find errors.Defects/flaws/bugs found at this stage will be sent back to the developer for a fixand have to be re-tested. This phase is iterative as long as the bugs are fixed to meetthe requirements.Activities in this phase - Plan Verification and Validation, Execute Verification andvalidation Tasks, Collect and Analyze Metric Data, Plan Testing, Develop TestRequirements\u00ae, Execute TestsInstallation. The so developed and tested software will finally need to be installed atthe client place. Careful planning has to be done to avoid problems to the user afterinstallation is done Activities in this phase - Plan Installation, Distribution of Software, Installation ofSoftware, Accept\u00aeSoftware in Operational Environment.Operation and Support. Support activities are usually performed by theorganization that developed the software. Both the parties usually decide on theseactivities before the system is developed.Activities in this phase - Operate the System, Provide Technical Assistance andC\u00aeonsulting, Maintain Support Request Log.Maintenance. The process does not stop once it is completely implemented andinstalled at user place; this phase undertakes development of new features,enhancements etc.Activities in this phase - Reapplying Software Life Cycle.The way you approach a particular application for testing greatly depends on the lifecycle model it follows. This is because, each life cycle model places emphasis ondifferent aspects of the software i.e. certain models provide good scope and time fortesting whereas some others don't. So, the number of test cases developed, featurescovered, time spent on each issue depends on the life cycle model the applicationfollows.No matter",
  "page307": "Analysis. Involves activities that - develop functional validation based on BusinessRequirements (writing test cases basing on these details), develop test case format(time estimates and priority assignments), develop test cycles (matrices andtimelines), identify test cases to be\u00aeautomated (if applicable), define area of stressand performance testing, plan the test cycles required for the project and regressiontesting, define procedures for data maintenance (backup, restore, validation), reviewdocumentation.Design. Activities in the design phase - Revise test plan based on changes, revise testcycle matrices and timelines, verify that test plan and cases are in a database orrequisite, continue to write test cases and add new ones based on changes, developRisk Asses\u00aesment Criteria, formalize details for Stress and Performance testing, finalizetest cycles (number of test case per cycle based on time estimates pertest case andpriority), finalize the Test Plan, (estimate resources to support development in unittesting).Construction (Unit Testing Phase). Complete all plans, co\u00aemplete Test Cycle matricesand timelines, complete all test cases (manual), begin Stress and Performance testing,test the automated testing system and fix bugs, (support development in unittesting), run QA acceptance test suite to certify software is ready to turn over to QA.Test Cycle(s) / Bug Fixes (Re-Testi\u00aeng/System Testing Phase). Run the test cases (frontand back end), bug reporting, verification, and revise/add test cases as required.Final Testing and Implementation (Code Freeze Phase). Execution of all front end testcases - manual and automated, execution of all back end test cases - manual andautomated, execute all Stress and Performance tests, provide on-going defecttracking metrics, provide on-going complexity and design metrics, update estimatesfor test cases and test plans, document test cycles, regression testing, and updateaccordingly.Post Implementation. Post implementation evaluation meeting can be conducted toreview entire project. Activities in this phase - Prepare final Defeviation from standards etc.",
  "page308": "Constantly changing software requirements cause a lot of confusion and pressureboth on the development and testing teams. Often, a new feature added or existingfeature removed can be linked to the other modules or components in the software.Overlooking such issues causes bugs. Al\u00aeso, fixing a bug in one part/component of the software might arise another in adifferent or same component. Lack of foresight in anticipating such issues can causeserious problems and increase in bug count. This is one of the major issues because ofwhich bugs occur since developers are very often subject to pressure related totimelines; frequently changing requirements, increase in the number of bugs etc. Designing and re-designing, UI interfaces, integration of modules, databasemanagement\u00aeall these add to the complexity of the software and the system as awhole. Fundamental problems with software design and architecture can cause problemsin programming. Developed software is prone to error as programmers can makemistakestoo. As a tester you can check for, data reference/declaration errors, contr\u00aeolflow errors, parameter errors, input/output errors etc. Rescheduling of resources, re-doing or discarding already completed work,changes in hardware/software requirements can affect the software too. Assigning anew developer to the project in midway can cause bugs. This is possible if propercoding standards\u00aehave not been followed, improper code documentation, ineffectiveknowledge transfer etc. Discarding a portion of the existing code might just leave itstrail behind in other parts of the software; overlooking or not eliminating such codecan cause bugs. Serious bugs can especially occur with larger projects, as it getstougher to identify the problem area. Programmers usually tend to rush as the deadline approaches closer. This is thetime when most of the bugs occur. It is possible that you will be able to spot bugs ofall types and severity.Complexity in keeping track of all the bugs can again cause bugs by itself. This getsharder when a bug has a very complex life cycle i.e. when the number of tieveloperfor a fix. The developer thenaccepts if valid.",
  "page309": "Not Accepted/Won't fix: If the developer considers the bug as low level or does notaccept it as a bug, thus pushing it into Not Accepted/Won't fix state.Such bugs will be assigned to the project manager who will decide if the bug needs afix. If it needs, then assigns it\u00aeback to the developer, and if it doesn't, then assigns itback to the tester who will have to close the bug.Pending: A bug accepted by the developer may not be fixed immediately. In suchcases, it can be put under Pending state.Fixed: Programmer will fix the bug and resolves it as Fixed.Close: The fixed bug will be assigned to the tester who will put it in the Close state.Re-Open: Fixed bugs can be re-opened by the testers in case the fix producesproblems elsewhere.Costs are logarithmi\u00aec; they increase in size tenfold as the time increases. A bug foundand fixed during the early stages - requirements or product spec stage can be fixed bya brief interaction with the concerned and might cost next to nothing.During coding, a swiftly spotted mistake may take only very less effort to fix. Duri\u00aengintegration testing, it costs the paperwork of a bug report and a formally documentedfix, as well as the delay and expense of a re-test.During system testing it costs even more time andmay delay delivery. Finally, duringoperations it may cause anything from a nuisance to a system failure, possibly withcatas\u00aetrophic consequences in a safety-critical system such as an aircraft or anemergency service.It is difficult to determine when exactly to stop testing. Here are a few commonfactors that help you decide when you can stop or reduce testing: Deadlines (release deadlines, testing deadlines, etc.) Test cases completed with certain percentage passed Test budget depleted Coverage of code/functionality/requirements reaches a specified point Bug rate falls below a certain level Beta or alpha testing period ends There are basically three levels of testing i.e. Unit Testing, Integration Testing andSystem Testing. Various types of testing come under these levels.Unit TestingTo verify a single program or a section of a single programIntegration TestingTo verify interaction between system componentsPrerequisite:unit testing completed on all components that compose a system.System TestingTo verify and validate behaviors of the entire system against the original systemobjectives.",
  "page310": "Software testing is a process that identifies the correctness, completeness, andquality of software.Following is a list of various types of software testing and their definitions in arandom order: Formal Testing: Performed by test engineers Informal Testing: Performed by the deve\u00aelopers Manual Testing: That part of software testing that requires human input, analysis,or evaluation. Automated Testing: Software testing that utilizes a variety of tools to automatethe testing process. Automated testing still requires a skilled quality assuranceprofessional with knowledge of the automation tools and the software being testedto set up the test cases. Black box Testing: Testing software without any knowledge of the back-end of thesystem, structure or language of the modul\u00aee being tested. Black box test cases arewritten from a definitive source document, such as a specification or requirementsdocument. White box Testing: Testing in which the software tester has knowledge of theback-end, structure and language of the software, or at least its purpose. Unit Testing: Unit testing is\u00aethe process of testing a particular complied program,i.e., a window, a report, an interface, etc. independently as a stand-alonecomponent/program. The types and degrees of unit tests can vary among modifiedand newly created programs.Unit testing is mostly performed by the programmerswho are also responsible\u00aefor the creation of the necessary unit test data. Incremental Testing: Incremental testing is partial testing of an incompleteproduct. The goal of incremental testing is to provide an early feedback to softwaredevelopers. System Testing: System testing is a form of black box testing. The purpose ofsystem testing is to validate an application's accuracy and completeness inperforming the functions as designed.Integration Testing: Testing two or more modules or functions together with theintent of finding interface defects between the modules/functions. System Integration Testing: Testing of software components that have beendistributed across multiple platforms (e.g., client, web server, application server, anddatabase server) to produce failures caused by system integration defects (i.e. defectsinvolving distribution and back- Functional Testing: Verifying that a module functions as stated in the specificationand establishing confidence that a program does what it is supposed to do.",
  "page311": "Acceptance Testing: Testing the system with the intent of confirming readiness ofthe product and customer acceptance. Also known as User Acceptance Testing. Adhoc Testing: Testing without a formal test plan or outside of atest plan. Withsome projects this type of testing is carri\u00aeed out as an addition to formal testing.Sometimes, if testing occurs very late in the development cycle, this will be the onlykind of testing that can be performed - usually done by skilled testers. Sometimes adhoc testing is referred to as exploratory testing. Configuration Testing: Testing to determine how well the product works with abroad range of hardware/peripheral equipment configurations as well as on differentoperating systems and software. Load Testing: Testing with the inte\u00aent of determining how well the product handlescompetition for system resources. The competition may come in the form of networktraffic, CPU utilization or memory allocation. Stress Testing: Testing done to evaluate the behavior when the system is pushedbeyond the breaking point. The goal is to expose the weak l\u00aeinks and to determine ifthe system manages to recover gracefully. Performance Testing: Testing with the intent of determining how efficiently aproduct handles a variety of events. Automated test tools geared specifically to testand fine-tune performance are used most often for this type of testing. Usability\u00aeTesting: Usability testing is testing for 'user-friendliness'. A way toevaluate and measure how users interact with a software product or site. Tasks aregiven to users and observations are made. Installation Testing: Testing with the intent of determining if the product iscompatible with a variety of platforms and how easily it installs. Recovery/Error Testing: Testing how well a system recovers from crashes,hardware failures, or other catastrophic problems. Security Testing: Testing of database and network software in order to keepcompany data and resources secure from mistaken/accidental users, hackers, andother malevolent attackers. Penetration Testing: Penetration testing is testing how well the system isprotected against unauthorized internal or external access, or willful damage. Thistype of testing usually requires sophisticated testing techniques.Compatibility Testing: Testing used to determine whether other system softwarecomponents such as browsers, utilities,",
  "page312": "Following are the most common software errors that aid you in software testing. Thishelps you to identify errors systematically and increases the efficiency andproductivity of software testing.User Interface Errors: Missing/Wrong Functions Doesn't do what the user expects,Mi\u00aessing information, Misleading, Confusing information, Wrong content in Help text,Inappropriate error messages. Performance issues - Poor responsiveness, Can'tredirect output, Inappropriate use of key board Error Handling: Inadequate - protection against corrupted data, tests of userinput, version control; Ignores - overflow, data comparison, Error recovery - abortingerrors, recovery from hardware problems. Boundary related errors: Boundaries in loop, space, time, memory, mishandl\u00aeing ofcases outside boundary. Calculation errors: Bad Logic, Bad Arithmetic, Outdated constants, Calculationerrors, incorrect conversion from one data representation to another, Wrongformula, incorrect approximation. Initial and Later states: Failure to - set data item to zero, to initialize a loopcontrol varia\u00aeble, or re-initialize a pointer, to clear a string or flag, Incorrectinitialization. Control flow errors: Wrong returning state assumed, Exception handling basedexits, Stack underflow/overflow, Failure to block or un-block interrupts, Comparisonsometimes yields wrong result, Missing/wrong default, and Data Ty\u00aepe errors. Errors in Handling or Interpreting Data: Un-terminated null strings, overwriting afile after an error exit or user abort. Race Conditions: Assumption that one event or task finished before anotherbegins, Resource races, Tasks starts before its prerequisites are met, Messages crosor don't arrive in the order sent.Load Conditions: Required resources are not available, No available large memoryarea, Low priority tasks not put off, doesn't erase old files from mass storage, anddoesn't return unused memory.Hardware: Wrong Device, Device unavailable, Underutilizing device intelligence,Misunderstood status or return code, Wrong operation or instruction codes. Source, Version and ID Control: No Title or version ID, Failure to updatemultiplecopies of data or program files.Testing Errors: Failure to notice/report a problem, Failure to use the mostpromising test case, Corrupted data files, Misinterpreted specifications ordocumentation, Failure to",
  "page313": "Test Policy - A document characterizing the organization's philosophy towardssoftware testing.Test Strategy - A high-level document defining the test phases to be performed andthe testing within those phases for a program. It defines the process to be followed ineach project\u00ae. This sets the standards for the processes, documents, activities etc. thatshould be followed for each project.For example, if a product is given for testing, you should decide if it is better to useblack-box testing or white-box testing and if you decide to use both, when will youapply each and to which part of the software? All these details need to be specified inthe Test Strategy.Project Test Plan - a document defining the test phases to be performed and thetesting within those phases\u00aefor a particular project.A Test Strategy should cover more than one project and should address the followingissues: An approach to testing high risk areas first, Planning for testing, How toimprove the process based on previous testing, Environments/data used, Testmanagement - Configuration management, Problem\u00aemanagement, What Metrics arefollowed, Will the tests be automated and if so which tools will be used, What are theTesting Stages and Testing Methods, Post Testing Review process, Templates.Test planning needs to start as soon as the project requirements are known. The firstdocument that needs to be produced\u00aethen is the Test Strategy/Testing Approach thatsets the high level approach for testing and covers all the other elements mentionedabove.Once the approach is understood, a detailed test plan can be written. Usually, thistest plan can be written in different styles. Test plans can completely differ fromproject to project in the same organization.PurposeTo describe the scope, approach, resources, and schedule of the testing activities. Toidentify the items being tested, the features to be tested, the testing tasks to beperformed, the personnel responsible for each task, and the risks associated with thisplan.OUTLINEA test plan shall have the following structure: Test plan identifier. A unique identifier assign to the test plan. Introduction: Summarized the software items and features to be tested andthe need for them to be included. Test items: Identify the test items, their transmittalmedia which impact their Features to be tested Features not to be testedApproachItem pass/fail",
  "page314": "Like any other process in software testing, the major tasks in test planning are to -Develop Test Strategy, Critical Success Factors, Define Test Objectives, IdentifyNeeded Test Resources, Plan Test Environment, Define Test Procedures, IdentifyFunctions To Be Tested, Identif\u00aey Interfaces With Other Systems or Components, WriteTest Scripts, Define Test Cases, Design Test Data, Build Test Matrix, Determine TestSchedules, Assemble Information, Finalize the Plan.A test case is a detailed procedure that fully tests a feature or an aspect of a feature.While the test plan describes what to test, a test case describes how to perform aparticular test. You need to develop test cases for each test listed in the test planAs a tester, the best way to determine the complian\u00aece of the software torequirements is by designing effective test cases that provide a thorough test of aunit. Various test case design techniques enable the testers to develop effective testcases. Besides, implementing the design techniques, every tester needs to keep inmind general guidelines that will aid in\u00aetest case design:a. The purpose of each test case is to run the test in the simplest way possible.[Suitable techniques - Specification derived tests, Equivalence partitioning]b. Concentrate initially on positive testing i.e. the test case should show that thesoftware does what it is intended to do. [Suitable\u00aetechniques - Specification derivedtests, Equivalence partitioning, State-transition testing]c. Existing test casesshould be enhanced and further test cases should be designedto show that the software does not do anything that it is not specified to do i.e.Negative Testing [Suitable techniques - Error guessing, Boundary value analysis,Internal boundary value testing, State-transition testing]d. Where appropriate, test cases should be designed to address issues such asperformance, safety requirements and security requirements [Suitable techniques -Specification derived tests]e. Further test cases can then be added to the unit test specification to achievespecific test coverage objectives. Once coverage tests have been designed, the testprocedure can be developed and the tests executed [Suitable techniques - Branchtesting, Condition testing, Data definition-use testing, State-transition testing]The manner in which a test case is depicted varies between organizations. Anyho categories:",
  "page315": "For example, if a program accepts integer values only from 1 to 10. The possible testcases for such a program would be the range of all integers. In such a program, allintegers up to 0 and above 10 will cause an error. So, it is reasonable to assume that if11 will fail, all value\u00aes above it will fail and vice versa.If an input condition is a range of values, let one valid equivalence class is the range (0or 10 in this example). Let the values below and above the range be two respectiveinvalid equivalence values (i.e. -1 and 11). Therefore, the above three partition valuescan be used as test cases for the above example.Boundary Value AnalysisThis is a selection technique where the test data are chosen to lie along theboundaries of the input domain or the output rang\u00aee. This technique is often called asstress testing and incorporates a degree of negative testing in the test design byanticipating that errors will occur at or around the partition boundaries.For example, a field is required to accept amounts of money between $0 and $10. Asa tester, you need to check if it mean\u00aes up to and including $10 and $9.99 and if $10 isacceptable. So, the boundary values are $0, $0.01, $9.99 and $10.Now, the following tests can be executed. A negative value should be rejected, 0should be accepted (this is on the boundary), $0.01 and $9.99 should be accepted,null and $10 should be rejected. In\u00aethis way, it uses the same concept of partitions asequivalence partitioning.State Transition TestingAs the name suggests, test cases are designed to test the transition between thestates by creating the events that cause the transition.Branch TestingIn branch testing, test cases are designed to exercise control flow branches ordecision points in a unit. This is usually aimed at achieving a target level of DecisionCoverage. Branch Coverage, need to test both branches of IF and ELSE. All branchesand compound conditions (e.g. loops and array handling) within the branch should beexercised at least once.Condition TestingThe object of condition testing is to design test cases to show that the individualcomponents of logical conditions and combinations of the individual components arecorrect. Test cases are designed to test the individual elements of logical expressions,both within branch conditions and within other expressions in a unit.Data Definition set. Data use isanywhere that a data item is read or used.",
  "page316": "Internal Boundary Value TestingIn many cases, partitions and their boundaries can be identified from a functionalspecification for a unit, as described under equivalence partitioning and boundaryvalue analysis above. However, a unit may also have internal boundary values thatcan\u00aeonly be identified from a structural specification.Error GuessingIt is a test case design technique where the testers use their experience to guess thepossible errors that might occur and design test cases accordingly to uncover them.Using any or a combination of the above described test case design techniques; youcan develop effective test cases.A use case describes the system's behavior under various conditions as it responds toa request from one of the users. The user initiates an\u00aeinteraction with the system toaccomplish some goal. Different sequences of behavior, or scenarios, can unfold,depending on the particular requests made and conditions surrounding the requests.The use case collects together those different scenarios.Use cases are popular largely because they tell coherent storie\u00aes about how thesystem will behave in use. The users of the system get to see just what this newsystem will be and get to react early.As discussed earlier, defect is the variance from a desired product attribute (it can bea wrong, missing or extra data). It can be of two types - Defect from theproduct or\u00aeavariance from customer/user expectations. It is a flaw in the software system and hasno impact until it affects the user/customer and operational system.With the knowledge of testing so far gained, you can now be able to categorize thedefects you have found. Defects can be categorized into different types basing on thecore issues they address. Some defects address security or database issues whileothers may refer to functionality or UI issues.Security Defects: Application security defects generally involve improper handling ofdata sent from the user to the application. These defects are the most severe andgiven highest priority for a fix.Examples:Authentication: Accepting an invalid username/password Authorization: Accessibility to pages though permission not givenData Quality/Database Defects: Deals with improper handling of data in thedatabase.Examples: Values not deleted/inserted into the database properly Improper/wrong/null values inserted in place of the actual valuesCriticaaffect the functionality of the application.Examples:",
  "page317": "User Interface Defects: As the name suggests, the bugs deal with problems related toUI are usually considered less severe.Examples:Improper error/warning/UI messages Spelling mistakes Alignment problemsOnce the test cases are developed using the appropriate techniques, they areex\u00aeecuted which is when the bugs occur. It is very important that these bugs bereported as soon as possible because, the earlier you report a bug, the more timeremains in the schedule to get it fixed.Simple example is that you report a wrong functionality documented in the Help file afew months before the product release, the chances that it will be fixed are very high.If you report the same bug few hours before the release, the odds are that it won't befixed. The bug is still the same t\u00aehough you report it few months or few hours beforethe release, but what matters is the time.It is not just enough to find the bugs; these should also be reported/communicatedclearly and efficiently, not to mention the number of people who will be reading thedefect.Defect tracking tools (also known as bug tracki\u00aeng tools, issue tracking tools orproblem trackers) greatly aid the testers in reporting and tracking the bugsfound in software applications. They provide a means of consolidating a keyelement of project information in one place. Project managers can then seewhich bugs have been fixed, which are outstanding an\u00aed how long it is taking tofix defects. Senior management can use reports to understand the state of thedevelopment process.You should provide enough detail while reporting the bug keeping in mind the peoplewho will use it - test lead, developer, project manager, other testers, new testersassigned etc. This means that thereport you will write should be concise, straight andclear. Following are the details your report should contain:Bug Title Bug identifier (number, ID, etc.) The application name or identifier and version The function, module, feature, object, screen, etc. where the bug occurredEnvironment (OS, Browser and its version)-Bug Type or Category/Severity/Priorityo Bug Category: Security, Database, Functionality (Critical/General), UIo Bug Severity: Severity with which the bug affects the application - Very High,High, Medium, Low, Very Lowo Bug Priority: Recommended priority to be given for a fix of this bug - P0, P1,P2, P3, P4, P5 (P0 Comments",
  "page318": "Once the reported defect is fixed, the tester needs to re-test to confirm the fix. This isusually done by executing the possible scenarios where the bug can occur. Onceretesting is completed, the fix can be confirmed and the bug can be closed. Thismarks the end of the bug life cy\u00aecle.The documents outlined in the IEEE Standard of Software Test Documentation coverstest planning, test specification, and test reporting.Test reporting covers four document types: A Test Item Transmittal Report identifies the test items being transmitted fortesting from the development to the testing group in the event that a formalbeginning of test execution is desiredDetails to be included in the report - Purpose, Outline, Transmittal-Report Identifier,Transmitted Items, Location, Stat\u00aeus, and Approvals.. A Test Log is used by the test team to record what occurred during test executionDetails to be included in the report - Purpose, Outline, Test-Log Identifier,Description, Activity and Event Entries, Execution Description, Procedure Results,Environmental Information, Anomalous Events, Inciden\u00aet-Report Identifiers A Test Incident report describes any event that occurs during the test executionthat requires further investigationDetails to be included in the report - Purpose, Outline, Test-Incident-Report Identifier,Summary, Impact A test summary report summarizes the testing activities associated wi\u00aeth one ormore test-design specificationsDetails to be included in the report - Purpose, Outline, Test-Summary-ReportIdentifier, Summary, Variances, Comprehensiveness Assessment, Summary of Results,Summary of Activities, and Approvals.Automating testing is no different from a programmer using a coding language towrite programs to automate any manual process. One of the problems with testinglarge systems is that it can go beyond the scope of small test teams. Because only asmall number of testers are available the coverage and depth of testing provided areinadequate for the task at hand.Expanding the test team beyond a certain size also becomes problematic withincrease in work over head. Feasible way to avoid this without introducing a loss ofquality is through appropriate use of tools that can expand individual's capacityenormously while maintaining the focus (depth) of testing upon the critical elements.Consider the following factors that help determine to make changes in the current ways you perform testing.",
  "page319": "Fully manual testing has the benefit of being relatively cheap and effective. But asquality of the product improves the additional cost for finding further bugs becomesmore expensive. Large scale manual testing also implies large scale testing teams withthe related costs of space\u00aeoverhead and infrastructure. Manual testing is also farmore responsive and flexible than automated testing but is prone to tester errorthrough fatigue.Fully automated testing is very consistent and allows the repetitions of similar testsat very little marginal cost. The setup and purchase costs of such automation are veryhigh however and maintenance can be equally expensive. Automation is alsorelatively inflexible and requires rework in order to adapt to changing requirements.Partial Auto\u00aemation incorporates automation only where the most benefits can beachieved. The advantage is that it targets specifically the tasks for automation andthus achieves the most benefit from them. It also retains a large component ofmanual testing which maintains the test team's flexibility and offers redundanc\u00aey bybacking up automation with manual testing. The disadvantage is that it obviouslydoes not provide as extensive benefits as either extreme solution.Take time to define thetool requirements in terms of technology, process,applications, people skills, and organization.During tool evaluation, prioritize which\u00aetest typesare the most critical to yoursuccess and judge the candidate tools on those criteria. Understand the tools and their trade-offs. You may need to use a multi-tool solutionto get higher levels of test-type coverage. For example, you will need to combinethe capture/play-back tool with a load-test tool to cover your performance testcases. Involve potential users in the definition of tool requirements and evaluation criteria. Build an evaluation scorecard to compare each tool's performance against acommon set of criteria. Rank the criteria in terms of relative importance to theorganization. Buying the Wrong Tool Inadequate Test Team Organization Lack of Management Support Incomplete Coverage of Test Types by the selected tool Inadequate Tool Training Difficulty using the tool Lack of a Basic Test Process or Understanding of What to Test Lack of Configuration Management Processes Lack of Tool Compatibility and InteroperabilityLack of Tool Availability",
  "page320": " oversight, Software subcontractmanagement, Software quality assurance, Software configuration managementLevel 3 - Defined: Key practice areas - Organization process focus, Organizationprocess definition, Training program, integrated software management, Softwareproduct engi\u00aeneering, intergroup coordination, Peer reviewsLevel 4 - Manageable: Key practice areas - Quantitative Process Management,Software Quality ManagementLevel 5 - Optimizing: Key practice areas - Defect prevention, Technology changemanagement, Process change managementSix Sigma is a quality management program to achieve \"six sigma\" levels of quality. Itwas pioneered by Motorola in the mid-1980s and has spread too many othermanufacturing companies, notably General Electric Corporatio\u00aen (GE).Six Sigma is a rigorous and disciplined methodology that uses data and statisticalanalysis to measure and improve a company's operational performance by identifyingand eliminating \"defects\" from manufacturing to transactional and from product toservice. Commonly defined as 3.4 defects per million oppor\u00aetunities, Six Sigma can bedefined and understood at three distinct levels: metric, methodology andphilosophy.Training Sigma processes are executed by Six Sigma Green Belts and Six Sigma BlackBelts, andare overseen by Six Sigma Master Black BeltsISO - International Organization for Standardization is a network\u00aeof the nationalstandards institutes of 150 countries, on the basis of one member per country, with aCentral Secretariat in Geneva, Switzerland, that coordinates the system. ISO is a nongovernmental organization. ISO has developed over 13, 000 International Standardson a variety of subjects.",
  "page321": "Capability Maturity git Model - Developed by the software community in 1986 withleadership from the SEI. The CMM describes the principles and practices underlyingsoftware process maturity. It is intended to help software organizations improve thematurity oftheir software processe\u00aes in terms of an evolutionary path from ad hoc,chaotic processes to mature, disciplined software processes. The focus is onidentifying key process areas and the exemplary practices that may comprise adisciplined software process.What makes up the CMM? The CMM is organized into five maturity levels: Initial Repeatable Defined ManageableOptimizingExcept for Level 1, each maturity level decomposes into several key process areasthat indicate the areas an organization should focus on to improve\u00aeits softwareprocess.Level 1 - Initial Level: Disciplined process, Standard, Consistent process, Predictableprocess, Continuously Improving processLevel 2 - Repeatable: Key practice areas - Requirements management, Softwareproject planning, Software project tracking ",
  "page322": "Following are some facts that can help you gain a better insight into the realities ofSoftware Engineering.1. The best programmers are up to 28 times better than the worst programmers.2. New tools/techniques cause an initial LOSS of productivity/quality.3. The answer to a feasibi\u00aelity study is almost always \"yes\".4. A May 2002 report prepared for the National Institute of Standards andTechnologies (NIST)(1) estimate the annual cost of software defects in the UnitedStates as $59.5 billion.5. Reusable components are three times as hard to build6.For every 25% increase in problem complexity, there is a 100% increase in solutioncomplexity.7. 80% of software work is intellectual. A fair amount of it is creative. Little of it isclerical.8. Requirements errors a\u00aere the most expensive to fix during production.9. Missing requirements are the hardest requirement errors to correct.10. Error-removal is the most time-consuming phase of the life cycle.11. Software is usually tested at best at the 55-60% (branch) coverage level.12. 100% coverage is still far from enough.13. Ri\u00aegorous inspections can remove up to 90% of errors before the first test case isrun.15. Maintenance typically consumes 40-80% of software costs. It is probably the mostimportant life cycle phase of software.16. Enhancements represent roughly 60% of maintenance costs.17. There is no single best approach to soft\u00aeware error removal.",
  "page323": "Testing plays an important role in achieving and assessing the quality of a softwareproduct [25]. On the one hand, we improve the quality of the products as we repeata test-find defects-fix cycle during development. On the other hand, we assess howgood our system is whe\u00aen we perform system-level tests before releasing a product.Thus, as Friedman and Voas [26] have succinctly described, software testing is averification process for software quality assessment and improvement. Generallyspeaking, the activities for software quality assessment can be divided into twobroad categories, namely, static analysis and dynamic analysis. Static Analysis: As the term \"static\" suggests, it is based on the examination of a number of documents, namely requiremen\u00aets documents, softwaremodels, design documents, and source code. Traditional static analysisincludes code review, inspection, walk-through, algorithm analysis, andproof of correctness. It does not involve actual execution of the code underdevelopment. Instead, it examines code and reasons over all possible beha\u00aeviors that might arise during run time. Compiler optimizations are standardstatic analysis.Dynamic Analysis: Dynamic analysis of a software system involves actualprogram execution in order to expose possible program failures. The behavioral and performance properties of the program are also observed. Programs\u00aeare executed with both typical and carefully chosen input values.Often, the input set of a program can be impractically large. However, forpractical considerations, a finite subset of the input set can be selected.Therefore, in testing, we observe some representative program behaviorsand reach a conclusion about the quality of the system. Careful selectionof a finite test set is crucial to reaching a reliable conclusion.",
  "page324": "By performing static and dynamic analyses, practitioners want to identify as manyfaults as possible so that those faults are fixed at an early stage of the softwaredevelopment. Static analysisand dynamic analysis are complementary in nature,and for better effectiveness, both must\u00aebe performed repeatedly and alternated.Practitioners and researchers need to remove the boundaries between static anddynamic analysis and create a hybrid analysis that combines the strengths of bothapproachesTwo similar concepts related to software testing frequently used by practitioners areverification and validation. Both concepts are abstract in nature, and each can berealized by a set of concrete, executable activities. The two concepts are explainedas follows: Verification: This kin\u00aed of activity helps us in evaluating a software systemby determining whether the product of a given development phase satisfiesthe requirements established before the start of that phase. One may notethat a product can be an intermediate product, such as requirement specification, design specification, code, us\u00aeer manual, or even the final product.Activities that check the correctness of a development phase are calledverification activities. Validation: Activities of this kind help us in confirming that a productmeets its intended use. Validation activities aim at confirming that a productmeets its customer's e\u00aexpectations. In other words, validation activities focuson the final product, which is extensively tested from the customer point ofview. Validation establishes whether the product meets overall expectationsof the users.Late execution of validation activities is often risky by leading tohigher development cost. Validation activities may be executed at earlystages of the software development cycle [28]. An example of early execution of validation activities can be found in the eXtreme Programming",
  "page325": "(XP) software development methodology. In the XP methodology, the customer closely interacts with the software development group and conductsacceptance tests during each development iteration [29].The verification process establishes the correspondence of an implementationphase o\u00aef the software development process with its specification, whereas validationestablishes the correspondence between a system and users'expectations. One cancompare verification and validation as follows: Verification activities aim at confirming that one is building the product correctly, whereas validation activities aim at confirming that one is buildingthe correct product [30]. Verification activities review interim work products, such as requirementsspecification, design, code, an\u00aed user manual, during a project life cycle toensure their quality. The quality attributes sought by verification activitiesare consistency, completeness, and correctness at each major stage of system development. On the other hand, validation is performed toward theend of system development to determine if the\u00aeentire system meets thecustomer's needs and expectations. Verification activities are performed on interim products by applying mostlystatic analysis techniques, such as inspection, walkthrough, andreviews,and using standards and checklists. Verification can also include dynamicanalysis, such as actual p\u00aerogram execution. On the other hand, validationis performed on the entire system by actually running the system in its realenvironment and using a variety of testsIn the literature on software testing, one can find references to the terms failure,error, fault, and defect. Although their meanings are related, there are importantdistinctions between these four concepts. In the following, we present first threeterms as they are understood in the fault-tolerant computing community",
  "page326": "Failure: A failure is said to occur whenever the external behavior of asystem does not conform to that prescribed in the system specification. Error: An error is a state of the system. In the absence of any correctiveaction by the system, an error state could lead to a fail\u00aeure which wouldnot be attributed to any event subsequent to the error. Fault: A fault is the adjudged cause of an error.A fault may remain undetected for a long time, until some event activates it. Whenan event activates a fault, it first brings the program into an intermediate error state.If computation is allowed to proceed from an error state without any correctiveaction, the program eventually causes a failure. As an aside, in fault-tolerant computing, corrective actions can be t\u00aeaken to take a program out ofan error state intoa desirable state such that subsequent computation does not eventually lead to afailure. The process of failure manifestation can therefore be succinctly representedas a behavior chain [31] as follows: faulterrorfailure. The behavior chaincan iterate f\u00aeor a while, that is, failure of one component can lead to a failure ofanother interacting component.The above definition of failure assumes that the given specification is acceptable to the customer. However, if the specification does not meet the expectationsof the customer, then, of course, even a fault-fre\u00aee implementation fails to satisfy thecustomer. It is a difficult task to give a precise definition of fault, error, or failureof software, because of the \"human factor\" involved in the overall acceptance of asystem. In an article titled \"What Is Software Failure\" [32], Ram Chillarege commented that in modern software business software failure means \"the customer'sexpectation has not been met and/or the customer is unable to do useful work withproduct,\" p. 354.Roderick Rees [33] extended Chillarege's comments of software failure bypointing out that \"failure is a matter of function only [and is thus] related to purpose,not to whether an item is physically intact or not\" (p. 163). To substantiate this,Behrooz Parhami [34] provided three interesting examples to show the relevanceof such a view point in wider context. One of the examples is quoted here",
  "page327": "The above definition of failure assumes that the given specification is acceptable to the customer. However, if the specification does not meet the expectationsof the customer, then, of course, even a fault-free implementation fails to satisfy thecustomer. It is a difficult task\u00aeto give a precise definition of fault, error, or failureof software, because of the \"human factor\" involved in the overall acceptance of asystem. In an article titled \"What Is Software Failure\" [32], Ram Chillarege commented that in modern software business software failure means \"the customer'sexpectation has not been met and/or the customer is unable to do useful work withproduct. As an aside, in fault-tolerant computing, corrective actions can be taken to\u00aetake a program out ofan error state intoa desirable state such that subsequent computation does not eventually lead to afailure. The process of failure manifestation can therefore be succinctly representedas a behavior chain [31] as follows: faulterrorfailure. The behavior chaincan iterate for a whi\u00aele, that is, failure of one component can lead to a failure ofanother interacting component.The above definition of failure assumes that the given specification is acceptable to the customer. However, if the specification does not meet the expectationsof the customer, then, of course, even a fault-free implem\u00aeentation fails to satisfy thecustomer. It is a difficult task to give a precise definition of fault, error, or failureof software, because of the \"human factor\" involved in the overall acceptance of asystem. In an article titled \"What Is Software Failure\" [32], Ram Chillarege commented that in modern software business software failure means \"the customer'sexpectation has not been met and/or the customer is unable to do useful work withproduct,\" p. 354.Roderick Rees [33] extended Chillarege's comments of software failure bypointing out that \"failure is a matter of function only [and is thus] related to purpose,not to whether an item is physically intact or not\" (p. 163). To substantiate this,Behrooz Parhami [34] provided three interesting examples to show the relevanceof such a view point in wider context. One of the examples is quoted here",
  "page328": " dissatisfactions are errors in the organization's state. The organization's personnel or departmentsprobably begin to malfunction as result of the errors, in turn causing an overall degradation of performance. The end result can be the organization's failure to ac\u00aehieveits goal.There is a fine difference between defects and faults in the above example, thatis, execution of a defective policy may lead to a faulty promotion. In a softwarecontext, a software system may be defective due to design issues; certain systemstates will expose a defect, resulting in the development of faults defined as incorrect signal values or decisions within the system. In industry, the term defect iswidely used, whereas among researchers the term fault is more prevalent.\u00aeFor allpractical purpose, the two terms are synonymous. In this book, we use the twoterms interchangeably as required. An approach to testing high risk areas first, Planning for testing, How toimprove the process based on previous testing, Environments/data used, Testmanagement - Configuration management, Probl\u00aeem management, What Metrics arefollowed, Will the tests be automated and if so which tools will be used, What are theTesting Stages and Testing Methods, Post Testing Review process, Templates.Test planning needs to start as soon as the project requirements are known. The firstdocument that needs to be produce\u00aed then is the Test Strategy/Testing Approach thatsets the high level approach for testing and covers all the other elements mentionedabove.Once the approach is understood, a detailed test plan can be written. Usually, thistest plan can be written in different styles. Test plans can completely differ fromproject to project in the same organization.PurposeTo describe the scope, approach, resources, and schedule of the testing activities. Toidentify the items being tested, the features to be tested, the testing tasks to beperformed, the personnel responsible for each task, and the risks associated with thisplan.OUTLINEA test plan shall have the following structure: Test plan identifier. A unique identifier assign to the test plan. Introduction: Summarized the software items and features to be tested andthe need for them to be included. Test items: Identify the test items, their transmittalmedia which impact their Features to be tested Features not to be testedApproachItem pass/fail",
  "page329": "Roderick Rees [33] extended Chillarege's comments of software failure bypointing out that \"failure is a matter of function only [and is thus] related to purpose,not to whether an item is physically intact or not\" (p. 163). To substantiate this,Behrooz Parhami [34]\u00aeprovided three interesting examples to show the relevanceof such a view point in wider context. One of the examples is quoted here (p. 451):Consider a small organization. Defects in the organization's staffpromotion policies cancause improper promotions, viewed as faults. The resulting ineptitudes Static Analysis: As the term \"static\" suggests, it is based on the examination of a number of documents, namely requirements documents, softwaremodels, design documents, and sourc\u00aee code. Traditional static analysisincludes code review, inspection, walk-through, algorithm analysis, andproof of correctness. It does not involve actual execution of the code underdevelopment. Instead, it examines code and reasons over all possible behaviors that might arise during run time. Compiler optimiza\u00aetions are standardstatic analysis.Dynamic Analysis: Dynamic analysis of a software system involves actualprogram execution in order to expose possible program failures. The behavioral and performance properties of the program are also observed. Programs are executed with both typical and carefully chosen inpu\u00aet values.Often, the input set of a program can be impractically large. However, forpractical considerations, a finite subset of the input set can be selected.Therefore, in testing, we observe some representative program behaviorsand reach a conclusion about the quality of the system. Careful selectionof a finite test set is crucial to reaching a reliable conclusion.",
  "page330": "No matter how many times we run the test-find faults-fix cycle during softwareNo matter how many times we run the test-find faults-fix cycle during softwaredevelopment, some faults are likely to escape our attention, and these will eventually surface at the cu\u00aestomer site. Therefore, a quantitative measure that is usefulin assessing the quality of a software is its reliability [35]. Software reliability isdefined as the probability of failure-free operation of a software system for a specified time in a specified environment. The level of reliability of a system depends onthose inputs that cause failures to be observed by the end users. Software reliabilitycan be estimated via random testing, as suggested by Hamlet [36]. Since the notionof relia\u00aebility is specific to a \"specified environment,\" test data must be drawn fromthe input distribution to closely resemble the future usage of the system. Capturing the future usage pattern of a system in a general sense is described in a formcalled the operational profileThe stakeholders in a test proce\u00aess are the programmers, the test engineers, theproject managers, and the customers. A stakeholder is a person or an organizationwho influences a system's behaviors or who is impacted by that system [39].Different stakeholders view a test process from different perspectives as explainedbelow",
  "page331": "It does work: While implementing a program unit, the programmer maywant to test whether or not the unit works in normal circumstances. Theprogrammer gets much confidence if the unit works to his or her satisfaction. The same idea applies to an entire system as well once a systemh\u00aeas been integrated, the developers may want to test whether or not thesystem performs the basic functions. Here, for the psychological reason,the objective of testing is to show that the system works, rather than itdoes not work.It does not work: Once the programmer (or the development team) issatisfied that a unit (or the system) works to a certain degree, more testsare conducted with the objective of finding faults in the unit (or the system).Here, the idea is to tryto make the unit (or\u00aethe system) fail.Reduce the risk of failure: Most of the complex software systems containfaults, which cause the system to fail from time to time. This concept of\"failing from time to time\" gives rise to the notion of failure rate. Asfaults are discovered and fixed while performing more and more tests\u00ae, thefailure rate of a system generally decreases. Thus, a higher level objectiveof performing tests is to bring down the risk of failing to an acceptablelevel.Reduce the cost of testing: The different kinds of costs associated with atest process includethe cost of designing, maintaining, and executing test c\u00aeases,the cost of analyzing the result of executing each test case,the cost of documenting the test cases, andthe cost of actually executing the system and documenting it.",
  "page332": "Therefore, the lessthe number of test cases designed, the less will be theassociated cost of testing. However, producing a small number of arbitrarytest cases is not a good way of saving cost. The highest level of objectiveof performing tests is to produce low-risk software with\u00aefewer numberof test cases. This idea leads us to the concept of effectiveness of testcases. Test engineers must therefore judiciously select fewer, effective testcasesIn its most basic form, a test case is a simple pair of < input, expected outcome >.If a program under test is expected to compute the square root of nonnegativenumbers, then four examples of test cases are as shown in Figure 1.3.In stateless systems, where the outcome depends solely on the current input,test cases are very s\u00aeimple in structure, as shown in Figure 1.3. A program tocompute the square root of nonnegative numbers is an example of a statelesssystem. A compiler for the C programming language is another example of astateless system. A compiler is a stateless system because to compile a program itdoes not need to know abou\u00aet the programs it compiled previously.In state-oriented systems, where the program outcome depends both on thecurrent state of the system and the current input, a test case may consist of asequence of < input, expected outcome > pairs. A telephone switching system andan automated teller machine (ATM) are exam\u00aeples of state-oriented systems. For anATM machine, a test case for testing the withdraw function is shown in Figure 1.4.Here, we assume that the user has already entered validated inputs, such as the cashcard and the personal identification number (PIN).",
  "page333": "In the test case TS1, \"check balance\" and \"withdraw\" in the first, second, andfourth tuples represent the pressing of the appropriate keys on the ATM keypad. It isassumed that the user account has $500.00 on it, and the user wants to withdraw anamount of $200.\u00ae00. The expected outcome \"$200.00\" in the third tuple representsthe cash dispensed by the ATM. After the withdrawal operation, the user makessure that the remaining balance is $300.00.For state-oriented systems, most of the test cases include some form of decision and timing in providing input to the system. A test case may include loopsand timers, which we do not show at this moment.An outcome of program execution is a complex entity that may include thefollowing:Values produced\u00aeby the program:Outputs for local observation(integer, text, audio, image)Outputs (messages) for remote storage, manipulation, or observationState change:State change of the programState change of the database (due to add, delete, and update operations) A sequence or set of values which must be interpreted toge\u00aether for theoutcome to be validAn important concept in test design is the concept of an oracle. An oracleis any entity program, process, human expert, or body of data that tells us theexpected outcome of a particular test or set of tests [40]. A test case is meaningfulonly if it is possible to decide on the a\u00aecceptability of the result produced by theprogram under test.Ideally, the expected outcome of a test should be computed while designingthe test case. In other words, the test outcome is computed before the program is executed with the selected test input.",
  "page334": "The idea here is that one should be able tocompute the expected outcome from an understanding of the program's requirements. Precomputation of the expected outcome will eliminate any implementationbias in case the test case is designed by the developer.In exceptional cases,\u00aewhere it is extremely difficult, impossible,or evenundesirable to compute a single expected outcome, one should identify expectedoutcomes by examining the actual test outcomes, as explained in the following:Execute the program with the selected input.Observe the actual outcome of program execution.Verify that the actual outcome is the expected outcome. Use the verified actual outcome as the expected outcome in subsequentruns of the test case It is not unusual to find people making claims s\u00aeuch as \"I have exhaustively testedthe program.\" Complete, or exhaustive, testing means there are no undiscoveredfaults at the end of the test phase. All problems must be known at the end ofcomplete testing. For most of the systems, complete testing is near impossiblebecause of the following reasons: T\u00aehe domain of possible inputs of a program is too large to be completelyused in testing a system. There are both valid inputs and invalid inputs.The program may have a large number of states. There may be timingconstraints on the inputs, that is, an input may be valid at a certain timeand invalid at other time\u00aes. An input value which is valid but is not properlytimed is called an inopportune input. The input domain of a system canbe very large to be completely used in testing a program.",
  "page335": "The design issues may be too complex to completely test. The design mayhave included implicit design decisions and assumptions. For example,a programmer may use a global variable or a static variable to controlprogram execution. It may not be possible to create all possible execu\u00aetion environments of thesystem. This becomes more significant when the behavior of the softwaresystem depends on the real, outside world, such as weather, temperature,altitude, pressure, and so on.We must realize that though the outcome of complete testing, that is, discovering allfaults, is highly desirable, it is a near-impossible task, and it may not be attempted.The next best thing is to select a subset of the input domain to test a program.Referring to Figure 1.5, let D be the input d\u00aeomain of a program P. Suppose thatwe select a subset D1 of D, that is, D1 \u2282 D, to test program P. It is possible thatD1 exercises only a part P1, that is, P1 \u2282 P, of the execution behavior of P, inwhich case faults with the other part, P2, will go undetected.By selecting a subset of the input domain D\u00ae1, the test engineer attemptsto deduce properties of an entire program P by observing the behavior of a partP1 of the entire behaviorof P on selected inputs D1. Therefore, selection of thesubset of the input domain must be done in a systematic and careful manner sothat the deduction is as accurate and complet\u00aee as possible. For example, the ideaof coverage is considered while selecting test cases In order to test a program, a test engineer must perform a sequence of testingactivities. Most of these activities have been shown in Figure 1.6 and are explainedin the following. ",
  "page336": "Identify an objective to be tested: The first activity is to identify anobjective to be tested. The objective defines the intention, orpurpose, ofdesigning one or more test cases to ensure that the program supports theobjective. A clear purpose must be associated with every test\u00aecaseSelect inputs: The second activity is to select test inputs. Selection of testinputs can be based on the requirements specification, the source code,or our expectations. Test inputs are selected by keeping the test objectivein mind.Compute the expected outcome: The third activity is to compute theexpected outcome of the program with the selected inputs. In most cases,this can be done from an overall, high-level understanding of the testobjective and the specification of the program und\u00aeer test.Set up the execution environment of the program: The fourth step is toprepare the right execution environment of the program. In this step all theassumptions external to the program must be satisfied. A few examples ofassumptions external to a program are as follows:Initialize the local system, external\u00aeto the program. This may includemaking a network connection available, making the right databasesystem available, and so on.Initialize any remote, external system (e.g., remote partner process in adistributed application.) For example, to test the client code, we mayneed to start the server at a remote siteE\u00aexecute the program: In the fifth step, the test engineer executes theprogram with the selected inputs and observes the actual outcome of theprogram. To execute a test case, inputs may be provided to the program atdifferent physical locations at different times. Theconcept of test coordination is used in synchronizing different components of a test case",
  "page337": "Analyze the test result: The final test activity is to analyze the result oftest execution. Here, the main task is to compare the actual outcome ofprogram execution with the expected outcome. The complexity of comparison depends on the complexity of the data to be observed. The o\u00aebserveddata type can be as simple as an integer or a string of characters or ascomplex as an image, a video, or an audio clip. At the end of the analysis step, a test verdict is assigned to the program. There are three majorkinds of test verdicts, namely, pass, fail, and inconclusive, as explainedbelow.If the program produces the expected outcome and the purpose of thetest case is satisfied, then a pass verdict is assigned.If the program does not produce the expected outcome, then a fail v\u00aeerdictis assigned.However, in some cases it may not be possible to assign a clear passor fail verdict. For example, if a timeout occurs while executing atest case on a distributed application, we may not be in a position toassign a clear pass or fail verdict. In those cases, an inconclusive testverdict is assig\u00aened. An inconclusive test verdictmeans that furthertests are needed to be done to refine the inconclusive verdict into aclear pass or fail verdict A test report must be written after analyzing the test result. Themotivation for writing a test report is to get the fault fixed if the test revealeda fault. A tes\u00aet report contains the following items to be informative:Explain how to reproduce the failure.Analyze the failure to be able to describe it.A pointer to the actual outcome and the test case, complete with theinput, the expected outcome, and the execution environment.",
  "page338": "Testing is performed at different levels involving the complete system or parts ofit throughout the life cycle of a software product. A software system goes throughfour stages of testing before it is actually deployed. These four stages are knownas unit, integration, system, and\u00aeacceptance level testing. The first three levels oftesting are performed by a number of different stakeholders in the developmentorganization, where as acceptance testing is performed by the customers. The fourstages of testing have been illustrated in the form of what is called the classical Vmodel In unit testing, programmers test individual program units, such as a procedures, functions, methods, or classes, in isolation. After ensuring that individualunits work to a satisfactory extent\u00ae, modules are assembled to construct larger subsystems by following integration testing techniques. Integration testing is jointlyperformed by software developers and integration test engineers. The objective ofintegration testing is to construct a reasonably stable system that can withstandthe rigor of system-\u00aelevel testing. System-level testing includes a wide spectrumof testing, such as functionality testing, security testing, robustness testing, loadtesting, stability testing, stress testing, performance testing, and reliability testing.System testing is a critical phase in a software development process because\u00aeof theneed to meet a tight schedule close to delivery date, to discover most of the faults,and to verify that fixes are working and have not resulted in new faults. Systemtesting comprises a number of distinct activities: creating a test plan, designinga test suite, preparing test environments, executing the tests by following a clearstrategy, and monitoring the process of test execution.",
  "page339": "Regression testing is another level of testing that is performed throughout thelife cycle of a system. Regression testing is performed whenever a component ofthe system is modified. The key idea in regression testing is to ascertain that themodification has not introduced any new\u00aefaults in the portion that was not subjectto modification. To be precise, regression testing is not a distinct level of testing.Rather, it is considered as a subphase of unit, integration, and system-level testing,as illustrated in Figure 1.8 [41].In regression testing, new tests are not designed. Instead, tests are selected,prioritized, and executed from the existing pool of test cases to ensure that nothingis broken in the new version of the software. Regression testing is an expensivep\u00aerocess and accounts for a predominant portion of testing effort in the industry. Itis desirable to select a subset of the test cases from the existing pool to reduce thecost. A key question is how many and which test cases should be selected so thatthe selected test cases are more likely to uncover new faults [\u00ae42-44].After the completion of system-level testing, the product is delivered to thecustomer. The customer performs their own series of tests, commonly known asacceptance testing. The objective of acceptance testing is to measure the qualityof the product, rather than searching for the defects, which is\u00aeobjective of systemtesting. A key notion in acceptance testing is the customer's expectations from thesystem. By the time of acceptance testing, the customer should have developedtheir acceptance criteria based on their own expectations from the system. Thereare two kinds of acceptancetesting as explained in the following: User acceptance testing (UAT)Business acceptance testing (BAT)",
  "page340": "User acceptance testing is conducted by the customer to ensure that the systemsatisfies the contractual acceptance criteria before being signed off as meeting userneeds. On the other hand, BAT is undertaken within the supplier's developmentorganization. The idea in having a\u00aeBAT is to ensure that the system will eventuallypass the user acceptance test. It is a rehearsal of UAT at the supplier's premises.Designing test cases has continued to stay in the foci of the research communityand the practitioners. A software developmentprocess generates a large body ofinformation, such as requirements specification, design document, and source code.In order togenerate effective tests at a lower cost, test designers analyze thefollowing sources of information: Requi\u00aerements and functional specificationsSource codeinput and output domainsOperational profileFault modelRequirements and Functional Specifications The process of software development begins by capturing user needs. The nature and amount of user needsidentified at the beginning of system development will vary depe\u00aending on thespecific life-cycle model to be followed. Let us consider a few examples. In theWaterfall model [45] of software development, a requirements engineer tries tocapture most of the requirements. Onthe other hand, in an agile software development model, such as XP [29] or the Scrum [46-48], only\u00aea few requirementsare identified in the beginning. A test engineer considers all the requirementsthe program is expected to meet whichever life-cycle model is chosen to test aprogram",
  "page341": "The requirements might have been specified in an informal manner, such asa combination of plaintext, equations, figures, and flowcharts. Though this form ofrequirements specification may be ambiguous, it is easily understood by customers.For example, the Bluetooth specification c\u00aeonsists of about 1100 pages of descriptions explaining how various subsystems of a Bluetooth interface is expected towork. The specification is written in plaintext form supplemented with mathematical equations, state diagrams, tables, and figures. For some systems, requirementsmay have been captured in the form of use cases, entity-relationship diagrams,and class diagrams. Sometimes the requirements of a system may have been specified in a formal language or notation, such as Z, SDL,\u00aeEstelle, or finite-statemachine. Both the informal and formal specifications are prime sources of testcasesSource Code Whereas a requirements specification describes the intendedbehavior of a system, the source code describes the actual behavior of the system.High-level assumptions and constraints take concret\u00aee form in an implementation.Though a software designer may produce a detailed design, programmers mayintroduce additional details into the system. For example, a step in the detaileddesign can be \"sort array A.\" To sort an array, there are many sorting algorithmswith different characteristics, such\u00aeas iteration, recursion, and temporarily usinganother array. Therefore, test cases must be designed based on the program [50].Input and Output Domains Some values in the input domain of a programhave special meanings, and hence must be treated separately [5]. To illustrate thispoint, let us consider the factorial function.",
  "page342": "without considering the special case of n 0. The above wrong implementationwill produce the correct result for all positive values of n, but will fail for n = 0.Sometimes even some output values have special meanings, and a programmust be tested to ensure that it produces the sp\u00aeecial values for all possible causes.In the above example, the output value 1 has special significance: (i) it is theminimum value computed by the factorial function and (ii) it is the only valueproduced for two different inputs.In the integer domain, the values 0 and 1 exhibit special characteristicsif arithmetic operations are performed. These characteristics are 0 X x = 0 and1 X x = x for all values of x. Therefore, all the special values in the input andoutput domains of a pr\u00aeogram must be considered while testing the program.Operational Profile As the term suggests, an operational profile is a quantitative characterization of how a system will be used. It was created to guide testengineers in selecting test cases (inputs) using samples of system usage. The notionof operational prof\u00aeiles, or usage profiles, was developed by Mills et al.The idea isto infer, from the observed test results, the future reliability of the software whenit is in actual use. To do this, test inputs are assigned a probability distribution, orprofile, according to their occurrences in actual operation. The ways te\u00aest engineersassign probability and select test cases to operate a system may significantly differfrom the ways actual users operate a system. However, for accurate estimationof the reliability of a system it is important to test a system by considering theways it will actually be used in the field.",
  "page343": "Fault Model Previously encountered faults are an excellent source of information in designing new test cases. The knownfaults are classified into differentclasses, such as initialization faults, logic faults, and interface faults, and stored ina repository [55, 56]. Test engineer\u00aes can use these data in designing tests to ensurethat a particular class of faults is not resident in the program.There are three types offault-based testing: error guessing, fault seeding,and mutation analysis. In error guessing, a test engineer applies his experienceto (i) assess the situation and guess where and what kinds of faults might exist,and (ii) design tests to specifically expose those kinds of faults. In fault seeding,known faults are injected into a program, and the test suit\u00aee is executed to assessthe effectiveness of the test suite. Fault seeding makes an assumption that a testsuite that finds seeded faults is also likely to find other faults. Mutation analysis issimilar to fault seeding, except that mutations to program statements are made inorder to determine the fault detection\u00aecapability of the test suite. If the test cases arenot capable of revealing such faults, the test engineer may specify additional testcases to reveal the faults. Mutation testing is based on the idea of fault simulation,whereasfault seeding is based onthe idea of fault injection. In the fault injectionapproa\u00aech, a fault is inserted into a program, and an oracle is available to assert thatthe inserted fault indeed made the program incorrect. On the other hand, in faultsimulation, a program modification is not guaranteed to lead to a faulty program.In fault simulation, one may modify an incorrect program and turn it into a correctprogram.",
  "page344": "A key idea in Section was that test cases need to be designed by considering information from several sources, such as the specification, source code, andspecial properties of the program's input and output domains. This is because allthose sources provide complementary inf\u00aeormation to test designers. Two broad concepts in testing, based on the sources of information for test design, are white-boxand black-box testing. White-box testing techniques are also called structural testing techniques, whereas black-box testing techniques are called functional testingtechniques.In structural testing, one primarily examines source code with a focus on control flow and data flow. Control flow refers to flow of control from one instructionto another. Control passes from\u00aeone instruction to another instruction in a numberof ways, such as one instruction appearing after another, function call, messagepassing, and interrupts. Conditional statements alter the normal, sequential flowof control in a program. Data flow refers to the propagation of values from onevariable or constant t\u00aeo another variable. Definitions and uses of variables determinethe data flow aspect in a program.In functional testing, one does not have access to the internal detailsof aprogram and the program is treated as a black box. A test engineer is concernedonly with the part that is accessible outside the program,\u00aethat is, just the inputand the externally visible outcome. A test engineer applies input to a program,observes the externally visible outcome of the program, and determines whetheror not the program outcome is the expected outcome. Inputs are selected fromthe program's requirements specification and properties of the program's input andoutput domains. A test engineer is concerned only with the functionality and thefeatures found in the program's specification",
  "page345": "At this point it is useful to identify a distinction between the scopes ofstructural testing and functional testing. One applies structural testing techniquesto individual units of a program, whereas functional testing techniques can beapplied to both an entire system and the ind\u00aeividual program units. Since individualprogrammers know the details of the source code they write, they themselvesperform structural testing on the individual program units they write. On the otherhand, functional testing is performed at the external interface level of a system,and it is conducted by a separate software quality assurance group.Let us consider a program unit U which is a part of a larger program P.A program unit is just a piece of source code with a well-defined objective a\u00aendwell-defined input and output domains. Now, if a programmer derives test casesfor testing U from a knowledge of the internal details of U , then the programmeris said to be performing structural testing. On the other hand, if the programmerdesigns test cases from the stated objective of the unit U and from hi\u00aes or herknowledge of the special properties of the input and output domains of U , then heor she is said to be performing functional testing on the same unit U .a of functional testing.Neither structural testing nor functional testing is by itself good enough todetect most of the faults. Even if one selects a\u00aell possible inputs, a structural testingtechnique cannot detect all faults if there are missing paths in a program. Intuitively,a path is said to be missing if there is no code to handle a possible condition.",
  "page346": "Similarly, without knowledge of the structural details of a program, many faultswill go undetected. Therefore, a combination of both structural and functionaltesting techniques must be used in program testing.The purpose of system test planning, or simply test planning, is to get\u00aeready andorganized for test execution. A test plan provides a framework, scope, details ofresource needed, effort required, schedule of activities, and a budget. A frameworkis a set of ideas, facts, or circumstances within which the tests will be conducted.The stated scope outlines the domain, or extent, of the test activities. The scopecovers the managerial aspects of testing, rather than the detailed techniques andspecific test cases.Test design is a critical phase of software testing.\u00aeDuring the test designphase, the system requirements are critically studied, system features tobetested are thoroughly identified, and the objectives of test cases and the detailedbehavior of test cases are defined. Test objectives are identified from differentsources, namely, the requirement specification and\u00aethe functional specification,and one or more test cases are designed for each test objective. Each test case isdesigned as a combination of modular test components called test steps. Thesetest steps can be combined together to create more complex, multistep tests. Atest case is clearly specified so that other\u00aes can easily borrow, understand, andreuse it ",
  "page347": "Monitoring and measurement aretwo key principles followed in every scientific andengineeringendeavor. The same principles are also applicable to the testing phasesof software development. It is important to monitor certain metrics which trulyrepresent the progress of testing and\u00aereveal the quality level of the system. Basedon those metrics, the management can trigger corrective and preventive actions. Byputting a small but critical set of metrics in place the executive management willbe able to know whether they are on the right track [58]. Test execution metricscan be broadly categorized into two classes as follows: Metrics for monitoring test execution Metrics for monitoring defectsThe first class of metrics concerns the process of executing test cases, whereast\u00aehe second class concerns the defects found as a result of test execution. Thesemetrics need to be tracked and analyzed on a periodic basis, say, daily or weekly.In order to effectively control a test project, it is important to gather valid andaccurate information about the project. One such example is to preci\u00aesely knowwhen to trigger revert criteria for a test cycle and initiate root cause analysis ofthe problems before more tests can be performed. By triggering such a revertcriteria, a test manager can effectively utilize the time of test engineers, and possibly money, by suspending a test cycle on a product with\u00aetoo many defects tocarry out a meaningful system test. A management team must identify and monitor metrics while testing is in progress so that important decisions can be made",
  "page348": " It is important to analyze and understand the test metrics, rather than justcollect data and make decisions based on those raw data. Metrics are meaningful only if they enable the management to make decisions which result in lowercost of production, reduced delay in delivery, an\u00aed improved quality of softwaresystems.Quantitative evaluation is important in every scientific and engineering field.Quantitative evaluation is carried out through measurement. Measurement lets oneevaluate parameters of interest in a quantitative manner as follows: Evaluate the effectiveness of a technique used in performing a task. Onecan evaluate the effectiveness of a test generation technique by countingthe number of defects detected by test cases generated by following thetechnique an\u00aed those detected by test cases generated by other means.Evaluate the productivity of the development activities. One can keep trackof productivity by counting the number of test cases designed per day, thenumber of test cases executed per day, and so on. Evaluate the quality of the product. By monitoring the nu\u00aember of defectsdetected per week of testing, one can observe the quality level of thesystem. Evaluate the product testing. For evaluating a product testing process, thefollowing two measurements are critical:incises in test design. The need for more testing occursas test engineers get new ideas while executi\u00aeng the planned testcases.Test effort effectiveness metric: It is important to evaluate the effectiveness of the testing effort in the development of a product. After aproduct is deployed at the customer's site, one is interested to knowthe effectiveness of testing that was performed",
  "page349": "A common measureof test effectiveness is the number of defects found by the customersthat were not found by the test engineers prior to the release of theproduct. These defects had escaped our test effort.In general, software testing is a highly laborintensive task. This is becau\u00aese test casesare to a great extent manually generated and often manually executed. Moreover,the results of test executions are manually analyzed. The durations of those taskscan be shortened by using appropriate tools. A test engineer can use a variety oftools, such as a static code analyzer, a test data generator, and a network analyzer,if a network-based application or protocol is under test. Those tools are useful inincreasing the efficiency and effectiveness of testing.Test automation\u00aeis essential for any testing and quality assurance division ofan organization to move forward to become more efficient. The benefits of testautomation are as follows: Increased productivity of the testers Better coverage of regression testing Reduced durations of the testing phases Reduced cost of software main\u00aetenance Increased effectiveness of test casesTest automation provides an opportunity to improve the skills of the testengineers by writing programs, and hence their morale. They will be more focusedon developing automated test cases to avoid being a bottleneck in product deliveryto the market. Consequently, s\u00aeoftware testing becomes less of a tedious job.Test automation improves the coverage of regression testing because of accumulation of automated test cases over time. Automation allows an organization tocreate a rich library of reusable test cases and facilitates the execution of a consistent set of test cases. Here consistency means our ability to produce repeatedresults for the same set of tests",
  "page350": " It may be very difficult to reproduce test results inmanual testing, because exact conditions at the time and point of failure may notbe precisely known. In automated testing it is easier to set up the initial conditionsof a system, thereby making it easier to reproduce test res\u00aeults. Test automationsimplifies the debugging work by providing a detailed, unambiguous log of activities and intermediate test steps. This leads to a more organized, structured, andreproducible testing approach.Automated execution of test cases reduces the elapsed time for testing, and,thus, it leads to a shorter time to market. The same automated test cases can beexecuted in an unsupervised manner at night, thereby efficiently utilizing the different platforms, such as hardware and confi\u00aeguration. In short, automation increasestest execution efficiency. However, at the end of test execution, it is important toanalyze the test results to determine the number of test cases that passed or failed.And, if a test case failed, one analyzes the reasons for its failure.In the long run, test automation i\u00aes cost-effective. It drastically reduces the software maintenance cost. In the sustaining phase of a software system, the regressiontests required after each change to the system are too many. As a result, regressiontesting becomes too time and labor intensive without automation.A repetitive type of testing i\u00aes very cumbersome and expensive to performmanually, but it can be automated easily using software tools. A simple repetitivetype of application can reveal memory leaks in a software. However, the applicationhas to be run for a significantly long duration, say, for weeks, to reveal memoryleaks. Therefore, manual testing may not be justified, whereas with automation itis easy to reveal memory leaks.",
  "page351": "For example, stress testing is a prime candidate forautomation. Stress testing requires a worst-case load for an extended period of time,which is very difficult to realize by manual means. Scalability testing is anotherarea that can be automated. Instead of creating a large test\u00aebed with hundreds ofequipment, one can develop a simulator to verify the scalability of the system.Test automation is very attractive, but it comes with a price tag. Sufficienttime and resources need to be allocated for the development of an automated testsuite. Development of automated test cases need to be managed like a programmingproject. That is, it should be done in an organized manner; otherwise it is highlylikely to fail. An automated test suite may take longer to develop because t\u00aehe testsuite needs to be debugged before it can be used for testing. Sufficient time andresources need to be allocated for maintaining an automated test suite and setting upa test environment. Moreover, every time the system is modified, the modificationmust be reflected in the automated test suite. Therefore,\u00aean automated test suiteshould be designed as a modular system, coordinated into reusable libraries, andcross-referenced and traceable back to the feature being tested.It is important to remember that test automation cannot replace manual testing. Human creativity, variability, and observability cannot be mimi\u00aecked throughautomation. Automation cannot detect some problems that can be easily observedby a human being. Automated testing does not introduce minor variationsthe waya human can. Certain categories of tests, such as usability, interoperability, robustness, and compatibility, are often not suitedfor automation. It is too difficult toed.",
  "page352": "The objective of test automation is not to reduce the head counts in thetesting department of an organization, but to improve the productivity, quality, andefficiency of test execution. In fact, test automation requires a larger head count inthe testing department in the first ye\u00aear, because the department needs to automatethe test cases and simultaneously continue the execution of manual tests. Even afterthe completion of the development of a test automation framework and test caselibraries, the head count in the testing department does not drop below its originallevel. The test organization needs to retain the original team members in order toimprove the quality by addingmore test cases to the automated test case repository.Before a test automation project can pr\u00aeoceed, the organization must assessand address a number of considerations. The following list of prerequisites mustbe considered for an assessment of whether the organization is ready for testautomation:The test cases to be automated are well defined. Test tools and an infrastructure are in placeThe test automa\u00aetion professionals have prior successful experience inautomation. Adequate budget should have been allocated for the procurement of software tools.Testing is a distributed activity conducted at different levels throughout the lifecycle of a software. These different levels are unit testing, integration testin\u00aeg, system testing, and acceptance testing. It is logical to have different testing groups inan organization for each level of testing. However, it is more logical and is thecase in reality that unit-level tests be developed and executed by the programmersthemselves rather than an independent group of unit test engineers. The programmer who develops a software unit should take the ownership and responsibilityof producing good-quality software to his or her satisfaction.",
  "page353": "System integrationtesting is performed by the system integration test engineers. The integration testengineers involved need to know the software modules very well. This means thatall development engineers who collectively built all the units being integratedneed to be involved i\u00aen integration testing. Also, the integration test engineersshould thoroughly know the build mechanism, which is key to integrating largesystems.A team for performing system-level testing is truly separated from the development team, and it usually has a separate head count and a separate budget. Themandate of this group is to ensure that the system requirements have been met andthe system is acceptable. Members of the system test group conduct different categories of tests, such as functio\u00aenality, robustness, stress, load, scalability, reliability,and performance. They also execute business acceptance tests identified in the useracceptance test plan to ensure that the system will eventually pass user acceptancetesting at the customer site. However, the real user acceptance testing is executedby t\u00aehe client's special user group. The user group consists of people from different backgrounds, such as software quality assurance engineers, business associates,and customer support engineers. It is a common practice to create a temporaryuser acceptance test group consisting of people with different backg\u00aerounds, suchas integration test engineers, system test engineers, customer support engineers,and marketing engineers. Once the user acceptance is completed, the group is dismantled. It is recommended to have at least two test groups in an organization:integration test group and system test group.Hiring and retaining test engineers are challenging tasks. Interview is theprimary mechanism for evaluating applicants. Interviewing is a skill that improveswith practice. It is necessary to have a recruiting process in place in order to beeffective in hiring excellent test engineers. In order to retain test engineers, themanagement must recognize the importance of testing efforts at par with development efforts. The management should treat the test engineers as professionals andas a part of the overall team that delivers quality products",
  "page354": "With the above high-level introduction to quality and software testing, we are nowin a position to outline the remaining chapters. Each chapter in the book coverstechnical, process, and/or managerial topics related to software testing. The topicshave been designed and organized t\u00aeo facilitate the reader to become a software testspecialist. In Chapter 2 we provide a self-contained introduction to the theory andlimitations of software testing.Chapters 3-6 treat unit testing techniques one by one, as quantitativelyas possible. These chapters describe both static and dynamic unit testing. Staticunit testing has been presented within a general framework called code review,rather than individual techniques called inspection and walkthrough. Dynamic unittesting, or e\u00aexecution-based unit testing, focuses on control flow, data flow, anddomain testing. The JUnit framework, which is used to create and execute dynamicunit tests, is introduced. We discuss some tools foreffectively performing unittesting.Chapter 7 discusses the concept of integration testing. Specifically, five ki\u00aendsof integration techniques, namely, top down, bottom up, sandwich, big bang, andincremental, are explained. Next, we discuss the integration of hardware and software components to form a complete system. We introduce a framework to developa plan for system integration testing. The chapter is completed with\u00aea brief discussion of integration testing of off-the-shelf components.Chapters 8-13 discuss various aspects of system-level testing. These sixchapters introduce the reader to the technical details of system testing that is thepractice in industry. These chapters promote both qualitative and quantitative evaluation of a system testing process. The chapters emphasize the need for having anindependent system testing group. A process for monitoring and controlling system testing is clearly explained. Chapter 14 is devoted to acceptance testing, whichincludes acceptance testing criteria, planning for acceptance testing, and acceptancetest execution.",
  "page355": "execution.Chapter 15 contains the fundamental concepts of software reliability and theirapplication to software testing. We discuss the notion of operation profile and itsapplication in system testing. We conclude the chapter with the description of anexample and the time of rele\u00aeasing a system by determining the additional lengthof system testing. The additional testing time is calculated by using the idea ofsoftware reliability.In Chapter 16, we present the structure of test groups and how these groupscan be organized in a software company. Next, we discuss how to hire and retaintest engineers by providing training, instituting a reward system, and establishingan attractive career path for them within the testing organization. We conclude thischapter with the des\u00aecription of how to build and manage a test team with a focuson teamwork rather than individual gain.Chapters 17 and 18 explain the concepts of software quality and differentmaturity models. Chapter 17 focuses on quality factors and criteria and describesthe ISO 9126 and ISO 9000:2000 standards. Chapter 18 cover\u00aes the CMM, whichwas developed by the SEI at Carnegie Mellon University. Two test-related models,namely the TPI model and the TMM, are explained at the end of Chapter 18.We define the key words used in the book in a glossary at the end of the book.The reader will find about 10 practice exercises at the end of\u00aeeach chapter. A listof references is included at the end of each chapter for a reader who would like tofind more detailed discussions of some of the topics. Finally, each chapter, exceptthis one, contains a literature review section that, essentially, provides pointers tomore advanced material related to the topics. The more advanced materials arebased on current research and alternate viewpoints.",
  "page356": "Any approach to testing is based on assumptionsabout the way program faultsoccur. Faults are due to two main reasons:Faults occur due to our inadequate understanding of all conditions withwhich a program must deal.Faults occur due to our failure to realize that certain combinatio\u00aens of conditions require special treatments.Goodenough and Gerhart classify program faults as follows:Logic Fault: This class of faults means a program produces incorrectresults independent of resources required. That is, the program fails becauseof the faults present in the program and not because of a lack of resources.Logic faults can be further split into three categories:Requirements fault: This means our failure to capture the real requirements of the customer.Design fault: This repr\u00aeesents our failure to satisfy an understoodrequirement.Construction fault: This represents our failure to satisfy a design. Suppose that a design step says \"Sort array A.\" To sort the array with Nelements, one may choose one of several sorting algorithms. Let {:}be the desired for loop construct to so\u00aert the array. If a programmerwrites the for loop in the formfor (i = 0; i <= N; i ){:}then there is a construction error in the implementation.Performance Fault: This class of faults leads to a failure of the programto produce expected results within specified or desired resource limitations.A thorough test\u00aemust be able to detect faults arising from any of the abovereasons. Test data selection criteria must reflect information derived from each stageof software development. Since each type of fault is manifested as an impropereffect produced by an implementation, it is useful to categorize the sources of faultsin implementation terms as folows: ",
  "page357": "Missing Control Flow Paths: Intuitively, a control flow path, or simply apath, is a feasible sequence of instructions in a program. A path may bemissing from a program if we fail to identify a condition and specify apath to handle that condition. An example of a missingpath is ou\u00aer failureto test for a zero divisorbefore executing a division. If we fail to recognizethat a divisor can take a zero value, then we will not include a piece ofcode to handle the special case. Thus, a certain desirable computation willbe missing from the program.Inappropriate Path Selection: A program executes an inappropriate path ifa condition is expressed incorrectly. we show a desiredbehavior and an implemented behavior. Both the behaviors are identicalexcept in the condition part of\u00aethe if statement. The if part of the implemented behavior contains an additional condition B. It is easy to see that both the desired part and the implemented part behave in the same wayfor all combinations of values of A and B except when A = 1 and B = 0.Inappropriate or Missing Action: There are three instanc\u00aees of this class offault One may calculate a value using a method that does not necessarilygive the correct result. For example, a desired expression is x = x X w,whereas it is wrongly written as x = x  w. These two expressionsproduce identical results for several combinations of x and w, such asx = 1.5\u00aeand w = 3, for example.Failing to assign a value to a variable is an example of a missing action. Calling a function with the wrong argument list is a kind of inappropriateaction.",
  "page358": "The main danger due to an inappropriate or missing action is that the action isincorrect only under certain combinations of conditions. Therefore, one must dothe following to find test data that reliably reveal errors: Identify all the conditions relevant to the correct operation\u00aeof a program.Select test data to exercise all possible combinations of these conditions.The above idea of selecting test data leads us to define the following terms:Test Data: Test data are actual values from the input domain of a programthat collectively satisfy some test selection criteria.Test Predicate: A test predicate is a description of conditions and combinations of conditions relevant to correct operation of the program:Test predicates describe the aspects of a program that are t\u00aeo be tested.Test data cause these aspects to be tested.Test predicates are the motivating force for test data selection.Components of test predicates arise first and primarily from the specifications for a program.Further conditions and predicates may be added as implementations areconsidered.A set of test pred\u00aeicates must at least satisfy the following conditions to have anychance of being reliable. These conditions are key to meaningful testing:Every individual branching condition in a program must be represented byan equivalent condition in C.Everypotential termination condition in the program, for example, an ov\u00aeerflow, must be represented by a condition in C.Every condition relevant to the correct operation of the program that isimplied by the specification and knowledge of the data structure of theprogram must be represented as a condition in C.",
  "page359": " The concepts of reliability and validity have been defined with respect tothe entire input domain of a program. A criterion is guaranteed to be bothreliable and valid if and only if it selects the entire domain as a single test.Since such exhaustive testing is impractical, onewi\u00aell have much difficultyin assessing the reliability and validity of a criterion.The concepts of reliability and validity have been defined with respect to aprogram. A test selection criterion that is reliable and valid for one programmay not be so for another program. The goodness of a test set should beindependent of individual programs and the faults therein.Neither validity nor reliability is preserved throughout the debugging process. In practice, as program failures are observed, the\u00aeprogram is debuggedto locate the faults, and the faults are generally fixed as soon as they arefound. During this debugging phase, as the program changes, so does theidealness of a test set. This is because a fault that was revealed beforedebugging is no more revealed after debugging and fault fixing. Thus,prop\u00aeerties of test selection criteria are not even \"monotonic\" in the senseof being either always gained or preserved or always lost or preserved.A key problem in the theory of Goodenough and Gerhart is that the reliability andvalidity of a criterion depend upon the presence of faults in a program and t\u00aeheirtypes. Weyuker and Ostrand [18] provide a modified theory in which the validityand reliability of test selection criteria are dependent only on the program specification, rather than a program. They propose the concept of a uniformly ideal test",
  "page360": "An ideal goal in software development is to find out whether or not a program iscorrect, where a correct program is void of faults. Much research results have beenreported in the field of program correctness. However, due to the highly constrainednature of program verification te\u00aechniques, no developer makes any effort to provethe correctness of even small programs of, say, a few thousand lines, let alonelarge programs with millions of linesof code. Instead, testing is accepted in theindustry as a practical way of finding faults in programs. The flip side of testingis that it cannot be used to settle the question of program correctness, which is theideal goal. Even though testing cannot settle the program correctness issue, thereis a need for a testing theory to en\u00aeable us to compare the power of different testmethods.To motivate a theoretical discussion of testing, we begin with an ideal processfor software development, which consists of the following steps:A customer and a development team specify the needs. The development team takes the specification and attempts to w\u00aerite a program to meet the specification. A test engineer takes both the specification and the program and selectsa set of test cases. The test cases are based on the specification and theprogram.The program is executed with the selected test data, and the test outcomeis compared with the expected outcome. Th\u00aee program is said to have faults if some tests fail.One can say the program to be ready for use if it passes all the test cases.We focus on the selection of test cases and the interpretation of their results.We assume that the specification is correct, and the specification is the sole arbiterof the correctness of the program. ",
  "page361": "Program Dependent: In this case, T :M (P), that is, test cases arederived solely based on the source code of a system. This is calledwhite-box testing. Here, a test method has complete knowledge of theinternal details of a program. However, from the viewpoint of practicaltesting,\u00aea white-box method is not generally applied to an entire program.One applies such a method to small units of a given large system. A unitrefers to a function, procedure, method, and so on. A white-box methodallows a test engineer to use the details of a program unit. Effective use ofa program unit requires a thorough understanding of the unit. Therefore,white-box test methods are used by programmers to test their own code. Specification Dependent: In this case, T = M (S ), that is, test c\u00aeasesare derived solely based on the specification of a system. This is calledblack-box testing. Here, a test method does not have access to the internaldetails of a program. Such a method uses information provided in thespecification of a system. It is not unusual to use an entire specificationin the generation\u00aeof test cases because specifications are much smaller insize than their corresponding implementations. Black-box methods aregenerally used by the development team and an independent system testgroup. Expectation Dependent: In practice, customers may generate test casesbased on theirexpectations from th\u00aee product at the time of taking deliveryof the system. These test cases may include continuous-operation tests,usability tests, and so on.",
  "page362": "Ideally, all programs should be correct, that is, there is no fault in a program. Dueto the impractical nature of proving even small programs to be correct, customersand software developers rely on the efficacy of testing. In this section, we introducetwo main limitations of test\u00aeing: Testing means executing a program with a generally small, proper subsetof the input domain of the program. A small, proper subset of the inputdomain is chosen because cost may not allow a much larger subset tobe chosen, let alone the full input set. Testing with the full input set isknown as exhaustive testing. Thus, the inherent need to test a programwith a small subset of the input domain poses a fundamental limit on theefficacy of testing. The limit is in the form of ourinability t\u00aeo extrapolate thecorrectness of results for a proper subset of the input domain to programcorrectness. In other words, even if a program passes a test set T, wecannot conclude that the program is correct. Once we have selected a subset of the input domain, we are faced with theproblem of verifying the correctne\u00aess of the program outputs for individualtest input. That is, a program output is examined to determine if the programperformed correctly on the test input. The mechanism which verifies thecorrectness of a program output is known as an oracle. The concept ofan oracle is discussed in detail in Chapter 9. Determ\u00aeining the correctnessof a program output is not a trivial task. If either of the following twoconditions hold, a program is considered non testable [20]:There does not exist an oracle.",
  "page363": "Ideally, all programs should be correct, that is, there is no fault in a program. Dueto the impractical nature of proving even small programs to be correct, customersand software developers rely on the efficacy of testing. In this section, we introducetwo main limitations of test\u00aeing: Testing means executing a program with a generally small, proper subsetof the input domain of the program. A small, proper subset of the inputdomain is chosen because cost may not allow a much larger subset tobe chosen, let alone the full input set. Testing with the full input set isknown as exhaustive testing. Thus, the inherent need to test a programwith a small subset of the input domain poses a fundamental limit on theefficacy of testing. The limit is in the form of our inability\u00aeto extrapolate thecorrectness of results for a proper subset of the input domain to programcorrectness. In other words, even if a program passes a test set T, wecannot conclude that the program iscorrect. Once we have selected a subset of the input domain, we are faced with theproblem of verifying the correctne\u00aess of the program outputs for individualtest input. That is, a program output is examined to determine if the programperformed correctly on the test input. The mechanism which verifies thecorrectness of a program output is known as an oracle. The concept ofan oracle is discussed in detail in Chapter 9. Determ\u00aeining the correctnessof a program output is not a trivial task. If either of the following twoconditions hold, a program is considered non testable [20]:There does not exist an oracle.",
  "page364": "In this chapter we consider the first level of testing, that is, unit testing. Unit testingrefers to testing program units in isolation. However, there is no consensus on thedefinition of a unit. Some examples of commonly understood units are functions,procedures, or methods. Eve\u00aen a class in an object-oriented programming languagecan be considered as a program unit. Syntactically, a program unit is a piece ofcode, such as a function or method of class, that is invoked from outside the unitand that can invoke other program units. Moreover, a program unit is assumed toimplement a well-defined function providing a certain level of abstraction to theimplementation of higher level functions. The function performed by a program unitmay not have a direct association with\u00aea system-level function. Thus, a programunit may be viewed as a piece of code implementing a \"low\"-level function. Inthis chapter, we use the terms unit and module interchangeably.Now, given that a program unit implements a function, it is only natural totest the unit before it is integrated with oth\u00aeer units. Thus, a program unit is testedin isolation, that is, ina stand-alone manner. There are two reasons for testing aunit in a stand-alone manner. First, errors found during testing can be attributedto a specific unit so that it can be easily fixed. Moreover, unit testing removesdependencies on other pro\u00aegram units. Second, during unit testing it is desirableto verify that each distinct execution of a program unit produces the expectedresult. In terms of code details, a distinct executionrefers to a distinct path in theunit.",
  "page365": "Second, during unit testing it is desirableto verify that each distinct execution of a program unit produces the expectedresult. In terms of code details, a distinct execution refers to a distinct path in theunit. Ideally, all possible or as much as possible distinct executions a\u00aere to beconsidered during unit testing. This requires careful selection of input data for eachdistinct execution. A programmer has direct access to the input vector of the unit byexecuting a program unit in isolation. This direct access makes it easier to executeas many distinct pathsas desirable or possible. If multiple units are put together fortesting, then a programmer needs to generate test input with indirect relationshipwith the input vectors of several units under test. The said in\u00aedirectrelationshipmakes it difficult to control the execution of distinct paths in a chosen unit.Unit testing has a limited scope. A programmer will need to verify whetheror not a code works correctly by performing unit-level testing. Intuitively, a programmer needs to test a unit as follows:Execute every line\u00aeof code. This is desirable because the programmer needsto know what happens when a line of code is executed. In the absence ofsuch basic observations, surprises at a later stage can be expensive. Execute every predicate in the unit to evaluate them to true and falseseparately. Observe that the unit performs i\u00aets intended function and ensure that itcontains no known errors.In spite of the above tests, there e is no guarantee that a satisfactorily tested unitis functionally correct from a systemwide perspective. ",
  "page366": "Even though it is not possible to find all errors in a program unit in isolation, itis still necessary to ensure that a unit performs satisfactorily before it is used byother program units. It serves no purpose to integrate an erroneous unit with otherunits for the following reas\u00aeons: (i) many of the subsequent tests will be a wasteof resources and (ii) finding the root causes of failures in an integrated system ismore resource consuming.Unit testing is performed by the programmer who writes the program unitbecause the programmer is intimately familiar with the internal details of the unit.The objective for the programmer is to be satisfied that the unit works as expected.Since a programmer is supposed to construct a unit with no errors in it, a unittest is perform\u00aeed by him or her to their satisfaction in the beginning and to thesatisfaction of other programmers when the unit is integrated with other units. Thismeans that all programmers are accountable for the quality of their own work,which may include both new code and modifications to the existing code. The ideahere\u00aeis to push the quality concept down to the lowest level of the organization andempower each programmer to be responsible for his or her own quality. Therefore,it is in the best interest of the programmer to take preventive actions to minimizethe number of defects inthe code. The defects found during unit test\u00aeing are internalto the software development group and are not reported up the personnel hierarchyto be counted in quality measurement metrics. The source code of a unit is notused for interfacing by other group members until the programmer completes unit ",
  "page367": "testing and checks in the unit to the version control system.Unit testing is conducted in two complementary phases: Static unit testingDynamic unit testingIn static unit testing, a programmer does not execute the unit; instead, the code isexamined over all possible behaviors that\u00aemight arise during run time. Static unittesting is also known as non-execution-based unit testing, whereas dynamic unittesting is execution based. In static unit testing, the code of each unit is validatedagainst requirements of the unit by reviewing thecode. During the review process,potential issues are identified and resolved. For example, in the C programminglanguage the two program-halting instructions are abort() and exit(). While the twoare closely related, they have different effe\u00aects as explained below:Abort(): This means abnormal program termination. By default, a call toabort() results in a run time diagnostic and program self-destruction. Theprogram destruction may or may not flush and close opened files or removetemporary files, depending on the implementation. Exit(): This means gr\u00aeaceful program termination. That is, the exit() callcloses the opened files and returns a status code to the execution environment.Whether to use abort() or exit() depends on the context that can be easilydetected and resolved during static unit testing. More issues caught earlier lead tofewer errors being id\u00aeentified in the dynamic test phase and result in fewer defectsin shipped products. Moreover, performing static tests is less expensive than performing dynamic tests. Code review is one component of the defect minimizationprocess and can help detect problems that are common to software development.After a round of code review, dynamic unit testing is conducted.",
  "page368": "In dynamic unittesting, a program unit is actually executed and its outcomes are observed. Dynamicunit testing means testing the code by actually running it. It may be noted that staticunit testing is not an alternative to dynamic unit testing. A programmer performsboth kinds of\u00aetests. In practice, partial dynamic unit testing is performed concurrently with static unit testing. If the entire dynamic unit testing has been performedand a static unit testing identifies significant problems, the dynamic unit testingmust be repeated. As a result of this repetition, the development schedule may beaffected. To minimize the probability of such an event, it is required that static unittesting be performed prior to the final dynamic unit testingStatic unit testing is conduc\u00aeted as a part of a larger philosophical belief that asoftware product should undergo a phase of inspection and correction at eachmilestone in its life cycle. At a certain milestone, the product need not be in itsfinal form. For example, completion of coding is a milestone, even though codingof all the units may\u00aenot make the desired product. After coding, the next milestoneis testing all or a substantial number of units forming the major components of theproduct. Thus, before units are individually tested by actually executing them, thoseare subject to usual review and correction as it is commonly understood. The id\u00aeeabehind review is to find the defects as close to their points of origin as possible sothat those defects are eliminated with less effort, and the interim product containsfewer defects before the next task is undertaken.",
  "page369": "Inspection: It is a step-by-step peer group review of a work product, witheach step checked against predetermined criteria. Walkthrough: It isa review where the author leads the team through amanual or simulated execution of the product using predefined scenarios.Regardless of wh\u00aeether a review is called an inspection or a walkthrough, it isa systematic approach to examining sourcecode in detail. The goal of such anexercise is to assess the quality of the software in question, not the quality of theprocess used to develop the product [3]. Reviews of this type are characterizedby significant preparation by groups of designers and programmers with varyingdegree of interest in the software development project. Code examination can betime consuming. Moreover, no examin\u00aeation process is perfect. Examiners may takeshortcuts, may not have adequate understanding of the product, and may accept aproduct which should not be accepted. Nonetheless, a well-designed code reviewprocess can find faults that may be missed by execution-based testing. The key tothe success of code review is\u00aeto divide and conquer, that is, having an examinerinspect small parts of the unit in isolation, while making sure of the following:(i) nothing is overlooked and (ii) the correctness of all examined parts of themodule implies the correctness of the whole module. The decomposition of thereview into discrete ste\u00aeps must assure that each step is simple enough that it canbe carried out without detailed knowledge of the others.The objective of code review is to review the code, not to evaluate the authorof the code",
  "page370": "review is to review the code, not to evaluate the authorof the code. A clash may occur betweenthe author of the code and the reviewers,and this may make the meetings unproductive. Therefore, code review must beplanned and managed in a professional manner. There is a need for mutu\u00aeal respect,openness, trust, and sharing of expertise in the group. The general guidelines forperforming code review consists of six steps as outlined in Figure 3.1: readiness,preparation, examination, rework, validation, and exit. The input to the readinessstep is the criteria that must be satisfied before the start of the code review process,and the process produces two types of documents, a change request (CR) and areport. These steps and documents are explained in the following. Readine\u00aess The author of the unit ensures that the unit under test isready for review. A unit is said to be ready if it satisfies the followingcriteria. Completeness: All the code relating to the unit to be reviewed must beavailable. This is because the reviewers are going to read the code andtry to understand it. It i\u00aes unproductive to review partially written codeor code that is going to be significantly modified by the programmer Minimal Functionality: The code must compile and link. Moreover,the code must have been tested to some extent to make sure that itperforms its basic functionalities.Readability: Since code revie\u00aew involves actual reading of code byother programmers, it is essential that the code is highly readable.Some code characteristics that enhance readability are proper formatting, using meaningful identifier names, straightforward use of programming language constructs, and an appropriate level of abstractionusing function calls. ",
  "page371": "Complexity: There is no need to schedule a group meeting to reviewstraightforward code which can be easily reviewed by the programmer.The code to be reviewed must be of sufficient complexity to warrantgroup review. Here, complexity is a composite term referring to thenumber of co\u00aenditional statements in the code, the number of input dataelements of the unit, the number of output data elements produced bythe unit, real-time processing of the code, and the number of other unitswith which the code communicates.Requirements and Design Documents: The latest approved versionof the low-level design specification or other appropriate descriptions Hierarchy of System Document of program requirements (see Table 3.1) should be available. Thesedocuments help the reviewers in v\u00aeerifying whether or not the codeunder review implements the expected functionalities. If the low-leveldesign document is available, it helps the reviewers in assessing whetheror not the code appropriately implements the design.All the people involved in the review process are informed of thegroup review meeting\u00aeschedule two or three days before the meeting.They are also given a copy of the work package for their perusal. Reviewsare conducted in bursts of 1-2 hours. Longer meetings are less and lessproductive because of the limited attention span of human beings. Therate of code review is restricted to about 12\u00ae5 lines of code (in a high-levellanguage) per hour. Reviewing complex code at a higher rate will resultin just glossing over the code, thereby defeating the fundamental purposeof code review. The composition of the review group involves a numberof people with different roles. These roles are explained as follows",
  "page372": "Moderator: A review meeting is chaired by the moderator. The moderator is a trained individual who guides the pace of the review process.The moderator selects the reviewers and schedules the review meetings.Myers suggests that the moderator be a member of a group from anunrelated\u00aeproject to preserve objectivity Author: This is the person who has written the code to be reviewed.Presenter: A presenter is someone other than the author of the code.The presenter reads the code beforehand to understand it. It is thepresenter who presents the author's code in the review meeting forthe following reasons: (i) an additional software developer will understand the work within the software organization; (ii) if the originalprogrammer leaves the company with a short notice\u00ae, at least one otherprogrammer in the company knows what is being done; and (iii) theoriginal programmer will have a good feeling about his or her work, ifsomeone else appreciates their work. Usually, the presenter appreciatesthe author's work. Recordkeeper: The recordkeeper documents the problems found du\u00aering the review process and the follow-up actions suggested. The personshould be different than the author and the moderator.Reviewers: These are experts in the subject area of the code underreview. The group size depends on the content of the material underreview. As a rule of thumb, the group size is betwee\u00aen 3 and 7. Usuallythis group does not have manager to whom the author reports. Thisis because it is the author's ongoing work that is under review, andneither a completed work nor the author himself is being reviewed.Observers: These are people who want to learn about the code underreview. These people do not participate in the review process but aresimply passive observers.",
  "page373": " Preparation Before the meeting, each reviewer carefully reviews thework package. It is expected that the reviewers read the code and understand its organization and operation before the review meeting. Eachreviewer develops the following: List of Questions: A reviewer prepares a\u00aelist of questions to be asked,if needed, of the author to clarify issues arising from his or her reading.A general guideline of what to examine while reading the code isoutlined in Table 3.2. Potential CR: A reviewer may make a formal request to make achange. These are called change requests rather than defect reports.At this stage, since the programmer has not yet made the code public, it is more appropriate to make suggestions to the author to makechanges, rather than report a defect. T\u00aehough CRs focus on defects inthe code, these reports are not included in defect statistics related tothe productSuggested Improvement Opportunities: The reviewers may suggesthow to fix the problems, if there are any, in the code under review.Since reviewers are experts in the subject area of the code, it is not\u00aeunusual for them to make suggestions for improvements. Examination The examination process consists of the followingactivities: The author makes a presentation of the procedural logic used in thecode, the paths denoting major computations, and the dependency ofthe unit under review on other units.The presente\u00aer reads the code line by line. The reviewers may raisequestions if the code is seen to have defects. However, problems are notresolved in the meeting. The reviewers may make general suggestionson how to fix the defects, but it is up to the author of the code to takecorrective measures after the meeting ends.",
  "page374": " The recordkeeper documents the change requests and the suggestionsfor fixing the problems, if there are any. A CR includes the followingdetails Give a brief description of the issue or action item. Assign a priority level (major or minor) to a CR.Assign a person to follow up the\u00aeissue. Since a CR documents apotential problem, there is a need for interaction between the author of the code and one of the reviewers, possibly the reviewer whomade the CR.Set a deadline for addressing a CR.The moderator ensures that the meeting remains focused on the reviewprocess. The moderator makes sure that the meeting makes progress ata certain rate so that the objective of the meeting is achieved.At the end of the meeting, a decision is taken regarding whether ornot to call anoth\u00aeer meeting to further review the code. If the reviewprocess leads to extensive rework of the code or critical issues areidentified in the process, then another meeting is generally convened.Otherwise, a second meeting is not scheduled, and the author is giventhe responsibility of fixing the CRs Rework At the en\u00aed of the meeting, the recordkeeper produces a summary of the meeting that includes the following information: A list of all the CRs, the dates by which those will be fixed, and thenames of the persons responsible for validating the CRs A list of improvement opportunitiesThe minutes of the meeting (optional)",
  "page375": "A copy of the report is distributed to all the members of the review group.After the meeting, the author works on the CRs to fix the problems. Theauthor documents the improvements made to the code in the CRs. Theauthor makes an attempt to address the issues within the agreed-upon\u00aetime frame using the prevailing coding conventions Validation The CRs are independently validated by the moderatoror another person designated for this purpose. The validation processinvolves checking the modified code as documented in the CRs andensuring that the suggested improvements have been implementedcorrectly. The revised and final version of the outcome of the reviewmeeting is distributed to all the group members.Exit Summarizing the review process, it is said to be complete if al\u00ael ofthe following actions have been taken: Every line of code in the unit has been inspected. If too many defects are found in a module, the module is once againreviewed after corrections are applied by the author. As a rule of thumb,if more than 5% of the total lines of code are thought to be contentious,then\u00aea second review is scheduled. The author and the reviewers reach a consensus that when correctionshave been applied the code will be potentially free of defects. All the CRs are documented and validated by the moderator or someoneelse. The author's follow-up actions are documented. A summary report of th\u00aee meeting including the CRs is distributed toall the membersof the review group.",
  "page376": "The effectiveness of static testing is limited by the ability of a reviewer tofind defects in code by visual means. However, if occurrences of defects depend onsome actual values of variables, then it is a difficult task to identify those defectsby visual means. Therefore, a unit\u00aemust be executed to observe its behaviors inresponse to a variety of inputs. Finally, whatever may be the effectiveness of statictests, one cannot feel confident without actually running the code.Code Review Metrics It is important to collect measurement data pertinent toa review process, so thatthe review process can be evaluated, made visible tothe upper management as a testing strategy, and improved to be more effective.Moreover, collecting metrics during code review facilitates estima\u00aetion of reviewtime and resources for future projects. Thus, code review is a viable testing strategythat can beeffectively used to improve the quality of products at an early stage.The following metrics can be collected from a code review:Number of lines of code (LOC) reviewed per hour Number of CRs generated p\u00aeer thousand lines of code (KLOC) Number of CRs generated per hour Total number of CRs generated per project Total number of hours spent on code review per projectIt is in the best interest of the programmers in particular and the company in generalto reduce the number of CRs generated during code review. This\u00aeis because CRs areindications of potential problems in the code, and those problems must be resolvedbefore different program units are integrated. Addressing CRs means spending moreresources and potentially delaying the project. Therefore, it is essential toadoptthe concept of defect prevention during code development.",
  "page377": "Code Review Metrics It is important to collect measurement data pertinent toa review process, so that the review process can be evaluated, made visible tothe upper management as a testing strategy, and improved to be more effective.Moreover, collecting metrics during code review\u00aefacilitates estimation of reviewtime and resources for future projects. Thus, code review is a viable testing strategythat can be effectively used to improve the quality of products at an early stage.The following metrics can be collected from a code review: Number of lines of code (LOC) reviewed per hourNumber of CRs generated per thousand lines of code (KLOC)Number of CRs generated per hour Total number of CRs generated per project Total number of hours spent on code review per projectIt\u00aeis in the best interest of the programmers in particular and the company in generalto reduce the number of CRs generated during code review. This is because CRs areindications of potential problems in the code, and those problems must be resolvedbefore different program units are integrated. Addressing CRs mea\u00aens spending moreresources and potentially delaying the project. Therefore, it is essential to adoptthe concept of defect prevention during code development. In practice, defectsare inadvertently introduced by programmers. Those accidents can be reduced bytaking preventive measures. It is useful to develop a s\u00aeet of guidelines to constructcode for defect minimization as explained in the following. These guidelines focuson incorporating suitable mechanisms into the code: ",
  "page378": "Build internal diagnostic tools, also known as instrumentation code, intothe units. Instrumentation codes are useful in providing information aboutthe internal states of the units. These codes allow programmers to realizebuilt-in tracking and tracing mechanisms. Instrumentation p\u00aelays a passiverole in dynamic unit testing. The role is passive in the sense of observingand recording the internal behavior without actively testing a unit. Use standard controls to detect possible occurrences of error conditions.Some examples of error detection in the code are divides by zero and arrayindex out of bounds. Ensure that code exists for all return values, some of which may be invalid.Appropriate follow-up actions need to be taken to handle invalid returnvalues. Ensure that c\u00aeounter data fields and buffer overflow and underflow areappropriately handled. Provide error messages and help texts from a common source so thatchanges in the text do not cause inconsistency. Good error messagesidentify the root causes of the problems and help users in resolving theproblems .Validate input dat\u00aea, such as the arguments, passed to a function. Use assertions to detect impossible conditions, undefined uses of data, andundesirable program behavior. An assertion is a Boolean statement whichshould never be false or can be false only if an error has occurred. In otherwords, an assertion is a check on a con\u00aedition which is assumed to be true,but it can cause a problem if it not true. Assertion should be routinely usedto perform the following kinds of checks:",
  "page379": "Ensure that preconditions are satisfied before beginning to execute aunit. A precondition is a Boolean function on the states of a unit specifying our expectation of the state prior to initiating an activity in thecode.Ensure that the expectedpostconditions are true while exiting\u00aefrom theunit. A postcondition is a Boolean function on the state of a unit specifying our expectation of the state after an activity has been completed.The postconditions may include an invariance. Ensure that the invariants hold. That is, check invariant states conditions which are expected not to change during the execution of apiece of code. Leave assertions in the code. You may deactivate them in the releasedversion of code in order to improve the operational performance of thesystem.\u00aeFully document the assertions that appear to be unclear. After every major computation, reverse-compute the input(s) from theresults in the code itself. Then compare the outcome with the actual inputsfor correctness. For example, suppose that a piece of code computes thesquare root of a positive number. Then s\u00aequare the output value and compare the result with the input. It may be needed to tolerate a margin oferror in the comparison process. In systems involving message passing, buffer management is an importantinternal activity. Incoming messages are stored in an already allocatedbuffer. It is useful to generate\u00aean event indicating low buffer availabilitybefore the system runs out of buffer. Develop a routine to continuallymonitor the availability of buffer after every use, calculate the remainingspace available in the buffer, and call an error handling routine if theamount of available buffer space is too low",
  "page380": "Develop a timer routine which counts down from a preset time until iteither hits zero or is reset. If the software is caught in an infinite loop, thetimer will expire and an exception handler routine can be invoked.Include a loop counter within each loop. If the loop is ever exec\u00aeuted lessthan the minimum possible number of times or more than the maximumpossible number of times, then invoke an exception handler routine.Define a variable to indicate the branch of decision logic that will be taken.Check this value after the decision has been made and the right branch hassupposedly been taken. If the value of the variable has not been preset,there is probably a fall-through condition in the logic.Execution-based unit testing is referred to as dynamic unit testing. In\u00aethis testing,a program unit is actually executed in isolation, as we commonly understand it.However, this execution differs from ordinary execution in the following way: A unit under test is taken out of its actual execution environment. The actual execution environment is emulated by writing more code(explaine\u00aed later in this section) so that the unit and the emulatedenvironment can be compiled togetherThe above compiled aggregate is executed with selected inputs. Theoutcome of such an execution is collected in a variety of ways, such asstraightforward observation on a screen, logging on files, and softwareinstrume\u00aentation of the code to reveal run time behavior. The resultis compared with the expected outcome. Any difference between theactual and expected outcome implies a failure and the fault is inthe code",
  "page381": "An environment for dynamic unit testing is created by emulating the contextof the unit under test, as shown in Figure 3.2. The context of a unit test consistsof two parts: (i) a caller of the unit and (ii) all the units called by the unit. Theenvironment of a unit is emulated bec\u00aeause the unit is to be tested in isolationand the emulating environment must be a simple one so that anyfault foundas a result of running the unit can be solely attributed to the unit under test.The caller unit is known as a test driver, and all the emulations of the unitscalled by the unit under test are called stubs. The test driver and the stubs aretogether called scaffolding. The functions of a test driver and a stub are explained asfollows:Test Driver: A test driver is a program that\u00aeinvokes the unit under test.The unit under test executes with input values received from the driverand, upon termination, returns a value to the driver. The driver comparesthe actual outcome, that is, the actual value returned by the unit under test,with the expected outcome from the unit and reports the ensuin\u00aeg test result.The test driver functions as the main unit in the execution process. Thedriver not only facilitates compilation, but also provides input data to theunit under test in the expected format. Stubs: A stub is a \"dummy subprogram\" that replaces a unit that is calledby the unit under test. S\u00aetubs replace the units called by the unit under test.A stub performs two tasks. ",
  "page382": "First, it shows an evidence that the stub was, in fact, called. Such evidence can be shown by merely printing a message.Second, the stub returns a precomputed value to the caller so that the unitunder test can continue its execution.The driver and the stubs are never discarded af\u00aeter the unit test is completed.Instead, those are reused in the future in regression testing of the unit if there issuch a need. For each unit, there should be one dedicated test driver and severalstubs as required. If just one test driver is developed to test multiple units, thedriver will be a complicated one. Any modification to the driver to accommodatechanges in one of the units under test may have side effects in testing the otherunits. Similarly, the test driver should not depend on\u00aethe external input data filesbut, instead, should have its own segregated set of input data. The separate inputdata file approach becomes a very compelling choice for large amounts of testinput data. For example, if hundreds of input test data elements are required to testmore than one unit, then it is better\u00aeto create a separate input test data file ratherthan to include the same set of input test data in each test driver designed to testthe unit.Thetest driver should have the capability to automatically determine thesuccess or failure of the unit under test for each input test data. If appropriate,the drivershou\u00aeld also check for memory leaks and problems in allocation anddeallocation of memory. If the module opens and closes files, the test driver shouldcheck that these files are left in the expected open or closed state after each test.",
  "page383": "Mutation testing has a rich and long history. It can be traced back to the late 1970s[8-10]. Mutation testing was originally proposed by Dick Lipton, and the articleby DeMillo, Lipton, and Sayward [9] is generally cited as the seminal reference.Mutation testing is a techniqu\u00aee that focuses on measuring the adequacy of test data(or test cases). The original intention behind mutation testing was to expose andlocate weaknesses in test cases. Thus, mutation testing is a way to measure thequality of test cases, and the actual testing of program units is an added benefit.Mutation testing is not a testing strategy like control flow or data flow testing. Itshould be used to supplement traditional unit testing techniques.A mutation of a program is a modification of the\u00aeprogram created by introducing a single, small, legal syntactic change in the code. A modified program soobtained is called a mutant. The term mutant has been borrowed from biology.Some of these mutants are equivalent to the original program, whereas others arefaulty. A mutant is said to be killed whenthe exec\u00aeution of a test case causes it tofail and the mutant is considered to be dead.Some mutants are equivalent to the given program, that is, such mutantsalways produce the same output as the original program. In the real world, largeprograms are generally faulty, and test cases too contain faults.",
  "page384": "The result of executing a mutant may be differentfrom the expected result, but a test suite doesnot detect the failure because it does not have the righttest case. In this scenariothe mutant is called killable or stubborn, that is, the existing set of test casesis insufficient to\u00aekill it. A mutation score for a set of test cases is the percentage of nonequivalent mutants killed by the test suite. The test suite is said to bemutationadequate if its mutation score is 100%. Mutation analysis is a two-stepprocess: The adequacy of an existing test suite is determined to distinguish thegiven program from its mutants. A given test suite may not be adequateto distinguish all the nonequivalent mutants. As explained above, thosenonequivalent mutants that could not be identi\u00aefied by the given test suiteare called stubborn mutants. New test cases are added to the existing test suite to kill the stubbornmutants. The test suite enhancement process iterates until the test suitehas reached a desired level of mutation score.If we run the modified programs against the test suite, we will\u00aeget the followingresults:Mutants 1 and 3: The programs will completely pass the test suite. In otherwords, mutants 1 and 3 are not killed.Mutant 2: The program will fail test case 2.Mutant 4: The program will fail test case 1 and test case 2.If we calculate the mutation score, we see that we created four muta\u00aents, andtwo of them were killed. This tells us that the mutation score is 50%, assumingthat mutants 1 and 3 are nonequivalent.The score is found to be low. It is low because we assumed that mutants 1 and3 are nonequivalent to the original program. ",
  "page385": "We have to show that either mutants 1 and 3 are equivalent mutants or those are killable.If those are killable, we needto add new test cases to kill these two mutants. First, let us analyze mutant 1in order to derive a \"killer\" test. The difference between P and mutant\u00ae1 is thestarting point. Mutant 1 starts with i 1, whereas P starts with i = 2.There isno impact on the result r. Therefore, we conclude that mutant 1 is an equivalentmutant. Second, we add a fourth test case as follows:Test case 4:Input: 2 2 1Then program P will produce the output \"Value of the rank is 1\" and mutant 3will produce the output \"Value of the rank is 2.\" Thus, this test data kills mutant 3,which give us a mutation score of 100%.In order to use the mutation\u00aetesting technique to build a robust test suite, thetest engineer needs to follow the steps that are outlined below:Step 1: Begin with a program P and a set of test cases T known to be correct.Step 2: Run each test case in T against the program P. If a test case fails, thatis, the output is incorrect, program P\u00aemust be modified and retested. Ifthere are no failures, then continue with step 3.Step 3: Create a set of mutants {Pi}, each differing from P by a simple, syntactically correct modification of P",
  "page386": "Execute each test case in T against each mutant Pi . If the output of themutant Pi differs from the output of the original program P, the mutantPi is considered incorrect and is said to be killed by the test case. If Piproduces exactly the same results as the original program P f\u00aeor the testsin T, then one of the following is true: P and Pi are equivalent. That is, their behaviors cannot be distinguished by any set of test cases. Note that the general problem ofdeciding whether or not a mutant is equivalent to the original programis undecidable. Pi is killable. That is, the test cases are insufficient to kill the mutantPi . In this case, new test cases must be created.Step 5: Calculate the mutation score for the set of test cases T. The mutation score i\u00aes the percentage of nonequivalent mutants killed by the testdata, that is, Mutation score 100 X D/(N \u2212 E), where D is the deadmutants, N the total number of mutants, and E the number of equivalentmutants.Step 6: If the estimated mutation adequacy of T in step 5 is not sufficiently high,then design a\u00aenew test case that distinguishes Pi from P, add the newtest case to T, and go to step 2. If the computed adequacy of T is morethan an appropriate threshold, then accept T as a good measure of thecorrectness of P with respect to the set of mutant programs Pi , and stopdesigning new test cases.",
  "page387": "Competent Programmer Hypothesis: This assumption states that programmers are generally competent, and they do not create \"random\" programs.Therefore, we can assume that for a given problem a programmer will create a correct program except for simple errors. In other wor\u00aeds, the mutantsto be considered are the ones falling within a small deviation from theoriginal program. In practice, such mutants are obtained by systematicallyand mechanically applying a set of transformations, called mutation operators, to the program under test. These mutation operators are expected tomodel programming errors made by programmers. In practice, this maybe only partly true. Coupling Effect: This assumption was first hypothesized in 1978 byDeMillo et al. [9]. The assumption\u00aecan be restated as complex faultsare coupled to simple faults in such a way that a test suite detectingall simple faults in a program will detect most of the complex faults.This assumption has been empirically supported by Offutt [11] andtheoretically demonstrated by Wah [12]. The fundamental premise ofmutatio\u00aen testing as coined by Geist et al. [13] is: If the software containsa fault, there will usually be a set of mutants that can only be killed by atest case that also detect that faul Mutation testing helps the tester to inject, by hypothesis, different types offaults in the code and develop test cases to revea\u00ael them. In addition, comprehensivetesting can be performed by proper choice of mutant operations. However, a relatively large number of mutant programs need to be tested against many of the testcases before these mutants can be distinguished from the original program. Running the test cases, analyzing the results, identifying equivalent mutants [14], anddeveloping additional test cases to kill the stubborn mutants are all time consuming",
  "page388": "Robust automated testing tools such as Mothra canbe used to expeditethe mutation testing process. Recently, with the availability of massive computing power, there has been a resurgence of mutation testing processes within theindustrial community to use as a white-box methodolog\u00aey for unit testing [16, 17].Researchers have shown that with an appropriate choice of mutant programs mutation testing is as powerful as path testing, domain testing [18], and data flowtesting The programmer, after a program failure, identifies the corresponding fault andfixes it. The process of determining the cause of a failure is known as debugging.Debugging occurs as a consequence of a test revealing a failure. Myers proposedthree approaches to debugging in his book The Art of Software\u00aeTesting [20]:Brute Force: The brute-force approach to debugging is preferred by manyprogrammers. Here, \"let the computer find the error\" philosophy is used. Print statements are scattered throughout the source code. These print statements provide a crude trace of the way the source code has executed.\u00aeThe availability of a good debugging tool makes these print statementsredundant . A dynamic debugger allows the software engineer to navigateby stepping through the code, observe which paths have executed, andobserve how values of variables change during the controlled execution.A good tool allows the program\u00aemer to assign values to several variablesand navigate step by step through the code",
  "page389": "Instrumentation code can bebuilt into the source code to detect problems and to log intermediate values of variables for problem diagnosis. One may use a memory dumpafter a failure has occurred to understand the final state of the code beingdebugged. The log and memory dump are r\u00aeeviewed to understand whathappened and how the failure occurred. Cause Elimination: The cause elimination approach can be best describedas a process involving induction and deduction [21]. In the induction part,first, all pertinent data related to the failure are collected , such as whathappened and what the symptoms are. Next, the collected data are organized in terms of behavior and symptoms, and their relationship is studiedto find a pattern to isolate the causes. A cause hypothesis is\u00aedevised, and theabove data are used to prove or disprove the hypothesis. In the deductionpart, a list of all possible causes is developed in order of their likelihoods,and tests are conducted to eliminate or substantiate each cause in decreasing order of their likelihoods. If the initial tests indicate that a p\u00aearticularhypothesis shows promise, test data are refined in an attempt to isolate theproblem as needed.Backtracking: In this approach, the programmer starts at a point in thecode where a failure was observed and traces back the execution to the pointwhere it occurred. This technique is frequently used by prog\u00aerammers, andthis is useful in small programs. However, the probability of tracing backto the fault decreases as the program size increases, because the numberof potential backward paths may become too large.",
  "page390": "Often, software engineers notice other previously undetected problems whiledebugging and applying a fix. These newly discovered faults should not be fixedalong with the fix in focus. This is because the software engineer may not have afull understanding of the part of the code re\u00aesponsible for the new fault. The bestway to deal with such a situation is to file a CR. A new CR gives the programmer anopportunity to discuss the matter with other team members and software architectsand to get their approval on a suggestion made by the programmer. Once the CR isapproved, the software engineer must file a defect in the defect tracking databaseand may proceed with the fix. This process is cumbersome, and it interrupts thedebugging process, but it is useful for very critica\u00ael projects. However, programmersoftendo not follow this because of a lack of a procedure to enforce it.A Debugging Heuristic The objective of debugging is to precisely identify thecause of a failure. Once the cause is identified, corrective measures are taken to fix the fault. Debugging is conducted by programm\u00aeers, preferably by those whowrote the code, because the programmer isthe best person to know the source codewell enough to analyze the code efficiently and effectively. Debugging is usuallya time consuming and error-prone process, which is generally performed understress. Debugging involves a combination of s\u00aeystematic evaluation, intuition, and,sometimes, a little bit of luck. Given a symptom of a problem, the purpose is toisolate and determine its specific cause. The following heuristic may be followedto isolate and correct it",
  "page391": "Reproduce the symptom(s). Readthe troubleshooting guide of the product. This guide may includeconditions and logs, produced by normal code, or diagnostics codespecifically written for troubleshooting purpose that can be turned on. Try to reproduce the symptoms with diagnostics co\u00aede turned on.Gather all the information and conduct causal analysis The goal ofcausal analysis is to identify the root cause of the problem and initiateactions so that the source of defects is eliminated.Step 2: Formulate some likely hypotheses for the cause of the problem based onthe causal analysis.Step 3: Develop a test scenario for each hypothesis to be proved or disproved.This is done by designing test cases to provide unambiguous resultsrelated to a hypothesis. The test cases may be\u00aestatic (reviewing code anddocumentation) and/or dynamic in nature. Preferably, the test casesarenondestructive, have low cost, and need minimum additional hardwareneeds. A test case is said to be destructive if it destroys the hardwaresetup. For example, cutting a cable during testing is called destructivetesti\u00aeng.Step 4: Prioritize the execution of test cases.Test cases corresponding tothehighly probable hypotheses are executed first. Also, the cost factor cannotbe overlooked. Therefore, it is desirable to execute the low-cost test casesfirst followed by the more expensive ones. The programmer needs toconsider both\u00aefactors.Step 5: Execute the test cases in order to find the cause of a symptom. Afterexecuting a test case, examine the result for new evidence. If the testresult shows that a particular hypothesis is promising, test data are refinedin an attempt to isolate the defect. If necessary, go back to earlier stepsor eliminate a particular hypothesis.",
  "page392": "any side effects (collateral damage) due to the changes effected in themodule. After a possible code review, apply the fix. Retest the unit to confirm that the actual cause of failure had beenfound. The unit is properly debugged and fixed if tests show that theobserve\u00aed failure does not occur any more. If there are no dynamic unit test cases that reveal the problem, thenadd a new test case to the dynamic unit testing to detect possiblereoccurrences or other similar problems. For the unit under consideration, identify all the test cases that havepassed. Now, perform a regression test on the unit with those testcases to ensure that new errors have not been introduced. That is whyit is so important to have archived all the test cases that have\u00aebeendesigned for a unit. Thus, even unit-level test cases must be managedin a systematic manner to reduce the cost of software development.Step 7: Document the changes which have been made. Once a defect is fixed,the following changes are required to be applied: Document the changes in the source code its\u00aeelf to reflect the change. Update the overall system documentation. Changes to the dynamic unit test cases. File a defect in the defect tracking database if the problem was foundafter the code was checked in to the version control system.",
  "page393": "A TDD approach to code development is used in the XP methodology [22, 23]. Thekey aspect of the TDD approach is that a programmer writes low-level tests beforewriting production code. This is referred to as test first [24] in software development. Writing test-driven units is an\u00aeimportant concept in the XP methodology. InXP, a few unit tests are coded first, then a simple, partial system is implemented topass the tests. Then, one more new unit test is created, and additional code is written to pass the new test, but not more,until a new unit test is created. The processis continued until nothing is left to test. The process is illustrated in Figure 3.3 andoutlined below:Step 1: Pick a requirement, that is, a story.Step 2: Write a test case that will verify a small\u00aepart of the story and assign afail verdict to it.Step 3: Write the code that implements a particular part of the story to pass thetest.Step 4: Execute all tests Step 5: Rework the code, and test the code until all tests pass.Step 6: Repeat steps 2-5 until the story is fully implemented.The simple cycle in\u00aeFigure 3.3 shows that, at the beginning of each cycle,the intention is for all tests to pass except the newly added test case. The new testcase is introduced to drive the new code development. At the end of the cycle, theprogrammer executes all the unit tests, ensuring that each one passesand, hence,the plan\u00aened task of the code still works",
  "page394": "A TDD approach to code development is used in the XP methodology [22, 23]. Thekey aspect of the TDD approach is that a programmer writes low-level tests beforewriting production code. This is referred to as test first [24] in software development. Writing test-driven units is an\u00aeimportant concept in the XP methodology. InXP, a few unit tests are coded first, then a simple, partial system is implemented topass the tests. Then, one more new unit test is created, and additional code is written to pass the new test, but not more, until a new unit test is created. The processis continued until nothing is left to test. The process is illustrated in Figure 3.3 andoutlined below:Step 1: Pick a requirement, that is, a story.Step 2: Write a test case that will verify a smal\u00ael part of the story and assign afail verdict to it.Step 3: Write the code that implements a particular part of the story to pass thetest.Step 4: Execute all tests Step 5: Rework the code, and test the code until all tests pass.Step 6: Repeat steps 2-5 until the story is fully implemented.The simple cycle i\u00aen Figure 3.3 shows that, at the beginning of each cycle,the intention is for all tests to pass except the newly added test case. The new testcase is introduced to drive the new code development. At the end of the cycle, theprogrammer executes all the unit tests, ensuring that each one passes and, hence,the planned task of the code still works",
  "page395": " One may not write production code unless the first failing unit test iswritten. One may not write more of a unit test than is sufficient to fail. One may not write more production code than is sufficient to make thefailing unit test pass.These three laws ensure that one must wri\u00aete a portion of a unit test that failsand then write just enough production code to make that unit test pass. The goalof these three laws is not to follow them strictly it is to decrease the intervalbetween writing unit tests and production code.Creating unit tests helps a developer focus on what needs to be done. Requirements, that is, user stories, are nailed down firmly by unit tests. Unit tests arereleased into the code repository along with the code they test. Code without unittests m\u00aeay not be released. If a unit test is discovered to be missing, it must be created immediately. Creating unit tests independently before coding sets up checksand balances and improves the chances of getting the system right the first time.Unit tests provide a safety net of regression tests and validation tests\u00aeso that XPprogrammers can refactor and integrate effectively.In XP, the code is being developed by two programmers working together sideby side. The concept is called pair programming. The two programmers sit side byside in front of the monitor. One person develops the code tactically and the otherone inspect\u00aes it methodically by keeping in mind the story they are implementing.It is similar to the two-person inspection strategy proposed by Bisant and Lyle",
  "page396": "The JUnit is a unit testing framework for the Java programming language designedby Kent Beck and Erich Gamma. Experience gained with JUnit has motivated thedevelopment of the TDD [22] methodology. The idea in the JUnit framework hasbeen ported to other languages, including C# (NU\u00aenit), Python (PyUnit), Fortran(fUnit) and C  (CPPUnit). This family of unit testing frameworks is collectivelyreferred to as xUnit. This section will introduce the fundamental concepts of JUnitto the reader.Suppose that we want to test the individual methods of a class called PlanetClass. Let Move() be a method in PlanetClass such that Move() accepts only oneinput parameter of type integer and returns a value of type integer. One can followthe following steps, illustrated using pseudocode\u00aein Figure 3.4, to test Move(): Create an object instance of Planet lass. Let us call the instance Mars.Now we are interested in testing the method Move() by invoking it onobject Mars. Select a value for all the input parameters of Move() this function hasjust one input parameter. Let us represent the input val\u00aeue to Move() by x. Know the expected value to be returned by Move(). Let the expectedreturned value be y Invoke method Move() on object Mars with input value x. Let z denotethe value returned by Move().Now compare y with z. If the two values are identical, then the methodMove() in object Mars passes th\u00aee test. Otherwise, the test is said to havefailed",
  "page397": "In a nutshell, the five steps of unit testing are as follows:Create an object and select a method to execute. Select values for the input parameters of the method. Compute the expected values to be returned by the method. Execute the selected method on the created object using th\u00aee selected inputvalues. Verify the result of executing the method.Performing unit testing leads to a programmer consuming some resources,especially time. Therefore, it is useful to employ a general programming frameworkto code individual test cases, organize a set of test cases as a test suite, initialize atest environment, execute the test suite, clean up the test environment, and recordthe result of execution of individual test cases. In the example shown in Figure 3.4,creating the objec\u00aet Mars is a part of the initialization process. The two print()statements are examples of recording the result of test execution. Alternatively,one can write the result of test execution to a file. The JUnit framework has been developed to make test writing simple. Theframework provides a basic class, called Te\u00aestCase, to write test cases. Programmersneed to extend the TestCase class to write a set of individual test cases. It may benoted that to write, for example, 10 test cases, one need not write 10 subclasses ofthe class Testcase. Rather, one subclass, say Testcase, of Testcase, can contain10 methods one for eac\u00aeh test case. Programmers need to make assertions about the state of objects while extending the Testcase class to write test cases. For example, in each test case it isrequired to compare the actual outcome of a computation with the expected outcome.",
  "page398": " Though an if() statement can be used to compare the equality of two valuesor two objects, it is seen to be more elegant to write an assert statement to achievethe same. The class Testcase extends a utility class called Assert in the JUnitframework. Essentially, the Assert class\u00aeprovides methods, as explained in the following, to make assertions about the state of objects created and manipulated whiletesting.assert True(Boolean condition): This assertion passes if the condition is true;otherwise, it fails.assert Equals(Object expected, Object actual): This assertion passes if theexpected and the actual objects are equal according to the equals() method;otherwise, the assertion fails.assert Equals(int expected, int actual): This assertion passes if expected andactu\u00aeal are equal according to the = operator; otherwise, the assertion fails. For each primitive type int, float, double, char, byte, long, short, andBoolean, the assertion has an overloaded version.assert Equals(double expected, double actual, double tolerance): This assertion passes if the absolute value ofthe d\u00aeifference between expected andactual is less than or equal to the tolerance value; otherwise, the assertionfails. The assertion has an overloaded version for float inputs.assert Same(Object expected, Object actual): This assertion passes if theexpected and actual values refer to the same object in memory; oth\u00aeerwise,the assertion fails assert Null(Object testobject): This assertion passes if testobject is null; otherwise the assertion fails.assert False(Boolean condition): This is the logical opposite of assert True()",
  "page399": "The reader maynote that the above list of assertions is not exhaustive. Infact, one can build other assertions while extending the TestCase class. When anassertion fails, a programmer may want to know immediately the nature of thefailure. This can be done by displaying a message\u00aewhen the assertion fails. Eachassertion method listed above accepts an optional first parameter of type String ifthe assertion fails, then the String value is displayed. This facilitates the programmerto display a desired message when the assertion fails. As an aside, upon failure,the assert Equals() method displays a customized message showing the expectedvalue and the actual value At this point it is interesting to note that only failed tests are reported. Failedtests can be reported by\u00aevarious means, such as displaying a message, displaying anidentifier for the test case, and counting the total number of failed test cases. Essentially, an assertion method throws an exception, called AssertionFailedError, whenthe assertion fails, and JUnit catches the exception. The code shown in Figure 3.5ill\u00aeustrates how the assert True() assertion works: When the JUnit framework catchesan exception, it records the fact that the assertion failed and proceeds to the nexttest case. Having executed all the test cases, JUnit produces a list of all those teststhat have failed. MyTestSuite and invoke the two methods My\u00aeTest1() and MyTest2(). Whether ornot the two methods, namely Method1() and Method()2, are to be invoked on twodifferent instances of the class TestMe depends on the individual objectives ofthose two test cases. In other words, it is the programmer who decides whether ornot two instances of the class TestMe are to be created",
  "page400": "Programmers can benefit from using tools in unit testing by reducing testing timewithout sacrificing thoroughness. The well-known tools in everyday life are aneditor, a compiler, an operating system, and a debugger. However, in some cases, the real execution environment of a unit\u00aemay not be available to a programmerwhile the code is being developed. In such cases, an emulator of the environmentis useful in testing and debugging the code. Other kinds of tools that facilitateeffective unit testing are as follows:1. Code Auditor: This tool is used to check the quality of software to ensurethat it meets some minimum coding standards. It detects violations of programming, naming, and style guidelines. It can identify portions of code that cannotbe ported between differ\u00aeent operating systems and processors. Moreover, it cansuggest improvements to the structure and style of the source code. In addition, itcounts the number of LOC which can be used to measure productivity, that is, LOCproduced per unit time, and calculate defect density, that is, number of defects perKLOC.2. Bou\u00aend Checker: This tool can check for accidental writes into the instruction areas of memory or to any other memory location outside the data storage areaof the application. This fills unused memory space with a signature pattern (distinct binary pattern) as a way of determining at a later time whether any of t\u00aehismemory space has been overwritten. The tool can issue diagnostic messages whenboundary violations on data items occur. It can detect violation of the boundaries of array, for example, when the array index or pointer is outside its allowedrange. ",
  "page401": "Documenters: These tools read source code and automatically generatedescriptions and caller/called tree diagram or data model from the source code. Interactive Debuggers: These tools assist software developers in implementing different debugging approaches discussed in this chapt\u00aeer. Thesetoolsshould have the trace-back and breakpoint capabilities to enable the programmers tounderstand the dynamics of program execution and to identify problem areas in thecode. Breakpoint debuggers are based on deductive logic. Breakpoints are placedaccording to a heuristic analysis of code [32]. Another popular kind of debuggeris known as omniscient debugger (ODB), in which there is no deduction. It simplyfollows the trail of \"bad\" values back to their source no \"gue\u00aessing\" where to putthe breakpoints. An ODB is like \"the snake in the grass,\" that is, if you see a snakein the grass and you pull its tail, sooner or later you get to its head. In contrast,breakpoint debuggers suffer from the \"lizard in the grass\" problem, that is, whenyou see the lizar\u00aed and grab its tail, the lizard breaks off its tail and gets away [33].n-Circuit Emulators: An in-circuit emulator, commonly known as ICE,is an invaluable software development tool in embedded system design. It providesa high-speed Ethernet connection between a host debugger and a target microprocessor, enabl\u00aeing developers to perform common source-level debugging activities,such as watching memory and controlling large numbers of registers, in a matterof seconds. It is vital for board bring-up, solving complex problems, and manufacturing or testing of products. Many emulators have advanced features, such as ",
  "page402": "performance analysis, coverage analysis, buffering of traces, and advance triggerand breakpoint possibilities.6. Memory Leak Detectors: These tools test the allocation of memory to anapplication which requests for memory, but fails to deallocate. These detect thefollowing overflo\u00aew problems in application programs:Illegal read, that is, accesses to memory which is not allocated to theapplication or which the application is not authorized to access. Reads memory which has not been initialized. Dynamic memory overwrites to a memory location that has not been allocated to the application. Reading from a memory location not allocated, or not initialized, prior tothe read operation.The tools watch the heap, keep track of heap allocations to applications, anddetect memor\u00aey leaks. The tools also build profiles of memory use, for example,which line-of-code source instruction accesses a particular memory address.7. Static Code (Path) Analyzer: These tools identify paths to test, basedon the structure of the code such as McCabe's cyclometric complexity measure(Table 3.3). Such\u00aetools are dependent on source language and require the sourcecode to be recompiled with the tool. These tools can be used to improve productivity, resource management, quality, and predictability by providing complexitymeasurement metrics.8. Software Inspection Support: Tools can help schedule group inspecti\u00aeons.These can also provide status of items reviewed and follow-up actions and distributethe reports of problem resolution. They can be integrated with other tools, such asstatic code analyzers Test Coverage Analyzer: These tools measure internal test coverage, oftenexpressed in terms of the control structure of the test object, and report the coverage metric. Coverage analyzers track and report what paths were exercised duringdynamic unit testing. ",
  "page403": "Test coverage analyzers are powerful tools that increase confidence in product quality by assuring that tests cover all of the structural parts ofa unit or a program. An important aspect in test coverage analysis is to identifyparts of source code that were never touched by any d\u00aeynamic unit test. Feedbackfrom the coverage reports to the source code makes it easier to design new unittestcases to cover the specific untested paths. Test Data Generator: These tools assist programmers in selecting test datathat cause a program to behave in a desired manner. Test data generators can offerseveral capabilities beyond the basics of data generation:They have generate a large number of variations of a desired data set basedon a description of the characteristics which has be\u00aeen fed into the tool. They can generate test input data from source code. They can generate equivalence classes and values close to the boundaries. They can calculate the desired extent of boundary value testing. They can estimate the likelihood of the test data being able to reveal faults. They can generate da\u00aeta to assist in mutation analysis.Automatic generation of test inputs is an active area of research. Several tools,such as CUTE [34], DART [35], and EGT system [36], have been developed byresearchers to improve test coverage. Test Harness: This class of tools supports the execution of dynamic unittests by mak\u00aeing it almost painless to (i) install the unit under test in a test environment, (ii) drive the unit under test with input data in the expected input format, (iii)generate stubs to emulate the behavior of subordinate modules, and (iv) capturethe actual outcome as generatedby the unit under test and log or display it in ausable form. ",
  "page404": "Advanced tools may compare the expected outcome with the actualoutcome and log a test verdict for each input test data.Performance Monitors: The timing characteristics of software componentscan be monitored and evaluated by these tools. These tools are essential for anyreal-time\u00aesystem in order to evaluate the performance characteristics of the system,such as delay and throughput. For example, in telecommunication systems, thesetools can be used to calculate the end-to-end delay of a telephone call. Network Analyzers: Network operating systems such as software that runon routers, switches, and client/server systems are tested by network analyzers.These tools have the ability to analyze the traffic and identify problem areas.Many of these networking tools allow tes\u00aet engineers to monitor performance metrics and diagnose performance problems across the networks. These tools areenhanced to improve the network security monitoring (NSM) capabilities to detectintrusion Simulators and Emulators: These tools are used to replace the real software and hardware that are currently n\u00aeot available. Both kinds of tools are usedfor training, safety, and economy reasons. Some examples are flight simulators,terminal emulators, and emulators for base transceiver stations in cellular mobilenetworks. These tools are bundled with traffic generators and performance analyzersin order to generatea la\u00aerge volume of input data Traffic Generators: Large volumes of data needed to stress the interfacesand the integrated system are generated by traffic generators. These produce streamsof transactions or data packets. For example, in testing routers, one needs a trafficthat simulates streams of varying size Internet Protocol (IP) packets arriving fromdifferent sources. These tools can set parameters for mean packet arrival rate,duration, and packet size. Operational profiles can be used to generate traffic forload and stability testing.",
  "page405": "Version Control: A version control system provides functionalities to storea sequence of revisions of the software and associated information files underdevelopment. A system release is a collection of the associated files from a version control tool perspective. These files may\u00aecontain source code, compiled code,documentation, and environment information, such as version of the tool used towrite the software. The objective of version control is to ensure a systematic andtraceable software development process in which all changes are precisely managed, so that a software system is always in a well-defined state. With most of theversion control tools, the repository is a central place that holds the master copyof all the files.The configuration management system (C\u00aeMS) extends the version control fromsoftware and documentation to control the changes made to hardware, firmware,software, documentation, test, test fixtures, test documentation, and execution environments throughout the development and operational life of a system. Therefore,configuration management tools are\u00aelarger, better variations of version control tools.The characteristics of the version control and configuration management tools areas follows: Access Control: The tools monitor and control access to components.One can specify which users can access a component or group of components. One can also restrict ac\u00aecess to components currently undergoingmodification or testing.Cross Referencing: The tools can maintain linkages among related components, such as problem reports, components, fixes, and documentations. One can merge files and coordinate multiple updates from different versionsto produce one consolidated file.",
  "page406": "Tracking of Modifications: The tools maintain records of all modifications to components. These also allow merging of files and coordinatemultiple updates from different versions to produce one consolidated file.These can track similarities and differences among versions of code,\u00aedocumentation, and test libraries. They also provide an audit trail or history ofthe changes from version to version.Release Generation: The tools can automatically build new systemreleases and insulate the development, test, and shipped versions of theproduct. System Version Management: The tools allow sharing of common components across system versions and controlled use of system versions. Theysupport coordination of parallel development, maintenance, and integrationof multiple compone\u00aents among several programmers or project teams. Theyalso coordinate geographically dispersed development and test teams. Archiving: The tools support automatic archiving of retired componentsand system versions. This chapter began with a description of unit-level testing, which means identifyingfaults in a prog\u00aeram unit analyzed and executed in isolation. Two complementarytypes of unit testing were introduced: static unit testing and dynamic unit testing. Static unit testing involves visual inspection and analysis of code, whereas aprogram unit is executed in a controlled manner in dynamic unit testing.Next, we desc\u00aeribed a code review process, which comprises six steps: readiness, preparation, examination, rework, validation, and exit. The goal of codereview is to assess the quality of the software in question, not the quality of theprocess used to develop the product. We discussed a few basic metrics that canbe collected from the code review process. Those metrics facilitate estimation ofreview time and resources required for similar projects. Also, the metrics makecode review visible to the upper management and allow upper management to besatisfied with the viability of code review as a testing tool ",
  "page407": "We explained several preventive measures that can be taken during codedevelopment to reduce the number of faults in a program. The preventive measures were presented in the form of a set of guidelines that programmers canfollow to construct code. Essentially, the guidelines focus\u00aeon incorporating suitablemechanisms into the code.Next, we studied dynamic unit testing in detail. In dynamic unit testing, aprogram unit is actually executed, and the outcomes of program execution areobserved. The concepts of test driver and stubs were explained in the contextof a unit under test. A test driver is a caller of the unit under test and all the \"dummy modules\"called by the unit are known as stubs. We described how mutation analysis can be used to locate weaknesses\u00aein test data used for unit testing.Mutation analysis should be used in conjunction with traditional unit testing techniques such as domain analysis or data flow analysis. That is, mutation testing isnot an alternative to domain testing or data flow analysis.With the unit test model in place to reveal defects, w\u00aee examined how programmers can locate faults by debugging a unit. Debugging occurs as a consequenceof a test revealing a defect. We discussed three approaches to debugging: bruteforce, cause elimination, and backtracking. The objective of debugging is to precisely identify the cause of a failure. Given the sy\u00aemptom of a problem, the purposeis to isolate and determine its specific cause. We explained a heuristic to performprogram debugging. Next, we explained dynamic unit testing is an integral part of the XP softwaredevelopment process. In the XP process, unit tests are created prior to coding thisis known as test first.",
  "page408": "The test-first approach sets up checks and balances to improvethe chances of getting things right the first time. We then introduced the JUnitframework, which is used to create and execute dynamic unit tests.We concluded the chapter with a description of several tools that can be\u00aeuseful in improving the effectiveness of unit testing. These tools are of the followingtypes: code auditor, bound checker, documenters, interactive debuggers, in-circuitemulators, memory leak detectors, static code analyzers, tools for software inspection support, test coverage analyzers, test data generators, tools for creating testharness, performance monitors, network analyzers, simulators and emulators, trafficgenerators, and tools for version control.The Institute of Electrical and E\u00aelectronics Engineers (IEEE) standard 1028-1988(IEEE Standard for Software Reviews and Audits: IEEE/ANSI Standard) describesthe detailed examination process for a technical review, an inspection, a softwarewalkthrough, and an audit. For each of the examination processes, it includes anobjective, an abstract, spe\u00aecial responsibilities, program input, entry criteria, procedures, exit criteria, output, and auditability.Several improvements on Fagan's inspection techniques have been proposedby researchers during the past three decades. Those proposals suggest ways toenhance the effectiveness of the review process or\u00aeto fit specific applicationdomains. A number of excellent articles address various issues related to softwareinspection as follows Biffl, and M. Halling, \"Investigating the Defect Effectiveness and CostBenefit of Nominal Inspection Teams,\" IEEE Transactions on SoftwareEngineering, Vol. 29, No. 5, May 2003, pp. 385-397.A. A. Porter and P. M. Johnson, \"Assessing Software Review Meeting:Results of a Comparative Analysis of Two Experimental Studies,\" IEEE",
  "page409": "Transactions on Software Engineering, Vol. 23, No. 3, March 1997, pp.129-145.A. A. Porter, H. P. Say, C. A. Toman, and L. G. Votta, \"An Experimentto Assess the Cost-Benefits of Code Inspection in Large Scale SoftwareDevelopment,\" IEEE Transactions on Software Engin\u00aeeering, Vol. 23, No.6, June 1997, pp. 329-346.A. A. Porter and L. G. Votta, \"What Makes Inspection Work,\" IEEE Software,Vol. 14, No. 5, May 1997, pp. 99-102.C. Sauer, D. Jeffery, L. Land, and P. Yetton, \"The Effectiveness of Software Development Technical Reviews: A Behaviorally Motivated Programof Search,\" IEEE Transactions on Software Engineering, Vol. 26, No. 1,January 2000, pp. 1-14.An alternative non-execution-based technique is formal verification o\u00aef code.Formal verification consists of mathematical proofs to show that a program iscorrect. The two most prominent methods for proving program properties are thoseof Dijkstra and Hoare:E. W. Dijkstra, A Discipline of Programming, Prentice-Hall, EnglewoodCliffs, NJ, 1976.C. A. R. Hoare, \"An Axiomatic Basis\u00aeof Computer Programming,\" Communications of the ACM , Vol. 12, No. 10, October 1969, pp. 576-580. Hoare presented an axiomatic approach in which properties of program fragmentsare described using preconditions and postconditions. An example statement witha precondition and a postcondition is {PRE}\u00aeP {POST}, where PRE is the precondition, POST is the postcondition, and P is the program fragment. Both PREand POST are expressed in first-order predicate calculus, which means that theycan include the universal quantifier \"for all\" or \"for every\" (\"for all\") and existential quantifier \"there exists\" or \"for some\" (\"thereexists\"). The interpretation of the above statement is that if the program fragmentP starts executing in a state satisfying PRE, then if P terminates, P will do so in astate satisfying POST",
  "page410": "Hoare's logic led to Dijkstra's closely related \"calculus of programs,\" whichis based on the idea of weakest preconditions. The weakest preconditions R withrespect to a program fragment P and a postcondition POST is the set of all statesthat, when subject to P\u00ae, will terminate and leave the state of computation in POST.The weakest precondition is written as WP(P, POST).While mutation testing systematically implants faults in programs by applyingsyntactic transformations, perturbation testing is performed to test a program'srobustness by changing the values of program data during run time, so that thesubsequent execution will either fail or succeed. Program perturbation is based onthree parts of software hypothesis as explained in the follow\u00aeing: Execution: A fault must be executed. Infection: The fault must change the data state of the computation directlyafter the fault location. Propagation: The erroneous data state must propagate to an output variable.In the perturbation technique, the programmer injects faults in the data stateof an executing\u00aeprogram and traces the injected faults on the program's output.A fault injection is performed by applying a perturbation function that changesthe program's data state. A perturbation function is a mathematical function thattakes a data state as its input, changes the data state according to some sp\u00aeecifiedcriteria, and produces a modified data state as output. For the interested readers,two excellent references on perturbation testing are as follow",
  "page411": "M. A. Friedman and J. M. Voas, Software Assessment Reliability, Safety,Testability, Wiley, New York, 1995.J. M. Voas and G. McGraw, Software Fault Injection Inoculating ProgramsAgainst Errors,Wiley, New York, 1998.The paper by Steven J. Zeil (\"Testing for Perturbation of Pro\u00aegram Statement,\" IEEE Transactions on Software Engineering, Vol. 9, No. 3, May 1983,pp. 335-346) describes a method for deducing sufficient path coverage to ensurethe absence of prescribed errors in a program. It models the program computationand potential errors as a vector space. This enables the conditions for nondetectionof an error to be calculated. The above article is an advanced reading for studentswho are interested in perturbation analysis.Those readers actively involve\u00aed in software configuration management(SCM) systems or interested in a more sophisticated treatment of the topic mustread the article by Jacky Estublier, David Leblang, Andre V. Hoek, Reidar \u00b4Conradi, Geoffrey Clemm, Walter Tichy, and Darcy Wiborg-Weber (\"Impactof Software Engineering Research on the\u00aePractice of Software ConfigurationManagement,\" ACM Transactions on Software Engineering and Methodology,Vol. 14, No. 4, October 2005, pp. 383-430). The authors discussed the evolutionof software configuration management technology, with a particular emphasisonthe impact that university and industria\u00ael research has had along the way. Thisarticle creates a detailed record of the critical value of software configurationmanagement research and illustrates the research results that have shaped thefunctionality of SCM systems ",
  "page412": "Two kinds of basic statements in a program unit are assignment statements andconditional statements. An assignment statement is explicitly represented by usingan assignment symbol, such as x = 2*y;, where x and y are variables.Program conditions are at the core of conditional sta\u00aetements, such as if(), for()loop, while() loop, and goto. As an example, in if(x! = y), we are testing for theinequality of x and y. In the absence of conditional statements, program instructionsare executed in the sequence they appear. The idea of successive execution ofinstructions gives rise to the concept of control flow in a program unit. Conditionalstatements alter the default, sequential control flow in a program unit. In fact,even a small number of conditional statements can lead t\u00aeo acomplex control flowstructure in a program.Function calls area mechanism to provide abstraction in program design.A call to a program function leads to control entering the called function. Similarly,when the called function executes its return statement, we say that control exitsfrom the function. Though a\u00aefunction can have many return statements, for simplicity, one can restructure the function to have exactly one return. A program unit canbe viewed as having a well-defined entry point and a well-defined exit point. Theexecution of a sequence of instructions from the entry point to the exit point of aprogram u\u00aenit is called a program path. There can be a large, even infinite, numberof paths in a program unit. Each program path can be characterized by an inputand an expected output. A specific input value causes a specific program path to beexecuted; itis expected that the program path performs the desired computation,thereby producing the expected output value. ",
  "page413": "The overall idea of generating test input data for performing control flow testinghas been depicted in Figure 4.1. The activities performed, the intermediate resultsproduced by those activities, and programmer preferences in the test generationprocess are explained below.Inputs:\u00aeThe source code of a program unit and a set of path selection criteriaare the inputs to a process for generating test data. In the following, twoexamples of path selection criteria are given.Example. Select paths such that every statement is executed at least once.Example. Select paths such that every conditional statement, forexample, an if() statement, evaluates to true and false at least once ondifferent occasions. A conditional statement may evaluate to true in onepath and false in a s\u00aeecond path.Generation of a Control Flow Graph: A control flow graph (CFG) is adetailed graphical representation of a program unit. The idea behind drawing a CFG is to be able to visualize all the paths in a program unit. Theprocess of drawing a CFG from a program unit will be explained in thefollowing section.\u00aeIf the process of testgeneration is automated, a compilercan be modified to produce a CFG. Selection of Paths: Paths are selected from the CFG to satisfy the path selection criteria, and it is done by considering the structure of the CFG.Generation of Test Input Data: A path can be executed if and only if ace\u00aertain instance of the inputs to the program unit causes all the conditionalstatements along the path to evaluate to true or false as dictated by thecontrol flow. Such a path iscalled a feasible path. Otherwise, the path issaid to be infeasible. ",
  "page414": "It is essential to identify certain values of the inputsfrom a given path for the path to execute.Feasibility Test of a Path: The idea behind checking the feasibility of aselected path is to meet the path selection criteria. If some chosen pathsare found to be infeasible, then ne\u00aew pathsare selected to meet the criteria A CFG is a graphical representation of a program unit. Three symbols are usedto construct a CFG, as shown in Figure 4.2. A rectangle represents a sequential computation. A maximal sequential computation can be represented either by asingle rectangle or by many rectangles, each corresponding to one statement in thesource code.We label each computation and decision box with a unique integer. The twobranches of a decision box are labeled with T and F t\u00aeo represent the true and falseevaluations, respectively, of the condition within the box. We will not label a mergenode, because one can easily identify the paths in a CFG even without explicitlyconsidering the merge nodes. Moreover, not mentioning the merge nodes in a pathwill make a path description shorter.W\u00aee consider the open files() function shown in Figure 4.3 to illustrate theprocess of drawing a CFG. The function has three statements: an assignment statement int i 0;, a conditional statement if(), and a return(i) statement. The readermay note that irrespective of the evaluation of the if(), the function per\u00aeforms thesame action, namely, null. In Figure 4.4, we show a high-level representation of the control flow in openfiles() with three nodes numbered 1, 2, and 3. The flowgraph shows just two paths in open files().",
  "page415": "A closer examination of the condition part ofthe if() statement reveals thatthere are not onlyBoolean and relational operators in the condition part, but alsoassignment statements. Some of their examples are given below:Assignment statements: fptr1 fopen(\"file1\",\u00aeand i Relational operator: fptr1! = NULLBoolean operators: Execution of the assignment statements in the condition part of the if statementdepends upon the component conditions. For example, consider the following component condition in the if part:((( fptr1 fopen(\"file1\", \"r\")) != NULL) nThe above condition is executed as follows: Execute the assignment statement fptr1 fopen(\"file1\", \"r\"). Execute the relational operation fptr1! = NULL. If the above relational operator\u00aeevaluates to false, skip the evaluation ofthe subsequent condition components (i ) The two most prominent methods for proving program properties are thoseof Dijkstra and Hoare:E. W. Dijkstra, A Discipline of Programming, Prentice-Hall, EnglewoodCliffs, NJ, 1976.C. A. R. Hoare, \"An Axiomatic Basis of Compu\u00aeter Programming,\" Communications of the ACM , Vol. 12, No. 10, October 1969, pp. 576-580. Hoare presented an axiomatic approach in which properties of program fragmentsare described using preconditions and postconditions. An example statement witha precondition and a postcondition is {PRE} P {POST},\u00aewhere PRE is the precondition, POST is the postcondition, and P is the program fragment. Both PREand POST are expressed in first-order predicate calculus, which means that theycan include the universal quantifier \"for all\" or \"for every\" (\"for all\") and existential quantifier \"there exists\" or \"for some\" (\"thereexists\"). The interpretation of the above statement is that if the program fragmentP starts executing in a state satisfying PRE, then if P terminates, P will do so in astate satisfying POST",
  "page416": "A CFG, such as the one shown in Figure 4.7, can have a large number of differentpaths. One may be tempted to test the execution of each and every path in a programunit. For a program unit with a small number of paths, executing all the paths may be desirable and achievable as wel\u00ael. On the other hand, for a program unit with alarge number of paths, executing every distinct path may not be practical. Thus,it is more productive for programmers to select a small number of program pathsin an effort to reveal defects in the code. Given the set of all paths, one is facedwith a question \"What paths do I select for testing?\" The concept of path selectioncriteria is useful is answering the above question. In the following, we state theadvantages of selecting paths\u00aebased on defined criteria All program constructs are exercised at least once. The programmer needsto observe the outcome of executing each program construct, for example,statements, Boolean conditions, and returns. We do not generate test inputs which execute the same path repeatedly.Executing the same path se\u00aeveral times is a waste of resources. However,if each execution of a program path potentially updates the state of thesystem, for example, the database state, then multiple executions of thesame path may not be identical.We know the program features that have been tested and those not tested.For example, we ma\u00aey execute an if statement only once so that it evaluatesto true. If we do not execute it once again for its false evaluation, we are,at least, aware that we have not observed the outcome of the program witha false evaluation of the if statement.",
  "page417": "Now we explain the following well-known path selection criteria: Select all paths. Select paths to achieve complete statement coverage. Select paths to achieve complete branch coverage.Select paths to achieve predicate coverage If all the paths in a CFG are selected, then one can\u00aedetect all faults, except thosedue to missing path errors. However, a program may contain a large number ofpaths, or even an infinite number of paths. The small, loop-free openfiles() functionshown in Figure 4.3 contains more than 25 paths. One does not know whether ornot a path is feasible at the time of selecting paths, though only eight of all thosepaths are feasible. If one selects all possible paths in a program, then we say thatthe all-path selection criterion has been satisfied.Let\u00aeus consider the example of the openfiles() function. This function tries toopen the three files file1, file2, and file3. The function returns an integer representingthe number offiles it has successfully opened. A file is said to be successfullyopened with \"read\" access if the file exists. The existe\u00aence of a file is either \"yes\"or \"no.\" Thus, the input domain of the function consists of eight combinations ofthe existence of the three files, as shown in Table 4.2.We can trace a path in the CFG of Figure 4.5 for each input, that is, eachrow of Table 4.2. Ideally, we identify test inputs\u00aeto execute a certain path in a program; this will be explained later in this chapter. We give three examples of thepaths executed by the test inputs (Table 4.3). In this manner, we can identify eightpossible paths .The all-paths selection criterion is desirable since itcan detect faults; however, it is difficult to achieve in practice.",
  "page418": "Statement coverage refers to executing individual program statements and observing the outcome. We say that 100% statement coverage has beenachieved if allthe statements have been executed at least once. Complete statement coverage isthe weakest coverage criterion in program test\u00aeing. Any test suite that achieves lessthan statement coverage for new software is considered to be unacceptable.All program statements are represented in some form in a CFG. Referringto the ReturnAverage() method in Figure 4.6 and its CFG in Figure 4.7, the fourassignment statementsi 0;ti = 0;tv = 0;sum = 0;have been represented by node 2. The while statement has been represented as aloop, where the loop control condition(ti < AS value[i] ! -999) as been represented by nodes 3 and 4. Thus,\u00aecovering a statement in a programmeans visiting one or more nodes representing the statement, more precisely, selecting a feasible entry-exit path that includes the corresponding nodes. Since a singleentry-exit path includes many nodes, we need to select just a few paths to coverall the nodes of a CF\u00aeG. Therefore, the basic problem is to select a few feasiblepaths to cover all the nodes of a CFG in order to achieve the complete statementcoverage criterion. We follow these rules while selecting paths: Select short paths. Select paths of increasingly longer length. Unfold a loop several times ifthere is a n\u00aeeed.Select arbitrarily long, \"complex\" paths.One can select the two paths shown in Figure 4.4 to achieve complete statementcoverage.",
  "page419": "Syntactically, a branch is an outgoing edge from a node. All the rectangle nodeshave at most one outgoing branch (edge). The exit node of a CFG does not have anoutgoing branch. All the diamond nodes have two outgoing branches. Covering abranch means selecting a path that includes\u00aethe branch. Complete branch coveragemeans selecting a number of paths such that every branch is included in at leastone path.In a preceding discussion, we showed that one can select two paths, SCPath 1and SCPath 2 in Table 4.4, to achieve complete statement coverage. These twopaths cover all the nodes (statements) and most of the branches of the CFG shownin Figure 4.7. The branches which are not covered by these two paths have beenhighlighted by bold dashed lines in Figure 4.8. These unco\u00aevered branches correspond to the three independent conditions evaluating to false. This means that as a programmer we have not observed theoutcome of the program execution as a result of the conditions evaluating to false.Thus, complete branch coverage means selecting enough number of paths such thatevery cond\u00aeition evaluates to true at least once and to false at least once.We need to select more paths to cover the branches highlighted by the bolddashed lines We refer to the partial CFG of Figure 4.9a toexplain the concept of predicatecoverage. OB1, OB2, OB3, and OB are four Boolean variables. The programcomputes\u00aethe values of the individual variables OB1, OB2, and OB3 details oftheir computation are irrelevant to our discussion and have been omitted. Next, OBis computed as shown in the CFG. The CFG checks the value of OB and executeseither OBlock1 or OBlock2 depending on whether OB evaluates to true or false,respectively.",
  "page420": "We need to design just two test cases to achieve both statement coverageand branch coverage. We select inputs such that the four Boolean conditions inFigure 4.9a evaluate to the values shown in Table 4.6. The reader may note thatwe have shown just one way of forcing OB to true.If\u00aewe select inputs so that thesetwo cases hold, then we do not observe the effect of the computations taking placein nodes 2 and 3. There may be faults in the computation parts of nodes 2 and 3such that OB2 and OB3 always evaluate to false . Therefore, there is a need to design test cases such that a path is executedunder all possible conditions. The False branch of node 5 (Figure 4.9a) is executedunder exactly one condition, namely, when OB1 False, OB2 = False, and OB3 =False, whereas th\u00aee true branch executes under seven conditions. If all possiblecombinations of truth values of the conditions affecting a selected path have beenexplored under some tests, then we say that predicate coverage has been achieved.Therefore, the path taking the true branch of node 5 in Figure 4.9a must be executedfor\u00aeall seven possible combinations of truth values of OB1, OB2, and OB3 whichresult in OB = True.A similar situation holds forthe partial CFG shown in Figure 4.9b, whereAB1, AB2, AB3, and AB are Boolean variables. ",
  "page421": "In Section 4.5 we explained the concept of path selection criteria to cover certainaspects of a program with a set of paths. The program aspects we consideredwere all statements, true and false evaluations of each condition, and combinationsof conditions affecting execution of a\u00aepath. Now, having identified a path, thequestion is how to select input values such that when the program is executedwith the selected inputs, the chosen paths get executed. In other words, we needto identify inputs to force the executions of the paths. In the following, we definea few terms and give an example of generating test inputs for a selected path.1. Input Vector: An input vector is a collection of all data entities read bythe routine whose values must be fixed prior to entering t\u00aehe routine. Members ofan input vector of a routine can take different forms as listed below:Input arguments to a routine Global variables and constants Files Contents of registers in assembly language programmingNetwork connections TimersA file is a complex input element. In one case, mere existence of a file c\u00aean beconsidered as an input, whereas in another case, contents of the file are considered to be inputs. Thus, the idea of an input vector is more general than the concept ofinput arguments of a function.Example. An input vector for openfiles() (Figure 4.3) consists of individual presence or absence of the fil\u00aees file1, file2, and file3.Example. The input vector of the ReturnAverage() method shown in Figure 4.6is < value [], AS, MIN, MAX > .2. Predicate: A predicate is a logical function evaluated at a decision point.Example. The construct ti < AS is the predicate in decision node 3 of Figure 4.7.",
  "page422": "he construct OB is the predicate in decision node 5 of Figure 4.9.3. Path Predicate: A path predicate is the set of predicates associated witha path.The path in Figure 4.10 indicates that nodes 3, 4, 6, 7, and 10 are decision nodes. The predicate associated with node 3 appears tw\u00aeice in the path; inthe first instance it evaluates to true and in the second instance it evaluates tofalse. The path predicate associated with the path under consideration is shown inFigure 4.11.We also specify the intended evaluation of the component predicates as foundin the path specification. For instance, we specify that value[i] ! 999 mustevaluate to true in the path predicate shown in Figure 4.11. We keep this additionalinformation for the following two reasons:In the absence of thi\u00aes additional information denoting the intended evaluation of a predicate, we will have no way to distinguish between the twoinstances of the predicate ti < AS, namely 3(T) and 3(F), associated withnode 3.We must know whether the individual component predicates of a pathpredicate evaluate to true or false in ord\u00aeer to generate path forcing inputs.4. Predicate Interpretation: The path predicate shown in Figure 4.11 is composed of elements of the input vector < value[], AS, MIN, MAX >, a vector oflocal variables < i, ti, tv >, and the constant \u2212999. The local variables are notvisible outside a function but are use\u00aed to hold intermediate results, point to array elements, and control loop iterations. ",
  "page423": "In other words, they play no roles in selecting inputs that force the paths to execute.Therefore, we can easily substitute all the local variables in a predicate with theelements of the input vector by using the idea of symbolic substitution. Let usconsider the method shown in Fi\u00aegure 4.12. The input vector for the method inFigure 4.12 is given by < x1, x2 > . The method defines a local variable y and alsouses the constants 7 and 0.The predicatex1  y > 0can be rewritten asx1  x2  7 >= 0by symbolically substituting y with x 2  7. The rewritten predicatex1  x2  7 >= 0has been expressed solely in terms of the input vector < x1,x2 > and the constantvector < 0,7 > . Thus, predicate interpretation is defined as the process of symbolically substituting operations al\u00aeong a path in order to express the predicates solelyin terms of the input vector and a constant vector.In a CFG, there may be several different paths leading up to a decision pointfrom the initial node, with each path doing different computations. Therefore, apredicate may have different interpretations dependi\u00aeng on how control reaches thepredicate under consideration. Path Predicate Expression: An interpreted path predicate is called a pathpredicate expression. A path predicate expression has the following properties:It is void of local variables and is solely composed of elements of the inputvector and possibly a\u00aevector of constants.It is a set of constraints constructed from the elements of the input vectorand possibly a vector of constants.Path forcing input values can be generated by solving the set of constraintsin a path predicate expression.",
  "page424": "If the set of constraints cannot be solved, there exist no input which cancause the selected path to execute. In other words, the selected path is saidto be infeasible. An infeasible path does not imply that one or more components of a pathpredicate expression are unsatisfiable.\u00aeIt simply means that the total combination of all the components in a path predicate expression is unsatisfiable.Infeasibility of a path predicate expression suggests that one considers otherpaths in an effort to meet a chosen path selection criterion.Example. Consider the path shown in Figure 4.10 from the CFG of Figure 4.7.Table 4.7 shows the nodes of the path in column 1, the corresponding descriptionof each node in column 2, and the interpretation of each node in column 3. The intended\u00aeevaluation of each interpreted predicate can be found in column 1 of thesame row.We show the path predicate expression of the path under consideration in Figure 4.13 for the sake of clarity. The rows of Figure 4.13 have beenobtained from Table 4.11 by combining each interpreted predicate in column 3 withits in\u00aetended evaluation in column 1. Now the reader may compare Figures 4.11and 4.13 to note that the predicates in Figure 4.13 are interpretations of the corresponding predicates in Figure 4.11 We show in Figure 4.14 an infeasible path appearing in the CFG ofFigure 4.7. The path predicate and its interpretation ar\u00aee shown in Table 4.8, and thepath predicate expression is shown in Figure 4.15. The path predicate expression isunsolvable because the constraint 0 > 0 = True is unsatisfiable. Therefore, the pathshown in Figure 4.14 is an infeasible path.",
  "page425": "Generating Input Data from Path Predicate Expression: We must solvethe corresponding path predicate expression in order to generate input data whichcan force a program to execute a selected path. Let us consider the path predicateexpression shown in Figure 4.13. We observe that c\u00aeonstraint 1 is always satisfied.Constraints 1 and 5 must be solved together to obtain AS 1. Similarly, constraints2, 3, and 4 must be solved together. We note that MIN < = value[0] < = MAXand value[0]! = \u2212999. Therefore, we have many choices to select values of MIN,MAX, and value[0]. An instance of the solutions of the constraints of Figure 4.13is shown in Figure 4.16 We give examples of selected test data to achieve complete statement and branchcoverage. We show four sets of test da\u00aeta in Table 4.9. The first two data sets coverall statements of the CFG in Figure 4.7. However, we need all four sets of testdata for complete branch coverage.If we execute the method ReturnAverage shown in Figure 4.6 with the foursets of test input data shown in Figure 4.9, then each statement of the method is\u00aeexecuted at least once, and every Boolean condition evaluates once to true andonce to false. We have thoroughly tested the method in the sense of completebranch coverage. However, it is possible to introduce simple faults in the methodwhich can go undetected when the method with the above four sets of test da\u00aeta isexecuted. Two examples of fault insertion are given below. in the method. Here the fault is that the method computes the average of thetotal number of inputs, denoted by ti, rather than the total number of valid inputs,denoted by tv.",
  "page426": "he JPA specification only allows properties/paths in the ORDER BY clause if the SELECT clause projects the same properties/paths. The following queries may be nonportable but work in Hibernate: select i.name from Item i order by i.buyNowPrice asc select i from Item i order by i.s\u00aeeller.username des Be careful with implicit inner joins in path expressions and ORDER BY: The last query returns only Item instances that have a seller. This may be unexpected, as the same query without the ORDER BY clause would retrieve all Item instances. (Ignoring for a moment that in our model the Item always has a seller, this issue is visible with optional references.) You'll find a more detailed discussion of inner joins and path. expressions later in this chapter. You now kno\u00aew how to write the FROM, WHERE, and ORDER BY clauses. You know how. to select the entities, you want to retrieve instances of and the necessary expressions. and operations to restrict and order the result. All you need now is the ability to project the data of this result to what you need in your application. I\u00aen simple terms, selection and restriction in a query is the process of declaring which. tables and rows you want to query. Projection is defining the \"columns\" you want returned to the application: the data you need. The SELECT clause in JPQL performs projections. As promised earlier,this criteria q\u00aeuery shows how you can add several Roots by calling the from() method several times. To add several elements to your projection, either call the tuple() methodof CriteriaBuilder, or the shortcut multiselect().",
  "page427": " Because this is a product, the result contains every possible combination of Item. and Bid rows found in the two underlying tables. Obviously, this query isn't useful, but you shouldn't be surprised to receive a collection of Object  as a query result. Hibernate manag\u00aees all Item and Bid entity instances in persistent state, in the persistence. context. Note how the HashSets filter out duplicate Item and Bid instances. Alternatively, with the Tuple API, in criteria queries you get typed access to the result list. Start by calling createTupleQuery() to create a CriteriaQuery<Tuple>. Then, refine the query definition by adding aliases for the entity classes The Object  returned by this query contain a Long at index 0, a String at index 1, and an Address\u00aeat index 2. The first two are scalar values; the third is an embedded class instance. None are managed entity instances! Therefore, these values aren't in any persistent state, like an entity instance would be. They aren't transactional and obviously aren't checked automatically for dirty state.\u00aeWe say that all of these values are transient. This is the kind of query you need to write for a simple reporting screen, showing all user names and their home addresses. You have now seen path expressions several times: using dot-notation, you can reference properties of an entity, such as User#username wi\u00aeth u.username. For a nested embedded property, for example, you can write the path u.homeAddress.city.zipcode.These are single-valued path expressions, because they don't terminate in a mapped collection property A more convenient alternative than Object[] or Tuple, especially for report queries, is dynamic instantiation in projections, which is next",
  "page428": "Let's say you have a reporting screen in your application where you need to show. some data in a list. You want to show all auction items and when each auction ends. You don't want to load managed Item entity instances, because no data will be modified: you only readdat\u00aea. First, write a class called ItemSummary with a constructor that takes a Long for the item's identifier, a String for the item's name, and a Date for the item's auction end timestamp: We sometimes call these kinds of classes data transfer objects (DTOs), because their main purpose is to shuttle data around in the application. The ItemSummary class isn't mapped to the database, and you can add arbitrary methods (getter, setter, printing of values) as needed by your re\u00aeporting user interface. Hibernate can directly return instances of Item Summary from a query with the new keyword in JPQL and the construct() method in criteria We sometimes call these kinds of classes data transfer objects (DTOs), because their main purpose is to shuttle data around in the application. The It\u00aeemSummary class isn't mapped to the database, and you can add arbitrary methods (getter, setter, printing of values) as needed by your reporting user interface. Hibernate can directly return instances of ItemSummary from a query with thenew keyword in JPQL and the construct() method in criteria ",
  "page429": "your DTO class doesn't have the right constructor, and you want to populate it from a query result through setter methods or fields, apply a ResultTransformer, as shown in in section 16.1.3. Later, we have more examples of aggregation and grouping. Next, we're going to\u00aelook at an issue with projection that is frequently confusing for many engineers: handling duplicates When you create a projection in a query, the elements of the result aren't guaranteed. to be unique. For example, item names aren't unique, so the following query may return the same name more than once: It's difficult to see how it could be meaningful to have two identical rows in a query result, so if you think duplicates are likely, you normally apply the DISTINCT keywor\u00aed or distinct() method This eliminates duplicates from the returned list of Item descriptions and translates. directly into the SQL DISTINCT operator. The filtering occurs at the database level. Later in this chapter, we show you that this isn't always the case. Earlier, you saw function calls in restrict\u00aeions, in the WHERE clause. You can also call functions in projections, to modify the returned data within the query If an Item doesn't have a buyNowPrice, a BigDecimal for the value zero is returned instead of null. Similar to coalesce() but more powerful are case/when expressions. The following query\u00aereturns the username of each User and an additional String with either \"Germany\", \"Switzerland For the built-in standard functions, refer to the tables in the previous section. Unlike function calls in restrictions, Hibernate won't pass on an unknown function call in a projection to the database as a plain direct SQL function call. Any function you'd like to call in a projection must be known to Hibernate and/or invoked with the special function() operation of JPQL.",
  "page430": " This projection returns the name of each auction Item and the number of days between item creation and auction end, calling the SQL datediff() function of the H2 database If instead you want to call a function directly, you give Hibernate the function's return type, so it c\u00aean parse the query. You add functions for invocation in projections by extending your configured org.hibernate.Dialect. The datediff() function is already registered for you in the H2 dialect. Then, you can either call it as shown with function(), which works in other JPA providers when accessing H2, or directly as datediff(), which most likely only works in Hibernate. Check the source code of the dialect for your database; you'll probably find many other proprietary SQL functions. al\u00aeready registered there. Furthermore, you can add SQL functions programmatically on boot to Hibernate by calling the method applySqlFunction() on a Hibernate MetadataBuilder See the Javadoc of SQLFunction and its subclasses for more information.  Next, we look at aggregation functions, which are the most usefu\u00ael functions in reporting queries. Reporting queries take advantage of the database's ability to perform efficient grouping and aggregation of data. For example, a typical report query would retrieve the highest initial item price in a given category. This calculation can occur in the database\u00ae, and you don't have to load many Item entity instances into memory. The aggregation functions standardized in JPA are count(), min(), max(), sum(), and avg(). This query returns a BigDecimal, because the amount property is of type BigDecimal. The sum() function also recognizes the BigInteger property type and returns Long for all other numeric property types",
  "page431": "When you call an aggregation function in the SELECT clause, without specifying any grouping in a GROUP BY clause, you collapse the results down to a single row, containing. the aggregated value(s). This means (in theabsence of a GROUP BY clause) any SELECT. clause that contains a\u00aen aggregation function must contain only aggregation function for more advanced statistics and for reporting, you need to be able to perform. grouping, which is up next JPA standardizes several features of SQL that are most commonly used for reporting although they're also used for other things. In reporting queries, you write the SELECT. clause for projection and the GROUP BY and HAVING clauses for aggregation. Just like in SQL, any property or alias that appears outside of an aggr\u00aeegate function. in the SELECT clause must also appear in the GROUP BY clause in this example, the u.lastname property isn't inside an aggregation function, so projected data has to be \"grouped by\" u.lastname. You also don't need to specify the property you want to count; the count(u)expressi\u00aeon is automatically translated into. count(u.id) When grouping, you may run into a Hibernate limitation. The following query is specification compliant but not properly handled in Hibernate The JPA specification allows grouping by an entity path expression, group by i. But Hibernate doesn't automatically\u00aeexpand the properties of Item in the generated SQL GROUP BY clause, which then doesn't match the SELECT clause. You have to expand the The JPA specification allows grouping by an entity path expression, group by i. But Hibernate doesn't automatically expand the properties of Item in the generated SQL GROUP BY clause, which then doesn't match the SELECT clause. You have to expand the fix",
  "page432": "Join operations combine data in two (or more) relations. Joining data in a query also enables you to fetch several associated instances and collections in a single query: for example, to load an Item and all its bids in one round trip to the database. We now show you how basic jo\u00aein operations work and how to use them to write such dynamic fetching strategies. Let's first look at how joins work in SQL queries, without JPA Let's start with the example we already mentioned: joining the data in the ITEM and BID tables, as shown in figure 15.1. The database contains three items: the first has three bids, the second has one bid, and the third has no bids. Note that we don't show. all columns; hence the dotted lines. What most people think of when they he\u00aear the word join in the context of SQL databases is an inner join. An inner join is the most important of several types of joins and the easiest to understand. Consider the SQL statement and result in figure 15.2. This SQL statement contains an ANSI-style inner join in the FROM clause. If you join the ITEM and\u00aeBID tables with an inner join, with the condition that the ID of an ITEM row must match the ITEM_ID value of a BID row, you get items combined with their bids in the result. Note thatthe result of this operation contains only items that have bids",
  "page433": "You can think of a join as working as follows: first you take a product of the two. tables, by taking all possible combinations of ITEM rows with BID rows. Second, you filter these combined rows with a join condition: the expression in the ON clause. (Any good database engine has\u00aemuch more sophisticated algorithms to evaluate a join; it usually doesn't build a memory-consuming product and then filter out rows.) The join condition is a Boolean expression that evaluates to true if the combined row is to be included in the result. It's crucial to understand that the join condition can be any expression that evaluates to true. You can join data in arbitrary ways; you aren't limited to comparisons of identifier values. For example, the join condit\u00aeion on i.ID b.ITEM_ID and amount > 100 would only include rows from the BID table that also have an AMOUNT greater than 100. that a BID has a reference to an ITEM row. This doesn't mean you can only join by comparing primary and foreign key columns. Key columns are of course the most common operands in a\u00aejoin condition, because you often want to retrieve related information together. If you want all items, not just the ones which have related bids, and NULL instead of bid data when there is no corresponding bid, then you write a (left) outer join",
  "page434": " In case of the left outer join, each row inthe (left) ITEM table that never satisfies the join condition is also included in the result, with NULL returned for all columns of BID. Right outer joins are rarely used; developers always think from left to right and put the \"dri\u00aeving\" table of a join operation first. In figure 15.4, you can see the same result with BID instead of ITEM as the driving table, and a right outer join. In SQL, you usually specify the join condition explicitly. Unfortunately, it isn't possible. to use the name of a foreign key constraint to specify how two tables are to be joined: select * from ITEM join BID on FK_BID_ITEM_ID doesn't work. You specify the join condition in the ON clause for an ANSI-style join or in the WH\u00aeERE. clause for a so-called theta-style join: select * from ITEM, BID b where i.ID b.ITEM_ID. This is an inner join; here you see that a product is created first in the FROM clause. We now discuss JPA join options. Remember thatHibernate eventually translates all queries into SQL, so even if the syntax is slig\u00aehtly different, you should always refer to the illustrations shown in this section and verify that you understand what the resulting SQL and result set looks like JPA provides four ways of expressing (inner and outer) joins in queries: An implicit association join with path expressions. An ordinary join in\u00aethe FROM clause with the join operator A fetch join in the FROM clause with the join operator and the fetch keyword for eager fetching A theta-style join in the WHERE clause. Let's start with implicit association joins. In JPA queries, you don't have to specify a join condition explicitly. Rather, you specify the name of a mapped Java class association. This is the same feature we'd prefer to have in SQL: a join condition expressed with a foreign key constraint name. Because you've mapped most, if not all, foreign key relationships of your database schema, you can use the names of these mapped associations in the query language. This is syntactical sugar, but it's convenient",
  "page435": " For example, the Bid entity class has a mapped many-to-one association named item, with the Item entity class. If you refer to this association in a query, Hibernate has enough information to deduce the join expression with a key column comparison. This helps make queries\u00aeless verbose and more readable. Earlier in this chapter, we showed you property path expressions, using dot-notation: single-valued path expressions such as user.homeAddress.zipcode and collectionvalued path expressions such as item.bids. You can create a path expression in an implicit inner join query The path b.item.name creates an implicit join on the many-to-one associations from Bid to Item the name of this association is item. Hibernate knows that you mapped this association w\u00aeith the ITEM_ID foreign key in the BID table and generates the SQL join condition accordingly. Implicit joins are always directed along many-to-one or one-to-one associations, never through a collection-valued association (you can't write item.bids.amount). This query joins rows from the BID, the ITEM, and\u00aethe USER tables. We frown on the use of this syntactic sugar for more complex queries. SQL joins are important, and especially when optimizing queries, you need to be able to see at a glance exactly how many of them there are How many joins are required to express such a query in SQL? Even if you get the an\u00aeswer right, it takes more than a few seconds to figure out. The answer is two. The generated SQL looks something like this: Alternatively, instead of joins with such complex path expressions, you can write ordinary joins explicitly in the FROM clause",
  "page436": "JPA differentiates between purposes you may have for joining. Suppose you're querying items; there are two possible reasons you may be interested in joining them with bids. You may want to limit the items returned by the query based on some criterion to apply to their bids.\u00aeFor example, you may want all items that have a bid of more than 100, which requires an inner join. Here, you aren't interested in items that have no bids. On the other hand, you may be primarily interested in the items but may want to execute an outer join just because you want to retrieve all bids for the queried items in a single SQL statement, something we called eager join fetching earlier. Remember that you prefer to map all associations lazily by default, so an eager fetch qu\u00aeery will override the default fetching strategy at runtime for a particular use case. Let's first write some queries that use joins for the purpose of restriction. If you want to retrieve Item instances and restrict the result to items that have bids with a certain amount, you have to assign an alias to a\u00aejoined association. Then yourefer to the alias in a WHERE clause to restrict the data you want This query assigns the alias b to the collection bids and limits the returned Item instances to those with Bid#amount greater than 100. So far, you've only written inner joins. Outer joins are mostly used for\u00aedynamic fetching, which we discuss soon. Sometimes, you want to write a simple query with an outer join without applying a dynamic fetching strategy. For example, the following query and retrieves items that have no bids, and items with bids of a minimum bid amount:",
  "page437": "This query returns ordered pairs of Item and Bid, in a List<Object[]>. The first thing that is new in this query is the LEFT keyword and JoinType.LEFT in the criteria query. Optionally you can write LEFT OUTER JOIN and RIGHT OUTER JOIN in JPQL, but we usually prefer the short fo\u00aerm. The second change is the additional join condition following the ON keyword. If instead you placethe amount > 100 expression into the WHERE clause, you restrict the result to Item instances that have bids. This isn't what you want here: you want to retrieve items and bids, and even items that don't have bids. If an item has bids, the bid amount must be greater than 100. By adding an additional join condition in the FROM clause, you can restrict the Bid instances and still re\u00aetrieve all Item instances, whether they have bids or not The SQL query will always contain the implied join condition of the mapped association, i.ID b.ITEM_ID. You can only append additional expressions to the join condition. JPA and Hibernate don't support arbitrary outer joins without a mapped en\u00aetity association or collection. Hibernate has a proprietary WITH keyword, it's the same as the ON keyword in JPQL. You may see it in older code examples, because JPA only recently standardized ON. You can write a query returning the same data with a right outer join, switching the driving table This ri\u00aeght outer join query is more important than you may think. Earlier in this book, we told you to avoid mapping a persistent collection whenever possible. If you don't have a one-to-many Item#bids collection, you need a right outer join to retrieve all Items and their Bid instances. You drive the query from the \"other\" side: the many-toone Bid#item. ",
  "page438": "All the queries you saw in the previous sections have one thing in common: the returned Item instances have a collection named bids. This @OneToMany collection, if mapped as FetchType.LAZY (the default for collections), isn't initialized, and an additional SQL statemen\u00aet is triggered as soon as you access it. The same is true for all single-valued associations, like the @ManyToOne association seller of each Item. By default, Hibernate generates a proxy and loads the associated User instance lazily and only on demand. What options do you have to change this behavior? First, you can change the fetch. plan in your mapping metadata and declare a collection or single-valued association. as FetchType.EAGER. Hibernate then executes the necessary SQL to g\u00aeuarantee that. the desired network of instances is loaded at all times. This also means a single JPA. query may result in several SQL operations! As an example, the simple query selects I from Item i may trigger additional SQL statements to load the bids of each Item, the seller of each Item, and so on. In cha\u00aepter 12, we made the case for a lazy global fetch plan in mapping metadata, where you shouldn't have FetchType.EAGER on association and collection mappings. Then, for a particular use case in your application, you dynamically override the lazy fetch plan and write a query that fetches the data you need a\u00aes efficiently as possible. For example, there is no reason you need several SQL statements to fetch all Item instances and to initialize their bids collections, or to retrieve the seller for each Item. You can do this at the same time, in a single SQL statement, with a join operation. Eager fetching of associated data is possible with the FETCH keyword in JPQL and the fetch() method in the criteria query API",
  "page439": "You've already seen the SQL query this produces and the result set in figure 15.3. This query returns a List<Item>; each Item instance has its bids collection fully initialized. This is different than the ordered pairs returned by the queries in the previous section! Be ca\u00aereful youmay not expect the duplicate results from the previous query: Make sure you understand why these duplicates appear in the result List. Verify the number of Item \"rows\" in the result set, as shown in figure 15.3. Hibernate preserves the rows as list elements; you may need the correct row count to make rendering a report table in the user interface easier. You can filter out duplicate Item instances by passing the result List through a LinkedHashSet, which doesn't al\u00aelow duplicate elements but preserves the order of elements. Alternatively, Hibernate can remove the duplicate elements with the DISTINCT operation and distinct() criteria method:Understand that in this case the DISTINCT operation does not execute in the database. There will be no DISTINCT keyword in the SQL sta\u00aetement. Conceptually, you can't remove the duplicate rows at the SQL ResultSet level. Hibernate performs deduplication in memory, just as you would manually with a LinkedHashSet This query returns a List<Item>, and each Item has its bids collection initialized. The seller of each Item is loaded as well.\u00aeFinally, the bidder of each Bid instance is loaded. You can do this in one SQL query by joining rows of the ITEM, BID, and USERS tables. If you write JOIN FETCH without LEFT, you get eager loading with an inner join (also if you use INNER JOIN FETCH) ",
  "page440": "An eager inner join fetch makes sense if there must be a fetched value: an Item must have a seller, and a Bid must have abidder. There are limits to how many associations you should eagerly load in one query and how much data you should fetch in one round trip. Consider the foll\u00aeowing query, which initializes the Item bids and Item images collections: This is a bad query, because it creates a Cartesian product of bids and images, with a potentially extremely large result set. We covered this issue in section 12.2.2. To summarize, eager dynamic fetching in queries has the following caveats: Never assign an alias to any fetch-joined association or collection for further restriction or projection. The query left join fetch i.bids b where b.amount ... is invalid. Yo\u00aeu can't say, \"Load the Item instances and initialize their bids collections, but only with Bid instances that have a certain amount.\" You can assign an alias to a fetch-joined association for further fetching: for example, retrieving the bidder of each Bid: left join fetch i.bids b join fet\u00aech b.bidder. You shouldn't fetch more than one collection; otherwise, you create a Cartesian product. You can fetch as many single-valued associations as you like without creating a product Queries ignore any fetching strategy you've defined in mapping metadata with @org.hibernate.annotations.Fetch.\u00aeFor example, mapping the bids collection with org.hibernate.annotations.FetchMode.JOIN has no effect on the queries you write. The dynamic fetching strategy of your query ignores the global fetching strategy. On the other hand, Hibernate doesn't ignore the mapped fetch plan: Hibernate always considers a FetchType.EAGER, and you may see several additional SQL statements when you execute your query.",
  "page441": "If you eager-fetch a collection, the List returned by Hibernate preserves the number of rows in the SQL result as duplicate references. You can filter out the duplicates in-memory either manually with a LinkedHashSet or with the special DISTINCT operation in the query. There is o\u00aene more issue to be aware of, and it deserves some special attention. You can't paginate a result set at the database level if you eagerly fetch a collection. For example, for the query select i from Item i fetch i.bids, how should Query#setFirstResult(21) and Query#setMaxResults(10) be handled? Clearly, you expect to get only 10 items, starting with item 21. But you also want to load all bids of each Item eagerly. Therefore, the database can't do the paging operatio\u00aen; you can't limit the SQL result to 10 arbitrary rows. Hibernate will execute paging in-memory if a collection is eagerly fetched in a query. This means all Item instances will be loaded into memory, each with the bids collection fully initialized. Hibernate then gives you the requested page of items: for\u00aeexample, only items 21 to 30. Not all items might fit into memory, and you probably expected the paging to occur in the database before it transmitted the result to the application! Therefore, Hibernate will log a warning message if your query contains fetch [collectionPath] and you call setFirstResult() or\u00aesetMaxResults(). We don't recommend the use of fetch [collectionPath] with setMaxResults() or setFirstResult() options. Usually there is an easier query you can write to get the data you want to render and we don't expect that you load data page by page to modify it. For example, if you want to showseveral pages of items and for each item the number of bids, writea report query ",
  "page442": "In traditional SQL, a theta-style join is a Cartesian product together with a join condition in the WHERE clause, which is appliedon the product to restrict the result. In JP queries, the theta-style syntax is useful when your join condition isn't a foreign key relationship\u00aemapped to a class association. For example, suppose you store the User's name in log records instead of mapping an association from LogRecord to User.The classes don't know anything about each other, because they aren't associated. You can then find all the Users and their Log Records with the following theta-style join The join condition here is a comparison of username, present as an attribute in both. classes. If both rows have the same username, they're joined (wit\u00aeh an inner join) in the result. The query result consists of ordered pairs You probably won't need to use the theta-style joins often. Note that it's currently not. possible in JPA to outer join two tables that don't have a mapped association thetastyle joins are inner joins. Another more\u00aecommon case for theta-style joins is comparisons of primary key or foreign key values to either query parameters or other primary or foreign key values in the WHERE clause: This query returns pairs of Item and Bid instances, where the bidder is also the seller. This is an important query in CaveatEmptor becau\u00aese it lets you detect people who bid on their own items. You probably should translate this query into a database constraint and not allow such a Bid instance to be stored.  The previous query also has an interesting comparison expression: i.seller b.bidder. This is an identifier comparison, our next topic",
  "page443": "In this query, i.seller refers to the SELLER_ID foreign key column of the ITEM table, referencing the USERS table. The alias u refers to the primary key of the USERS table. (on the ID column). Hence, this query has a theta-style join and is equivalent to the easier, readable alte\u00aernative A path expression ending with id is special in Hibernate: the id name always refers to the identifier property of an entity. It doesn't matter what the actual name of the property annotated with @Id is you can always reach it with entityAlias.id. That's why we recommend you always name the identifier property of yourentity classes id, to avoid confusion in queries. Note that this isn't a requirement or standardized in JPA; only Hibernate treats an id path element spe\u00aecially. You may also want to compare a key value to a query parameter, perhaps to find all Items for a given seller (a User) The first query pair uses an implicit table join; the second has no joins at all!  This completes our discussion of queries that involve joins. Our final topic is nesting selects within\u00aeselects: subselect Sub selects are an important and powerful feature of SQL. Asubselect is a select query embedded in another query, usually in the SELECT, FROM, or WHERE clause. JPA supports subqueries in the WHERE clause. Subselects in theFROM clause aren't supported because the query languages doesn\\u00aeu2019t have transitive closure. The result of a query may not be usable for further selection in a FROM clause. The query language also doesn't support subselects in the SELECT clause, but you map can subselects to derived properties with @org.hibernate.annotations.Formula, as shown in section 5.1.3. Subselects can be either correlated with the rest of the query or uncorrelated.",
  "page444": "The result of a subquery may contain either a single row or multiple rows. Typically, subqueries that return single rows perform aggregation. The following subquery returns the total number of items sold by a user; the outer query returns all users who have sold more than one ite\u00aem: The subquery in this example returns the maximum bid amount in the entire system; the outer query returns all bids whose amount is within one (U.S. dollar, Euro, and so on) of that amount. Note that in both cases, parentheses enclose the subquery in JPQL. This is always required Uncorrelated subqueries are harmless, and there is no reason not to use them when convenient. You can always rewrite them as two queries, because they don't. reference each other. You should think more care\u00aefully about the performance impact. of correlated subqueries. On a mature database, the performance cost of a simple correlated subquery is similar to the cost of a join. But it isn't necessarily possible to rewrite a correlated subquery using several separate queries.  If a subquery returns multiple rows\u00ae, you combine it with quantification The following quantifiers are standardized: ALL The expression evaluates to true if the comparison is true for all values in the result of the subquery. It evaluates to false if a single value of the subquery result fails the comparison test. ANY The expression evaluates\u00aeto true if the comparison is true for some (any) value in the result of the subquery. If the subquery result is empty or no value satisfies the comparison, it evaluates to false. The keyword SOME is a synonym for ANY. EXISTS Evaluates to true if the result of the subquery consists of one or more values",
  "page445": "This chapter explains query options that you may consider optional or advanced: transforming query results, filtering collections,and the Hibernate criteria query API. First, we discuss Hibernate's ResultTransformer API, with which you can apply a result transformer to a que\u00aery result to filter or marshal the result with your own code instead of Hibernate'sdefault behavior. In previous chapters, we always advised you to be careful when mapping collections, because it's rarely worth the effort. In this chapter, we introduce collection filters, a native Hibernate feature that makes persistent collections more valuable. Finally, we look at another proprietary Hibernate feature, the org.hibernate.Criteria query API, and some situations when you mi\u00aeght prefer it to the standard JPA query-by-criteria. Let's start with the transformation of query results. Transforming query results You can apply a result transformer to a query result so that you can filter or marshal the result with your own procedure instead of the Hibernate default behavior. Hibern\u00aeates default behavior provides a set of default transformers that you can replace and/or customize. The result you're going to transform is that of a simple query, but you need to access the native Hibernate API org.hibernate.Query through the Session, as shown in the following listing Each object array\u00aeis a \"row\" of the query result. Each element of that tuple can be accessed by index: here index 0 is a Long, index 1 a String, and index 2 a Date. The first result transformer we introduce instead returns a List of Lists",
  "page446": "In section 15.3.2, we showed how a query can return instances of a JavaBean dynamically by calling the ItemSummary constructor. In JPQL, you achieve this with the new operator. For criteria queries, you use the construct() method. The ItemSummary class must have a constructor tha\u00aet matches the projected query result. Alternatively, if your JavaBean doesn't have the right constructor, you can still instantiate and populate its values through setters and/or fields with the AliasToBeanResultTransformer.  You create the transformer with the JavaBean class you want to instantiate, here ItemSummary. Hibernate requires that this class either has no constructor or a public nonargument constructor. When transforming the query result, Hibernate looks for\u00aesetter methods and fields with the same names as the aliases in the query. The ItemSummary class must either have the fields itemId, name, unauctioned, or the setter methods setItemId(), setName(), and setAuctionEnd(). The fields or setter method parameters must be of the right type. If you have fields that map\u00aeto some query aliases and setter methods for the rest, that's fine too.  You should also know how to write your own ResultTransformer when none of the built-in ones suits you The built-in transformers in Hibernate aren't sophisticated; there isn't much difference between result tuples r\u00aeepresented as lists, maps, or object arrays.  Next, we show you how to implement a ResultTransformer. Let's assume that you want a List<ItemSummary> returned from the query shown in listing 16.1, but you can't let Hibernate create an instance of ItemSummary through reflection on a constructor. Maybe your ItemSummary class is predefined and doesn't have the right constructor, fields, and setter methods. Instead, you have an ItemSummaryFactory to produce instances of ItemSummary",
  "page447": "For each result \"row,\" an Object[] tuple must be transformed into the desired result value for that row. Here you access each projection element by index in the tuple array and then call the ItemSummaryFactory to produce the query result value. Hibernate passes the meth\u00aeod the aliases found in the query, for each tuple element. You don't need the aliases in this transformer, though. C You can wrap or modify the result list after transforming the tuples. Here you make. the returned List unmodifiable: ideal for a reporting screen where nothing should change the data. As you can see in the example, you transform query results in two steps: first you customize how to convert each \"row\" or tuple of the query result to whatever value you desire.\u00aeThen you work on the entire List of these values, wrapping or converting again. Next, we discuss another convenient Hibernate feature (where JPA doesn't have an equivalent): collection filters. In chapter 7, you saw reasons you should (or rather, shouldn't) map a collection in your Java domain model\u00ae. The main benefit of a collection mapping is easier access to data: you can call item.getImages() or item.getBids() to access all images and bids associated with an Item. You don't have to write a JPQL or criteria query; Hibernate will execute the query for you when you start iterating through the colle\u00aection elements. The most obvious problem with this automatic data access is that Hibernate will always write the same query, retrieving all images or bids for an Item. You can customize the order of collection elements, but even that is a static mapping. What would you do to render two lists of bids for an Item, in ascending and descending order by creation date? ",
  "page448": " Instead, you can use a Hibernate proprietary feature, collection filters, that makes writing these queries easier, using the mapped collection. Let's say you have a persistent Item instance in memory, probably loaded with the EntityManager API. You want to list all bids mad\u00aee for this Item but further restrict the result to bids made by a particular User. You also want the list sorted in descending order by Bid#amount.The session.createFilter() method accepts a persistent collection and a JPQL query fragment. This query fragment doesn't require a select or from clause; here it only has a restriction with the where clause and an order by clause. The alias this always refers to elements of the collection, here Bid instances. The filter created is an ordina\u00aery org.hibernate.Query, prepared with a bound parameter and executed with list(), as usual. Hibernate doesn't execute collection filters in memory. The Item bids collection may be uninitialized when you call the filter and, and if so, remains uninitialized. Furthermore, filters don't apply to transie\u00aent collections or query results. You may only apply them to a mapped persistent collection currently referenced by an entity instance managed by the persistence context. The term filter is somewhat misleading, because the result of filtering is a completely new and different collection; the original collectio\u00aen isn't touched. To the great surprise of everyone, including the designer of this feature, even trivial filters turn out to be useful. For example, you can use an empty query to paginate collection elements:",
  "page449": "Here, Hibernate executes the query, loading the collection elements and limiting the returned rows to two, starting with row zero of the result. Usually, you'd use an order by with paginated queries. You don't need a from clause in a collection filter, but you can have\u00aeone if that's your style. A collection filter doesn't even need to return elements of the collection being filtered. This next filter returns any Item sold by any of the bidders All this is a lot of fun, but the most important reason for the existence of collection filters is to allow your application to retrieve collection elements without initializing the entire collection. For large collections, this is important to achieve acceptable performance. The following query r\u00aeetrieves all bids made for the Item with an amount greater or equal to 100: Again, this doesn't initialize the Item#bids collection but returns a new collection. Before JPA 2, query-by-criteria was only available as a proprietary Hibernate API. Today, the standardized JPA interfaces are equally as powerfu\u00ael as the old org.hibernate.Criteria API, so you'll rarely need it. But several features are still only available in the Hibernate API, such as query-by-example and embedding of arbitrary SQL fragments. In the following section, you find a short overview of the org.hibernate .Criteria API and\u00aesome of its unique options Using the org.hibernate.Criteria and org.hibernate.Example interfaces, you can build queries programmatically by creating and combining org.hibernate.criterion.* instances. You see how to use these APIs and how to express selection, restriction, joins, and projection. We assume that you've read the previous chapter and that you know how these operations are translated into SQL.",
  "page450": ". All query examples shown here have an equivalent JPQL or JPA criteria example in the previous chapter, so you can easily flip back andforth if you need to compare all three APIs. Let's start with some basic selection examples. When you're ready to execute the query,\u00ae\"attach\" it to a Session with getExecutableCriteria(). Note that this is a unique feature of the Hibernate criteria API. With JPA, you always need at least an EntityManagerFactory to get a CriteriaBuilder. You can declare the order of the results, equivalent to an order by clause in JPQL. The following query loads all User instances sorted in ascending order by first and last name: In this example, the code is written in the fluent style (using method chaining); method's\u00aesuch as add Order() return the original org.hibernate.Criteria.  Next, we look at restricting the selected records The Restrictions interface is the factory for individual Criterion you can add to the Criteria. Attributes are addressed with simple strings, here Item#name with \"name\". You can also match subs\u00aetrings, similar to the like operator in JPQL. The following query loads all User instances with username starting with \"j\" or \"J\" A unique feature of the Hibernate Criteria API is the ability to write plain SQL fragments in restrictions. This query loads all User instances with a use\u00aername shorter than eight characters Hibernate sends the SQL fragment to the database as is. You need the {alias} placeholder to prefix any table alias in the final SQL; it always refers to the table the root entity is mapped to (USERS, in this case). You also apply a position parameter (named parameters aren't supported by this API) and specify its type as StandardBasicTypes.INTEGER",
  "page451": "The result of this query is a List of Object[], one array for each tuple. Each array contains a Long (or whatever the type of the user's identifier is), a String, and an Address. Just as with restrictions, you can add arbitrary SQL expressions and function calls to projecti\u00aeons This query returns a List of Strings, where strings have the form \"[Item name]:[Auction end date]\". The second parameter for the projection is the name of the alias(es) you used in the query: Hibernate needs this to read the value of the ResultSet. The type of each projected element/alias is also needed: here, StandardBasicTypes.STRING. Hibernate supports grouping and aggregation. This query counts users' last names This query returns all Bid instances of an\u00aey Item sold by User \"johndoe\" that doesn't have a buyNowPrice. The first inner join of the Bid#item association is made with createCriteria(\"item\") on the root Criteria of the Bid. This nested Criteria now represents the association path, on which anotherinner join is made with createCrit\u00aeeria(\"seller\"). Further restrictions are placed on each join Criteria; they will be combined with logical and in the where clause of the final SQL query. This query returns all Item instances, loads the Item#bids collection with an outer join, and loads Bid#bidder with an inner join. The Item#seller is also\u00aeloaded: because it can't be null, it doesn't matter whether an inner or outer join is used. As always, don't fetch several collections in one query, or you'll create a Cartesian products (see section 15.4.5).  Next, you see that subqueries with criteria also work with nested Criteria instances.",
  "page452": "The DetachedCriteria is a query that returns the number of items sold restricted by a given User. The restriction relies on the alias u, so this is a correlated subquery. The \"outer\" query then embeds the DetachedCriteria and provides the alias u. Note that the subquery\u00aeis theright operand of the lt() (less than) operation, which translates into 1 < ([Result of count query]) in SQL. Again, the position of the operands dictates that the comparison is based on geAll() (greater or equal than all) to find the bids with \"less or equal than 10\" amount. So far, there are a few good reasons to use the old org.hibernate.Criteria API. You really should use the standardized JPA query languages in new applications, though. The most interesting features of\u00aethe old proprietary API we've shown are embedded SQL expressions in restrictions and projections. Another Hibernate-only feature you may find interesting is query-by-example The idea behind example queries is that you provide an example entity instance, and Hibernate loads all entity instances that \"\u00aelook like the example.\" This can be convenient if you have a complex search screen in your user interface, because you don't have to write extra classes to hold the enteredsearch terms. Let's say you have a search form in your application where you can search for User instances by last name. Y\u00aeou can bind the form field for \"last name\" directly to the User#lastname property and then tell Hibernate to load \"similar\" User instances",
  "page453": "Create an \"empty\" instance of User as a template for your search, and set the property values you're looking for: people with the last name \"Doe\" Create an instance of Example with the template. This APIallows you to fine-tune the search. You want the cas\u00aee of the last name to be ignored, and a substring search, so \"Doe\", \"Dex\", or \"Doe Y\" will match. D The User class has a Boolean property called activated. As a primitive, it can't be null, and its default value is false, so Hibernate would include it in the search and only return users that aren't activated. You want all users, so tell Hibernate to ignore that property. E The Example is added to a Criteria as a restriction. Because you've writ\u00aeten the User entity class following JavaBean rules, binding it to a UI form should be trivial. It has regular getter and setter methods, and you can create an \"empty\" instance with the public no-argument constructor (remember our discussion of constructor design in section 3.2.3.) One obvious d\u00aeisadvantage of the Example API is that any string-matching options, such as ignoreCase() and enableLike(), apply to all string-valued properties of the template. If you searched for both last name and first name, both would be case insensitive substring matches. nsensitive substring matches. By default, all\u00aenon-null valued properties of the given entity template are added to the restriction of the example query. As shown in the last code snippet, you can manually exclude properties of the entity template by name with excludeProperty",
  "page454": "Other exclusion options are exclusion of zero-valued properties (such as int or long) with excludeZeroes() and disabling exclusion altogether with excludeNone(). If no properties are excluded, any null property of the template is added to the restriction in the SQL query with an\u00aeis null check. If you need more control over exclusion and inclusion of properties, you can extend Example and write your own PropertySelector: After adding an Example restriction to a Criteria, you can add further restrictions to the query. Alternatively, you can add multiple example restrictions to a single query. The following query returns all Item instances with names starting with \"B\" or \"b\" and a seller matching a User example: You used the ResultTransformer AP\u00aeI to write custom code to process a query result, returning a list of lists and a list of maps, and mapping aliases to bean properties. We covered Hibernate's collection-filtering interfaces as well as making better use of mapped persistent collections. You explored the older Hibernate Criteria query faci\u00aelity and when you might use it instead of the standardized criteria queries in JPA. We covered all the relational and Hibernate goodies using this API: selection and ordering, restriction, projection and aggregation, joins, subselects, and example queries.",
  "page455": "In this chapter, we cover customizing and embedding SQL in a Hibernate application. SQL was created in the 1970s, but ANSI didn't standardized it until 1986. Although each update of the SQL standard has seen new (and many controversial) features, every DBMS product that supp\u00aeorts SQL does so in its own unique way. The burden of portability is again on the database application developers. This is where Hibernate helps: its built-in query languages produce SQL that depends on the configured database dialect. Dialects also help produce all other automatically generated SQL (for example, when Hibernate has to retrieve a collection on demand). With a simple dialect switch, you can run your application on a different DBMS. Hibernate generates all SQL statements for\u00aeyou, for all create, read, update, and delete (CRUD) operations. Sometimes, though, you need more control than Hibernate and theJava Persistence API provide: you need to work at a lower level of abstraction. With Hibernate, you can write your own SQL statements: Fall back to the JDBC API, and work directly wit\u00aehthe Connection, PreparedStatement, and ResultSet interfaces. Hibernate provides the Connection, so you don't have to maintain a separate connection pool, and your SQL statements execute within the same (current) transaction. Write plain SQL SELECT statements, and either embed them wit\u00aehin your Java code or externalize them (in XML files or annotations) as named queries. You execute these SQLqueries with the Java Persistence API, just like a regular JPQL query. Hibernate can then transform the query result according to your mapping. This also works with stored procedure calls.",
  "page456": "Replace SQL statements generated by Hibernate with your own hand-written SQL. For example, when Hibernate loads an entity instance with em.find() or loads a collection on-demand, your own SQL query can perform the load. You can also write your own Data Manipulation Language (DML)\u00aestatements, such as UPDATE, INSERT, and DELETE. You might even call a stored procedure to preform a CRUD operation. You can replace all SQL statements automatically generated by Hibernate with custom statements. We start with JDBC fallback usage and then discuss Hibernate's automatic result-mapping capabilities. Then, we show you how to override queries and DML statements in Hibernate. Last, we discuss integration with stored database procedures. Sometimes you want Hibernate to\u00aeget out of the way and directly access the database through the JDBC API. To do so, you need a java.sql.Connection interface to write and execute your own PreparedStatement and direct access to your statement ResultSet. Because Hibernate already knows how to obtain and close database connections, it can\u00aeprovide your application with a Connection and release it when you're done. This functionality is available with the org.hibernate.jdbc.Work API, acallbackstyle interface. You encapsulate your JDBC \"work\" by implementing this interface; Hibernate calls your implementation providing a Con\u00aenection. The following example executes an SQL SELECT and iterates through the ResultSet For this \"work,\" an item identifier is needed, enforced with the final field and the constructor paramet",
  "page457": "The execute() method is called by Hibernate with a JDBC Connection. You don't have to close the connection when you're done. D You have to close and release other resources you've obtained, though, such as the PreparedStatement and ResultSet In this case, Hibernate\u00aehas already enlisted the JDBC Connection it provides with the current system transaction. Your statements are committed when the system transaction is committed, and all operations, whether executed with the EntityManager or Session API, are part of the same unit of work. Alternatively, if you want to return a value from your JDBC \"work\" to the application, implement the interface org.hibernate.jdbc.ReturningWork. There are no limits on the JDBC operations you can perform\u00aein a Work implementation. Instead of a PreparedStatement, you may use a CallableStatement and execute a stored procedure in the database; you have full access to the JDBC API. For simple queries and working with a ResultSet, such as the one in the previous example, a more convenient alternative is available.\u00aeWhen you execute an SQL SELECT query with the JDBC API or execute a stored procedures that returns a ResultSet, you iterate through each row of this result set and retrieve the data you need. This is a labor-intensive task, and you end up duplicating the same lines of code repeatedly. When you execute an SQL\u00aeSELECT query with the JDBC API or execute a stored procedure that returns a ResultSet, you iterate through each row of this result set and retrieve the data you need. This is a labor-intensive task, and you end up duplicating the same lines of code repeatedly.",
  "page458": "The returned Item instances are in persistent state, managed by the current persistence context. The result is therefore the same as with the JPQL query select i from Item i. For this transformation, Hibernate reads the result set of the SQL query and tries to discover the colum\u00aen names andtypes as defined in your entity mapping metadata. If the column AUCTIONEND is returned, and it's mapped to the Item#auctionEnd property, Hibernate knows how to populate that property and returns fully loaded entity instances. Note that Hibernate expects the query to return all columns required to create an instance of Item, including all properties, embedded components, and foreign keycolumns. If Hibernate can't find a mapped column (by name) in the result set, an exc\u00aeeption is thrown. You may have to use aliases in SQL to return the same column names as defined in your entity mapping metadata. The interfaces javax.persistence.Query and org.hibernate.SQLQuery both support parameter binding. The following query returns only a single Itementity instance Although available in\u00aeHibernate for both APIs, the JPA specification doesn't consider named parameter binding for native queries portable. Therefore, some JPA providers may not support named parameters for native queries.  If your SQL query doesn't return the columns as mapped in your Java entity class, and you\u00aerewrite the query with aliases to rename columns in the result, you must create a result-set mapping",
  "page459": "The following query returns a List of managed Item entity instances. All columns of the ITEM table are included in the SQL projection, as required for the construction of an Item instance. But the query renames the NAME column to EXTENDED_NAME with an alias in the projection Hibe\u00aernate can no longer automatically match the result set fields to Item properties: the NAME column is missing from the result set. You therefore specify a \"result mapping\" with You map all fields of the result set to properties of the entity class. Even if only one field/column doesn't match the already mapped column name (here EXTENDED_NAME), all other columns and properties have to be mapped as well SQL result mappings in annotations are difficult to read and as usua\u00ael with JPAannotations, they only work when declared ona class, not in a package-info.java metadata file. We prefer externalizing such mappings into XML files. The following provides the same mapping: If both result-set mappings have the same name, the mapping declared in XML overrides the one defined with\u00aeannotations. You can also externalize the actual SQL query with @NamedNativeQuery or <namednative-query>, as shown in section 14.4. In all following examples, we keep the SQL statement embedded in the Java code, because this will make it easier for you to understand what the code does. But most of the\u00aetime, you'll see result-set mappings in the more succinct XML syntax.",
  "page460": "With the Hibernate API, you can perform the result-set mapping directly within the query through alias placeholders. When calling addEntity(), you provide an alias, here i. In the SQL string, you then let Hibernate generate the actual aliases in the projection with placeholders s\u00aeuch as {i.name} and {i.auctionEnd}, which refer to properties of the Item entity. No additional result-set mapping declaration is necessary; Hibernate generatesthe aliases in the SQL string and knows how to read the property values from the query ResultSet. This is much more convenient than the JPA result-set mapping option. Or, if you can't or don't want to modify the SQL statement, use add Root() and add Property() on the org.hibernate.SQLQuery to perform the mapping This is e\u00aeffectively an eager fetch of the association Item#seller. Hibernate knows that each row contains the fields for an Itemand a User entity instance, linked by the SELLER_ID. The duplicate columns in the result set would be i.ID and u.ID, which both have the same name. You've renamed them with an alias to IT\u00aeEM_ID and USER_ID, so you have to map how the result set is to be transformed As before, you have to map all fields of each entity result to column names, even if only two have different names as the original entity mapping. This query is much easier to map with the Hibernate API:",
  "page461": "Hibernate will add auto-generated unique aliases to the SQL statement for the {i.*} and {u.*} placeholders, so the query won't return duplicate column names.  You may have noticed the dot syntax in the previous JPA result mapping for the home Address embedded component in a\u00aeUser. Let's look at this special case again We've shown this dot syntax several times before when discussing embedded components: you reference the street property of home Address with homeAddress.street. For nested embedded components, you can write homeAddress.city.name if City isn't just a string but another embeddable class. Hibernate's SQL query API also supports the dot syntax in alias placeholders for component properties. Here are the same query and result-set\u00aemapping: The query (outer) joins the ITEM and BID tables. The projection returns all columns required to constructItem and Bid instances. The query renames duplicate columns such as ID with aliases, so field names are unique in the result. C Because of the renamed fields, you have to map each column to its res\u00aepective entity property. D Add a Fetch Return for the bids collection with the alias of the owning entity and map the key and element special properties to the foreign key column BID_ITEM_ID and the identifier of the Bid. Then the code maps each property of Bid to a field of the result set. Some fields are m\u00aeapped twice, as required by Hibernate for construction of the collection The number of rows in the result set is a product: one item has three bids, one item has one bid, and the last item has no bids, for a total of five rows in the result. F The first element of the result tuple is the Item instance; Hibernate initialized the bids collection",
  "page462": "The second element of the result tuple is each Bid. Alternatively, if you don't have to manually map the result because the field names returned by your SQL query match the already-mapped columns of the entities, you can let Hibernate insert aliases into your SQL statement w\u00aeith placeholders: Eager fetching of collections with dynamic SQL result mappings is only available with the Hibernate API; it's not standardized in JPA So far, you've seen SQL queries returning managed entity instances. You can also return transient instances of any class with the right constructor. The returned column types have to match the constructor parameter types; Hibernate would default to BigInteger for the ID column, so you map it to a Long with the class attribute. T\u00aehe Hibernate API gives you a choice. You can either use an existing result mapping for the query by name, or apply a result transformer, as you saw for JPQL queries in section 16.1: You can use an existing result mapping. C Alternatively, you can map the fields returned by the query as scalar values. With\u00aeout a result transformer, you'd get an Object[] for each result row. D Apply a built-in result transformer to turn the Object[] into instances of ItemSummary. As explained in section 15.3.2, Hibernate can use any class constructor with such a mapping. Instead of ItemSummary, you can construct Item instan\u00aeces. They will be in either transient or detached state, depending on whether you return and map an identifier value in your query.  You can also mix different kinds of result mappings or return scalar values directly.",
  "page463": "Hibernate usually generates CRUD SQL statements at startup. It caches the SQL statements internally for future use, thus avoiding any runtime costs of SQL generation forthe most common operations. You've seen how you can override the R in CRUD, sonow, let's do the same\u00aefor CUD. For each entity, you can define custom CUD SQL statements with the Hibernate annotations @SQLInsert, @SQLUpdate, and @SQLDelete,respectivelyYou need to be careful with the binding of arguments for SQL statements and parameter placeholders (the ? in the statements). For CUD customization, Hibernate onlysupports positional parameters. What is the right order for the parameters? There's an internal order to howHibernate binds arguments to SQL parameters for CUD statements. The e\u00aeasiest way tofigure out the right SQL statement and parameter order is to let Hibernate generatean example for you. Without any customSQL statements, enable DEBUG logging forthe org.hibernate.persister.entity category, and search the Hibernate startupoutput for lines like the following:These automatically gener\u00aeated SQL statements show the right parameter order, andHibernate always binds the values in that order. Copy the SQL statements you want tocustomize into annotations, and make the necessary changes. A special case is properties of an entity class mapped to another table with@SecondaryTable. The CUD customizat\u00aeion statements we've shown so far only refer tothe columns of the main entity table. Hibernate still executes automatically generatedSQL statements for insertion, updates, and deletion of rows in the secondary table(s).",
  "page464": "statements in an XML file. This also simplifies ad hoc testing, because you can copyand paste SQL statements between an XML file and your SQL database console. You've probably noticed that all the SQL examples in the previous sections weretrivial. In fact, none of the exampl\u00aees required a query written in SQL we could haveused JPQL in each case. To make the next example moreinteresting, we write a query that can't be expressed inJPQL, only in SQL. This is the mapping of the association a regular@ManyToOne of the PARENT_ID foreign key column:Categories form a tree. The root of the tree is a Category node without a parent. Thedatabase data for the example tree is in figure 17.2. You can also represent this data as a tree diagram, as shown in figure 17.3. Al\u00aeternatively, you can use a sequence of paths and the level of each node:Now, consider how your application loads Category instances. You may wantto findthe root Category of the tree. This is a trivial JPQL query:You can easily query for the categories in a particular level of the tree, such as all children of t\u00aehe root:This querywill only return direct children of the root: here, categories Two and Three. How can you load the entire tree (or a subtree) in one query? This isn't possible withJPQL, because it would require recursion: \"Load categories at this level, then all the children on thenext level, then\u00aeall the children of those, and so on.\" In SQL, you can writesuch a query, using a common table expression (CTE), a feature alsoknown as subquery factoring.",
  "page465": "It's a complex query, and we won't spend too much time on it here. To understand it,read the last SELECT, querying the CATEGORY_LINK view. Each row in that view represents a node in the tree. The view is declared here in the WITH() AS operation. TheCATEGORY_LINK view is\u00aea combined (union) result of two other SELECTs. You add additional information to the view during recursion, such as the PATH and the LEVEL ofeach node.The XML maps the ID, CAT_NAME, and PARENT_ID fields to properties of the Categoryentity. The mapping returns the PATH and LEVEL as additional scalar values. To execute the named SQL query and access the result, write the following:Each tuple contains a managed, persistent Category instance; its path in the tree as astring (such as /One, /O\u00aene/Two, and so on); and the tree level of the node. Alternatively, you can declare and map an SQL query in a Hibernate XML metadata file:We left out the SQL query in this snippet; it's the same as the SQL statement shownearlier in the JPA example. As mentioned in section 14.4, with regard to the execution\u00aein Java code, it doesn'tmatter which syntax you declare your named queries in: XML file or annotations. Eventhe language doesn't matter it can be JPQL or SQL. Both Hibernate and JPA queryinterfaces have methods to \"get a named query\" and execute it independently fromhow you defined it. Thi\u00aes concludes our discussion of SQL result mapping for queries. The next subjectis customization of SQL statements for CRUD operations, replacing the SQL automatically generated by Hibernate for creating, reading, updating, and deleting data in thedatabase.",
  "page466": "The first custom SQL you write loads an entity instance of the User class. All the following code examples show the same SQL that Hibernate executes automatically bydefault, without much customization this helps you understand the mapping technique more quickly. You can customize\u00aeretrieval of an entity instance with a loaderHibernate has two requirements when you override an SQL query to load an entityinstance: Write a named query that retrieves the entity instance. We show an example inSQL, but as always, you can also write named queries in JPQL. For an SQL query,you may need a custom result mapping, as shown earlier in this chapter.Activate the query on an entity class with @org.hibernate.annotationsLoader. This enables the query as the replacement forthe Hibern\u00aeate-generated query.Let's override how Hibernate loads an instance of the User entity, as shown in the following listing.Annotations declare the query to load an instance of User; you can also declare it inan XML file (JPA or Hibernate metadata). You can call this named query directly inyour data-accesscod\u00aee when needed. The query must have exactly one parameter placeholder, which Hibernate sets as theidentifier value of the instance to load. Here it's a positional parameter, but a namedparameter would also work. For this trivial query, you don't need a custom result-set mapping. The User class mapsal\u00ael fields returned by the query. Hibernate can automatically transform the result.",
  "page467": "Setting the loader for an entity class to a named query enables the query for all operations that retrieve an instance of User from the database. There's no indication of thequery language or where you declared it; this is independent of the loader declaration.In a named loa\u00aeder query for an entity, you have to SELECT (that is, perform a projection for) the following properties of the entity class: The value of the identifier property or properties, if a composite primary key isused. All scalar properties of basic type. All properties of embedded components. An entity identifier value for each @JoinColumn of each mapped entity association such as @ManyToOne owned by the loaded entity class. All scalar properties, embedded component properties, and association\u00aejoinreferences that are inside a @SecondaryTable annotation. If you enable lazy loading for some properties, through interception and bytecodeinstrumentation, you don't need to load the lazy properties (see section 12.1.3).Hibernate always calls the enabled loader query when a User has to be retrieved from\u00aethe database by identifier. When you call em.find(User.class, USER_ID), your custom query will execute. When you call someItem.getSeller().getUsername(), andthe Item#seller proxy has to be initialized, your custom query will load the data. You may also want to customize how Hibernate creates, updates, and del\u00aeetes aninstance of User in the database.",
  "page468": "Hibernate usually generates CRUD SQL statements at startup. It caches the SQL statements internally for future use, thus avoiding any runtime costs of SQL generation forthe most common operations. You've seen how you can override the R in CRUD, sonow, let's do the same\u00aefor CUD. For each entity, you can define custom CUD SQL statements with the Hibernate annotations @SQLInsert, @SQLUpdate, and @SQLDelete,respectivelyYou need to be careful with the binding of arguments for SQL statements and parameter placeholders (the ? in the statements). For CUD customization, Hibernate onlysupports positional parameters. What is the right order for the parameters? There's an internal order to howHibernate binds arguments to SQL parameters for CUD statements. The e\u00aeasiest way tofigure out the right SQL statement and parameter order is to let Hibernate generatean example for you. Without any custom SQL statements, enable DEBUG logging forthe org.hibernate.persister.entity category, and search the Hibernate startupoutput for lines like the following:These automatically gene\u00aerated SQL statements show the right parameter order, andHibernate always binds the values in that order. Copy the SQL statements youwant tocustomize into annotations, and make the necessary changes. A special case is properties of an entity class mapped to another table with@SecondaryTable. The CUD customizat\u00aeion statements we've shown so far only refer tothe columns of the main entity table. Hibernate still executes automatically generatedSQL statements for insertion, updates, and deletion of rows in the secondary table(s).",
  "page469": "You can customize this SQL by adding the @org.hibernate.annotations.Table annotation to your entity class and setting its sqlInsert, sqlUpdate, and sqlDelete attributes. If you prefer to have your CUD SQL statementsin XML, your only choice is to mapthe entire entity in a Hibernat\u00aee XML metadata file. The elements in this proprietarymapping format for custom CUD statements are <sql-insert>, <sql-update>, and<sql-delete>. Fortunately, CUD statements are usually much more trivial than queries, so annotations are fine in most applications. You've now added custom SQL statements for CRUD operations of an entityinstance. Next, we show how to override SQL statements loading and modifying acollection.Let's override the SQL stafortement Hibernate uses when loadin\u00aeg the Item#images collection. This is a collection of embeddable components mapped with @ElementCollection; the procedure is the same for collections of basic types or many-valuedentity associations (@OneToMany or @ManyToMany)As before, you declare that a named query will load the collection. This time, however\u00ae,you must declare and map the result of the query in a Hibernate XML metadata file,which is the only facility that supports mapping of query results to collection properties:The query has to have one (positional or named) parameter. Hibernate sets its valueto the entity identifier that owns the collection. Wh\u00aeenever Hibernate need to initializethe Item#images collection, Hibernate now executes your custom SQL query.",
  "page470": "QUOTING SQL IDENTIFIERS- From time to time, especially in legacy databases, you'll encounter identifiers with strange characters or whitespace, or wish to force case sensitivity. Or, as in the previous example, the automatic mapping of a class or property would require a tab\u00aele or column name that is a reserved keyword.  Hibernate 5 knows the reserved keywords of your DBMS through the configureddatabase dialect. Hibernate 5 can automatically put quotes around such strings when generating SQL. You can enable this automatic quoting with hibernate.auto_quote _keyword true in your persistence unit configuration. If you're using an older version of Hibernate, or you find that the dialect's information is incomplete, you must still apply quotes on n\u00aeames manually in your mappings if there is a conflict with a keyword.  If you quote a table or column name in your mapping with backticks, Hibernate always quotes this identifier in the generated SQL. This still works in latest versions of Hibernate, but JPA 2.0 standardized this functionality as delimited ide\u00aentifiers with double quotes. If you have to quote all SQL identifiers, create an orm.xml file and add the setting <delimited-identifiers/> to its <persistence-unit-defaults> section, as shown in listing 3.8. Hibernate then enforces quoted identifiers everywhere.  You should consider renaming tables or\u00aecolumns with reserved keyword names whenever possible. Ad hoc SQL queries are difficult to write in an SQL console if you have to quote and escape everything properly by hand.  Next, you'll see how Hibernate can help when you encounter organizations with strict conventions for database table and column names.To summarize, our recommendations on identifier generator strategies are as follows:ng names- Let's first talk about the naming of entity classes and tables. If you only specify @Entity on the persistence-capable class, the default mapped table name is the same as the class name. Note that we write SQL artifact names in UPPERCASE to make them easier to distinguish SQL is actually case insensitive. So the Java entity class Item maps to the ITEM table. You can override the table name with the JPA @Table annotation, as shown next.",
  "page471": "Entities are the coarser-grained classes of your system. Their instances have an independent life cycle and their own identity, and many other instances can reference them. Value types, on the other hand, are dependent on a particular entity class. A value type instance is\u00aebound to its owning entity instance, and only one entity instance can reference it it has no individual identity. We looked at Java identity, object equality, and database identity, and at what makes good primary keys. You learned which generators for primary key values Hibernate provides out of the box, and how to use and extend this identifier system. We discussed some useful class mapping options, such as naming strategies and dynamic SQL generation. After spending the previous chapt\u00aeer almost exclusively on entities and the respective class- and identity-mapping options, we now focus on value types in their various forms. We split value types into two categories: basic value-typed classes that come with the JDK, such as String, Date, primitives, and their wrappers; and developer\u00aedefined value-typed classes, such as Address and MonetaryAmount in CaveatEmptor. In this chapter, we first map persistent properties with JDK types and learn the basic mapping annotations. You see how to work with various aspects of properties: overriding defaults, customizing access, and generated values. You al\u00aeso see how SQL is used with derived properties and transformed column values. We wrap up basic properties with temporal properties and mapping enumerations. We then discuss custom value-typed classes and map them as embeddable components. You learn how classes relate to the database schema and make your classes embeddable, while allowing for overriding embedded attributes. We complete embeddable componentQL statements will be large for even the simplest operations (say, only one column needs updating), you should disable this startup SQL generationand switch to dynamic statements generated at runtime. An extremely large number of entities can also impact startup time, because Hibernate has to generate all SQL statements for CRUD up front. Memory consumption for this query statement cache will also be high if a dozen statements must be cached for thousands of entities.",
  "page472": "This configuration by exception approach means you don't have to annotate a property to make it persistent; you only have to configure the mapping in an exceptional case. Several annotations are available in JPA to customize and control basic property map. Overriding b\u00aeasic property defaults- You might not want all properties of an entity class to be persistent. For example, although it makes sense to have a persistent Item#initialPrice property, an Item#totalPriceIncludingTax property shouldn't be persistent if you only compute and use its value at runtime, and hence shouldn't be stored in the database. To exclude a property, mark the field or the getter method of the property with the @javax.persistence.Transient annotation or use the J\u00aeava transient keyword. The transient keyword usually only excludes fields for Java serialization, but it's also recognized by JPA providers.  We'll come back to the placement of the annotation on fields or getter methods in a moment. Let's assume as we have before that Hibernate will acces\u00aes fields directly because @Id has been placed on a field. Therefore, all other JPA and Hibernate mapping annotations are also on fields. We have to admit that this annotation isn't very useful. It only has two parameters: the one shown here, optional, marks the property as not optional at the Java\u00aeobject level. By default, all persistent properties are nullable and optional; an Item may have an unknown initialPrice. Mapping the initialPrice property as non-optional makes sense if you have a NOT NULL constraint on the INITIALPRICE column in your SQL schema. If Hibernate is generating the SQL schema, it will include a NOT NULL constraint automatically for non-optional properties. My isn't what you want, and you should always map Java classes instead of storing a heap of bytes in the database. Imagine maintaining a database with this binary information when the application is gone in a few years. Otherwise, Hibernate will throw an exception on startup, complaining that it doesn't understand the type of the property",
  "page473": "The Column annotation has a few other parameters, most of which control SQL-level details such as catalog and schema names. They're rarely needed, and we only show them throughout this book when necessary.  Property annotations aren't always on fields, and you may not\u00aewant Hibernate to access fields directly. Customizing property access- The persistence engine accesses the properties of a class either directly through fields or indirectly through getter and setter methods. An annotated entity inherits the default from the position of the mandatory @Id annotation. For example, if you've declared @Id on a field, not a getter method, all other mapping annotations for that entity are expected on fields. Annotations are never on the setter methods.  T\u00aehe default access strategy isn't only applicable to a single entity class. Any @Embedded class inherits the default or explicitly declared access strategy of its owning root entity class. We cover embedded components later in this chapter. Furthermore, Hibernate accesses any @MappedSuperclass p\u00aeroperties with the default or explicitly declared access strategy of the mapped entity class. Inheritance is the topic of chapter 6.  The JPA specification offers the @Access annotation for overriding the default behavior, with the parameters AccessType.FIELD and AccessType.PROPERTY. If you set @Access on th\u00aee class/entity level, Hibernate accesses all properties of the class according to the selected strategy. You then set any other mapping annotations, including the @Id, on either fields or getter methods, respectively. Now, when you store an Item and forget to set a value on the initialPrice field, Hibernate will complain with an exception before hitting the database with an SQL statement. Hibernate knows that a valueautomatically. The @Column annotation can also override the mapping of the property name to the database column:",
  "page474": "Hibernate reads your Java domain model classes and mapping metadata and generates schema DDL statements. You can export them into a text file or execute them directly on your database whenever you run integration tests. Because schema languages are mostly vendor-specific, every o\u00aeption you put in your mapping metadata has the potential to bind the metadata to a particular database product keep this in mind when using schema features. Hibernate creates the basic schema for your tables and constraints automatically; it even creates sequences, depending on the identifier generator you select. But there are some schema artifacts Hibernate can't and won't create automatically. These include all kinds of highly vendor-specific performance options and any other\u00aeartifacts that are relevant only for the physical storage of data (tablespaces, for example). Besides these physical concerns, your DBA will often supply custom additional schema statements to improve the generated schema. DBAs should get involved early and verify the automatically generated schema from Hibern\u00aeate. Never go into production with an unchecked automatically generated schema. If your development process allows, changes made by the DBA can flow back into your Java systems, for you to add to mapping metadata. In many projects, the mapping metadata can contain all the necessary schema changes from a DBA.\u00aeThen Hibernate can generate the final production schema during the regular build by including all comments, constraints, indexes, and so on. In the following sections, we show you how to customize the generated schema and how to add auxiliary database schema artifacts (we call them objects sometimes; we don't mean Java objects here). We discuss custom data types, additional integrity rules, indexes, ",
  "page475": "This property defines when the create and drop scripts should be executed. Your custom SQL scripts will contain CREATE DOMAIN statements, which must be executed before the tables using these domains are created. With these settings, the schema generator runs the create script fir\u00aest before reading your ORM metadata (annotations, XML files) and creating the tables. The drop script executes after Hibernate drops the tables, giving you a chance to clean up anything you created. Other options are metadata (ignore custom script sources) and script (only use a custom script source; ignore ORM metadata in annotations and XML files). D This is the location of the custom SQL script for creation of the schema. The path is (a) the location of the script resource on the classp\u00aeath; (b) the location of the script as a file:// URL; or, if neither (a) nor (b) matches, (c) the absolute or relative file path on the local file system. This example uses (a). E This is the custom SQL script for dropping the schema. F This load script runs after the tables have been created. We've mentio\u00aened that DDL is usually highly vendor-specific. If your application has to support several database dialects, you may need several sets of create/drop/load scripts to customize the schema for each database dialect. You can solve this with several persistence unit configurations in the persistence.xml file. A\u00aelternatively, Hibernate hasits own proprietary configuration for schema customization in an hbm.xml mapping file, You can hook the following three types of custom SQL scriptsinto Hibernates schema-generation process: The create script executes when the schema is generated. A custom create script can run before, after, or instead of Hibernates automatically generated scripts. In other words, you",
  "page476": "Table constraints An integrity rule that applies to several columns or several rows is a table constraint. A typical declarative table constraint is UNIQUE: all rows are checked for duplicate values (for example, each user must have a distinct email address). A rule affecting onl\u00aey a single row but multiple columns is \"the auction end time has to be after the auction start time.\" Database constraints If a rule applies to more than one table, it has database scope. You should already be familiar with the most common database constraint, the foreign key. This rule guarantees the integrity of references between rows, usually in separate tables, but not always (self-referencing foreign key constraints aren't uncommon). Other database constraints involvin\u00aeg several tables aren't uncommon: for example, a bid can only be stored if the auction end time of the referenced item hasn't been reached. Most (if not all) SQL database management systems support these kinds of constraints and the most important options of each. In addition to simple keywords such a\u00aes NOT NULL and UNIQUE, you can usually also declare more complex rules with the CHECK constraint that applies an arbitrary SQL expression. Still, integrity constraints are one of the weak areas in the SQL standard, and solutions from vendors can differ significantly. Furthermore, non-declarative and procedur\u00aeal constraints are possible with database triggers that intercept data-modification operations. A trigger can then implement the constraint procedure directly or call an existing stored procedure. Integrity constraints can be checked immediately when a data-modification statement is executed, or the check can be deferred until the end of a transaction. The violation response in SQL databases CHAR data type can hold character strings:",
  "page477": "This constraint restricts valid username values to a maximum length of 15 characters, and the string can't begin with admin to avoid confusion. You can call any SQL functions supported by your DBMS; the column Definition is always passed through into the exported schema. No\u00aete that you have a choice: creating and using a domain or adding a single column constraint has the same effect. Domains are usually easier to maintain and avoid duplicating. At the time of writing, Hibernate doesn't support its proprietary annotation @org.hibernate.annotations.Check on individual properties; you use it for table level constraints. Hibernate appends table constraints to the generated CREATE TABLE statement, which can contain arbitrary SQL expressions. You can implem\u00aeent multirow table constraints with expressions that are more complex. You may need a sub select in the expression to do this, which may not be supported by your DBMS. But there are some common multirow table constraints, like UNIQUE, that you can add directly in the mappings. You've already seen the @Colu\u00aemn(unique true | false) option in the previous section. Now all pairs of USERNAME and EMAIL must be unique, for all rows in the USERS table. If you don't provide a name for the constraint here, UNQ_USERNAME_EMAIL an automatically generated and probably ugly name is used. The last kinds of constraints we\u00aediscuss are database-wide rules that span several tables. Hibernate passes on database constraint violations in error exceptions; check whether the exception in your transaction has a cause, somewhere in the exception chain, of type org.hibernate.exception.ConstraintViolationException. This exception can provide more information about the error, such as the name of the failed database constraint. The SQL standard intly, so be careful with database-specific SQL",
  "page478": "This statement declares the foreign key constraint for the ITEM_ID column in the BID table, referencing the primary key column of the ITEM table. You can customize the name of the constraint with the foreign Key option in an @JoinColumn mapping A foreign Key attribute is also sup\u00aeported in @PrimaryKeyJoinColumn, @MapKeyJoinColumn, @JoinTable, @CollectionTable, and @AssociationOverride mappings. The @ForeignKey annotation has some rarely needed options we haven't shown: You can write your own foreign Key Definition, an SQL fragment such as FOREIGN KEY ([column]) REFERENCES [table]([column]) ON UPDATE [action]. Hibernate will use this SQL fragment instead of the provider-generated fragment, it can be in the SQL dialect supported by your DBMS. The Constraint Mo\u00aede setting is useful if you want to disable foreign key generation completely, with the value NO_CONSTRAINT. You can then write the foreign key constraint yourself with an ALTER TABLE statement, probably in a load script as we've shown. Naming constraints properly is not only good practice, but also helps\u00aesignificantly when you have to read exception messages. This completes our discussion of database integrity rules. Next, we look at some optimization you might want to include in your schema for performance reasons. Indexes are a key feature when optimizing the performance of a database application. The que\u00aery optimizer in a DBMS can use indexes to avoid excessive scans of the data tables. Because they're relevant only in the physical implementation of a database, indexes aren't part of the SQL standard, and the DDL and available indexing options are product specific. You can, however, embed the most common schema artifacts for typical indexes in mapping metadata A user can only make bids until an auctiontial integrity rules. They're widely known as foreign keys, which are a combination of two things: a key value copy from a related row and a constraint that guarantees that the referenced value exists. Hibernate creates foreign key constraints automatically for all foreign key columns in association mappings. If you check the schema produced by Hibernate, you'll notice that these constraints also have automatically generated database identifiers names that aren't easy to read and that make debugging more difficult. You see this kind of statement in the generated schema",
  "page479": "If you encounter a USERS table in a legacy schema, it's likely that USERNAME is the primary key. In this case, you have no surrogate identifier that Hibernate generates automatically. Instead, your application has to assign the identifier value when saving an instance of the\u00aeUser class Hibernate calls the protected no-argument constructor when it loads a User from the database and then assigns the username field value directly. When you instantiate a User, call the public constructor with a username. If you don't declare an identifier generator on the @Id property, Hibernate expects the application to take care of the primary key value assignment. Composite (natural) primary keys require a bit more work Suppose the primary key of the USERS table is a co\u00aemposite of the two columns USERNAME and DEPARTMENTNR. You write a separate composite identifier class that declares just the key properties and call this class User Id This class has to be @Embeddable and Serializable any type used as an identifier type in JPA must be Serializable. C You don't have to mark\u00aethe properties of the composite key as @NotNull; their database columns are automatically NOT NULL when embedded as the primary key of an entity. D The JPA specification requires a public no-argument constructor for an embeddable identifier class. Hibernate accepts protected visibility. E The only public con\u00aestructor should havethe key values as arguments. F You have to override the equals() and hashCode() methods with the same semantics the composite key has in your database. In this case, this is a straightforward comparison of the username and department values. Many queries in Caveat Emptor will probably involve the username of a User entity. You can speed up these queries by creating an index for the column of thismpact performance. Unfortunately, many legacy schemas use (natural) composite keys heavily; and for the reason we discourage the use of composite keys, it may be difficult to change the legacy schema to use non-composite natural or surrogate keys. Therefore, JPA supports natural and composite primary and foreign keys.",
  "page480": "We've already shown the @SecondaryTable annotation in an inheritance mapping in section 6.5. It helped to break out properties of a particular subclass into a separate table. This generic functionality has more uses but be aware that a properly designed system should have, s\u00aeimplified, more classes than tables. Suppose that in a legacy schema, you aren't keeping a user's billing address information with the other user details in the USERS main entity table, but in a separate table. Figure 9.5 shows this schema. The user's home address is stored in the columns STREET, ZIPCODE, and CITY of the USERS table. The user's billing address is stored in the BILLING_ADDRESS table, which has the primary key column USER_ID, which is also a foreign key\u00aeconstraint referencing the ID primary key of the USERS table The User class has two properties of embedded type: home Address and billing Address. The first is a regular embedded mapping, and the Address class is @Embeddable. Just as in section 5.2.3, you can use the @AttributeOverrides annotation to override\u00aethe mapping of embedded properties. Then, @Column maps the individual properties to the BILLING_ADDRESS table, with its table option. Remember that an @AttributeOverride replaces all mapping information for a property: any annotations on the Address fields are ignored if you override. Therefore, you have to\u00aespecify nullability and length again in the @Column override. We've shown you a secondary table mapping example with an embeddable property. Of course, you could also break out simple basic properties like the username string in a secondary table. Keep in mind that reading and maintaining these mappings can be a problem, though; you should only map legacy unchangeable schemas with secondary tables. A foreign key consName attribute of @JoinColumn to declare this relationship. Hibernate now knows that the referenced target column is a natural key, and not the primary key, and manages the foreign key relationship accordingly. If the target natural key is a composite key, use @JoinColumns instead as in the previous section.",
  "page481": "strategies for runtime data management. These strategies are crucial to the performance and correct behavior of your applications. In this chapter, we discuss the life cycle of entity instances how an instance becomes persistent, and how it stops being considered persistent and\u00aethe method calls and management operations that trigger these transitions. The JPA Entity Manager is your primary interface for accessing data. Before we look at the API, let's start with entity instances, their life cycle, and the events that trigger a change of state. Although some of the material may be formal, a solid understanding of the persistence life cycle is essential. Because JPA is a transparent persistence mechanism classes are unaware of their own persistence capability\u00aeit's possible to write application logic that's unaware whether the data it operates on represents persistent state or temporary state that exists only in memory. The application shouldn't necessarily need to care that an instance ispersistent when invoking its methods. You can, for example, inv\u00aeoke the Item #calculateTotalPrice() business method without having to consider persistence at all (for example, in a unit test). Any application with persistent state must interact with the persistence service whenever it needs to propagate state held in memory to the database (or vice versa). In other words\u00ae, you have to call the Java Persistenceinterfaces to store and load data. When interacting with the persistence mechanism that way, the application must concern itself with the state and life cycle of an entity instance with respect to persistence. We refer to this as the persistence life cycle: the states an entity instance goes through during its life. We also use the term unit of work: a set of (posning to and intercepting events, auditing and versioning with Hibernate Envers, and filtering datadynamically. After reading this part, you'll know how to work with Hibernate and Java Persistence programming interfaces and how to load, modify, and store objects efficiently. You'll understand how transactions work and why conversational processing can open up new approaches for application design.",
  "page482": "A persistent entity instance has a representation in the database. It's stored in the database or it will be stored when the unit of work completes. It's an instance with a database identity, as defined in section 4.2; its database identifier is set to the primary key v\u00aealue of the database representation. The application may have created instances and then made them persistent by calling Entity Manager #persist(). There may be instances that became persistent when the application created a reference to the object from another persistent instance that the JPA provider already manages. A persistent entity instance may be an instance retrieved from the database by execution of a query, by an identifier lookup, or by navigating the object graph starting fr\u00aeom another persistent instance. Persistent instances are always associated with a persistence context. Yousee more about this in a moment You can delete a persistent entity instance from the database in several ways: For example, you can remove it with Entity Manager #remove(). It may also become available for\u00aedeletion if you remove a reference to it from a mapped collection with orphan removal enabled. An entity instance is then in the removed state: the provider will delete it at the end of a unit of work. You should discard any references you may hold to it in the application after you finish working with it f\u00aeor example, after you've rendered the removal confirmation screen your users see Think of the persistence context as a service that remembers all the modifications and state changes you made to data in a particular unit of work (this is somewhat simplified, but it's a good starting point). We now dissect all these terms: entity states, persistence contexts, and managed scope. You're probably more accst like new Long() and new Big Decimal(). Hibernate doesn't provide any rollback functionality for transient instances; if you modify the price of a transient Item, you can't automatically undo the change. For an entity instance to transition from transient to persistent state, to become managed, requires either a call to the Entity Manager #persist() method or the creation of a reference from an already-persistent instance and enabled cascading of ",
  "page483": "The persistence context acts as a first-level cache; it remembers all entity instances you've handled in a particular unit of work. For example, if you ask Hibernate to load an entity instance using a primary key value (a lookup by identifier), Hibernate can first check the\u00aecurrent unit of work in the persistence context. If Hibernate finds the entity instance in the persistence context, no database hit occurs this is a repeatable read for an application. Consecutive em.find(Item. Class, ITEM_ID) calls with the same persistence context will yield the same result. This cache also affects results of arbitrary queries, executed for example with the javax.persistence.Query API. Hibernate reads the SQL result set of a query and transforms it into entity instances\u00ae. This process first tries to resolve every entity instance in the persistence context by identifier lookup. Only if an instance with the same identifier value can't be found in the current persistence context does Hibernate read the rest of the data from the result-set row. Hibernate ignores any potential\u00aely newer data in the result set, due to read-committed transaction isolation at the database level, if the entity instance is already present in the persistence context. The persistence context cache is always on it can't be turned off. It ensures the following The persistence layer isn't vulnerabl\u00aee to stack overflows in the case of circular references in an object graph. There can never be conflicting representations of the same database row at the end of a unit of work. The provider can safely write all changes made to an entity instance to the database. Likewise, changes made in a particular persistence context are always immediately visible to all other code executed inside that unit of work and its persiase the state of instances monitored by a persistence context, either automatically or on demand. Typically, when a unit of work completes, the provider propagates state held in memory to the database through the execution of SQL INSERT, UPDATE, and DELETE statements (all part of the Data Modification Language [DML]) This flushing procedure may also occur at other times. For example, Hibernate may synchronize with the database before execution of a query. This ensures that queries are aware of changes made earlier during the unit of work.",
  "page484": "Hibernate joins the in-progress system transaction when an Entity Manager is created and waits for the transaction to commit. When Hibernate is notified (by the JTA engine) of the commit, it performs dirty checking of the persistence context and synchronizes with the database. Yo\u00aeu can also force dirty checking synchronization manually by calling EntityManager#flush() at any time during a transaction. You decide the scope of the persistence context by choosing when to close() the Entity Manager. You have to close the persistence context at some point, so always place the close() call in a finally block. How long should the persistence context be open? Let's assume for the following examples that you're writing a server, and each client request will be p\u00aerocessed with one persistence context and system transaction in a multithreaded environment. If you're familiar with servlets, imagine the code in listing 10.1 embedded in a servlet's service() method. Within this unit of work, you access the Entity Manager to load and store data. A new transient Item\u00aeis instantiated as usual. Of course, you may also instantiate it before creating the EntityManager. A call to persist() makes the transient instance of Item persistent. It's now managed by and associated with the current persistence context. To store the Item instance in the database, Hibernate has to\u00aeexecute an SQL INSERT statement. When the transaction of this unit of work commits, Hibernate flushes the persistence context, and the INSERT occurs at that time. Hibernate may even batch the INSERT at the JDBC level with other statements. When you call persist(), only the identifier value of the Item is assigned. Alternatively, if your identifier generator isn't pre-insert, the INSERT statement willwrite empty catch clauses in your code, though you'll have to roll back the transaction and handle exceptions. Creating an Entity Manager starts its persistence context. Hibernate won't access the database until necessary; the Entity Manager doesn't obtain a JDBC Connection from the pool until SQL statements have to be executed. You can create and close an Entity Manager without hitting the database.",
  "page485": "You can modify the Item instance, and the persistence context will detect these changes and record them in the database automatically. When Hibernate flushes the persistence context during commit, it executes the necessary SQL DML statements to synchronize the changes with the da\u00aetabase. Hibernate propagates state changes to the database as late as possible, toward the end of the transaction. DML statements usually create locks in the database that are held until the transaction completes, so Hibernate keeps the lock duration in the database as short as possible. Hibernate writes the new Item#name to the database with an SQL UPDATE. By default, Hibernate includes all columns of the mapped ITEM table in the SQL UPDATE statement, updating unchanged columns to their\u00aeold values. Hence, Hibernate can generate these basic SQL statements at startup, not at runtime. If you want to include only modified (or non-nullable for INSERT) columns in SQL statements, you can enable dynamic SQL generation as discussed in section 4.3.2. Hibernate detects the changed name by comparing the\u00aeItem with a snapshot copy it took before, when the Item was loaded from the database. If your Item is different from the snapshot, an UPDATE is necessary. This snapshot in the persistence context consumes memory. Dirty checking with snapshots can also be time consuming, because Hibernate has to compare all in\u00aestances in the persistence context with their snapshot during flushing. It's better (but not required) to fully initialize the Item instance before managing it with a persistence context. The SQL INSERT statement contains the values that were held by the instance at the point when persist() was called. If you don't set the name of the Item before making it persistent, a NOT NULL constraint may b9s a generic method, and its return type is set as a side effect of the first parameter. The retrieved entity instance is in persistent state, and you can now modify it inside the unit of work. If no persistent instance with the given identifier value can be found, find() returns null. The find() operation always hits the database if there was no hit for the",
  "page486": "As soon as you call any method such as Item#getName() on the proxy, a SELECT is executed to fully initialize the placeholder. The exception to this rule is a mapped database identifier getter method, such as getId(). A proxy may look like the real thing, but it's only a plac\u00aeeholder carrying the identifier value of the entity instance it represents. If the database record no longer exists when the proxy is initialized, an EntityNotFoundException is thrown. Note that the exception can be thrown when Item#getName() is called. E Hibernate has a convenient static initialize() method that loads the proxy's data. F After the persistence context is closed, item is in detached state. If you don't initialize the proxy while the persistence context is still op\u00aeen, you get a LazyInitializationException if you access the proxy. You can't load data on demand once the persistence context is closed. The solution is simple: load the data before you close the persistence context. We'll have much more to say about proxies, lazy loading, and on-demand fetching in c\u00aehapter 12. If you want to remove the state of an entity instance from the database, you have to make it transient. To make an entity instance transient and delete its database representation, call the remove() method on the EntityManager If you call find(), Hibernate executes a SELECT to load the Item. If yo\u00aeu call getReference(), Hibernate attempts to avoid the SELECT and returns a proxy. C Calling remove() queues the entity instance for deletion when the unit of work completes; it's now in removed state. If remove() is called on a proxy, Hibernate executes a SELECT to load the data. You may want to customize how Hibernate detects dirty state, using an extension point. Set the property hibernate.entity_dirtiness_stference() without hitting the database. Furthermore, if no persistent instance with that identifier is currently managed, Hibernate produces a hollow placeholder: a proxy. This means getReference() won't access the database, and it doesn't return null, unlike find(). C JPA offers PersistenceUnitUtil helper methods such as isLoaded() to detect whether you're working with an uninitialized proxy.",
  "page487": "Java Persistence also offers bulk operations that translate into direct SQL DELETE statements without life cycle interceptors in the application. We'll discuss these operations in section 20.1. Let's say you load an entity instance from the database and work with the d\u00aeata. For some reason, you know that another application or maybe another thread of your application has updated the underlying row in the database. Next, we'll see how to refresh the data held in memory. After you load the entity instance, you realize (how isn't important) that someone else changed the data in the database. Calling refresh() causes Hibernate to execute a SELECT to read and marshal a whole result set, overwriting changes you already made to the persistent instance\u00aein application memory. If the database row no longer exists (someone deleted it), Hibernate throws an EntityNotFoundException on refresh(). Most applications don't have to manually refresh in-memory state; concurrent modifications are typically resolved at transaction commit time. The best use case for\u00aerefreshing is with an extended persistence context, which might span several request/response cycles and/or system transactions. While you wait for user input with an open persistence context, data gets stale, and selective refreshing may be required depending on the duration of the conversation and the dialo\u00aegue between the user and the system. Refreshing can be useful to undo changes made in memory during a conversation, if the user cancels the dialogue. We'll have more to say about refreshing in a conversation in section. An entity instance must be fully initialized during life cycle transitions. You may have life cycle callback methods or an entity listener enabled (see section 13.2), and the inlue after removal of an entity instance. In the previous code example, the identifier value is reset to the default value of null (it's a Long). The Item is now the same as intransient state, and you can save it again in a new persistence context.",
  "page488": "The persistence context is a cache of persistent instances. Every entity instance in persistent state is associated with the persistence context. Many Hibernate users who ignore this simple fact run into an OutOfMemoryException. This is typically the case when you load thousands\u00aeof entity instances in a unit of work but never intend to modify them. Hibernate still has to create a snapshot of each instance in the persistence context cache, which can lead to memory exhaustion. (Obviously, you should execute a bulk data operation if you modify thousands of rows we'll get back to this kind of unit of work in section 20.1.) The persistence context cache never shrinks automatically. Keep the size of your persistence context to the necessary minimum. Often, many p\u00aeersistent instances in your context are there by accident for example, because you needed only a few items but queried for many. Extremely large graphs can have a serious performance impact and require significant memory for state snapshots. Check that your queries return only data you need, and consider the fo\u00aellowing ways to control Hibernate's caching behavior. You can call EntityManager#detach(i) to evict a persistent instance manually from the persistence context. You can call EntityManager#clear() to detach all persistent entity instances, leaving you with an empty persistence context. The native Sessio\u00aen API has some extra operations you might find useful. You can set the entire persistence context to read-only mode. This disables state snapshots and dirty checking, and Hibernate won't write modifications to the database. Replication is useful, for example, when you need to retrieve data from one database and store it in another. Replication takes detached instances loaded in one persistence context and makes them pnt databases. An example case is a product upgrade: if the new version of your application requires a new database (schema), you may want to migrate and replicate the existing data once. The persistence context does many things for you: automatic dirty checking, guaranteed scope of object identity, and so on. It's equally important that you know some of the details of its management, and that you sometimes influence what goes on behind the scenes. ",
  "page489": "Next, we look at the detached entity state. We already mentioned some issues you'll see when entity instances aren't associated with a persistence context anymore, such as disabled lazy initialization. Let's explore the detached state with some examples, so you kno\u00aew what to expect when you work with data outside of a persistence context. If a reference leaves the scope of guaranteed identity, we call it a reference to a detached entity instance. When the persistence context is closed, it no longer provides an identity-mapping service. You'll run into aliasing problems when you work with detached entity instances, so make sure you understand how to handle the identity of detached instances. If you look up data using the same database identifier\u00aevalue in the same persistence context, the result is two references to the same in-memory instance on the JVM heap. Consider the two units of work shown next. In the first unit of work at begin() B, you start by creating a persistence context C and loading some entity instances. Because references a and b are\u00aeobtained from the same persistence context, they have the same Java identity D. They're equal from the same persistence context, they have the same Java identity D. They're equal E because by default equals() relies on Java identity comparison. They obviously have the same database identity F. They reference\u00aethe same Item instance, in persistent state, managed by the persistence context for that unit of work. The first part of this example finishes by committing the transaction. By default, Hibernate flushes the persistence context of an EntityManager and synchronizes changes with the database whenever the joined transaction is committed. All the previous code examples, except some in the last sectabase by calling EntityManager#flush(). This concludes our discussion of the transient, persistent, and removed entity states, and the basic usage of the EntityManager API. Mastering these state transitions and API methods is essential; every JPA application is built with these operations.",
  "page490": "When you begin a journey, it's a good idea to have a mental map ofthe terrain you'll be passing through. The same is true for an intellectual journey, such as learning to write computer programs. In this case, you'll need to know the basics of what computers are an\u00aed how they work. You'll want to have some idea of what a computer program is and how one is created. Since you will be writing programs in the Java programming language, you'll want to know something about that language in particular and about the modern, networked computing environment for which Java is designed. As you read this chapter, don't worry if you can't understand everything in detail. (In fact, it would be impossible for you to learn all the details from the\u00aebrief expositions in this chapter.) Concentrate on learning enough about the big ideas to orient yourself, in preparation for the rest of the book. Most of what is covered in this chapter will be covered in much greater detail later in the book. A computer is a complex system consisting of many different compo\u00aenents. But at the heart or the brain, if you want of the computer is a single component that does the actual computing. This is the Central Processing Unit, or CPU. In a modern desktop computer, the CPU is a single \"chip\" on the order of one square inch in size. The job of the CPU is to execute prog\u00aerams. A program is simply a list of unambiguous instructions meant to be followed mechanically by a computer. A computer is built to carry out instructions that are written in a very simple type of language called machine language. Each type of computer has its own machine language, and the computer can directly execute a program only if the program is expressed in that language. (It can execute programs writtchine language instructions in main memory. It does this by repeatedly reading, or fetching, an instruction from memory and then carrying out, or executing, that instruction. This process fetch an instruction, execute it, fetch another instruction, execute it, and so on forever is called the fetch-and-execute cycle. With one exception, which will be covered in the next section,",
  "page491": "Machine language instructions are expressed as binary numbers. A binary number is made up of just two possible digits, zero and one. So, a machine language instruction is just a sequence of zeros and ones. Each particular sequence encodes some particular instruction. The data tha\u00aet the computer manipulates is also encoded as binary numbers. A computer can work directly with binary numbers because switches can readily represent such numbers: Turn the switch on to represent a one; turn it off to represent a zero. Machine language instructions are stored in memory as patterns of switches turned on or off, When a machine language instruction is loaded into CPU, all that happens is that certain switches are turned on or off in the pattern that encodes that particular in\u00aestruction. The CPU is built to respond to this pattern by executing the instruction it encodes; it does this simply because of the way all the other switches in the CPU are wired together. So, you should understand this much about how computers work: Main memory holds machine language programs and data. These a\u00aere encoded as binary numbers. The CPU fetches machine language instructions from memory one after another and executes them. It does this mechanically, without thinking about or understanding what it does-and therefore the program it executes must be perfect, complete in all details, and unambiguous because t\u00aehe CPU can do nothing but execute it exactly as written. Here is a schematic view of this first-stage understanding of the computer Asynchronous Events: Polling Loops and Interrupts The CPU spends almost all of its time fetching instructions from memory and executing them. However, the CPU and main memory are only two out of many components in areal computer system, A complete system contains other devices such as: A hard ere is a device driver, which consists of software that the CPU executes when it has to deal with the device. Installing a new device on a system generally has two steps: plugging the device physically into the computer, and installing the device driver software. Without the device driver, the actual physical device would be useless, since the CPU would not be able to communicate with it.",
  "page492": "Again, you should understand that this is a purely mechanical process: A device signals an interrupt simply by turning on a wire. The CPU is built so that when that wire is turned on, the CPU saves enough information about what it is currently doing so that it can return to the s\u00aeame state later. This information consists of the contents of important internal registers such as the program counter. Then the CPU jumps to some predetermined memory location and begins executing the instructions stored there. Those instructions make up an interrupt handler that does the processing necessary to respond to the interrupt. (This interrupt handler is part of the device driver software for the device that signalled the interrupt.) At the end of the interrupt handler is an ins\u00aetruction that tells the CPU to jump back to whatit was doing; it does that by restoring its previously saved state. Interrupts allow the CPU to deal with asynchronous events. In the regular fetch-and execute cycle, things happen in a predetermined order; everything that happens is \"synchronized\" with\u00aeeverything else. Interrupts make it possible for the CPU to deal efficiently with events that happen \"asynchronously,\" that is, at unpredictable times. As another example of how interrupts are used, consider what happens when the CPU need to access data that is stored on the hard disk. The CPU can a\u00aeccess data directly only if it is in main memory. Data on the disk has to be copied into memory before it can be accessed. Unfortunately, on the scale of speed at which the CPU operates, the disk drive is extremely slow. When the CPU needs data from the disk, it sends a signal to the disk drive telling it to locate the data and get it ready. (This signal is sent synchronously, under the control of a regular program.alled a thread. (Or a process; there are technical differences between threads and processes, but they are not important here.) At any given time, only one thread can actually be executed by a CPU. The CPU will continue running the same thread until one of several things happens The thread might voluntarily yield control, to give other threads a chance to run. The thread might have to waitven programming\" has a very different feel from the more traditional straight-through, synchronous programming.",
  "page493": "The Java Virtual Machine Machine language consists of very simple instructions that can be executed directly by the CPU of a computer. Almost all programs, though, are written in high-level programming languages such as Java, Pascal, or C++. A program written in a high-level lang\u00aeuage cannot be run directly on any computer. First, it has to be translated into machine language. This translation can be done by a program called a compiler. A compiler takes a high-level-language program and translates it into an executable machine-language program. Once the translation is done, the machine-language program can be run any number of times, but of course it can only be run on one type of computer (since each type of computer has its own individual machine language). If th\u00aee program is to run on another type of computer it has to be re-translated, using a different compiler, into the appropriate machine language. There is an alternative to compiling a high-level language program. Instead of using a compiler, which translates the program all at once, you can use an interpreter, wh\u00aeich translates it instruction-by-instruction, as necessary. An interpreter is a program that acts much like a CPU, with a kind of fetch-and-execute cycle. In order to execute a program, the interpreter runs in a loop in which it repeatedly reads one instruction from the program, decides what is necessary to c\u00aearry out that instruction, and then performs the appropriate machine-language commands to do so. one use of interpreters is to execute high-level language programs. For example, the programming language Lisp is usually executed by an interpreter rather than a compiler. However, interpreters have another purpose: they can let you use a machine-language program meant for one type of computer on a completely different type ofputer. However, one of the main selling points of Java is that it can actually be used on any computer. All that the computer needs is an interpreter for Java bytecode. Such an interpreter simulates the Java virtual machine in the same way that Virtual PC simulates a PC computer. Of course, a different Java bytecode interpreter is needed for each type of computer, but once a computer has act-oriented language. I should also note that the really hard part of platform-independence is providing a \"Graphical User Interface\" with windows, buttons, etc. that will work on all the platforms that support Java.",
  "page494": "Programs must be designed. No one can just sit down at the computer and compose a program of any complexity. The discipline called software engineering is concerned with the construction of correct, working, well-written programs. The software engineer tends to use accepted and\u00aemethods for analyzing the problem to be solved and for designing a program to solve that problem. During the 1970s and into the 80s, the primary software engineering methodology was structured programming. The structured programming approach to program design was based on the following advice: To solve a large problem, break the problem into several pieces and work on each piece separately; to solve each piece, treat it as a new problem which can itself be broken down into smaller\u00ae; eventually, you will work your way down to problems that can be solved directly, without further decomposition. This approach is called top-down programming. There is nothing wrong with top-down programming. It is a valuable and often-used approach to problem-solving. However, it is incomplete. For one\u00aething, it deals almost entirely with producing the instructions necessary to solve a problem. But as time went on, people realized that the design of the data structures for a program was as least as important as the design of subroutines and control structures. Top-down programming doesn't give adequate\u00aeconsideration to the data that the program manipulates. Another problem with strict top-down programming is that it makes it difficult to reuse work done for other projects. By starting with a particular problem and subdividing it into convenient pieces, top-down programming tends to produce a design that is unique to that problem. It is unlikely that you will be able to take a large chunk of programming from another program antware modules is to contain some data, along with some subroutines for manipulating that data. For example, a mailing-list module might contain a list of names and addresses along with a subroutine for adding a new name, a subroutine for printing mailing labels, and so forth. In such modules, the data itself is often hidden inside the module; a program that uses the module can then manipuld to be better models of the way the world itself works, and that they are therefore easier to write, easier to understand, and more likely to be correct. You should think of objects as \"knowing\" how to respond to certain messages.",
  "page495": "For example, consider a drawing program that lets the user draw lines, rectangles, ovals, polygons, and curves on the screen. In the program, each visible object on the screen could be represented by a software object in the program. There would be five classes of objects in the\u00aeprogram, one for each type of visible object that can be drawn. All the lines would belong to one class, all the rectangles to another class, and so on. These classes are obviously related; all of them represent \"drawable objects.\" They would, for example, all presumably be able to respond to a \"draw yourself\" message. Another level of grouping, based on the data to represent each type of object, is less obvious, but would be very useful in a program: We can group poly\u00aegons and curves together as \"multipoint objects,\" while lines, rectangles, and ovals are \"two-point objects.\" (A line is determined by its endpoints, a rectangle by two of its corners, and an oval by two corners of the rectangle that contains it.) We could diagram these relationships as foll\u00aeows DrawableObject, MultipointObject, and TwoPointObject would be classes in the program. MultipointObject and TwoPointObject would be subclasses of DrawableObject. The class Line would be a subclass of TwoPointObject and (indirectly) of DrawableObject. A subclass of a class is said to inherit the properties\u00aeof that class. The subclass can add to its inheritance and it can even \"override\" part of that inheritance (by defining a different response to some method). Nevertheless, lines, rectangles, and so on are drawable objects, and the class DrawableObject expresses this relationship. Inheritance is a powerful means for organizing a program. It is also related to the problem of reusing software components. A class is the computer types back itsresponse. Early personal computers also used typed commands and responses, except that there was only one person involved at a time. This type of interaction between a user and a computer is called a command-line interface. Today, of course, most people interact with computers in a completely different way. They use a Graphical User Interface, or GUI. Theer, and is used in preference to the AWT in most modern Java programs. The applet that is shown above uses components that are part of Swing.",
  "page496": "The use of the term \"message\" here is deliberate. Messages, as you saw in the previous section, are sent to objects. In fact, Java GUI components are implemented as objects. Java includes many predefined classes that represent various types of GUI components. Some of th\u00aeese classes are subclasses of others. Here is a diagram showing some of Swing's GUI classes and their relationships. Don't worry about the details for now, but try to get some feel about how object-oriented programming and inheritance are used here. Note that all the GUI classes are subclasses, directly or indirectly, of a class called JComponent, which represents general properties that are shared by all Swing components. Two of the direct subclasses of JComponent themselves hav\u00aee subclasses. The classes JTextArea and JTextField, which have certain behaviors in common, are grouped together as subclasses of JTextComponent. Similarly JButton and JToggleButton are subclasses of JAbstractButton, which represents properties common to both buttons and checkboxes. (JComboBox, by the way, is t\u00aehe Swing class that represents pop-up menus.) Just from this brief discussion, perhaps you can see how GUI programming can make effective use of object-oriented design. In fact, GUI's, with their \"visible objects,\" are probably a major factor contributing to the popularity of OOP. The Internet\u00aeand the World-Wide Web - Computers can be connected together on networks. A computer on a network can communicate with other computers on the same network by exchanging data and files or by sending and receiving messages.Computers on a network can even work together on a large computation. Today, millions of computers throughout the world are connected to a single huge network called the Internet. New computers are some data being sent from one computer to another, along with addressing information that indicates where on the Internet that data is supposed to go. Think of a packet as an envelope with an address on the outside and a message on the inside. (The message is the data.) Thepacket also includes a \"return address,\" that is, the address of the sender. A packet can hold only AP, are used to fetch messages from an email account so that the recipient can read them. A person who uses email, however, doesn't need to understand or even know about these protocols.",
  "page497": "On a basic level (the level of machine language), a computer can perform only very simple operations. A computer performs complex tasks by stringing together large numbers of such operations. Such tasks must be \"scripted\" in complete and perfect detail by programs. Crea\u00aeting complex programs will never be really easy, but the difficulty can be handled to some extent by giving the program a clear overall structure. The design of the overall structure of a program is what I call \"programming in the large.\" Programming in the small, which is sometimes called coding, would then refer to filling in the details of that design. The details are the explicit, step-by-step instructions for performing fairly small-scale tasks. When you do coding, you are w\u00aeorking fairly \"close to the machine,\" with some of the same concepts that you might use in machine language: memory locations, arithmetic operations, loops and branches. In a high-level language such as Java, you get to work with these concepts on a level several steps above machine language. However,\u00aeyou still have to worry about getting all the details exactly right. This chapter and the next examine the facilities for programming in the small in the Java programming language. Don't be misled by the term \"programming in the small\" into thinking that this material is easy or unimportant.\u00aehis material is an essential foundation for all types of programming. If you don't understand it, you can't write programs, no matter how good you get at designing their large-scale structure. The Basic Java Application - A program is a sequence of instructions that a computer can execute to perform some task. A simple enough idea, but for the computer to make any use of the instructions, they must be writtect program is one that does what you want it to. Furthermore, a program can be syntactically and semantically correct but still be a pretty bad program. Using the language correctly is not the same as using it well. For example, a good program has \"style.\" It is written in away that will make it easy for people to read and to understand. It follows conventions that will b details here of how you do each of these steps; it depends on the particular computer and Java programming environment that you are using. See Section 2.6 for information about creating and running Java programs in specific programming environments.",
  "page498": "Here is a Java program to display the message \"Hello World!\". Don't expect to understand what's going on here just yet some of it you won't really understand until a few chapters from now: // A program to display the message // \\\"Hello World!\\\" on sta\u00aendard output public class HelloWorld { public static void main(String[] args) { System.out.println(\\\"Hello World!\\\"); } } // end of class HelloWorld The command that actually displays the message is: System.out.println(\\\"Hello World!\\\"); This command is an example of a subroutine call statement. It uses a \"built-in subroutine\" named System.out.println to do the actual work. Recall that a subroutine consists of the instructions for performing some task, chunked together an\u00aed given a name. That name can be used to \"call\" the subroutine whenever that task needs to be performed. A built-in subroutine is one that is already defined as part of the language and therefore automatically available for use in any program. When you run this program, the message \"Hello World!\\u00aeu201d (without the quotes) will be displayed on standard output. Unfortunately, I can't say exactly what that means! Java is meant to run on many different platforms, and standard output will mean different things on different platforms. However, you can expect the message to show up in some convenient\u00ae(If you use a command-line interface, like that in Sun Microsystem's Java Development Kit, you type in a command to tell the computer to run the program. The computer will type the output from the program, Hello World!, on the next line.) You must be curious about all the other stuff in the above program. Part of it consists of comments. Comments in a program are entirely ignored by the computer; thehe main() subroutine, and the statements that it contains are executed. These statements make up the script that tells the computer exactly what to do when the program is executed. The main() routine can call subroutines that are defined in the same class or even in other classes, but it is the main() routine that determines how and in what order the other subroutines are used. The wtics of the language. The computer doesn't care about layout you could run the entire program together on one line as far as it is concerned. However, layout is important to human readers.",
  "page499": "Variables and the Primitive Types - Names are fundamental to programming. In programs, names are used to refer to many different sorts of things. In order to use those things, a programmer must understand the rules for giving names to things and the rules for using the names to w\u00aeork with those things. That is, the programmer must understand the syntax and the semantics of names. According to the syntax rules of Java, a name is a sequence of one or more characters. It must begin with a letter or underscore and must consist entirely of letters, digits, and underscores. (\"Underscore\" refers to the character ' '.) For example, here are some legal names: No spaces are allowed in identifiers; HelloWorld is a legal identifier, but Hello World\u00ae01d is not. Upper case and lower case letters are considered to be different, so that HelloWorld, helloworld, HELLOWORLD, and hElloWorLD are all distinct names. Certain names are reserved for special uses in Java, and cannot be used by the programmer for other purposes. These reserved words include: class, public, stati\u00aec, if, else, while, and several dozen other words. Java is actually pretty liberal about what counts as a letter or a digit. Java uses the Unicode character set, which includes thousands of characters from many different languages and different alphabets, and many of these characters count as letters\u00aeHowever, I will be sticking to what can be typed on a regular English keyboard. The pragmatics of naming includes style guidelines about how to choose names for things. For example, it is customary for names of classes to begin with upper case letters, while names of variables and of subroutines begin with lower case letters; you can avoid a lot of confusion by following the same convention in your own programsimple or compound that can be used to refer to something in Java. (Note that the reserved words are not identifiers, since they can't be used as names for things.) Variables - Programs manipulate data that are stored in memory. In machine language, data can only be referred to by giving the numerical address of the location in memory where it is stored. In a high-level language such acuting a program, it evaluates the expression and puts the resulting data value into the variable. For example, consider the simple assignment statement rate = 0.07; The variable in this assignment statement is rate, and the expression is the number 0.07.",
  "page500": "Types and Literals- A variable in Java is designed to hold only one particular type of data; it can legally hold that type of data and no other. The compiler will consider it to be a syntax error if you try to violate this rule. We say that Java is a strongly typed language becau\u00aese it enforces this rule. There are eight so-called primitive types built into Java. The primitive types are named byte, short, int, long, float, double, char, and boolean. The first four types hold integers (whole numbers such as 17, -38477, and 0). The four integer types are distinguished by the ranges of integers they can hold. The float and double types hold real numbers (such as 3.6 and -145.99). Again, the two real types are distinguished by their range and accuracy. A variable of\u00aechar holds a single character from the Unicode character set. And a variable of type boolean holds one of the two logical values true or false. Any data value stored in the computer's memory must be represented as a binary number, that is as a string of zeros and ones. A single zero or one is called a bit. A\u00aestring of eight bits is called a byte. Memory is usually measured in terms of bytes. Not surprisingly, the byte data type refers to a single byte of memory. A variable of type byte holds a string of eight bits, which can represent any of the integers between -128 and 127, inclusive.\u00aeeight bits can represent 256 two raised to the power eight different values.) As for the other integer types, short corresponds to two bytes (16 bits). Variables of type short have values in the range -32768 to 32767. int corresponds to four bytes (32 bits). Variables of type int have values in the range -2147483648 to 2147483647. long corresponds to eight bytes (64 bits). Variables of type long have values inrogram, it must be surrounded by single quotes; for example: 'A', '*', or 'x'. Without the quotes, A would be an identifier and * would be a multiplication operator. The quotes are not part of the value and are not stored in the variable; they are just a convention for naming a particular character constant in a program. A name for a constant value is cae for real numbers.) Even for integer literals, there are some complications. Ordinary integers such as 177777 and -32 are literals of type byte, short, or int, depending on their size. You can make a literal of type long by adding \"L\" as a suffix."
}

const dataC = {
  "page1": "be on one page and those starting with D on the next page. Alternatively, each page shows auction items that have reached a certain threshold value for their highest bid amount. The\u00aeoffset method performs much worse if you fetch higher page numbers. If you jump to page 5,000, the database must count all the rows and prepare 5,000pages of data before it can skip 4,999 of them to give you the result.\u00aeA common work around is to restrict how far a user can jump: for example, only allowing direct jumps to the first 100 pages and forcing the user to refine the query restrictions to get a smaller result.\u00aeThe seek method is usually faster than the offset method, even on low page numbers. The database query optimizer can skip directly to the start of the desired page and efficiently limit the index range to scan. Records shown on previous pages don't have to be considered or counted.\u00aeThe offset method may sometimes show incorrect results. Although the result may be consistent with whats'in the database, your users may consider it incorrect. When applications insert new records or delete existing records while the user is browsing, anomalies may occur. Imagine the user looking at page 1 and someone adding\u00aea new record that would appear on page 1. If the user now retrieves page 2, some record they may have seen on page 1 is pushed forward onto page 2. If a record on page 1 was deleted, the user may miss data on page2 because a record was pulled back to page 1.",
  "page2": "The seek method avoids these anomalies; records don't mysteriously reappear or vanish.We now show you how to implement both paging techniques by extending the persistence layer.\u00aeYou start with a simple model that holds the current paging and sortingsettings of a data table viewWhen you want to coordinate querying and rendering data pages, you need to keep some details about the size of the pages\u00aeand what page you're currently viewing. Following is a simple model class encapsulating this information; it's an abstract superclass that will work for both the offset and seek techniques:\u00aeThe model holds the size of each page and the number of records shown per page. The value -1 is special, meaning \"no limit; show all records.\"Keeping the number of total records is necessary for some calculations: for example,to determine whether there is a \"next\" page.\u00aePaging always requires a deterministic record order. Typically, you sort by a particularattribute of your entity classes in ascending or descending order. javax.persistence.metamodel.SingularAttribute is an attribute of either an entity or an embeddableclass in JPA; it's not a collection (you can't \"order by collection\u00aein a query).The allowedAttributes list is set when creating the page model. It restricts the possible sortable attributes to the ones you can handle in your queries.Some methods of the Page class that we haven't shown are trivial mostly getters andsetters. The abstract createQuery() method, however, is what subclasses must implement: it's how paging settings are applied to a CriteriaQuery before the query isexecuted.",
  "page3": "For offset-based paging, you need to know which page you're on. By default, you start with page 1. Test whether the sorting attribute of this page can be resolved against the\u00aeattribute path and therefore the model used by the query. The method throws an exception ifthe sorting attribute of the page wasn't available on the model class referenced in thequery. This is a safety mechanism that\u00aeproduces a meaningful error message if youpair the wrong paging settings with the wrong query. Add an ORDER BY clause to the query. Set the offset of the query: the starting result row. Cut the result off\u00aewith the desired page size.We haven't shown all the methods involved here, such as getRangeStartInteger(),which calculates the number of the first row that must be retrieved for the currentpage based on page size. You can find other simple convenience methods in thesource code. Note\u00aethat the result order may not be deterministic: if you sort by ascending itemname, and several items have the same name, the database will return them in whatever order the DBMS creators deemed appropriate. You should either sort by a uniquekey attribute or add an additional order criterion with a key attribute. Although manydev\u00aeelopers get away with ignoring deterministic sorting problems with the offsetmethod, a predictable record order is mandatory for the seek strategy.",
  "page4": "For seek paging, you need to add restrictions to a query. Let's assume that the previous page showed all items sorted by ascending name up to Coffee Machine, as shownin figure\u00ae19.2, and that you want to retrieve the next page with an SQL query. Youremembered the last value of the previous page, the Coffee Machine record and itsidentifier value (let's say 5), so you can write in SQL:The\u00aefirst restriction of the query says, \"Give me all items with a name greater than orequal to [Coffee Machine],\" which seeks forward to the end of the previous page. Thedatabase may perform this\u00aeefficient restriction with an index scan. Then you furtherrestrict the result by saying you want no records named Coffee Machine, thus skippingthe last record you've already shown on the previous page.But there may be two items named Coffee Machine in the database. A unique keyis nec\u00aeessary to prevent items from falling through the cracks between pages. You mustorder by and restrict the result with that unique key. Here you use the primary key,thus guaranteeing that the database includes only items not named Coffee Machine or(even if named Coffee Machine) with a greater identifier value than the one youshowed\u00aeon the previous page.Of course, if the item name (or any other column you sort by) is unique, you won'tneed an additional unique key. The generic example code assumes that you alwaysprovide an explicit unique key attribute. Also note that the fact that item identifiersare incremented numerical values isn't important; the important aspect is that this keyallows deterministic sorting.",
  "page5": "This kind of restriction \"message\" expression works even in JPQL in Hibernate. JPA doesn't standardized this, though; it's unavailable with the criteria API and not supported by all\u00aedatabase products. We prefer the slightly more verbose variation, which works everywhere. If you sort in descending order, you invert the comparisons from greater than toless thanIn addition to the regular sorting attribute\u00aebute, the seek technique requires a paging attribute that's a guaranteed unique key. This can be any unique attribute of your entitymodel, but it's usually the primary key attribute. For both\u00aethe sorting attribute and the unique key attribute, you must remember theirvalues from the \"last page.\" You can then retrieve the next page by seeking those values. Any Comparable value is fine, as required by the restriction API in criteria queries. You must always sort the resu\u00aelt by both the sorting attribute and the unique keyattribute. Add any necessary additional restrictions (not shown) to the where clause of thequery, seeking beyond the last known values to the target page.Cut off the result with the desired page size.The full applySeekRestriction() method can be found in the example code; this\u00aeis criteria query code we don't have enough space for here. The final query is equivalentto the SQL example you saw earlier. Let's test the new paging feature of the persistence layer.",
  "page6": "When you call -the ItemDAO#getItemBidSummaries() method now, you must provide aPage instance. A service or UI layer client on top of the persistence layer executes thefollowing code:\u00aeWith the offset strategy, all you had to know to jump to a page was the page number.With the seek strategy, you must remember the last values shown on the previous page:This is clearly more work for a client of the persistance\u00aelayer. Instead of a simple number, it must be able to remember the last values dynamically, depending on the sortand the unique attribute. We implemented the UIs for earlier screenshots with JSF\u00aeand CDI, including thenecessary glue code for our paging API. Look at the source code for inspiration onhow to integrate this technique with your own service and UI layer. Even if you don't use JSF, it should be straightforward to adapt our paging and sorting solutions to other web f\u00aerameworks. If you plan to use JSF, the next section is foryou: we dive into complex use cases such as placing a bid and editing an auction itemwith a JSF web interface and a stateful service layer.In the previous example, you can click an auction item name in the catalog. Thisbrings you to the auction page for that item, with more\u00aedetailed information, asshown in figure 19.3. The item auction page has one form, allowing you to place a bid on the item. This is the first use case to implement: placing a bid. You can handle this with a simplerequest-scoped service.",
  "page7": "The page is bookmarkable that is, it can and will be called with a query parameter,the identifier value of an Item. The view metadata tells JSF it should bind this parameter value\u00aeto the back-end property id of the AuctionService. You implement this newservice in a minute.Rendering this information repeatedly calls the AuctionService#getItem() method,something you have to keep in mind when you work\u00aeon the service implementation.Finally, here's the form for placing a bid:You need to transmit the identifier value of the item when the form is submitted. Theback-end service is request-scoped, so\u00aeit needs to be initialized for every request: thiscalls the AuctionService#setId() method. The entered bid amount is set by JSF with the AuctionService#setNewBidAmount()method when the POSTback of this form is processed. After all values have been bound, the action method AuctionService#\u00aeplaceBid() iscalled.You don't need to hold state across requests for this use case. A service instance is created when the auction page view is rendered with a GET request, and JSF binds therequest parameter by calling setId(). The service instance is destroyed after rendering is complete. Your server doesn't hold any\u00aestate between requests. When the auction form is submitted and processing of that POST request starts, JSF calls setId() tobind the hidden form field, and you can again initialize the state of the service. The state you hold for each request is the identifier value of the Item the user is working on, the Item after it was loaded, the currently highest bid amount for that item,and the new bid amount entered by the user.",
  "page8": "Remember that in JSF, any property accessor methods bound in an XHTML templatemay be called multiple times. In the AuctionService, data will be loaded when accessor methods are called\u00aeand you have to make sure you don't load from the databasemultiple times by accident:When JSF calls setId() for the first time in a request, you load the Item with that identifier value once. Fail immediately if\u00aethe entity instance can't be found. You also loadthe currently highest bid amount to initialize the state of this service fully, before theview can be rendered or the action method called. Note that\u00aethis is a nontransactional method! Unlike EJBs, methods in a simple CDIbean don't require a transaction to be active by default. The produced EntityManagerand the persistence context used by the DAOs are unsynchronized, and you read datafrom the database in auto-commit mode. Even\u00aeif later, while still processing the samerequest, a transactional method is called, the same unsynchronized, already-produced, request-scoped EntityManager is reused.The @Transactional annotation is new in Java EE 7 (from JTA 1.2) and similar to@TransactionAttribute on EJB components. Internally, an interceptor wraps themethod call\u00aein a system transaction context, just like an EJB method. Perform transactional work and store a new bid in the database, and prevent concurrent bids. You must join the persistence context with the transaction. It doesn't matterwhich DAO you call: all of them share the same request-scoped EntityManager.",
  "page9": "If another transaction is committed for a higher bid in the time the user was thinkingand looking at the rendered auction page, fail and re-render the auction page with amessage.\u00aeyou must force a version increment of the Item at flush time to prevent concurrent bids.If another transaction runs at the same time as this and loads the same Item version andcurrent highest bid from the database in setId()\u00aeone of the transactions must fail inplaceBid(). This is a simple redirect-after-POST in JSF, so users can safely reload the page after submitting a bid.When the placeBid() method returns, the transaction\u00aeis committed, and the joinedpersistence context flushes changes to the database automatically. In Java EE 7, youcan finally have the most convenient feature of EJBs: the declarative transactiondemarcation, independent from EJBs. Another new feature in Java EE is the CDI conversation\u00aescope and its integrationin JSF.You can declare beans and produced instances in Java EE as @ConversationScoped.This special scope is manually controllable with the standard javax.enterprise.context.Conversation API. To understand conversation scope, think about theexamples in the previous chapter. The shortest possible conversation\u00aea unit of work from the perspective of theapplication user, is a single request/response cycle. A user sends a request, and theserver starts a conversation context to hold the data for that request.",
  "page10": " When theresponse is returned, this short-running transient conversation context ends and isclosed. Transient conversation scope is therefore the same as the request scope; both\u00aecontexts have the same life cycle. This is how the CDI specification maps the conversation scope to servlet requests and therefore also JSF and JAX-RS requests. With the Conversation API, you can promote a conversation on\u00aethe server andmake it long-running and no longer transient. If you promote the conversation during a request, the same conversation context is then available to the next request. Youcan use the context\u00aeto transport data from one request to the next. Usually you'dneed the session context for this, but the data from several parallel conversations (auser opens two browser tabs) would have to be manually isolated in the session. This iswhat the conversation context provides: automatic d\u00aeata isolation in a controllable context, which is more convenient than the regular session context. You implement the conversation context with server-side sessions, of course, andstore the data in the user's session. To isolate and identify conversations within a session, each conversation has an identifier value; you must\u00aetransmit that value with everyrequest. The parameter name cid has even been standardized for this purpose. Eachconversation also has an individual timeout setting so the server can free resources if auser stops sending requests. This allows the server to clean up expired conversationstate automatically without waiting until the entire user session expires.",
  "page11": "Another nice feature of the long-running (not transient) conversation context is that the server protects concurrent access to conversation-scoped data automatically: if a user\u00aequickly clicks an action button several times, and each request transmits the identifier value of a particular conversation, the server will terminate all except one ofthe requests with BusyConversationExceptions. We now\u00aewalk through an example of conversation scope usage in JSF. You implement a use case with a workflow that takes the user several requests to complete: putting an item up for auction If you think about it\u00aefor a minute, creating and editing an auction item are very similar tasks. These workflows are conversations: units of work from the perspective of the application user, and your users will expect them to look similar. Your goal is there for to avoid code duplication; you should implement\u00aeboth cases with a single UI and backend service. Your application guides the user through the workflow; figure 19.4 shows a graphical representation as a state chart. Here's how to read this diagram. The user may start the conversation without anydata, if no item exists. Alternatively, the user may have the identifier value\u00aeof an existing item, which is probably a simple numeric value. How the user obtained this identifier isn't important; maybe they executed some kind of search in a previous conversation. This is a common scenario, and many conversation workflows have several entry points (the solid filled circles in the state chart). From the perspective of the user, editing an item is a multistep process: each boxwith rounded corners represents a state of the application.",
  "page12": " In the Edit Item state, the application presents the user with a dialog or form where they can change the item'sdescription or the starting price for the auction. Meanwhile,\u00aethe application waits for the user to trigger an event; we call this user think-time. When the user triggers the Next event, the application processes it, and the user proceeds to the next (wait) state, Edit Images. Whe\u00aen the user finishes working on the item images, they end the conversation by submitting the item and images (the solid circles with an outer circle). Clicking Cancel on any page aborts everything. From\u00aethe application user's perspective, the entire conversation is an atomic unit: all changes are committed and final when the user clicks Submit. No other transition or outcome should result in any permanent changes to the database. You'll implement this with a wizard-style user in\u00aeterface with two web pages. On the first page, the user enters the details of the item to sell or edit, as shown in figure 19.5.Note that rendering this page won't start a long-running conversation context on the server. This would waste resources, because you don't know yet whether the user wants to follow through and\u00aework on the item details. You begin the long-running conversation context and hold state across requests on the server when the user clicks the Next button for the first time. If form validation passes, the server stores the conversational data in the user's session and advances the user to the Edit Images page(see figure 19.6).",
  "page13": "The unit of work is complete when the user submits the item after uploading images for the item. The user may at any time click Cancel to end the conversation or walk away from the\u00aeterminal and let the conversation expire. The user may work into browser tabs and run several conversations in parallel, so they must be isolated from each other on the server within the same user session. All conversation\u00aedata for a user is removed if the user session expires (or if, in a cluster of servers with sticky sessions, a server fails). The XHTML templates for this wizard have no markup relevant for the\u00aeconversation; JSF with CDI automates this. Conversation identifiers are transmitted automatically with regular JSF form submissions in a generated hidden field, if a long-running conversation context was discovered when the form was rendered. This works even across JSF redirects: the c id par\u00aeameter is appended automatically to the redirect destination URL. The conversation-scoped back-end service bound to those pages the Edit Item Service is where everything happens and where you control the contexts.The service instance is conversation-scoped. By default, the conversation context istransient and therefore behaves as\u00aea request-scoped service. The class must be Serializable, unlike a request-scope implementation. An instanceof EditItemService may be stored in the HTTP session, and that session data may beserialized to disk or sent across the network in a cluster. We took the easy way out inchapter 18 by using a stateful EJB, saying, \"It's not passivation capable.\" Anything inthe CDI conversation scope must be passivation-capable and therefore serializable.",
  "page14": "The injected DAO instances have dependent scope and are serializable. You may thinkthey aren't, because they have an EntityManager field, which isn't serializable. We\u00aetalkabout this mismatch in a second.The Conversation API is provided by the container. Call it to control the conversationcontext. You need it when the user clicks the Next button for the first time, promotingthe transient\u00aeconversation to long-running.This is the state of the service: the item the user is editing on the pages of the wizard.You start with a fresh Item entity instance in transient state. If this service is\u00aeinitializedwith an item identifier value, load the Item in setItemId(). This is a transient state of the service. You only need it temporarily when the userclicks Upload on the Edit Images page. The Part class of the Servlet API isn't serializable. It's not uncommon to have some\u00aetransient state in a conversational service, butyou must initialize it for every request when it's needed.setItemId() is called only if the request contains an item identifier value. You therefore have two entry points into this conversation: with or without an existing item'sidentifier value. If the user is editing an\u00aeitem, you must load it from the database. You're still relying ona request-scoped persistence context, so as soon as the request is complete, this Item instance is in detached state. You can hold detached entity instances in a conversational service's state and merge it when needed to persist changes (see section 10.3.4).",
  "page15": "Unlike stateful EJBs, you can't disable passivation of the conversation context. The CDIspecification requires that the class and all non-transient dependencies of a conversation\u00aescoped component be serializable. In fact, you'll get a deployment error if youmake a mistake and include state that can't be serialized in a conversation-scopedbean or any of its dependencies. To pass this\u00aetest, we lied earlier by saying that GenericDAO is java.io.Serializable.The EntityManager field in GenericDAOImpl is not serializable! This works because CDIuses contextual references smart placeholders.\u00aeThe EntityManager field of the DAOs isn't an actual instance of a persistence context at runtime. The field holds a reference to some EntityManager: some currentpersistence context. Remember that CDI produces and injects the dependencythrough the constructor. Because you declared\u00aeit request-scoped, at runtime a specialproxy is injected that only looks like a real EntityManager. This proxy delegates allcalls to an actual EntityManager it finds in the current request context. The proxy isserializable and doesn't hold a reference to an EntityManager after a request is complete. It can then be easily seri\u00aealized; and when it's deserialized, maybe even on a different JVM, it will continue to do its work and obtain a request-scoped EntityManagerwhenever called. Therefore, you're not serializing the entire persistence context only a proxy that can look up the current request-scoped persistence context.",
  "page16": "This may sound strange at first, but it's how CDI works: if a request-scoped bean isinjected into a conversation-, session-, or application-scoped bean, an indirect reference\u00aeis required. If you try to call the EntityManager proxy through a DAO when norequest context is active (say, in a servlet's init() method), you'll get a ContextNotActiveException because the proxy can'to\u00aeobtain a current EntityManager. The CDIspecification also defines that such proxies may be passivated (serialized), even whenthe component they represent may not be. Assume now that the user has filled out\u00aethe form on the first page of the wizardwith the item details and clicks Next. You have to promote the transient conversationcontext.Clicking Next in the wizard submits the form with the Item details and takes the user tothe Edit Images page. Because the EditItemService is bound to a\u00aetransient conversation context, processing this request happens in a new, transient conversation context.Remember that the transient conversation scope is the same as the request scope. When you process the request with an action method, you call the ConversationAPI to control the current transient conversation context:The action me\u00aethod is called after all the Item details have been set on the EditItemService#item property by the JSF engine. You make the transient conversation longrunning with an individual timeout setting. This timeout is obviously shorter than orequal to the timeout of the user's session; larger values don't make sense.",
  "page17": "The serverpreserves the state of the service in the user's session and renders an automaticallygenerated conversation identifier as a hidden field on any action form on the\u00aeitImages page. If you need it, you can also obtain the identifier value of the conversation with Conversation#getId(). You can even set your own identifier value in theConversation#begin() method call. The server now\u00aewaits for the next request with the identifier of the conversation,most likely originating from the Edit Images page. If this takes too long, becauseeither the long-running conversation or the entire session\u00aeexpired, a NonexistentConversationException is thrown on request.Create the Image entity instance from the submitted multipart form.You must add the transient Image to the transient or detached Item. This conversationwill consume more and more memory on the server as uploaded image data\u00aeis addedto conversational state and therefore the user's session.One of the most important issues when building a stateful server system is memoryconsumption and how many concurrent user sessions the system can handle. Youmust be careful with conversational data. Always question whether you must hold datayou get from the us\u00aeer or data you load from the database for the entire conversation. When the user clicks Submit Item, the conversation ends, and all transient anddetached entity instances must be stored.",
  "page18": "The system transaction interceptor wraps the method call.You must join the unsynchronized request-scoped persistence context with the systemtransaction if you want to store data.\u00aeThis DAO call makes the transient or detached Item persistent. Because you enabled itwith a cascading rule on the @OneToMany, it also stores any new transient or olddetached Item#images collection elements. According to\u00aethe DAO contract, you musttake the returned instance as the current state. Manually end the long-running conversation. This is effectively a demotion: the longrunning conversation becomes transient. You\u00aedestroy the conversation context andthis service instance when the request is complete. All conversational state is removedfrom the user's session. This is a redirect-after-POST in JSF to the auction item details page, with the new identifier value of the now-persistent Item.This comp\u00aecompletes our example of stateful services with JSF, CDI, and a JPA persistencelayer. We think JSF and CDI are great in combination with JPA; you get a well-testedand standardized programming model with little overhead, both in terms of resourceconsumption and lines of code. We now continue with CDI but, instead of JSF, introduce JAX-\u00aeRS combined with JPAin a stateless server design for any rich clients. One of the challenges you face in thisarchitecture is serializing data.",
  "page19": "When we first talked about writing persistence-capable classes in section 3.2.3, webriefly mentioned that the classes don't have to implement java.io.Serializable.You can apply\u00aethis marker interface when needed. One of the cases when this was necessary so far was in chapter 18. Domain modelinstances were transmitted between EJB client and server systems, and they were automatically serialized\u00aeon one end into some wire format and deserialized on the otherend. This worked flawlessly and without customization because both client and serverare Java virtual machines, and the Hibernate libraries\u00aewere available on both systems.The client used Remote Method Invocation (RMI) and the standardized Java serialization format (a stream of bytes representing Java objects). If your client isn't running in a Java virtual machine, you probably don't want toreceive a stream of bytes\u00aerepresenting Java objects from the server. A common scenario is a stateless server system that handles a rich client, such as a JavaScript application running in a web browser or a mobile device application. To implement this, youtypically create a Web API on the server that speaks HTTP and transmit either XML orJSON payloads,\u00aewhich clients must then parse. The clients also send XML or JSON datawhen changes must be stored on the server, so your server must be able to produceand consume the desired media types.",
  "page20": "You now write an HTTP server with the JAX-RS framework, producing and consumingXML documents. Although the examples are in XML, they're equally applicable toJSON, and the\u00aefundamental problems we discuss are the same in both.Let's start with the JAX-RS service. One service method delivers an XML document representing an Item entity instance when a client sends a GET HTTP request. Another\u00aeservice method accepts an XML document for updating an Item in a PUT request:When the server receives a request with the request path /item, the method on thisservice handles it. By default, the service\u00aeinstance is request-scoped, but you can applyCDI scoping annotations to change that. An HTTP GET request maps to this method. The container uses the path segment after /item as an argument value for the call,such as /item/123. You map it to a method parameter with @PathParam. This method\u00aeproduces XML media; therefore, someone has to serialize the method'sreturned value into XML. Be careful: this annotation isn't the same producer annotation as in CDI. It's in a different package! This method consumes XML media; therefore, someone has to deserialize the XMLdocument and transform it into a detached It\u00aeem instance.You want to store data in this method, so you must start a system transaction and jointhe persistence context with it.The JAX-RS standard covers automatic marshalling for the most important mediatypes and a whole range of Java types. A JAX-RS implementation, for example, must beable to produce and consume XML media for an application-supplied Java Architecture for XML Binding (JAXB) class. The domain model entity class Item must therefore become a JAXB class.",
  "page21": "JAXB, much like JPA, works with annotations to declare a class's capabilities. Theseannotations map the properties of a class to elements and attributes in an XML document.\u00aeThe JAXB runtime automatically reads and writes instances from and to XMLdocuments. This should sound familiar to you by now; JAXB is a great companion forJPA-enabled domain models.An Item instance maps to an <item> XML\u00aeelement. This annotation effectively enablesJAXB on the class. When serializing or deserializing an instance, JAXB should call the fields directly andnot the getter or setter methods. The reasoning behind\u00aethis is the same as for JPA:freedom in method design.The identifier and auction end date of the item become XML attributes, and all otherproperties are nested XML elements. You don't have to put any JAXB annotations ondescription and initialPrice; they map to elements by default. Si\u00aeSingular attributesof the domain model class are easy: they're either XML attributes or nested XML elements with some text. What about entity associations and collections? You can embed a collection directly in the XML document and thus eagerlyinclude it, as you did with the Item#bids:There is some optimization potential here:\u00aeif you say, \"Always eagerly include bids\"when your service returns an Item, then you should load them eagerly. Right now, several queries are necessary in JPA to load the Item and the default lazy-mappedItem#bids. The JAXB serializer automatically iterates through the collection elementswhen the response is prepared.",
  "page22": "Hibernate initializes the collection data either lazily or eagerly, and JAXB (or anyother serializer you have) serializes each element in turn. The fact that Hibernate usesspecial\u00aecollections internally, as discussed in section 12.1.2, makes no difference whenyou serialize. It will later be important when you deserialize an XML document, butlet's ignore this issue for now. When you don't\u00aewant to include a collection or a property in the XML document,use the @XmlTransient annotation:Collections are easy to handle, regardless of whether they're collections of primitives,embeddables,\u00aeor many-valued entity associations. Of course, you must be careful withcircular references, such as each Bid having a (back) reference to an Item. At somepoint, you must make a break and declare a reference transient. The most difficult issue you face when serializing an entity instance\u00aeloaded byHibernate is internal proxies: the placeholders used for lazy loading of entity associations. In the Item class, this is the seller property, referencing a User entity.Such a document would indicate to a client that the item has no seller. This is ofcourse wrong; an uninitialized proxy is not the same as null! You could a\u00aessign specialmeaning to an empty XML element and say, on the client, \"An empty element means aproxy\" and \"A missing element means null.\" Unfortunately, we've seen serializationsolutions, even designed for Hibernate, that don't make this distinction. Some serialization solutions, not designed for Hibernate, may even stumble and fail as soon asthey discover a Hibernate proxy. Usually, you must customize your serialization tool to deal with Hibernate proxiesin some meaningful way. In this application, you want the following XML data for anuninitialized entity proxy.",
  "page23": "This is the same data as the proxy: the entity class and the identifier value representedby the proxy. A client now knows that there is indeed a seller for this item and the identifier\u00aeof that user; it can request this data if needed. If you receive this XML document on the server when a user updates an item, you can reconstruct a proxy from theentity class name and identifier value. You should\u00aewrite a model class that represents such an entity reference and map itto XML elements and attributes:Next, you must customize marshalling and unmarshalling the Item, so instead of areal User, an EntityReference\u00aehandles the Item#seller property. In JAXB, you applya custom type adapter on the property:JAXB calls this constructor when it generates an XML document. In that case, youdon't need an EntityManager: the proxy contains all the information you need towrite an EntityReference.\u00aeJAXB must call this constructor when it reads an XML document. You need an EntityManager to get a Hibernate proxy from an EntityReference.When writing an XML document, take the Hibernate proxy and create a serializablerepresentation. This calls internal Hibernate methods that we haven't shown here.When reading an XML document,\u00aetake the serialized representation and create aHibernate proxy attached to the current persistence context.",
  "page24": "Finally, you need an extension for JAX-RS that will automatically initialize this adapterwith the current request-scoped EntityManager when an XML document has to beunmarshalled on\u00aethe server. You can find this EntityReferenceXMLReader extensionin the example code for this book. There are a few remaining points we need to discuss. First, we haven't talked aboutunmarshalling collections. Any\u00ae<bids> element in the XML document will be deserialized when the service is called, and detached instances of Bid will be created from thatdata. You can access them on the detached Item#bids when your\u00aeservice runs. Nothing else will happen or can happen, though: the collection created during unmarshalling by JAXB isn't one of the special Hibernate collections. Even if you had enabledcascaded merging of the Item#bids collection in your mapping, it would be ignoredby EntityManager#mer\u00aege(). This is similar to the proxy problem you solved in the previous section. You wouldhave to detect that you must create a special Hibernate collection when a particularproperty is unmarshalled in an XML document. You'd have to call some Hibernateinternal APIs to create that magic collection. We recommend that you consider\u00aecollections to be read-only; collection mappings in general are a shortcut for embeddingdata in query results when you send data to the client. When the client sends an XMLdocument to the server, it shouldn't include any <bids> element. On the server, youonly access the collection on the persistent Item after it's merged (ignoring the collection during merge).",
  "page25": " Second, you're probably wondering where our JSON examples are. We knowyou're most likely relying on JSON right now in your applications and not on a customXML media type\u00ae.JSON is a convenient format to parse in a JavaScript client. The badnews is that we couldn't figure out a way to customize JSON marshalling and unmarshalling in JAX-RS without relying on a proprietary framework.\u00aeAlthough JAX-RS maybe standardized, how it generates and reads JSON isn't standardized; some JAX-RSimplementations use Jackson, whereas others use Jettison. There is also the new standard Java API\u00aefor JSON Processing (JSONP), which some JAX-RS implementations mayrely on in the future. If you want to use JSON with Hibernate, you must write the same extension code wewrote for JAXB, but for your favorite JSON marshalling tool. You'll have to customizeproxy handling, how proxy data\u00aeis sent to the client, and how proxy data is turned backinto entity references with em.getReference(). You'll certainly have to rely on someextension API of your framework, just as we did with JAXB, but the same pattern applies.You've seen many ways to integrate Hibernate and JPA in a web application environment. You en\u00aeabled the EntityManager for CDI injection and improved thepersistence layer with CDI and a generic sorting and paging solution for finderqueries.You looked at JPA in a JSF web application: how to write request- and conversation-scoped services with a JPA persistence context.We discussed problems and solutions associated with entity data serialization,and how to resolve those in an environment with stateless clients and a JAX-RSserver.",
  "page26": "You use object/relational mapping to move data into the application tier in orderto use an object-oriented programming language to process that data. This is agood strategy when\u00aeimplementing a multiuser online transaction-processing application with small to medium size data sets involved in each unit of work. On the other hand, operations that require massive amounts of data aren't bestsuited\u00aefor the application tier. You should move the operation closer to where thedata lives, rather than the other way around. In an SQL system, the DML statementsUPDATE and DELETE execute directly in the\u00aedatabase and are often sufficient if youhave to implement an operation that involves thousands of rows. Operations thatare more complex may require additional procedures to run inside the database;therefore, you should consider stored procedures as one possible strategy. You canfall back t\u00aeo JDBC and SQL at any time in Hibernate applications. We discussed somethese options earlier, in chapter 17. In this chapter, we show you how to avoid falling back to JDBC and how to execute bulk and batch operations with Hibernateand JPA.A major justification for our claim that applications using an object/relational persistence\u00aelayer outperform applications built using direct JDBC is caching. Although weargue passionately that most applications should be designed so that it's possible toachieve acceptable performance without the use of a cache, there's no doubt that forsome kinds of applications, especially read-mostly applications or applications thatkeep significant metadata in the database, caching can have an enormous impact onperformance.",
  "page27": "Furthermore, scaling a highly concurrent application to thousands ofonline transactions per second usually requires some caching to reduce the load onthe database server(s). After\u00aediscussing bulk and batch operations, we explore Hibernate's caching system.First we look at standardized bulk statements in JPQL, such as UPDATE and DELETE, andtheir equivalent criteria versions. After that, we\u00aerepeat some of these operations withSQL native statements. Then, you learn how to insert and update a large number ofentity instances in batches. Finally, we introduce the special org.hibernate.StatelessSession\u00aeAPIThe Java Persistence Query Language is similar to SQL. The main difference betweenthe two is that JPQL uses class names instead of table names and property namesinstead of column names. JPQL also understands inheritance that is, whether you'requerying with a superclass or an\u00aeinterface. The JPA criteria query facility supports thesame query constructs as JPQL but in addition offers type-safe and easy programmaticstatement creation. The next statements we show you support updating and deleting data directly inthe database without the need to retrieve them into memory. We also provide a statement that c\u00aean select data and insert it as new entity instances, directly in the database.JPA offers DML operations that are a little more powerful than plain SQL. Let's look atthe first operation in JPQL: an UPDATE.",
  "page28": "This JPQL statement looks like an SQL statement, but it uses an entity name (classname) and property names. The aliases are optional, so you can also write updateItem set active\u00aetrue. You use the standard query API to bind named and positionalparameters. The executeUpdate call returns the number of updated entity instances,which may be different from the number of updated database rows, depending\u00aeon themapping strategy. This UPDATE statement only affects the database; Hibernate doesn't update anyItem instance you've already retrieved into the (current) persistence context. In theprevious\u00aechapters, we've repeated that you should think about state management ofentity instances, not how SQL statements are managed. This strategy assumes that theentity instances you're referring to are available in memory. If you update or deletedata directly in the database, what\u00aeyou've already loaded into application memory,into the persistence context, isn't updated or deleted. A pragmatic solution that avoids this issue is a simple convention: execute anydirect DML operations first in a fresh persistence context. Then, use the EntityManager to load and store entity instances. This convention\u00aeguarantees that the persistence context is unaffected by any statements executed earlier. Alternatively, you canselectively use the refresh() operation to reload the state of an entity instance in thepersistence context from the database, if you know it's been modified outside of thepersistence context.",
  "page29": "Hibernate knows how to execute this update, even if several SQL statements have to begenerated or some data needs to be copied into a temporary table; it updates rows inseveral base\u00aetables (because CreditCard is mapped to several superclass and subclasstables).JPQL UPDATE statements can reference only a single entity class, and criteria bulkoperations may have only one root entity; you can't\u00aewrite a single statement to updateItem and CreditCard data simultaneously, for example. Subqueries are allowed in theWHERE clause, and any joins are allowed only in these subqueries. You can update valu\u00aees of an embedded type: for example, update User u setu.homeAddress.street You can't update values of an embeddable type in acollection. This isn't allowed: update Item i set i.images.title Direct DML operations, by default, don't affect any version or timestamp values inth\u00aee affected entities (as standardized by JPA). But a Hibernate extension lets youincrement the version number of directly modified entity instances:The version of each updated Item entity instance will now be directly incremented inthe database, indicating to any other transaction relying on optimistic concurrencycontrol that you m\u00aeodified the data. (Hibernate doesn't allow use of the versionedkeyword if your version or timestamp property relies on a custom org.hibernate.usertype.UserVersionType.) With the JPA criteria API, you have to increment the version yourself.",
  "page30": "The same rules for UPDATE statements and CriteriaUpdate apply to DELETE andCriteriaDelete: no joins, single entity class only, optional aliases, or subqueriesallowed in the WHERE\u00aeclause. Another special JPQL bulk operation lets you create entity instances directly in thedatabase.Let's assume that some of your customers' credit cards have been stolen. You write twobulk operations to mark\u00aethe day they were stolen (well, the day you discovered thetheft) and to remove the compromised credit-card data from your records. Becauseyou work for a responsible company, you have to report the stole\u00aen credit cards to theauthorities and affected customers. Therefore, before you delete the records, youextract everything stolen and create a few hundred (or thousand) StolenCreditCardrecords. You write a new mapped entity class just for this purpose:Hibernate maps this class to the STOLEN\u00aeCREDITCARD table. Next, you need a statementthat executes directly in the database, retrieves all compromised credit cards, and creates new StolenCreditCard records. This is possible with the Hibernate-only INSERT... SELECT statement:This operation does two things. First, it selects the details of CreditCard records andthe respect\u00aeive owner (a User). Second, it inserts the result directly into the tablemapped by the StolenCreditCard class. Note the following: The properties that are the target of an INSERT ... SELECT (in this case, theStolenCreditCard properties you list) have to be for a particular subclass, notan (abstract) superclass. Because StolenCreditCard isn't part of an inheritancehierarchy, this isn't an issue.",
  "page31": "The types returned by the projection in the SELECT must match the typesrequired for the arguments of the INSERT. In the example, the identifier property of StolenCreditCard is in\u00aethe list ofinserted properties and supplied through selection; it's the same as the originalCreditCard identifier value. Alternatively, you can map an identifier generatorfor StolenCreditCard; but this works only for\u00aeidentifier generators that operate directly inside the database, such as sequences or identity fields.If the generated records are of a versioned class (with a version or timestampproperty), a fresh ve\u00aersion (zero, or the current timestamp) is also generated.Alternatively, you can select a version (or timestamp) value and add the version(or timestamp) property to the list of inserted properties.The INSERT ... SELECT statement was, at the time of writing, not supported by the JPAor Hiber\u00aenate criteria APIs.JPQL and criteria bulk operations cover many situations in which you'd usuallyresort to plain SQL. In some cases, you may want to execute SQL bulk operations without falling back to JDBC. In the previous section, you saw JPQL UPDATE and DELETE statements. The primaryadvantage of these statements is that the\u00aey work with class and property names and thatHibernate knows how to handle inheritance hierarchies and versioning when generating SQL. Because Hibernate parses JPQL, it also knows how to efficiently dirty-checkand flush the persistence context before the query and how to invalidate second-levelcache regions.",
  "page32": "With JPA native bulk statements, you must be aware of one important issue: Hibernatewill not parse your SQL statement to detect the affected tables. This means Hibernatedoesn't\u00aeknow whether a flush of the persistence context is required before the queryexecutes. In the previous example, Hibernate doesn't know you're updating rows inthe ITEM table. Hibernate has to dirty-check and\u00aeflush any entity instances in the persistence context when you execute the query; it can't only dirty-check and flush Iteminstances in the persistence context. You must consider another issue if you\u00aeenable the second-level cache (if you don't,don't worry): Hibernate has to keep your second-level cache synchronized to avoidreturning stale data, so it will invalidate and clear all second-level cache regions whenyou execute a native SQL UPDATE or DELETE statement. This means y\u00aeour second-levelcache will be empty after this query!With the addSynchronizedEntityClass() method, you can let Hibernate know whichtables are affected by your SQL statement and Hibernate will clear only the relevantcache regions. Hibernate now also knows that it has to flush only modified Item entityinstance in the persistence con\u00aetext, before the query. Sometimes you can't exclude the application tier in a mass data operation. Youhave to load data into application memory and work with the EntityManager to perform your updates and deletions, which brings us to batch processing.",
  "page33": "If you have to create or update a few hundred or thousand entity instances in onetransaction and unit of work, you may run out of memory. Furthermore, you have toconsider the time\u00aeit takes for the transaction to complete. Most transaction managershave a low transaction timeout, in the range of seconds or minutes. The Bitronixtransaction manager used for the examples in this book has a default\u00aetransactiontimeout of 60 seconds. If your unit of work takes longer to complete, you should firstoverride this timeout for a particular transaction:This is the UserTransaction API. Only future transactions\u00aestarted on this thread willhave the new timeout. You must set the timeout before you begin() the transaction. Next, let's insert a few thousand Item instances into the database in a batch.Every transient entity instance you pass to EntityManager#persist() is added to thepersistence c\u00aeontext cache, as explained in section 10.2.8. To prevent memory exhaustion, you flush() and clear() the persistence context after a certain number ofinsertions, effectively batching the inserts.Create and persist 100,000 Item instances. After 100 operations, flush and clear the persistence context. This executes the SQLINSERT stat\u00aeements for 100 Item instances, and because they're now in detached stateand no longer referenced, the JVM garbage collection can reclaim that memory.You should set the hibernate.jdbc.batch_size property in the persistence unit tothe same size as your batch, here 100. With this setting, Hibernate will batch theINSERT statements at the JDBC level, with PreparedStatement#addBatch().",
  "page34": "If you enable the shared second-level cache for the Item entity, you should thenbypass the cache for your batch (insertion) procedure; see section 20.2.5. A serious problem with\u00aemass insertions is contention on the identifier generator:every call of EntityManager#persist() must obtain a new identifier value. Typically, the generator is a database sequence, called once for every persisted entity\u00aeinstance. You have to reduce the number of database round trips for an efficientbatch procedure.Now use the generator with @GeneratedValue in your mapped entity classes. With increment_size set to 100, the\u00aesequence produces the \"next\" values 100,200, 300, 400, and so on. The pooled-lo optimizer in Hibernate generates intermediate values each time you call persist(), without another round trip to the database.Therefore, if the next value obtained from the sequence is 100, Hibernat\u00aee will generate the identifier values 101, 102, 103, and so on in the application tier. Once the optimizer's pool of 100 identifier values is exhausted, the database obtains the nextsequence value, and the procedure repeats. This means you only make one round tripto get an identifier value from the database per batch of 100 i\u00aensertions. Other identifier generator optimizers are available, but the pooled-lo optimizer covers virtually alluse cases and is the easiest to understand and configure. Be aware that an increment size of 100 will leave large gaps in between numericidentifiers if an application uses the same sequence but doesn't apply the same algorithm as Hibernate's optimizer. This shouldn't be too much of a concern; instead ofbeing able to generate a new identifier value each millisecond for 300 million years,you might exhaust the number space in 3 million years. You can use the same batching technique to update large number of entityinstances.",
  "page35": "Imagine that you have to manipulate many Item entity instances and that the changesyou need to make aren't as trivial as setting a flag (which you've done with a singleUP\u00aeDATE JPQL statement previously). Let's also assume that you can't create a databasestored procedure, for whatever reason (maybe because your application has to workon database-management systems that don't\u00aesupport stored procedures). Your onlychoice is to write the procedure in Java and to retrieve a massive amount of data intomemory to run it through the procedure. This requires working in batches and s\u00aecrolling through a query result with a database cursor, which is a Hibernate-only query feature. Please review our explanation ofscrolling with cursors in section 14.3.3 and make sure database cursors are properlysupported by your DBMS and JDBC driver. The following code loads 100 Item en\u00aetityinstances at a time for processing.You use a JPQL query to load all Item instances from the database. Instead of retrieving the result of the query completely into application memory, you open an onlinedatabase cursor. You control the cursor with the ScrollableResults API and move it along the result.Each call to next() forwar\u00aeds the cursor to the next record. The get(int i) call retrieves a single entity instance into memory: the record the cursor is currently pointing to.To avoid memory exhaustion, you flush and clear the persistence context before loading the next 100 records into it.",
  "page36": "For the best performance, you should set the size of the property hibernate.jdbc.batch_size in the persistence unit configuration to the same value as your procedure batch: 100.\u00aeHibernate batches at the JDBC level all UPDATE statements executedwhile flushing. By default, Hibernate won't batch at the JDBC level if you've enabledversioning for an entity class some JDBC drivers have trouble\u00aereturning the correctupdated row count for batch UPDATE statements (Oracle is known to have this issue).If you're sure your JDBC driver supports this properly, and your Item entity class hasan @Ve\u00aersion annotation, enable JDBC batching by setting the property hibernate.jdbc.batch_versioned_data to true. If you enable the shared second-levelcache for the Item entity, you should then bypass the cache for your batch (update)procedure; see section 20.2.5. Another option that avoids mem\u00aeory consumption in the persistence context (byeffectively disabling it) is the org.hibernate.StatelessSession interface.The persistence context is an essential feature of the Hibernate engine. Without apersistence context, you can't manipulate entity state and have Hibernate detect yourchanges automatically. Many other things\u00aewouldn't also be possible. Hibernate offers an alternative interface, however, if you prefer to work with yourdatabase by executing statements. The statement-oriented interface org.hibernate.StatelessSession, feels and works like plain JDBC, except that you get the benefit of mapped persistent classes and Hibernate's database portability. The mostinteresting methods in this interface are insert(), update(), and delete(), which allmap to the equivalent immediately executed JDBC/SQL operation.",
  "page37": "Open a StatelessSession on the Hibernate SessionFactory, which you can unwrapfrom an EntityManagerFactory.Use a JPQL query to load all Item instances from the database. Instead of\u00aeretrievingthe result of the query completely into application memory, open an online databasecursor.Scroll through the result with the cursor, and retrieve an Item entity instance. Thisinstance is in detached state; there\u00aeis no persistence context! Because Hibernate doesn't detect changes automatically without a persistence context, you have to execute SQL UPDATE statements manually.Disabling the persistence conte\u00aext and working with the StatelessSession interfacehas some other serious consequences and conceptual limitations (at least, if you compare it to a regular EntityManager and org.hibernate.Session): The StatelessSession doesn't have a persistence context cache and doesn'tinteract\u00aewith any other second-level or query cache. There is no automatic dirtychecking or SQL execution when a transaction commits. Everything you doresults in immediate SQL operations. No modification of an entity instance and no operation you call are cascaded toany associated instance. Hibernate ignores any cascading rules in your map\u00aepings. You're working with instances of a single entity class. You have no guaranteed scope of object identity. The same query executedtwice in the same StatelessSession produces two different in-memorydetached instances. This can lead to data-aliasing effects if you don't carefullyimplement the equals() and hashCode() methods in your persistent classes.",
  "page38": "Hibernate ignores any modifications to a collection that you mapped as anentity association (one-to-many, many-to-many). Only collections of basic orembeddable types are considered\u00ae. Therefore, you shouldn't map entity associations with collections but only many-to-one or one-to-one and handle the relationship through that side only. Write a query to obtain data you'd otherwiseretrieve\u00aeby iterating through a mapped collection. Hibernate doesn't invoke JPA event listeners and event callback methods foroperations executed with StatelessSession. StatelessSession bypasses anyenabled or\u00aeg.hibernate.Interceptor, and you can't intercept it through theHibernate core event system.Good use cases for a StatelessSession are rare; you may prefer it if manual batchingwith a regular EntityManager becomes cumbersome. In the next section, we introduce the Hibernate shared cachi\u00aeng system. Cachingdata on the application tier is a complementary optimization that you can utilize inany sophisticated multiuser application.In this section, we show you how to enable, tune, and manage the shared data cachesin Hibernate. The shared data cache is not the persistence context cache, whichHibernate never shares betwe\u00aeen application threads. For reasons explained in section10.1.2, this isn't optional. We call the persistence context a first-level cache. The shareddata cache the second-level cache is optional, and although JPA standardizes someconfiguration settings and mapping metadata for shared caching, every vendor hasdifferent solutions for optimization. Let's start with some background informationand explore the architecture of Hibernate's shared cache.",
  "page39": "A cache keeps a representation of current database state close to the application,either in memory or on disk of the application server machine. A cache is a local copyof the data\u00aeand sits between your application and the database. Simplified, to Hibernate a cache looks like a map of key/value pairs. Hibernate can store data in the cacheby providing a key and a value, and it can look up a value\u00aein the cache with a key. Hibernate has several types of shared caches available. You may use a cache toavoid a database hit whenever the following take place: The application performs an entity instance l\u00aeookup by identifier (primary key);this may get a hit in the entity data cache. Initializing an entity proxy on demandis the same operation and, internally, may hit the entity data cache instead ofthe database. The cache key is the identifier value of the entity instance, andthe cache valu\u00aee is the data of the entity instance (its property values). Theactual data is stored in a special disassembled format, and Hibernate assemblesan entity instance again when it reads from the entity data cache.The persistence engine initializes a collection lazily; a collection cache may holdthe elements of the collection. The cache\u00aekey is the collection role: for example, \"Item[1234]#bids\" would be the bids collection of an Item instance withidentifier 1234. The cache value in this case would be a set of Bid identifier values, the elements of the collection. (Note that this collection cache does nothold the Bid entity data, only the data's identifier values!).",
  "page40": "The application performs an entity instance lookup by a unique key attribute.This is a special natural identifier cache for entity classes with unique properties:for example, User#\u00aeusername. The cache key is the unique property, such as theusername, and the cached value is the User's entity instance identifier. You execute a JPQL, criteria, or SQL query, and the result of the actual SQLquery\u00aeis already stored in the query result cache. The cache key is the renderedSQL statement including all its parameter values, and the cache value is somerepresentation of the SQL result set, which may incl\u00aeude entity identifier values.It's critically important to understand that the entity data cache is the only type ofcache that holds actual entity data values. The other three cache types only hold entityidentifier information. Therefore, it doesn't make sense to enable the natur\u00aeal identifier cache, for example, without also enabling the entity data cache. A lookup in thenatural identifier cache will, when a match is found, always involve a lookup in theentity data cache. We'll further analyze this behavior below with some code examples. As we hinted earlier, Hibernate has a two-level cache architect\u00aeure.You can see the various elements of Hibernate's caching system in figure 20.1. Thefirst-level cache is the persistence context cache, which we discussed in section 10.1.2.Hibernate does not share this cache between threads; each application thread has itsown copy of the data in this cache. Hence, there are no issues with transaction isolation and concurrency when accessing this cache.",
  "page41": "The second-level cache system in Hibernate may be process-scoped in the JVM ormay be a cache system that can work in a cluster of JVMs. Multiple application threadsmay access the\u00aeshared second-level caches concurrently. The cache concurrency strategydefines the transaction isolation details for entity data, collection elements, andnatural identifier caches. Whenever an entry is stored or loaded\u00aein these caches,Hibernate will coordinate access with the configured strategy. Picking the right cacheconcurrency strategy for entity classes and their collections can be challenging, andwe'll guide\u00aeyou through the process with several examples later on. The query result cache also has its own, internal strategy for handling concurrentaccess and keeping the cached results fresh and coordinated with the database. Weshow you how the query cache works and for which queries it makes sens\u00aee to enableresult caching. The cache provider implements the physical caches as a pluggable system. For now,Hibernate forces you to choose a single cache provider for the entire persistence unit.The cache provider is responsible for handling physical cache regions the bucketswhere the data is held on the application tier (in memor\u00aey, in indexed files, or evenreplicated in a cluster). The cache provider controls expiration policies, such as whento remove data from a region by timeout, or keeping only the most-recently used datawhen the cache is full. The cache provider implementation may be able to communicate with other instances in a cluster of JVMs, to synchronize data in each instance'sbuckets. Hibernate itself doesn't handle any clustering of caches; this is fully delegated to the cache provider engine.",
  "page42": "In this section, you set up caching on a single JVM with the Ehcache provider, a simple but very powerful caching engine (originally developed for Hibernate specificallyas the easy\u00aeHibernate cache). We only cover some of Ehcache's basic settings; consult itsmanual for more information. Frequently, the first question many developers have about the Hibernate cachingsystem is, \"Will the ca\u00aeche know when data is modified in the database?\" Let's try toanswer this question before you get hands-on with cache configuration and usage.If an application does not have exclusive access to\u00aethe database, shared caching shouldonly be used for data that changes rarely and for which a small window of inconsistency is acceptable after an update. When another application updates the database,your cache may contain stale data until it expires. The other application may be adatabas\u00aee-triggered stored procedure or even an ON DELETE or ON UPDATE foreign keyoption. There is no way for Hibernate's cache system to know when another application or trigger updates the data in the database; the database can't send your application a message. (You could implement such a messaging system with database trigge\u00aersand JMS, but doing so isn't exactly trivial.) Therefore, using caching depends on thetype of data and the freshness of the data required by your business case. Let's assume for a moment that your application has exclusive access to the database. Even then, you must ask the same questions as a shared cache makes dataretrieved from the database in one transaction visible to another transaction. Whattransaction isolation guarantees should the shared cache provide? The shared cachewill affect the isolation level of your transactions, whether you read only committeddata or if reads are repeatable.",
  "page43": "For some data, it may be acceptable that updates by oneapplication thread aren't immediately visible by other application threads, providingan acceptable window of inconsisten\u00aecy. This would allow a much more efficient andaggressive caching strategy. Start this design process with a diagram of your domain model, and look at theentity classes. Good candidates for caching are classes that repre\u00aesent Data that changes rarely Noncritical data (for example, content-management data)Data that's local to the application and not modified by other applicationsBad candidates include Data that is up\u00aedated often Financial data, where decisions must be based on the latest update Data that is shared with and/or written by other applicationsThese aren't the only rules we usually apply. Many applications have a number ofclasses with the following properties: A small number of instanc\u00aees (thousands, not millions) that all fit into memory Each instance referenced by many instances of another class or classes Instances that are rarely (or never) updatedWe sometimes call this kind of data reference data. Examples of reference data are Zipcodes, locations, static text messages, and so on. Reference data is an excel\u00aelent candidate for shared caching, and any application that uses reference data heavily will benefit greatly from caching that data. You allow the data to be refreshed when the cachetimeout period expires, and some small window of inconsistency is acceptable after anupdate. In fact, some reference data (such as country codes) may have an extremelylarge window of inconsistency or may be cached eternally if the data is read-only. You must exercise careful judgment for each class and collection for which youwant to enable caching. You have to decide which concurrency strategy to use.",
  "page44": "A cache concurrency strategy is a mediator: it's responsible for storing items of data inthe cache and retrieving them from the cache. This important role defines the transaction\u00aeisolation semantics for that particular item. You'll have to decide, for each persistent class and collection, which cache concurrency strategy to use if you want toenable the shared cache. The four built-in\u00aeHibernate concurrency strategies represent decreasing levels ofstrictness in terms of transaction isolation: TRANSACTIONAL Available only in environments with a system transaction manager, this strategy gu\u00aearantees full transactional isolation up to repeatable read, ifsupported by the cache provider. With this strategy, Hibernate assumes that thecache provider is aware of and participating in system transactions. Hibernatedoesn't perform any kind of locking or version checking; it reli\u00aees solely on thecache provider's ability to isolate data in concurrent transactions. Use this strategy for read-mostly data where it's critical to prevent stale data in concurrenttransactions, in the rare case of an update. This strategy also works in a cluster ifthe cache provider engine supports synchronous distributed\u00aecaching. READ_WRITE Maintains read committed isolation where Hibernate can use atime-stamping mechanism; hence, this strategy only works in a non-clusteredenvironment. Hibernate may also use a proprietary locking API offered by thecache provider. Enable this strategy for read-mostly data where it's critical toprevent stale data in concurrent transactions, in the rare case of an update. Youshouldn't enable this strategy if data is concurrently modified (by other applications) in the database.",
  "page45": "NONSTRICT_READ_WRITE Makes no guarantee of consistency between thecache and the database. A transaction may read stale data from the cache. Usethis strategy if data hardly ever\u00aechanges (say, not every 10 seconds) and a window of inconsistency isn't of critical concern. You configure the duration of theinconsistency window with the expiration policies of your cache provider. Thisstrategy is u\u00aesable in a cluster, even with asynchronous distributed caching. Itmay be appropriate if other applications change data in the same database. READ_ONLY Suitable for data that never changes. You get an exc\u00aeeption if youtrigger an update. Use it for reference data only.With decreasing strictness come increasing performance and scalability. A clusteredasynchronous cache with NONSTRICT_READ_WRITE can handle many more transactionsthan a synchronous cluster with TRANSACTIONAL. You have to evalua\u00aete carefully theperformance of a clustered cache with full transaction isolation before using it in production. In many cases, you may be better off not enabling the shared cache for a particular class, if stale data isn't an option! You should benchmark your application with the shared cache disabled. Enable itfor good candi\u00aedate classes, one at a time, while continuously testing the scalability ofyour system and evaluating concurrency strategies. You must have automated testsavailable to judge the impact of changes to your cache setup. We recommend that youwrite these tests first, for the performance and scalability hotspots of your application,before you enable the shared cache. With all this theory under your belt, it's time to see how caching works in practice.First, you configure the shared cache.",
  "page46": "The shared cache mode controls how entity classes of this persistence unit becomecacheable. Usually you prefer to enable caching selectively for only some entityclasses. Options:\u00aeDISABLE_SELECTIVE, ALL, and NONE. Hibernate's second-level cache system has to be enabled explicitly; it isn't enabled bydefault. You can separately enable the query result cache; it's disabled by default\u00aeaswell. Pick a provider for the second-level cache system. For Ehcache, add the org.hibernate:hibernate-ehcache Maven artifact dependency to your classpath. Then, choose howHibernate uses Ehcache with th\u00aeis region factory setting; here you tell Hibernate to manage a single Ehcache instance internally as the second-level cache provider. Hibernate passes this property to Ehcache when the provider is started, setting thelocation of the Ehcache configuration file. All physical cache settings\u00aefor cacheregions are in this file. This controls how Hibernate disassembles and assembles entity state when data isstored and loaded from the second-level cache. The structured cache entry format isless efficient but necessary in a clustered environment. For a nonclustered secondlevel cache like the singleton Ehcache on this JVM,\u00aeyou can disable this setting and usea more efficient format. When you experiment with the second-level cache, you usually want to see what's happening behind the scenes. Hibernate has a statistics collector and an API to accessthese statistics. For performance reasons, it's disabled by default (and should be disabled in production).The second-level cache system is now ready, and Hibernate will start Ehcache whenyou build an EntityManagerFactory for this persistence unit. Hibernate won't cacheanything by default, though; you have to enable caching selectively for entity classesand their collections.",
  "page47": "We now look at entity classes and collections of the CaveatEmptor domain model andenable caching with the right concurrency strategy. In parallel, you'll configure thenecessary\u00aephysical cache regions in the Ehcache configuration file. First the User entity: this data rarely changes, but, of course, a user may changetheir user name or address from time to time. This isn't critical data in\u00ae a financialsense; few people make buying decisions based on a user's name or address. A smallwindow of inconsistency is acceptable when a user changes name or address information. Let's say t\u00aehere is no problem if, for a maximum of one minute, the old information is still visible in some transactions. This means you can enable caching with theNONSTRICT_READ_WRITE strategy:The @Cacheable annotation enables the shared cache for this entity class, but a Hibernate annotation is ne\u00aecessary to pick the concurrency strategy. Hibernate stores andloads User entity data in the second-level cache, in a cache region named your.package.name.User. You can override the name with the region attribute of the @Cache annotation. (Alternatively, you can set a global region name prefix with the hibernate.cache.region_prefix\u00aeproperty in the persistence unit.) You also enable the natural identifier cache for the User entity with @org.hibernate.annotations.NaturalIdCache. The natural identifier properties are marked with@org.hibernate.annotations.NaturalId, and you have to tell Hibernate whether theproperty is mutable. This enables you to look up User instances by username withouthitting the database. Next, configure the cache regions for both the entity data and the natural identifier caches in Ehcache.",
  "page48": "You can store a maximum 500 entries in both caches, and Ehcache won't keep themeternally. Ehcache will remove an element if it hasn't been accessed for 30 secondsand will\u00aeremove even actively accessed entries after 1 minute. This guarantees thatyour window of inconsistency from cache reads is never more than 1 minute. In otherwords, the cache region(s) will hold up to the 500 most-recently\u00aeused user accounts,none older than 1 minute, and shrink automatically. Let's move on to the Item entity class. This data changes frequently, although youstill have many more reads than writes. I\u00aef the name or description of an item ischanged, concurrent transactions should see this update immediately. Users makefinancial decisions, whether to buy an item, based on the description of an item.Therefore, READ_WRITE is an appropriate strategy:Hibernate will coordinate reads and write\u00aes when Item changes are made, ensuringthat you can always read committed data from the shared cache. If another application is modifying Item data directly in the database, all bets are off! You configure thecache region in Ehcache to expire the most-recently used Item data after one hour, toavoid filling up the cache bucket with\u00aestale data:Consider the bids collection of the Item entity class: A particular Bid in the Item#bidscollection is immutable, but the collection itself is mutable, and concurrent units ofwork need to see any addition or removal of a collection element immediately.",
  "page49": "It's critical to remember that the collection cache will not contain the actual Bid data.The collection cache only holds a set of Bid identifier values. Therefore, you mustenable\u00aecaching for the Bid entity as well. Otherwise, Hibernate may hit the cachewhen you start iterating through Item#bids, but then, due to cache misses, load eachBid separately from the database. This is a case where\u00aeenabling the cache will result inmore load on your database server! We've said that Bids are immutable, so you can cache this entity data as READ_ONLY:Hibernate's transparent caching behavior can\u00aebe difficult to analyze. The API for loading and storing data is still the EntityManager, with Hibernate automatically writingand reading data in the cache. Of course, you can see actual database access by logging Hibernate's SQL statements, but you should familiarize yourself with\u00aethe org.hibernate.stat.Statistics API to obtain more information about a unit of workand see what's going on behind the scenes. Let's run through some examples to seehow this works. You enabled the statistics collector earlier in the persistence unit configuration, insection 20.2.2. You access the statistics of the persi\u00aestence unit on the org.hibernate.SessionFactory:Here, you also get statistics for the data cache region for Item entities, and you can seethat there are several entries already in the cache. This is a warm cache; Hibernatestored data in the cache when the application saved Item entity instances. However,the entities haven't been read from the cache, and the hit count is zero.",
  "page50": "The statistics tell you that there are three Item#bids collections in the cache (one foreach Item). No successful cache lookups have occurred so far. The entity cache of Bid has\u00aefive records, and you haven't accessed it either. Initializing the collection reads the data from both caches. The cache found one collection as well as the data for its three Bid elements.The special natural identifier\u00ae cache for Users is not completely transparent. You needto call a method on the org.hibernate.Session to perform a lookup by naturalidentifier:The natural identifier cache region for Users has one ele\u00aement.The org.hibernate.Session API performs natural identifier lookup; this is the onlyAPI for accessing the natural identifier cache. You had a cache hit for the natural identifier lookup. The cache returned the identifier value \"johndoe\". You also had a cache hit for the entit\u00aey data of that User.The statistics API offers much more information than we've shown in these simpleexamples; we encourage you to explore this API further. Hibernate collects information about all its operations, and these statistics are useful for finding hotspots such asthe queries taking the longest time and the entities a\u00aend collections most accessedAs mentioned at the beginning of this section, Hibernate transparently writes andreads the cached data. For some procedures, you need more control over cacheusage, and you may want to bypass the caches explicitly. This is where cache modescome into play.",
  "page51": "JPA standardizes control of the shared cache with several cache modes. The followingEntityManager#find() operation, for example, doesn't attempt a cache lookup andhits the\u00aedaabase directly:The default CacheRetrieveMode is USE; here, you override it for one operation withBYPASS. A more common usage of cache modes is the CacheStoreMode. By default, Hibernate puts entity data in the cache when\u00aeyou call EntityManager#persist(). It alsoputs data in the cache when you load an entity instance from the database. But if youstore or load a large number of entity instances, you may not want to fill u\u00aep the available cache. This is especially important for batch procedures, as we showed earlier inthis chapter. You can disable storage of data in the shared entity cache for the entire unit ofwork by setting a CacheStoreMode on the EntityManager:Let's look at the special cache mode C\u00aeacheStoreMode.REFRESH. When you load anentity instance from the database with the default CacheStoreMode.USE, Hibernatefirst asks the cache whether it already has the data of the loaded entity instance. Then,if the cache already contains the data, Hibernate doesn't put the loaded data into thecache. This avoids a cache write,\u00aeassuming that cache reads are cheaper. With theREFRESH mode, Hibernate always puts loaded data into the cache without first querying the cache In a cluster with synchronous distributed caching, writing to all cache nodes is usually a very expensive operation. In fact, with a distributed cache, you should set theconfiguration property hibernate.cache.use_minimal_puts to true. This optimizessecond-level cache operation to minimize writes, at the cost of more frequent reads.",
  "page52": "If, however, there is no difference for your cache provider and architecture betweenreads and writes, you may want to disable the additional read with CacheStoreMode.REFRESH. (Note\u00aethat some cache providers in Hibernate may set use_minimal_puts: for example, with Ehcache this setting is enabled by default.) Cache modes, as you've seen, can be set on the find() operation and for theentire\u00aeEntity Manager. You can also set cache modes on the refresh() operation andon individual Querys as hints, as discussed in section 14.5. The per-operation and perquery settings override the cache mode of the\u00aeEntityManager. The cache mode only influences how Hibernate works with the caches internally.Sometimes you want to control the cache system programmatically: for example, toremove data from the cache.This is a simple API, and it only allows you to access cache regions of entity data. Youn\u00aeeed the org.hibernate.Cache API to access the other cache regions, such as the collection and natural identifier cache regions:You'll rarely need these control mechanisms. Also, note that eviction of the secondlevel cache is nontransactional: that is, Hibernate doesn't lock the cache regions during eviction. Let's m\u00aeove on to the last part of the Hibernate caching system: the query result cache.The query result cache is by default disabled, and every JPA, criteria, or native SQLquery you write always hits the database first. In this section, we show you why Hibernate disables the query cache by default and then how to enable it for particular queries when needed.",
  "page53": "You have to enable caching for a particular query. Without the org.hibernate.cachable hint, the result won't be stored in the query result cache. Hibernate executes the SQL\u00aequery and retrieves the result set into memory. Using the statistics API, you can find out more details. This is the first time you executethis query, so you get a cache miss, not a hit. Hibernate puts the query and its \u00aeresultinto the cache. If you run the same query again, the result will be from the cache. The entity instance data retrieved in the result set is stored in the entity cache region,not in the query result\u00aecache.The org.hibernate.cachable hint is set on the Query API, so it also works for criteriaand native SQL queries. Internally, the cache key is the SQL Hibernate uses to access thedatabase, with arguments rendered into the string where you had parameter markers. The query result cache do\u00aeesn't contain the entire result set of the SQL query. Inthe last example, the SQL result set contained rows from the ITEM table. Hibernateignores most of the information in this result set; only the ID value of each ITEMrecord is stored in the query result cache. The property values of each Item are storedin the entity cache\u00aeregion. Now, when you execute the same query again, with the same argument values forits parameters, Hibernate first accesses the query result cache. It retrieves the identifier values of the ITEM records from the cache region for query results.",
  "page54": "Then, Hibernate looks up and assembles each Item entity instance by identifier from the entitycache region. If you query for entities and decide to enable caching, make sure youalso\u00aeenable regular data caching for these entities. If you don't, you may end up withmore database hits after enabling the query result cache!If you cache the result of a query that doesn't return entity instances\u00aebut returnsonly scalar or embeddable values (for example, select i.name from Item i or selectu.homeAddress from User), the values are held in the query result cache regiondirectly. The query result ca\u00aeche uses two physical cache regions:The first cache region is where the query results are stored. You should let the cacheprovider expire the most-recently used result sets over time, such that the cache usesthe available space for recently executed queries. The second region, org.hiberna\u00aete.cache.spi.UpdateTimestampsCache, is special: Hibernate uses this region to decide whether a cached query result set is stale.When you re-execute a query with caching enabled, Hibernate looks in the timestampcache region for the timestamp of the most recent insert, update, or delete made tothe queried table(s). If the timestamp\u00aefound is later than the timestamp of the cachedquery results, Hibernate discards the cached results and issues a new database query.This effectively guarantees that Hibernate won't use the cached query result if anytable that may be involved in the query contains updated data; hence, the cachedresult may be stale. You should disable expiration of the update timestamp cache sothat the cache provider never removes an element from this cache. The maximumnumber of elements in this cache region depends on the number of tables in yourmapped model.",
  "page55": "The majority of queries don't benefit from result caching. This may come as a surprise. After all, it sounds like avoiding a database hit is always a good thing. There aretwo\u00aegood reasons this doesn't always work for arbitrary queries, compared to entityretrieval by identifier or collection initialization. First, you must ask how often you're going to execute the same query repeatedly,\u00aewith the same arguments. Granted, your application may execute a few queries repeatedly with exactly the same arguments bound to parameters and the same automaticallygenerated SQL statement. We consi\u00aeder this a rare case, but when you're certain you'reexecuting a query repeatedly, it becomes a good candidate for result set caching. Second, for applications that perform many queries and few inserts, deletes, orupdates, caching query results can improve performance and scalabi\u00aelity. On the otherhand, if the application performs many writes, Hibernate won't use the query resultcache efficiently. Hibernate expires a cached query result set when there is any insert,update, or delete of any row of a table that appears in the cached query result. Thismeans cached results may have a short lifetime, and e\u00aeven if you execute a queryrepeatedly, Hibernate won't use cached results due to concurrent modifications ofrows in the tables referenced by the query. For many queries, the benefit of the query result cache is nonexistent or, at least,doesn't have the impact you'd expect. But if your query restriction is on a unique natural identifier, such as select u from User u where u.username ?, you should consider natural identifier caching and lookup as shown earlier in this chapter.",
  "page56": "So, sure, Maven is an alternative to Ant, but Apache Ant continues to be a great, widely-used tool. Ithas been the reigning champion of Java builds for years, and you can integrate\u00aeAnt build scripts withyour project's Maven build very easily. This is a common usage pattern for a Maven project. On theother hand, as more and more open source projects move to Maven as a project management\u00aeplatform,working developers are starting to realize that Maven not only simplifies the task of build management, itis helping to encourage a common interface between developers and software projects. Maven is\u00aemore ofa platform than a tool, while you could consider Maven an alternative to Ant, you are comparing applesto oranges. \"Maven\" includes more than just a build tool.This is the central point that makes all of the Maven vs. Ant, Maven vs. Buildr, Maven vs. Gradlearguments irrelevant. M\u00aeaven isn't totally defined by the mechanics of your build system. It isn't aboutscripting the various tasks in your build as much as it is about encouraging a set of standards, a commoninterface, a life-cycle, a standard repository format, a standard directory layout, etc. It certainly isn'tabout what format the POM\u00aehappens to be in (XML vs. YAML vs. Ruby). Maven is much larger thanthat, and Maven refers to much more than the tool itself. When this book talks of Maven, it is referring tothe constellation of software, systems, and standards that support it. Buildr, Ivy, Gradle, all of these toolsinteract with the repository format that Maven helped create, and you could just as easily use a repositorymanager like Nexus to support a build written entirely in Ant.",
  "page57": "While Maven is an alternative to many of these tools, the community needs to evolve beyond seeingtechnology as a zero-sum game between unfriendly competitors in a competition for\u00aeusers and developers.This might be how large corporations relate to one another, but it has very little relevance to the way thatopen source communities work. The headline \"Who's winning? Ant or Maven?\" isn't very\u00aeconstructive.If you force us to answer this question, we're definitely going to say that Maven is a superior alternativeto Ant as a foundational technology for a build; at the same time, Maven\u00ae's boundaries are constantlyshifting and the Maven community is constantly trying to seek out new ways to become more ecumenical,more inter-operable, more cooperative. The core tenets of Maven are declarative builds, dependencymanagement, repository managers, universal reuse through\u00aeplugins, but the specific incarnation of theseideas at any given moment is less important than the sense that the open source community is collaboratingto reduce the inefficiency of \"enterprise-scale builds\".The authors of this book have no interest in creating a feud between Apache Ant and Apache Maven,but we are also cognizant\u00aeof the fact that most organizations have to make a decision between the twostandard solutions: Apache Ant and Apache Maven. In this section, we compare and contrast the tools.Ant excels at build process, it is a build system modeled after make with targets and dependencies. Eachtarget consists of a set of instructions which are coded in XML. There is a copy task and a javac taskas well as a jar task. When you use Ant, you supply Ant with specific instructions for compiling andpackaging your output. Look at the following example of a simple build.xml file.",
  "page58": "In this simple Ant example, you can see how you have to tell Ant exactly what to do. There is a compile goal which includes the javac task that compiles the source in the src/main/\u00aejava directory to thetarget/classes directory. You have to tell Ant exactly where your source is, where you want the resultingbytecode to be stored, and how to package this all into a JAR file. While there are some recent\u00aedevelopments that help make Ant less procedural, a developer's experience with Ant is in coding a procedurallanguage written in XML.Contrast the previous Ant example with a Maven example. In Mave\u00aen, to create a JAR file from some Javasource, all you need to do is create a simple pom.xml, place your source code in ${basedir}/src/main/javaand then run mvn install from the command line. The example Maven pom.xml that achieves thesame results as the simple Ant file listed in A Simple\u00aeAnt build.xml file is shown in A Sample Mavenpom.xml.That's all you need in your pom.xml. Running mvn install from the command line will processresources, compile source, execute unit tests, create a JAR, and install the JAR in a local repository forreuse in other projects. Without modification, you can run mvn site and then\u00aefind an index.html file intarget/site that contains links to JavaDoc and a few reports about your source code.",
  "page59": "Admittedly, this is the simplest possible example project containing nothing more than some source codeand producing a simple JAR. It is a project which closely follows Maven\u00aeconventions and doesn't requireany dependencies or customization. If we wanted to start customizing the behavior, our pom.xml is goingto grow in size, and in the largest of projects you can see collections of very complex\u00aeMaven POMs whichcontain a great deal of plugin customization and dependency declarations. But, even when your project'sPOM files become more substantial, they hold an entirely different kind of i\u00aenformation from the build fileof a similarly sized project using Ant. Maven POMs contain declarations: \"This is a JAR project\", and\"The source code is in src/main/java\". Ant build files contain explicit instructions: \"This is project\", \"Thesource is in src/main/java\", \"Run javac\u00aeagainst this directory\", \"Put the results in target/classes\", \"Createa JAR from the . . . .\", etc. Where Ant had to be explicit about the process, there was something \"built-in\"to Maven that just knew where the source code was and how it should be processed.Apache Ant Ant doesn't have formal conventions like a common p\u00aeroject directory structure or default behavior. You have to tell Ant exactly where to find the source and where to put the output. Informalconventions have emerged over time, but they haven't been codified into the product.",
  "page60": "Ant is procedural. You have to tell Ant exactly what to do and when to do it. You have to tell it tocompile, then copy, then compress.Ant doesn't have a lifecycle. You have to\u00aedefine goals and goal dependencies. You have to attach asequence of tasks to each goal manually.Apache Maven Maven has conventions. It knows where your source code is because you followed the convention.Maven's\u00aeCompiler plugin put the bytecode in target/classes, and it produces a JAR file in target. Maven is declarative. All you had to do was create a pom.xml file and put your source in the defaultdirectory. Mave\u00aen took care of the rest.Maven has a lifecycle which was invoked when you executed mvn install. This command toldMaven to execute a series of sequential lifecycle phases until it reached the install lifecycle phase. Asa side-effect of this journey through the lifecycle, Maven executed a nu\u00aember of default plugin goalswhich did things like compile and create a JAR.Maven has built-in intelligence about common project tasks in the form of Maven plugins. If you wantedto write and execute unit tests, all you would need to do is write the tests, place them in ${basedir}/src/test/java,add a test-scoped dependency on either\u00aeTestNG or JUnit, and run mvn test. If you wanted to deploy aweb application and not a JAR, all you would need to do is change your project type to war and put yourdocroot in ${basedir}/src/main/webapp.",
  "page61": "Sure, you can do all of this with Ant, but you will be writingthe instructions from scratch. In Ant, you would first have to figure out where the JUnit JAR file shouldbe. Then you\u00aewould have to create a classpath that includes the JUnit JAR file. Then you would tellAnt where it should look for test source code, write a goal that compiles the test source to bytecode, andexecute the unit tests with\u00aeJUnit.Without supporting technologies like antlibs and Ivy (even with these supporting technologies), Ant hasthe feeling of a c`ustom procedural build. An efficient set of Maven POMs in a project which\u00aeadheresto Maven's assumed conventions has surprisingly little XML compared to the Ant alternative. Anotherbenefit of Maven is the reliance on widely-shared Maven plugins. Everyone uses the Maven Surefireplugin for unit testing, and if someone adds support for a new unit testing frame\u00aework, you can gain newcapabilities in your own build by just incrementing the version of a particular Maven plugin in yourproject's POM.The decision to use Maven or Ant isn't a binary one, and Ant still has a place in a complex build. Ifyour current build contains some highly customized process, or if you've written\u00aesome Ant scripts tocomplete a specific process in a specific way that cannot be adapted to the Maven standards, you can stilluse these scripts with Maven. Ant is made available as a core Maven plugin. Custom Maven plugins canbe implemented in Ant, and Maven projects can be configured to execute Ant scripts within the Mavenproject lifecycle.",
  "page62": "The latest version of Maven currently requires the usage of Java 7 or higher. While older Maven versionscan run on older Java versions, this book assumes that you are running at le\u00aeast Java 7. Go with the mostrecent stable Java Development Kit (JDK) available for your operating system.Maven works with all certified JavaTM compatible development kits, and a few non-certified implementations of Java\u00ae. The examples in this book were written and tested against the official Java DevelopmentKit releases downloaded from the Oracle web site.When downloading Maven, you can download the latest available ver\u00aesion the latest available version ofMaven 3 in various branches. The latest version of Maven 3 when this book was last updated was Maven3.3.3. If you are not familiar with the Apache Software License, you should familiarize yourself with theterms of the license before you start using the\u00aeproduct. More information on the Apache Software Licensecan be found in Section 2.8.There are wide differences between operating systems such as Mac OS X and Microsoft Windows, andthere are subtle differences between different versions of Windows. Luckily, the process of installingMaven on all of these operating systems is relativ\u00aeely painless and straightforward. The following sectionsoutline the recommended best-practice for installing Maven on a variety of operating systems.",
  "page63": "You can download a binary release of Maven from http://maven.apache.org/download.html. Downloadthe current release of Maven in a format that is convenient for you to work with. Pic\u00aek an appropriate placefor it to live, and expand the archive there. If you expanded the archive into the directory /opt/apachemaven-3.2.5, you may want to create a symbolic link to make it easier to work with and to avo\u00aeid the needto change any environment configuration when you upgrade to a newer version:Once Maven is installed, you need to do a couple of things to make it work correctly. You need to add itsbin directo\u00aery in the distribution (in this example, /opt/maven/bin) to your command path.Installing Maven on Windows is very similar to installing Maven on Mac OSX, the main differences beingthe installation location and the setting of an environment variable. This book assumes a Maven installation\u00aedirectory of c:\\Program Files\\apache-maven-3.2.5, but it won't make a difference ifyou install Maven in another directory as long as you configure the proper environment variables. Onceyou've unpacked Maven to the installation directory, you will need to set the PATH environment variable.You can use the following comma\u00aends:Setting these environment variables on the command-line will allow you to run Maven in your currentsession, but unless you add them to the System environment variables through the control panel, you'llhave to execute these two lines every time you log into your system. You should modify both of thesevariables through the Control Panel in Microsoft Windows.",
  "page64": "If you see this output, you know that Maven is available and ready to be used. If you do not see thisoutput, and your operating system cannot find the mvn command, make sure that y\u00aeour PATH environmentvariable and M2_HOME environment variable have been properly set.Maven's download measures in at roughly 1.5 MiB, it has attained such a slim download size becausethe core of Maven has been desi\u00aegned to retrieve plugins and dependencies from a remote repository ondemand. When you start using Maven, it will start to download plugins to a local repository described inSection 2.5.1. In case you are\u00aecurious, let's take a quick look at what is in Maven's installation directory.LICENSE.txt contains the software license for Apache Maven. This license is described in some detaillater in the section Section 2.8. NOTICE.txt contains some notices and attributions required by libr\u00aeariesthat Maven depends on. README.txt contains some installation instructions. bin/ contains the mvn scriptthat executes Maven. boot/ contains a JAR file (classwords-1.1.jar) that is responsible for creating theClass Loader in which Maven executes. conf/ contains a global settings.xml that can be used to customizethe behavior of\u00aeyour Maven installation. If you need to customize Maven, it is customary to override anysettings in a settings.xml file stored in ~/.m2. lib/ contains a single JAR file (maven-core-3.0.3-uber.jar)that contains the core of Maven.",
  "page65": "If you've installed Maven on a Mac OSX or Unix machine according to the details in Section 2.3.1, itshould be easy to upgrade to newer versions of Maven when they become avail\u00aeable. Simply install thenewer version of Maven (/opt/maven-3.future) next to the existing version of Maven (/opt/maven-3.2.5).Then switch the symbolic link /opt/maven from /opt/maven-3.2.5 to /opt/maven-3.future. Since,\u00aeyou'vealready set your M2_HOME variable to point to /opt/maven, you won't need to change any environmentvariables.If you have installed Maven on a Windows machine, simply unpack Maven to c:\\P\u00aerogram Files\\maven3.future and update your M2_HOME variable.Most of the installation instructions involve unpacking of the Maven distribution archive in a directoryand setting of various environment variables. If you need to remove Maven from your computer, all youneed to do is delete yo\u00aeur Maven installation directory and remove the environment variables. You willalso want to delete the ~/.m2 directory as it contains your local repository.While this book aims to be a comprehensive reference, there are going to be topics we will miss andspecial situations and tips which are not covered. While the core of Maven is\u00aevery simple, the real workin Maven happens in the plugins, and there are too many plugins available to cover them all in one book.You are going to encounter problems and features which have not been covered in this book; in thesecases, we suggest searching for answers at the following locations.",
  "page66": "This will be the first place to look, the Maven web site contains a wealth of information and doc-umentation. Every plugin has a few pages of documentation and there are a series o\u00aef \"quick start\"documents which will be helpful in addition to the content of this book. While the Maven sitecontains a wealth of information, it can also be a frustrating, confusing, and overwhelming. Thereis a custom\u00aeGoogle search box on the main Maven page that will search known Maven sites forinformation. This provides better results than a generic Google search.The Maven User mailing list is the place for users\u00aeto ask questions. Before you ask a questionon the user mailing list, you will want to search for any previous discussion that might relateto your question. It is bad form to ask a question that has already been asked without firstchecking to see if an answer already exists in the archives.\u00aeThere are a number of useful mailing list archive browsers, we've found Nabble to the be the most useful. You can browse theUser mailing list archives here: http://www.nabble.com/Maven---Users-f178.html. You canjoin the user mailing list by following the instructions available here http://maven.apache.org/-mail-lists.html.So\u00aenatype maintains an online copy of this book and other tutorials related to Apache Maven.",
  "page67": "Apache Maven is released under the Apache Software License, Version 2.0. If you want to read thislicense, you can read ${M2_HOME}/LICENSE.txt or read this license on the Open Sourc\u00aee Initiative'sweb site here: http://www.opensource.org/licenses/apache2.0.php.There's a good chance that, if you are reading this book, you are not a lawyer. If you are wonderingwhat the Apache License, Versio\u00aen 2.0 means, the Apache Software Foundation has assembled a veryhelpful Frequently Asked Questions (FAQ) page about the license available here: http://www.apache.org/-foundation/licence-FAQ.html. Here\u00aeis the answer to the question \"I am not a lawyer. What does it allmean?\"freely download and use Apache software, in whole or in part, for personal, company internal, orcommercial purposes;use Apache software in packages or distributions that you create.redistribute any piece of Apache-orig\u00aeinated software without proper attribution; use any marks owned by The Apache Software Foundation in any way that might state or imply thatthe Foundation endorses your distribution; use any marks owned by The Apache Software Foundation in any way that might state or imply thatyou created the Apache software in questioninclude a co\u00aepy of the license in any redistribution you may make that includes Apache software;provide clear attribution to The Apache Software Foundation for any distributions that include Apachesoftware.include the source of the Apache software itself, or of any modifications you may have made to it, inany redistribution you may assemble that includes it; submit changes that you make to the software back to the Apache Software Foundation (though suchfeedback is encouraged).",
  "page68": "Maven projects, dependencies, builds, artifacts: all of these are objects to be modeled and described.These objects are described by an XML file called a Project Object Model. The\u00aePOM tells Maven whatsort of project it is dealing with and how to modify default behavior to generate output from source. In thesame way a Java web application has a web.xml that describes, configures, and customizes th\u00aee application,a Maven project is defined by the presence of a pom.xml. It is a descriptive declaration of a project forMaven; it is the figurative \"map\" that Maven needs to understand what it i\u00aes looking at when it builds yourproject.You could also think of the pom.xml as analogous to a Makefile or an Ant build.xml. When you are usingGNU make to build something like MySQL, you'll usually have a file named Makefile that containsexplicit instructions for building a binary fro\u00aem source. When you are using Apache Ant, you likely have afile named build.xml that contains explicit instructions for cleaning, compiling, packaging, and deployingan application. make, Ant, and Maven are similar in that they rely on the presence of a commonly namedfile such as Makefile, build.xml, or pom.xml, but that is where th\u00aee similarities end. If you look at a Mavenpom.xml, the majority of the POM is going to deal with descriptions: Where is the source code? Where arethe resources? What is the packaging? If you look at an Ant build.xml file, you'll see something entirelydifferent.",
  "page69": "You'll see explicit instructions for tasks such as compiling a set of Java classes. The MavenPOM is declarative, and although you can certainly choose to include some procedur\u00aeal customizationsvia the Maven Ant plugin, for the most part you will not need to get into the gritty procedural details ofyour project's build.The POM is also not specific to building Java projects. While most of\u00aethe examples in this book aregeared towards Java applications, there is nothing Java-specific in the definition of a Maven Project ObjectModel. While Maven's default plugins are targeted at building\u00aeJAR artifacts from a set of source, tests, andresources, there is nothing preventing you from defining a POM for a project that contains C# sources andproduces some proprietary Microsoft binary using Microsoft tools. Similarly, there is nothing stoppingyou from defining a POM for a techn\u00aeical book. In fact, the source for this book and this book's examplesis captured in a multi-module Maven project which uses one of the many Maven Docbook plugins toapply the standard Docbook XSL to a series of chapter XML files. Others have created Maven plugins tobuild Adobe Flex code into SWCs and SWFs, and yet others have\u00aeused Maven to build projects writtenin C.We've established that the POM describes and declares, it is unlike Ant or Make in that it doesn't provideexplicit instructions, and we've noted that POM concepts are not specific to Java. Diving into morespecifics, take a look at Figure 3.1 for a survey of the contents of a POM.",
  "page70": "This includes a project's name, the URL for a project, the sponsoring organization, and a list ofdevelopers and contributors along with the license for a project.In this secti\u00aeon, we customize the behavior of the default Maven build. We can change the locationof source and tests, we can add new plugins, we can attach plugin goals to the lifecycle, and we cancustomize the site generation param\u00aeeters.The build environment consists of profiles that can be activated for use in different environments.For example, during development you may want to deploy to a development server, whereas inproducti\u00aeon you want to deploy to a production server. The build environment customizes the buildsettings for specific environments and is often supplemented by a custom settings.xml in ~/.m2.This settings file is discussed in Chapter 5 and in the section Section 15.2A project rarely stands alone;\u00aeit depends on other projects, inherits POM settings from parentprojects, defines its own coordinates, and may include submodules.Before we dive into some examples of POMs, let's take a quick look at the Super POM. All Maven projectPOMs extend the Super POM, which defines a set of defaults shared by all projects. This Super P\u00aeOM isa part of the Maven installation. Depending on the Maven version it can be found in the maven-x.y.z-uber.jar or maven-model-builder-xy.z.jar file in ${M2_HOME}/lib. If you look in thisJAR file, you will find a file named pom-4.0.0.xml under the org.apache.maven.model package. Itis also published on the Maven reference site that is available for each version of Maven separately ande.g. for Maven 3.1.1 it can be found with the Maven Model Builder documentation. A Super POM forMaven is shown in The Super POM.",
  "page71": "The Super POM defines some standard configuration variables that are inherited by all projects. Thosevalues are captured in the annotated sections: The default Super POM defines a\u00aesingle remote Maven repository with an ID of central. This isthe Central Repository that all Maven clients are configured to read from by default. This settingcan be overridden by a custom settings.xml file. Note that t\u00aehe default Super POM has disabledsnapshot artifacts on the Central Repository. If you need to use a snapshot repository, you willneed to customize repository settings in your pom.xml or in your settings.\u00aexml. Settings and profilesare covered in Chapter 5 and in Section 15.2The Central Repository also contains Maven plugins. The default plugin repository is the centralMaven repository. Snapshots are disabled, and the update policy is set to \"never,\" which meansthat Maven will nev\u00aeer automatically update a plugin if a new version is released. The build element sets the default values for directories in the Maven Standard Directory layout. Starting in Maven 2.0.9, default versions of core plugins have been provided in the Super POM.This was done to provide some stability for users that are not specifying ver\u00aesions in their POMs.In newer versions some of this has been migrated out of the file. However you can still see theversions that will be used in your project using mvn help:effective-pom.",
  "page72": "All Maven POMs inherit defaults from the Super POM (introduced earlier in the section Section 3.2.1).If you are just writing a simple project that produces a JAR from some source i\u00aen src/main/java, want torun your JUnit tests in src/test/java, and want to build a project site using mvn site, you don't have tocustomize anything. All you would need, in this case, is the simplest possible POM sh\u00aeown in The SimplestPOM. This POM defines a groupId, artifactId, and version: the three required coordinates forevery projectSuch a simple POM would be more than adequate for a simple project e.g., a Java\u00aelibrary that producesa JAR file. It isn't related to any other projects, it has no dependencies, and it lacks basic informationsuch as a name and a URL. If you were to create this file and then create the subdirectory src/main/javawith some source code, running mvn package would pro\u00aeduce a JAR in target/simple-project-1.jar.Executing the effective-pom goal should print out an XML document capturing the merge betweenthe Super POM and the POM from The Simplest POM.Maven is something of a chameleon; you can pick and choose the features you want to take advantage of.Some open source projects may value the ability\u00aeto list developers and contributors, generate clean projectdocumentation, and manage releases automatically using the Maven Release plugin. On the other hand,someone working in a corporate environment on a small team might not be interested in the distributionmanagement capabilities of Maven nor the ability to list developers.",
  "page73": "The remainder of this chapter isgoing to discuss features of the POM in isolation. Instead of bombarding you with a 10-page listing of aset of related POMs, we're going to foc\u00aeus on creating a good reference for specific sections of the POM.In this chapter, we discuss relationships between POMs, but we don't illustrate such a project here.The POM is always in a file named pom.xml in the\u00aebase directory of a Maven project. This XML documentcan start with the XML declaration, or you can choose to omit it. All values in a POM are captured asXML elements.A project's version number is us\u00aeed to group and order releases. Maven versions contain the followingparts: major version, minor version, incremental version, and qualifier. In a version, these parts correspond to the following format:For example, the version \"1.3.5\" has a major version of 1, a minor version of 3, and\u00aean incremental versionof 5. The version \"5\" has a major version of 5 and no minor or incremental version. The qualifier existsto capture milestone builds: alpha and beta releases, and the qualifier is separated from the major, minor,and incremental versions by a hyphen. For example, the version \"1.3-beta-01\" has a major versio\u00aen of 1, aminor version of 3, no incremental version and a qualifier of \"beta-01\".Keeping your version numbers aligned with this standard will become very important when you want tostart using version ranges in your POMs. Version ranges, introduced in Section 3.4.3, allow you to specifya dependency on a range of versions, and they are only supported because Maven has the ability to sortversions based on the version release number format introduced in this section.",
  "page74": "If your version release number matches the format <major>.<minor>.<incremental>-<qualifier> then your versions will be compared properly; \"1.2.3\" will be evaluated as a more rece\u00aent build than\"1.0.2\", and the comparison will be made using the numeric values of the major, minor, and incrementalversions. If your version release number does not fit the standard introduced in this section, then yo\u00aeurversions will be compared as strings; \"1.0.1b\" will be compared to \"1.2.0b\" using a String comparison.One gotcha for release version numbers is the ordering of the qualifiers. Take the version rele\u00aease numbers\"1.2.3-alpha-2\" and \"1.2.3-alpha-10,\" where the \"alpha-2\" build corresponds to the 2nd alpha build, andthe \"alpha-10\" build corresponds to the 10th alpha build. Even though \"alpha-10\" should be consideredmore recent than \"a\u00aelpha-2,\" Maven is going to sort \"alpha-10\" before \"alpha-2\" due to a known issue inthe way Maven handles version numbers.Maven is supposed to treat the number after the qualifier as a build number. In other words, the qualifiershould be \"alpha\", and the build number should be 2. Even though Maven has bee\u00aen designed to separatethe build number from the qualifier, this parsing is currently broken. As a result, \"alpha-2\" and \"alpha10\" are compared using a String comparison, and \"alpha-10\" comes before \"alpha-2\" alphabetically. Toget around this limitation, you will need to left-pad your qualified build numbers. If you use \"alpha-02\"and \"alpha-10\" this problem will go away, and it will continue to work once Maven properly parses theversion build number.",
  "page75": "Maven versions can contain a string literal to signify that a project is currently under active development.If a version contains the string \"-SNAPSHOT,\" then Maven will\u00aeexpand this token to a date and timevalue converted to UTC (Coordinated Universal Time) when you install or release this component. Forexample, if your project has a version of \"1.0-SNAPSHOT\" and you deploy th\u00aeis project's artifacts to aMaven repository, Maven would expand this version to \"1.0-20080207-230803-1\" if you were to deploya release at 11:08 PM on February 7th, 2008 UTC. In other words\u00ae, when you deploy a snapshot, you arenot making a release of a software component; you are releasing a snapshot of a component at a specifictime.Why would you use this? SNAPSHOT versions are used for projects under active development. If yourproject depends on a software component that is\u00aeunder active development, you can depend on a SNAPSHOT release, and Maven will periodically attempt to download the latest snapshot from a repositorywhen you run a build. Similarly, if the next release of your system is going to have a version \"1.4\", yourproject would have a version \"1.4-SNAPSHOT\" until it was formally releas\u00aeed.As a default setting, Maven will not check for SNAPSHOT releases on remote repositories. To dependon SNAPSHOT releases, users must explicitly enable the ability to download snapshots using a repository or pluginRepository element in the POM.",
  "page76": "When releasing a project, you should resolve all dependencies on SNAPSHOT versions to dependencies on released versions. If a project depends on a SNAPSHOT, it is not stable as the\u00aedependencies maychange over time. Artifacts published to non-snapshot Maven repositories such as http://repo1.maven.org/-maven2 cannot depend on SNAPSHOT versions, as Maven's Super POM has snapshot's disabled\u00aefromthe Central repository. SNAPSHOT versions are for development onlyThe syntax for using a property in Maven is to surround the property name with two curly braces andprecede it with a dollar symbol.\u00aeFor example, consider the following POMWhen Maven reads a POM, it replaces references to properties when it loads the POM XML. Mavenproperties occur frequently in advanced Maven usage, and are similar to properties in other systems suchas Ant or Velocity. They are simply variables delimit\u00aeed by ${...}. Maven provides three implicit variableswhich can be used to access environment variables, POM information, and Maven Settings:The env variable exposes environment variables exposed by your operating system or shell. Forexample, a reference to ${env.PATH} in a Maven POM would be replaced by the ${PATH} environment var\u00aeiable (or %PATH% in Windows).The project variable exposes the POM. You can use a dot-notated (.) path to reference the valueof a POM element. For example, in this section we used the groupId and artifactId to setthe finalName element in the build configuration. The syntax for this property reference was:${project.groupId}-${project.artifactId}.",
  "page77": "The settings variable exposes Maven settings information. You can use a dot-notated (.) pathto reference the value of an element in a settings.xml file. For example, ${settings.off\u00aeline} wouldreference the value of the offline element in ~/.m2/settings.xml.In addition to the three implicit variables, you can reference system properties and any custom propertiesset in the Maven POM or in a build pr\u00aeofile:All properties accessible via getProperties() on java.lang.System are exposed asPOM properties. Some examples of system properties are: ${user.name}, ${user.home}, ${java.home},and ${os.name}. A fu\u00aell list of system properties can be found in the Javadoc for the System classArbitrary properties can be set with a properties element in a pom.xml or settings.xml, orproperties can be loaded from external files. If you set a property named fooBar in your pom.xml,that same property is ref\u00aeerenced with ${fooBar}. Custom properties come in handy when you arebuilding a system that filters resources and targets different deployment platforms. Here is thesyntax for setting ${foo}bar in a POM:Maven can manage both internal and external dependencies. An external dependency for a Java projectmight be a library such as Plex\u00aeus, the Spring Framework, or Log4J. An internal dependency is illustratedby a web application project depending on another project that contains service classes, model objects, orpersistence logic. Project Dependencies shows some examples of project dependencies.",
  "page78": "The first dependency is a compile dependency on the XFire SOAP library from Codehaus. You woulduse this type of dependency if your project depended on this library for compilation,\u00aetesting, and duringexecution. The second dependency is a test-scoped dependency on JUnit. You would use a testscoped dependency when you need to reference this library only during testing. The last dependency inProject\u00aeDependencies is a dependency on the Servlet 2.4 API. The last dependency is scoped as a provideddependency. You would use a provided scope when the application you are developing needs a library forcomp\u00aeilation and testing, but this library is supplied by a container at runtime.briefly introduced three of the five dependency scopes: compile, test, and provided. Scope controls which dependencies are available in which classpath, and which dependenciesare included with an application. Let\u00aeu2019s explore each scope in detail.compile is the default scope; all dependencies are compile-scoped if a scope is not supplied.compile dependencies are available in all classpaths, and they are packaged.provided dependencies are used when you expect the JDK or a container to provide them. Forexample, if you were developing a web\u00aeapplication, you would need the Servlet API available onthe compile classpath to compile a servlet, but you wouldn't want to include the Servlet API in thepackaged WAR; the Servlet API JAR is supplied by your application server or servlet container.provided dependencies are available on the compilation classpath (not runtime). They are nottransitive, nor are they packaged.",
  "page79": "runtime dependencies are required to execute and test the system, but they are not required forcompilation. For example, you may need a JDBC API JAR at compile time and the JDBC dr\u00aeiverimplementation only at runtime.test-scoped dependencies are not required during the normal operation of an application, andthey are available only during test compilation and execution phasesThe system scope is simi\u00aelar to provided except that you have to provide an explicit path to theJAR on the local file system. This is intended to allow compilation against native objects that maybe part of the system libraries.\u00aeThe artifact is assumed to always be available and is not looked upin a repository. If you declare the scope to be system, you must also provide the systemPathelement. Note that this scope is not recommended (you should always try to reference dependenciesin a public or custom Maven repos\u00aeitory).Assume that you are working on a library that provides caching behavior. Instead of writing a cachingsystem from scratch, you want to use some of the existing libraries that provide caching on the filesystem and distributed caches. Also assume that you want to give the end user an option to cache onthe file system or to use\u00aean in-memory distributed cache. To cache on the file system, you'll want touse a freely available library called EHCache (http://ehcache.sourceforge.net/), and to cache in a distributed in-memory cache, you want to use another freely available caching library named SwarmCache (http://swarmcache.sourceforge.net/ ).",
  "page80": "You'll code an interface and create a library that can be configuredto use either EHCache or SwarmCache, but you want to avoid adding a dependency on both cachinglibraries to\u00aeany project that depends on your library.In other words, you need both libraries to compile this library project, but you don't want both librariesto show up as transitive runtime dependencies for the project that\u00aeuses your library. You can accomplishthis by using optional dependencies as shown in Declaring Optional Dependencies.Since you've declared these dependencies as optional in my-project, if you'v\u00aee defined a project thatdepends on my-project which needs those dependencies, you'll have to include them explicitly in theproject that depends on my-project. For example, if you were writing an application which dependedon my-project and wanted to use the EHCache implementation, you\u00aewould need to add the followingdependency element to your projectIn an ideal world, you wouldn't have to use optional dependencies. Instead of having one large projectwith a series of optional dependencies, you would separate the EHCache-specific code to a my-project-ehcache submodule and the SwarmCache-specific code to a my\u00ae-project-swarmcache submodule. This way, instead of requiring projects that reference my-project to specifically add a dependency, projects can just reference a particular implementation project and benefit from the transitivedependency.",
  "page81": "Instead of a specific version for each dependency, you can alternatively specify a range of versions thatwould satisfy a given dependency. For example, you can specify that your pr\u00aeoject depends on version 3.8or greater of JUnit, or anything between versions 4.5 and 4.10 of JUnit. You do this by surrounding oneor more version numbers with the following characters:For example, if you wished to acce\u00aess any JUnit version greater than or equal to 3.8 but less than 4.0,your dependency would be as shown in Specifying a Dependency Range: JUnit 3.8 - JUnit 4.0A version before or after the comma is not req\u00aeuired, and means /- infinity. For example, \"[4.0,)\" meansany version greater than or equal to 4.0. \"(,2.0)\" is any version less than 2.0. \"[1.2]\" means only version1.2, and nothing else.project-a depends on project-b, which in turn depends on project-c, then project-c isconsidered\u00aea transitive dependency of project-a. If project-c depended on project-d, thenproject-d would also be considered a transitive dependency of project-a. Part of Maven's appealis that it can manage transitive dependencies and shield the developer from having to keep track of allof the dependencies required to compile and run an\u00aeapplication. You can just depend on something likethe Spring Framework and not have to worry about tracking down every last dependency of the SpringFramework.",
  "page82": "Maven accomplishes this by building a graph of dependencies and dealing with any conflicts and overlapsthat might occur. For example, if Maven sees that two projects depend on the\u00aesame groupId and artifactId, it will sort out which dependency to use automatically, always favoring the more recent version ofa dependency. Although this sounds convenient, there are some edge cases where transitive de\u00aependenciescan cause some configuration issues. For these scenarios, you can use a dependency exclusion.Each of the scopes outlined earlier in the section Section 3.4.1 affects not just the scope of the d\u00aeependencyin the declaring project, but also how it acts as a transitive dependency. The easiest way to convey thisinformation is through a table, as in Table 3.1. Scopes in the top row represent the scope of a transitivedependency. Scopes in the leftmost column represent the scope of a di\u00aerect dependency. The intersectionof the row and column is the scope that is assigned to a transitive dependency. A blank cell in this tablemeans that the transitive dependency will be omitted.To illustrate the relationship of transitive dependency scope to direct dependency scope, consider thefollowing example. If project-a contai\u00aens a test scoped dependency on project-b which contains acompile scoped dependency on project-c. project-c would be a test-scoped transitive dependencyof project-a.You can think of this as a transitive boundary which acts as a filter on dependency scope. Transitivedependencies which are provided and test scope usually do not affect a project. Transitive dependencies which are compile and runtime scoped usually affect a project regardless of the scope of a directdependency.",
  "page83": "Transitive dependencies which are compile scoped will have the same scope of the directdependency . Transitive dependencies which are runtime scoped will generally have the same sc\u00aeope ofthe direct dependency except when the direct dependency has a scope of compile. When a transitive dependency is runtime scoped and the direct dependency is compile scoped, the transitive dependency willhave an eff\u00aeective scope of runtime.There will be times when you need to exclude a transitive dependency, such as when you are dependingon a project that depends on another project, but you would like to either excl\u00aeude the dependency altogether or replace the transitive dependency with another dependency that provides the same functionality.Excluding a Transitive Dependency shows an example of a dependency element that adds a dependencyon project-a, but excludes the transitive dependency project-b.O\u00aeften, you will want to replace a transitive dependency with another implementation. For example, ifyou are depending on a library that depends on the Sun JTA API, you may want to replace the declaredtransitive dependency. Hibernate is one example. Hibernate depends on the Sun JTA API JAR, which isnot available in the central Maven\u00aerepository because it cannot be freely redistributed. Fortunately, theApache Geronimo project has created an independent implementation of this library that can be freelyredistributed. To replace a transitive dependency with another dependency, you would exclude the transitive dependency and declare a dependency on the project you wanted instead. Excluding and Replacing aTransitive Dependency shows an example of a such replacement.",
  "page84": "In Excluding and Replacing a Transitive Dependency, there is nothing marking the dependency on geronimojta_1.1_spec as a replacement, it just happens to be a library which provides\u00aethe same API as the originalJTA dependency. Here are some other reasons you might want to exclude or replace transitive dependencies:The groupId or artifactId of the artifact has changed, where the current project requ\u00aeires analternately named version from a dependency's version - resulting in 2 copies of the same project inthe classpath. Normally Maven would capture this conflict and use a single version of the p\u00aeroject,but when groupId or artifactId are different, Maven will consider this to be two differentlibraries. An artifact is not used in your project and the transitive dependency has not been marked as an optional dependency. In this case, you might want to exclude a dependency because it\u00aeisn't somethingyour system needs and you are trying to cut down on the number of libraries distributed with anapplication. An artifact which is provided by your runtime container thus should not be included with your build.An example of this is if a dependency depends on something like the Servlet API and you want tomake sure\u00aethat the dependency is not included in a web application's WEB-INF/lib directory. To exclude a dependency which might be an API with multiple implementations. This is the situation illustrated by Excluding and Replacing a Transitive Dependency; there is a Sun API whichrequires click-wrap licensing and a time-consuming manual install into a custom repository (Sun'sJTA JAR) versus a freely distributed version of the same API available in the central Maven repository (Geronimo's JTA implementation).=== Dependency Management.",
  "page85": "Once you've adopted Maven at your super complex enterprise and you have two hundred and twentyinter-related Maven projects, you are going to start wondering if there is a bett\u00aeer way to get a handle ondependency versions. If every single project that uses a dependency like the MySQL Java connector needsto independently list the version number of the dependency, you are going to run into probl\u00aeems when youneed to upgrade to a new version. Because the version numbers are distributed throughout your projecttree, you are going to have to manually edit each of the pom.xml files that reference a de\u00aependency to makesure that you are changing the version number everywhere. Even with find, xargs, and awk, you arestill running the risk of missing a single POM.Luckily, Maven provides a way for you to consolidate dependency version numbers in the dependencyManagement element. You'll\u00aeusually see the dependencyManagement element in a top-level parentPOM for an organization or project. Using the dependencyManagement element in a pom.xml allowsyou to reference a dependency in a child project without having to explicitly list the version. Maven willwalk up the parent-child hierarchy until it finds a project with a\u00aedependencyManagement element, itwill then use the version specified in this dependencyManagement element.For example, if you have a large set of projects which make use of the MySQL Java connector version 5.1.2, you could define the following dependencyManagement element in your multi-moduleproject's top-level POM.",
  "page86": "You should notice that the child project did not have to explicitly list the version of the mysql-connector-java dependency. Because this dependency was defined in the top-level PO\u00aeM's dependencyManagement element, the version number is going to propagate to the child project's dependency on mysqlconnector-java. Note that if this child project did define a version, it would override the\u00aeversionlisted in the top-level POM's dependencyManagement section. That is, the dependencyManagement version is only used when the child does not declare a version directly.Dependency management in\u00aea top-level POM is different from just defining a dependency on a widelyshared parent POM. For starters, all dependencies are inherited. If mysql-connector-java werelisted as a dependency of the top-level parent project, every single project in the hierarchy would havea reference to this\u00aedependency. Instead of adding in unnecessary dependencies, using dependencyManagement allows you to consolidate and centralize the management of dependency versions withoutadding dependencies which are inherited by all children. In other words, the dependencyManagement element is equivalent to an environment variable which allows\u00aeyou to declare a dependency anywherebelow a project without specifying a version number.One of the compelling reasons to use Maven is that it makes the process of tracking down dependencies(and dependencies of dependencies) very easy. When a project depends on an artifact produced by anotherproject we say that this artifact is a dependency. In the case of a Java project, this can be as simple as aproject depending on an external dependency like Log4J or JUnit. While dependencies can model externaldependencies, they can also manage the dependencies between a set of related projects.",
  "page87": "If projecta depends on project-b, Maven is smart enough to know that project-b must be built beforeproject-a.Relationships are not only about dependencies and figuring out what one\u00aeproject needs to be able to buildan artifact. Maven can model the relationship of a project to a parent, and the relationship of a project tosubmodules. This section gives an overview of the various relationships betwe\u00aeen projects and how suchrelationships are configured.Coordinates define a unique location for a project. Projects are related to one another using MavenCoordinates. project-a doesn't just depend on\u00aeproject-b; a project with a groupId, artifactId, and version depends on another project with a groupId, artifactId, and version. Toreview, a Maven Coordinate is made up of three components:A groupId groups a set of related artifacts. Group identifiers generally resemble a Java packagename\u00ae. For example, the groupId org.apache.maven is the base groupId for all artifacts produced by the Apache Maven project. Group identifiers are translated into paths in the Maven Repository; for example, the org.apache.maven groupId can be found in /maven2/org/apache/maven onrepo1.maven.org.The artifactId is the project's main\u00aeidentifier. When you generate an artifact, this artifact isgoing to be named with the artifactId. When you refer to a project, you are going to referto it using the artifactId. The artifactId, groupId combination must be unique. Inother words, you can't have two separate projects with the same artifactId and groupId;artifactId s are unique within a particular groupId.",
  "page88": "When an artifact is released, it is released with a version number. This version number is a numericidentifier such as \"1.0\", \"1.1.1\", or \"1.1.2-alpha-01\". You can also use w\u00aehat is known as a snapshotversion. A snapshot version is a version for a component which is under development, snapshot version numbers always end in SNAPSHOT; for example, \"1.0-SNAPSHOT\", \"1.1.1-SNAPSHOT\",and \"1-S\u00aeNAPSHOT\". Section 3.3.1.1 introduces versions and version ranges.You would use a classifier if you were releasing the same code but needed to produce two separateartifacts for technical reasons. For exa\u00aemple, if you wanted to build two separate artifacts of a JAR,one compiled with the Java 1.4 compiler and another compiled with the Java 6 compiler, you mightuse the classifier to produce two separate JAR artifacts under the same groupId:artifactId:versioncombination. If your project uses\u00aenative extensions, you might use the classifier to produce anartifact for each target platform. Classifiers are commonly used to package up an artifact's sources,JavaDocs or binary assemblies.When we talk of dependencies in this book, we often use the following shorthand notation to describe adependency: groupId:artifactId:ve\u00aersion. To refer to the 2.5 release of the Spring Framework,we would refer to it as org.springframework:spring:2.5. When you ask Maven to print out alist of dependencies with the Maven Dependency plugin, you will also see that Maven tends to print outlog messages with this shorthand dependency notation.",
  "page89": "There are going to be times when you want a project to inherit values from a parent POM. You mightbe building a large system, and you don't want to have to repeat the same dep\u00aeendency elements overand over again. You can avoid repeating yourself if your projects make use of inheritance via the parentelement. When a project specifies a parent, it inherits the information in the parent project\u00aeu2019s POM. It canthen override and add to the values specified in this parent POM.All Maven POMs inherit values from a parent POM. If a POM does not specify a direct parent usingthe parent element, that\u00aePOM will inherit values from the Super POM. Project Inheritance shows theparent element of project-a which inherits the POM defined by the a-parent project.Running mvn help:effective-pom in project-a would show a POM that is the result of merging the Super POM with the POM defined by a-p\u00aearent and the POM defined in project-a. Theimplicit and explicit inheritance relationships for project-a are shown in Figure 3.3.When a project specifies a parent project, Maven uses that parent POM as a starting point before it readsthe current project's POM. It inherits everything, including the groupId and version number. Y\u00aeou'llnotice that project-a does not specify either, both groupId and version are inherited from aparent. With a parent element, all a POM really needs to define is an artifactId. This isn'tmandatory, project-a could have a different groupId and version, but by not providing values,Maven will use the values specified in the parent POM. If you start using Maven to manage and buildlarge multi-module projects, you will often be creating many projects which share a common groupIdand version.",
  "page90": "When Maven inherits dependencies, it will add dependencies of child projects to the dependencies definedin parent projects. You can use this feature of Maven to specify widely used\u00aedependencies across allprojects which inherit from a top-level POM. For example, if your system makes universal use of theLog4J logging framework, you can list this dependency in your top-level POM. Any projects whichi\u00aenherit POM information from this project will automatically have Log4J as a dependency. Similarly, ifyou need to make sure that every project is using the same version of a Maven plugin, you can list thi\u00aesMaven plugin version explicitly in a top-level parent POM's pluginManagement section.Maven assumes that the parent POM is available from the local repository, or available in the parentdirectory (../pom.xml) of the current project. If neither location is valid this default behavior\u00aemay beoverridden via the relativePath element. For example, some organizations prefer a flat projectstructure where a parent project's pom.xml isn't in the parent directory of a child project. It might be in asibling directory to the project. If your child project were in a directory ./project-a and the parent projectwer\u00aee in a directory named ./a-parent, you could specify the relative location of parent-a's POM withthe following configuration:Maven can be used to manage everything from simple, single-project systems to builds that involve hundreds of inter-related submodules.",
  "page91": "Part of the learning process with Maven isn't just figuring out thesyntax for configuring Maven, it is learning the \"Maven Way\" the current set of best practices for organiz\u00aeing and building projects using Maven. This section attempts to distill some of this knowledge to helpyou adopt best practices from the start without having to wade through years of discussions on the Mavenmailing lists\u00aeIf you have a set of dependencies which are logically grouped together. You can create a project withpom packaging that groups dependencies together. For example, let's assume that your application\u00aeusesHibernate, a popular Object-Relational mapping framework. Every project which uses Hibernate mightalso have a dependency on the Spring Framework and a MySQL JDBC driver. Instead of having to includethese dependencies in every project that uses Hibernate, Spring, and MySQL you could cr\u00aeeate a specialPOM that does nothing more than declare a set of common dependencies. You could create a projectcalled persistence-deps (short for Persistence Dependencies), and have every project that needs todo persistence depend on this convenience project:If you create this project in a directory named persistence-deps, all you\u00aeneed to do is create thispom.xml and run mvn install. Since the packaging type is pom, this POM is installed in your localrepository. You can now add this project as a dependency and all of its dependencies will be added astransitive dependencies to your project. When you declare a dependency on this persistence-deps project,don't forget to specify the dependency type as pom.",
  "page92": "If you later decide to switch to a different JDBC driver (for example, JTDS), just replace the dependenciesin the persistence-deps project to use net.sourceforge.jtds:jtds instead\u00aeof mysql:mysql-java-connector and update the version number. All projects depending on persistence-deps will use JTDS if they decide to update to the newer version. Consolidating related dependencies is a good way to cu\u00aet down on the length of pom.xml files that start having to depend on a largenumber of dependencies. If you need to share a large number of dependencies between projects, youcould also just establish pare\u00aent-child relationships between projects and refactor all common dependencies to the parent project, but the disadvantage of the parent-child approach is that a project can have onlyone parent. Sometimes it makes more sense to group similar dependencies together and reference a pomdependen\u00aecy. This way, your project can reference as many of these consolidated dependency POMs as itneeds.There is a difference between inheriting from a parent project and being managed by a multimoduleproject. A parent project is one that passes its values to its children. A multimodule project simplymanages a group of other subprojects\u00aeor modules. The multimodule relationship is defined from thetopmost level downwards. When setting up a multimodule project, you are simply telling a project that itsbuild should include the specified modules. Multimodule builds are to be used to group modules togetherin a single build. The parent-child relationship is defined from the leaf node upwards.",
  "page93": "The parent-childrelationship deals more with the definition of a particular project. When you associate a child with itsparent, you are telling Maven that a project's POM is d\u00aeerived from another.To illustrate the decision process that goes into choosing a design that uses inheritance vs. multi-moduleor both approaches consider the following two examples: the Maven project used to generate th\u00aeis bookand a hypothetical project that contains a number of logically grouped modules.When we build this Maven book you are reading, we run mvn package in a multi-module projectnamed maven-book. This mul\u00aeti-module project includes two submodules: book-examples andbook-chapters. Neither of these projects share the same parent, they are related only in that theyare modules in the maven-book project. book-examples builds the ZIP and TGZ archives youdownloaded to get this book's example.\u00aeWhen we run the book-examples build from book-examples/directory with mvn package, it has no knowledge that it is a part of the larger maven-book project.book-examples doesn't really care about maven-book, all it knows in life is that its parent is thetop-most sonatype POM and that it creates an archive of examples. In this\u00aecase, the maven-bookproject exists only as a convenience and as an aggregator of modules.Each of the three projects: maven-book, book-examples, and book-chapters all list a shared\"corporate\" parent  sonatype.",
  "page94": "This is a common practice in organizations which have adoptedMaven, instead of having every project extend the Super POM by default, some organizations definea top-level corporate\u00aePOM that serves as the default parent when a project doesn't have any good reasonto depend on another. In this book example, there is no compelling reason to have book-examplesand book-chapters share the same paren\u00aet POM, they are entirely different projects which have a different set of dependencies, a different build configuration, and use drastically different plugins to createthe content you are now reading. Th\u00aee sonatype POM gives the organization a chance to customize thedefault behavior of Maven and supply some organization-specific information to configure deploymentsettings and build profiles.Let's take a look at an example that provides a more accurate picture of a real-world project\u00aewhere inheritance and multi-module relationships exist side by side. Figure 3.5 shows a collection of projects thatresemble a typical set of projects in an enterprise application. There is a top-level POM for the corporation with an artifactId of sonatype. There is a multi-module project named big-system whichreferences sub-module\u00aes server-side and client-side.What's going on here? Let's try to deconstruct this confusing set of arrows. First, let's take a look atbig-system. The big-system might be the project that you would run mvn package on to buildand test the entire system. big-system references submodules client-side and server-side.Each of these projects effectively rolls up all of the code that runs on either the server or on the client.",
  "page95": "Let's focus on the server-side project. Under the server-side project we have a project calledserver-lib and a multi-module project named web-apps. Under web-apps we have two\u00aeJava webapplications: client-web and admin-web.Let's start with the parent/child relationships from client-web and admin-web to web-apps. Sinceboth of the web applications are implemented in the same web applicatio\u00aen framework (let's say Wicket),both projects would share the same set of core dependencies. The dependencies on the Servlet API, theJSP API, and Wicket would all be captured in the web-apps project.\u00aeBoth client-web and admin-web also need to depend on server-lib, this dependency would be defined as a dependency betweenweb-apps and server-lib. Because client-web and admin-web share so much configuration by inheriting from web-apps, both client-web and admin-web will have very small P\u00aeOMscontaining little more than identifiers, a parent declaration, and a final build name.Next we focus on the parent/child relationship from web-apps and server-lib to server-side.In this case, let's just assume that there is a separate working group of developers which work on theserver-side code and another group of develop\u00aeers that work on the client-side code. The list of developers would be configured in the server-side POM and inherited by all of the child projects underneath it: web-apps, server-lib, client-web, and admin-web.We could also imagine thatthe server-side project might have different build and deployment settings which are unique to thedevelopment for the server side.",
  "page96": "The server-side project might define a build profile that only makessense for all of the server-side projects. This build profile might contain the database host and credentials, o\u00aer the server-side project's POM might configure a specific version of the Maven Jettyplugin which should be universal across all projects that inherit the server-side POM.In this example, the main reason to use par\u00aeent/child relationships is shared dependencies and commonconfiguration for a group of projects which are logically related. All of the projects below big-systemare related to one another as submodules, b\u00aeut not all submodules are configured to point back to parentproject that included it as a submodule. Everything is a submodule for reasons of convenience, to buildthe entire system just go to the big-system project directory and run mvn package. Look moreclosely at the figure and you see\u00aethat there is no parent/child relationship between server-side andbig-system. Why is this? POM inheritance is very powerful, but it can be overused. When it makessense to share dependencies and build configuration, a parent/child relationship should be used. Whenit doesn't make sense is when there are distinct differences betw\u00aeeen two projects. Take, for example,the server-side and client-side projects. It is possible to create a system where clientside and server-side inherited a common POM from big-system, but as soon as a significantdivergence between the two child projects develops, you then have to figure out creative ways to factorout common build configuration to big-system without affecting all of the children. Even thoughclient-side and server-side might both depend on Log4J, they also might have distinct pluginconfigurations.",
  "page97": "There's a certain point defined more by style and experience where you decide that minimal duplicationof configuration is a small price to pay for allowing projects like clien\u00aet-side and server-side toremain completely independent. Designing a huge set of thirty plus projects which all inherit five levelsof POM configuration isn't always the best idea. In such a setup, you might not have\u00aeto duplicate yourLog4J dependency more than once, but you'll also end up having to wade through five levels of POMjust figure out how Maven calculated your effective POM. All of this complexity to\u00aeavoid duplicatingfive lines of dependency declaration. In Maven, there is a \"Maven Way\", but there are also many waysto accomplish the same thing. It all boils down to preference and style. For the most part, you won't gowrong if all of your submodules turn out to define back-refer\u00aeences to the same project as a parent, butyour use of Maven may evolve over time.Maven models projects as nouns which are described by a POM. The POM captures the identity of aproject: What does a project contain? What type of packaging a project needs? Does the project havea parent? What are the dependencies? We've explored\u00aethe idea of describing a project in the previouschapters, but we haven't introduced the mechanism that allows Maven to act upon these objects. In Maventhe \"verbs\" are goals packaged in Maven plugins which are tied to a phases in a build lifecycle. A Mavenlifecycle consists of a sequence of named phases: prepare-resources, compile, package, and install amongother.",
  "page98": "There is phase that captures compilation and a phase that captures packaging. There are pre- andpost- phases which can be used to register goals which must run prior to compilation\u00ae, or tasks which mustbe run after a particular phase. When you tell Maven to build a project, you are telling Maven to stepthrough a defined sequence of phases and execute any goals which may have been registered with e\u00aeachphase.A build lifecycle is an organized sequence of phases that exist to give order to a set of goals. Those goalsare chosen and bound by the packaging type of the project being acted upon. There are\u00aethree standardlifecycles in Maven: clean, default (sometimes called build) and site. In this chapter, you are going tolearn how Maven ties goals to lifecycle phases and how the lifecycle can be customized. You will alsolearn about the default lifecycle phases.The interesting phase in the\u00aeclean lifecycle is the clean phase. The Clean plugin's clean goal (clean:clean) is bound to the clean phase in the clean lifecycle. The clean:clean goal deletes theoutput of a build by deleting the build directory. If you haven't customized the location of the builddirectory it will be the ${basedir}/target directory as\u00aedefined by the Super POM. When you execute theclean:clean goal you do not do so by executing the goal directly with mvn clean:clean, youdo so by executing the clean phase of the clean lifecycle. Executing the clean phase gives Maven anopportunity to execute any other goals which may be bound to the pre-clean phase.",
  "page99": "For example, suppose you wanted to trigger an antrun:run goal task to echo a notification on preclean, or to make an archive of a project's build directory before it is delete\u00aed. Simply running theclean:clean goal will not execute the lifecycle at all, but specifying the clean phase will use theclean lifecycle and advance through the three lifecycle phases until it reaches the clean phase. Tr\u00aeiggering a Goal on pre-clean shows an example of build configuration which binds the antrun:run goalto the pre-clean phase to echo an alert that the project artifact is about to be deleted. In this examp\u00aele,the antrun:run goal is being used to execute some arbitrary Ant commands to check for an existingproject artifact. If the project's artifact is about to be deleted it will print this to the screenIn addition to configuring Maven to run a goal during the pre-clean phase, you can al\u00aeso customizethe Clean plugin to delete files in addition to the build output directory. You can configure the plugin toremove specific files in a fileSet. The example below configures clean to remove all .class files in adirectory named target-other/ using standard Ant file wildcards: * and \\**.Maven does more than build software\u00aeartifacts from project, it can also generate project documentationand reports about the project, or a collection of projects. Project documentation and site generation havea dedicated lifecycle which contains four phases.",
  "page100": "The packaging type does not usually alter this lifecycle since packaging types are concerned primarilywith artifact creation, not with the type of site generated. The Site plugin k\u00aeicks off the execution of Doxiadocument generation and other report generation plugins. You can generate a site from a Maven projectby running the following command:The specific goals bound to each phase default to a se\u00aet of goals specific to a project's packaging. A projectwith packaging jar has a different set of default goals from a project with a packaging of war. Thepackaging element affects the steps required\u00aeto build a project. For an example of how the packagingaffects the build, consider two projects: one with pom packaging and the other with jar packaging.The project with pom packaging will run the site:attach-descriptor goal during the packagephase, and the project with jar packaging wil\u00ael run the jar:jar goal instead.The following sections describe the lifecycle for all built-in packaging types in Maven. Use these sectionsto find out what default goals are mapped to default lifecycle phasesJAR is the default packaging type, the most common, and thus the most commonly encountered lifecycleconfiguration. The defaul\u00aet goals for the JAR lifecycle .POM is the simplest packaging type. The artifact that it generates is itself only, rather than a JAR, SAR,or EAR. There is no code to test or compile, and there are no resources the process. The default goals forprojects with POM packaging .",
  "page101": "All software problems can be termed as bugs. A software bug usually occurs when the software does not do what it is intended to do or does something that it is not intended to do.\u00aeFlaws in specifications, design, code or other reasons can cause these bugs. Identifying and fixing bugs in the early stages of the software is very important as the cost of fixing bugs grows over time. So, the goal of\u00aea software tester is to find bugs and find them as early as possible and make sure they are fixed. Testing is context-based and risk-driven. It requires a methodical and disciplined approach to finding b\u00aeugs. A good software tester needs to build credibility and possess the attitude to be explorative, troubleshooting, relentless, creative, diplomatic and persuasive. As against the perception that testing starts only after the completion of coding phase, it actually begins even before the\u00aefirst line of code can be written. In the life cycle of the conventional software product, testing begins at the stage when the specifications are written, i.e. from testing the product specifications or product spec. Finding bugs at this stage can save huge amounts of time and money. Once the specifications are well understood, y\u00aeou are required to design and execute the test cases. Selecting the appropriate technique that reduces the number of tests that cover a feature is one of the most important things that you need to take into consideration while designing these test cases. Test cases need to be designed to cover all aspects of the software, i.e. security, database, functionality (critical and general) and the user interface. Bugs originate when the test cases are executed. As a tester you might have to perform testing under different circumstances, i.e. life cycle model that does not support much of formal testing or retesting. Further, testing using different operating systems, browsers and the configurations are to be taken care of. Reporting a bug may be the most important and sometimes the most difficult task that you as a software tester will perform. By using various tools and clearly communicating to the developer, you can ensure that the bugs you find are fixed. Using automated tools to execute tests, run scripts and tracking bugs improves efficiency and effectiveness of your tests. Also, keeping pace with the latest developments in the field will augment your career as a software test engineer.",
  "page102": "Software is a series of instructions for the computer that perform a particular task,called a program; the two major categories of software are system software andapplication softw\u00aeare. System software is made up of control programs. Applicationsoftware is any program that processes data for the user (spreadsheet, wordprocessor, payroll, etc.).A software product should only be released after it ha\u00aes gone through a properprocess of development, testing and bug fixing. Testing looks at areas such asperformance, stability and error handling by setting up test scenarios undercontrolled conditions and\u00aeassessing the results. This is why exactly any software hasto be tested. It is important to note that software is mainly tested to see that it meetsthe customers' needs and that it conforms to the standards. It is a usual norm thatsoftware is considered of good quality if it meets th\u00aee user requirements.Quality can briefly be defined as \"a degree of excellence\". High quality softwareusually conforms to the user requirements. A customer's idea of quality may cover abreadth of features - conformance to specifications, good performance onplatform(s)/configurations, completely meets operational requ\u00aeirements (even if notspecified!), compatibility to all the end-user equipment, no negative impact onexisting end-user base at introduction time.Quality software saves good amount of time and money. Because software will havefewer defects, this saves time during testing and maintenance phases. Greaterreliability contributes to an immeasurable increase in customer satisfaction as well aslower maintenance costs. Because maintenance represents a large portion of allsoftware costs, the overall cost of the project will most likely be lower than similarprojec that theprogram meets the program specification, as a test engineer you need to create testcases, procedures, scripts and generate data. You execute test procedures andscripts, analyze standards and evaluate results of system/integration/regressiontesting. You also... Speed up development process by identifying bugs at an early stage (e.g.specifications stage)Reduce the organization's risk of legal liability Maximize the value of the software Assure successful launch of the product, save money, time and reputation ofthe company by discovering bugs and design flaws at an early stage beforefailures occur in production, or in the field Promote continual improvement",
  "page103": "As software engineering is now being considered as a technical engineeringprofession, it is important that the software test engineer's posses certain traits witha relentless\u00aeattitude to make them stand out. Here are a few.Know the technology. Knowledge of the technology in which the application isdeveloped is an added advantage to any tester. It helps design better and powerfultest cases ba\u00aesing on the weakness or flaws of the technology. Good testers knowwhat it supports and what it doesn't, so concentrating on these lines will help thembreak the application quickly. Perfectionist and\u00aea realist. Being a perfectionist will help testers spot the problemand being a realist helps know at the end of the day which problems are reallyimportant problems. You will know which ones require a fix and which ones don't.Tactful, diplomatic and persuasive. Good software testers\u00aeare tactful and knowhow to break the news to the developers. They are diplomatic while convincing thedevelopers of the bugs and persuade them when necessary and have their bug(s)fixed. It is important to be critical of the issue and not let the person who developedthe application be taken aback of the findings. An explorer. A bit\u00aeof creativity and an attitude to take risk helps the testersventure into unknown situations and find bugs that otherwise will be looked over.Troubleshoot. Troubleshooting and figuring out why something doesn't workhelps testers be confident and clear in communicating the defects to the developers. Posses people skills and tenacity. Testers can face a lot of resistance fromprogrammers. Being socially smart and diplomatic doesn't mean being indecisive. Thebest testers are both-socially adept and tenacious where it matters.Organized. Best testers very well their findings that can be used as an evidence and doublecheck their findings.",
  "page104": "Testing can't show that bugs don't exist. An important reason for testing is toprevent defects. You can perform your tests, find and report bugs, but at no point canyou g\u00aeuarantee that there are no bugs. It is impossible to test a program completely. Unfortunately this is not possibleeven with the simplest program because - the number of inputs is very large, numberof outputs is ver\u00aey large, number of paths through the software is very large, and thespecification is subjective to frequent changes. You can't guarantee quality. As a software tester, you cannot test everything and\u00aeare not responsible for the quality of the product. The main way that a tester can failis to fail to report accurately a defect you have observed. It is important to rememberthat we seldom have little control over quality. Target environment and intended end user. Anticipating and testing\u00aetheapplication in the environment user is expected to use is one of the major factors thatshould be considered. Also, considering if the application is a single user system ormulti user system is important for demonstrating the ability for immediate readinesswhen necessary. The error case of Disney's Lion King illustrates th\u00aeis. Disney Companyreleased its first multimedia CD-ROM game for children, The Lion King AnimatedStorybook. It was highly promoted and the sales were huge. Soon there were reportsthat buyers were unable to get the software to work. It worked on a few systems -likely the ones that the Disney programmers used to create the game - but not on themost common systems that the general public used.No application is 100% bug free. It is more reasonable to recognize there arepriorities, which may leave some less critical problems unsolved or If the answer is zero, yourcomputer is just fine. If you get anything else, you have an old Intel Pentium CPU witha floating-point division bug.Be the customer. Try to use the system as a lay user. To get a glimpse of this, get aperson who has no idea of the application to use it for a while and you will be amazedto see the number of problems the person seem to come across. As you can see,there is no procedure involved. Doing this could actually cause the system toencounter an array of unexpected tests - repetition, stress, load, race etc.",
  "page105": " Build your credibility. Credibility is like quality that includes reliability, knowledge,consistency, reputation, trust, attitude and attention to detail. It is not instant butsho\u00aeuld be built over time and gives voice to the testers in the organization. Your keysto build credibility - identify your strengths and weaknesses, build good relations,demonstrate competency, and be willing to admi\u00aet mistakes, re-assess and adjust. Test what you observe. It is very important that you test what you can observeand have access to. Writing creative test cases can help only when you have the opportunity\u00aeto observe the results. So, assume nothing.Not all bugs you find will be fixed. Deciding which bugs will be fixed and whichwon't is a risk-based decision. Several reasons why your bug might not be fixed iswhen there is no enough time, the bug is dismissed for a new feature, fixing i\u00aet mightbe very risky or it may not be worth it because it occurs infrequently or has a workaround where the user can prevent or avoid the bug. Making a wrong decision can bedisastrous.Review competitive products. Gaining a good insight into various products of thesame kind and getting to know their functionality and general behavi\u00aeor will help youdesign different test cases and to understand the strengths and weaknesses of yourapplication. This will also enable you to add value and suggest new features andenhancements to your product.Follow standards and processes. As a tester, your need to conform to thestandards and guidelines set by the organization. These standards pertain toreporting hierarchy, coding, documentation, testing, reporting bugs, using automatedtools etc. The software life cycle typically includes the following: requirements analysis, design,coding, testing, installatizations provide solutions to customerrequirements by developing appropriate software that best suits theirspecifications.",
  "page106": "Coding. The development process tends to run iteratively through these phasesrather than linearly; several models (spiral, waterfall etc.) have been proposed todescribe this proces\u00aes.Activities in this phase - Create Test Data, Create Source, Generate Object Code,Create Operating Documentation, Plan Integration, Perform IntegrationTesting. The process of using the developed system with the intent\u00aeto find errors.Defects/flaws/bugs found at this stage will be sent back to the developer for a fixand have to be re-tested. This phase is iterative as long as the bugs are fixed to meetthe requirements.A\u00aectivities in this phase - Plan Verification and Validation, Execute Verification andvalidation Tasks, Collect and Analyze Metric Data, Plan Testing, Develop TestRequirements, Execute TestsInstallation. The so developed and tested software will finally need to be installed atthe client pla\u00aece. Careful planning has to be done to avoid problems to the user afterinstallation is done Activities in this phase - Plan Installation, Distribution of Software, Installation ofSoftware, Accept Software in Operational Environment.Operation and Support. Support activities are usually performed by theorganization that developed th\u00aee software. Both the parties usually decide on theseactivities before the system is developed.Activities in this phase - Operate the System, Provide Technical Assistance andConsulting, Maintain Support Request Log.Maintenance. The process does not stop once it is completely implemented andinstalled at user place; this phase undertakes development of new features,enhancements etc.Activities in this phase - Reapplying Software Life Cycle.The way you approach a particular application for testing greatly depends on the lifecycle model it follows. Thime others don't. So, the number of test cases developed, featurescovered, time spent on each issue depends on the life cycle model the applicationfollows.",
  "page107": "Analysis. Involves activities that - develop functional validation based on BusinessRequirements (writing test cases basing on these details), develop test case format(time estimat\u00aees and priority assignments), develop test cycles (matrices andtimelines), identify test cases to be automated (if applicable), define area of stressand performance testing, plan the test cycles required for the project\u00aeand regressiontesting, define procedures for data maintenance (backup, restore, validation), reviewdocumentation.Design. Activities in the design phase - Revise test plan based on changes, revise testcy\u00aecle matrices and timelines, verify that test plan and cases are in a database orrequisite, continue to write test cases and add new ones based on changes, developRisk Assessment Criteria, formalize details for Stress and Performance testing, finalizetest cycles (number of test case per cy\u00aecle based on time estimates per test case andpriority), finalize the Test Plan, (estimate resources to support development in unittesting).Construction (Unit Testing Phase). Complete all plans, complete Test Cycle matricesand timelines, complete all test cases (manual), begin Stress and Performance testing,test the automated testi\u00aeng system and fix bugs, (support development in unittesting), run QA acceptance test suite to certify software is ready to turn over to QA.Test Cycle(s) / Bug Fixes (Re-Testing/System Testing Phase). Run the test cases (frontand back end), bug reporting, verification, and revise/add test cases as required.Final Testing and Implementation (Code Freeze Phase). Execution of all front end testcases - manual and automated, execution of all back end test cases - manual andautomated, execute all Stress and Performance tests, provide on-going defecnd updateaccordingly.Post Implementation. Post implementation evaluation meeting can be conducted toreview entire project.",
  "page108": "Constantly changing software requirements cause a lot of confusion and pressureboth on the development and testing teams. Often, a new feature added or existingfeature removed can\u00aebe linked to the other modules or components in the software.Overlooking such issues causes bugs. Also, fixing a bug in one part/component of the software might arise another in adifferent or same component. Lack of for\u00aeesight in anticipating such issues can causeserious problems and increase in bug count. This is one of the major issues because ofwhich bugs occur since developers are very often subject to pressure rela\u00aeted totimelines; frequently changing requirements, increase in the number of bugs etc. Designing and re-designing, UI interfaces, integration of modules, databasemanagement all these add to the complexity of the software and the system as awhole. Fundamental problems with software design\u00aeand architecture can cause problemsin programming. Developed software is prone to error as programmers can makemistakes too. As a tester you can check for, data reference/declaration errors, controlflow errors, parameter errors, input/output errors etc. Rescheduling of resources, re-doing or discarding already completed work,chang\u00aees in hardware/software requirements can affect the software too. Assigning anew developer to the project in midway can cause bugs. This is possible if propercoding standards have not been followed, improper code documentation, ineffectiveknowledge transfer etc. Discarding a portion of the existing code might just leave itstrail behind in other parts of the software; overlooking or not eliminating such codecan cause bugs. Serious bugs can especially occur with larger projects, as it getstougher to identify the problem area. Programmers usually teseverity.Complexity in keeping track of all the bugs can again cause bugs by itself. This getsharder when a bug has a very complex life cycle i.e. when the number of times it hasbeen closed, re-opened, not accepted, ignored etc.",
  "page109": "Not Accepted/Won't fix: If the developer considers the bug as low level or does notaccept it as a bug, thus pushing it into Not Accepted/Won't fix state.Such bugs will be\u00aeassigned to the project manager who will decide if the bug needs afix. If it needs, then assigns it back to the developer, and if it doesn't, then assigns itback to the tester who will have to close the bug.Pendin\u00aeg: A bug accepted by the developer may not be fixed immediately. In suchcases, it can be put under Pending state.Fixed: Programmer will fix the bug and resolves it as Fixed.Close: The fixed bug will be a\u00aessigned to the tester who will put it in the Close state.Re-Open: Fixed bugs can be re-opened by the testers in case the fix producesproblems elsewhere.Costs are logarithmic; they increase in size tenfold as the time increases. A bug foundand fixed during the early stages - requireme\u00aents or product spec stage can be fixed bya brief interaction with the concerned and might cost next to nothing.During coding, a swiftly spotted mistake may take only very less effort to fix. Duringintegration testing, it costs the paperwork of a bug report and a formally documentedfix, as well as the delay and expense of a re-test\u00ae.During system testing it costs even more time and may delay delivery. Finally, duringoperations it may cause anything from a nuisance to a system failure, possibly withcatastrophic consequences in a safety-critical system such as an aircraft or anemergency service.It is difficult to determine when exactly to stop testing. Here are a few commonfactors that help you decide when you can stop or reduce testing: Deadlines (release deadlines, testing deadlines, etc.) Test cases completed with certain percentage passed Test budget depleted Coverage of code/functionality/requirements reaches a specified point Bug rate falls below a certain level Beta or alpha testing period ends There are basically three levels of testing i.e. Unit Testing, Integration Testing andSystem Testing. Various types of testing come under these levels.Unit TestingTo verify a single program or a section of a single programIntegration TestingTo verify interaction between system componentsPrerequisite: unit testing completed on all components that compose a system.System TestingTo verify and validate behaviors of the entire system against the original systemobjectives.",
  "page110": "Software testing is a process that identifies the correctness, completeness, andquality of software.Following is a list of various types of software testing and their definitions i\u00aen arandom order: Formal Testing: Performed by test engineers Informal Testing: Performed by the developers Manual Testing: That part of software testing that requires human input, analysis,or evaluation. Automated Testi\u00aeng: Software testing that utilizes a variety of tools to automatethe testing process. Automated testing still requires a skilled quality assuranceprofessional with knowledge of the automation tools and t\u00aehe software being testedto set up the test cases. Black box Testing: Testing software without any knowledge of the back-end of thesystem, structure or language of the module being tested. Black box test cases arewritten from a definitive source document, such as a specification or require\u00aementsdocument. White box Testing: Testing in which the software tester has knowledge of theback-end, structure and language of the software, or at least its purpose. Unit Testing: Unit testing is the process of testing a particular complied program,i.e., a window, a report, an interface, etc. independently as a stand-alonecomponen\u00aet/program. The types and degrees of unit tests can vary among modifiedand newly created programs. Unit testing is mostly performed by the programmerswho are also responsible for the creation of the necessary unit test data. Incremental Testing: Incremental testing is partial testing of an incompleteproduct. The goal of incremental testing is to provide an early feedback to softwaredevelopers. System Testing: System testing is a form of black box testing. The purpose ofsystem testing is to validate an application's accuracy and completeness inperforming the functions as designed.Integration Testing: Testing two or more modules or functions together with theintent of finding interface defects between the modules/functions. System Integration Testing: Testing of software components that have beendistributed across multiple platformsngnetwork communication, or interacting with other hardware, application, or system.",
  "page111": "Acceptance Testing: Testing the system with the intent of confirming readiness ofthe product and customer acceptance. Also known as User Acceptance Testing. Adhoc Testing: Testing\u00aewithout a formal test plan or outside of a test plan. Withsome projects this type of testing is carried out as an addition to formal testing.Sometimes, if testing occurs very late in the development cycle, this will be\u00aethe onlykind of testing that can be performed - usually done by skilled testers. Sometimes adhoc testing is referred to as exploratory testing. Configuration Testing: Testing to determine how well t\u00aehe product works with abroad range of hardware/peripheral equipment configurations as well as on differentoperating systems and software. Load Testing: Testing with the intent of determining how well the product handlescompetition for system resources. The competition may come in the form\u00aeof networktraffic, CPU utilization or memory allocation. Stress Testing: Testing done to evaluate the behavior when the system is pushedbeyond the breaking point. The goal is to expose the weak links and to determine ifthe system manages to recover gracefully. Performance Testing: Testing with the intent of determining how effici\u00aeently aproduct handles a variety of events. Automated test tools geared specifically to testand fine-tune performance are used most often for this type of testing. Usability Testing: Usability testing is testing for 'user-friendliness'. A way toevaluate and measure how users interact with a software product or site. Tasks aregiven to users and observations are made. Installation Testing: Testing with the intent of determining if the product iscompatible with a variety of platforms and how easily it installs. Recovery/Error Testing: Testing how well a system recovers from crashes,hardware failures, or other catastrophic problems. Security Testing: Testing of database and network software in order to keepcompany data and resources secure from mistaken/accidental users, hackers, andother malevolent attackers. Penetration Testing:",
  "page112": "Following are the most common software errors that aid you in software testing. Thishelps you to identify errors systematically and increases the efficiency andproductivity of soft\u00aeware testing.User Interface Errors: Missing/Wrong Functions Doesn't do what the user expects,Missing information, Misleading, Confusing information, Wrong content in Help text,Inappropriate error messages. Performa\u00aence issues - Poor responsiveness, Can'tredirect output, Inappropriate use of key board Error Handling: Inadequate - protection against corrupted data, tests of userinput, version control; Ignores -\u00aeoverflow, data comparison, Error recovery - abortingerrors, recovery from hardware problems. Boundary related errors: Boundaries in loop, space, time, memory, mishandling ofcases outside boundary. Calculation errors: Bad Logic, Bad Arithmetic, Outdated constants, Calculationerrors, i\u00aencorrect conversion from one data representation to another, Wrongformula, incorrect approximation. Initial and Later states: Failure to - set data item to zero, to initialize a loopcontrol variable, or re-initialize a pointer, to clear a string or flag, Incorrectinitialization. Control flow errors: Wrong returning state assumed,\u00aeException handling basedexits, Stack underflow/overflow, Failure to block or un-block interrupts, Comparisonsometimes yields wrong result, Missing/wrong default, and Data Type errors. Errors in Handling or Interpreting Data: Un-terminated null strings, overwriting afile after an error exit or user abort. Race Conditions: Assumption that one event or task finished before anotherbegins, Resource races, Tasks starts before its prerequisites are met, Messages crosor don't arrive in the order sent.Load Conditions: Required resources are not available, No available large memoryarea, Low priority tasks not put off, doesn't erase old files from mass storage, anddoesn't return unused memory.Hardware: Wrong Device, Device unavailable, Underutilizing device intelligence,Misunderstood status or return code, Wrong operation or instruction",
  "page113": "Test Policy - A document characterizing the organization's philosophy towardssoftware testing.Test Strategy - A high-level document defining the test phases to be performed an\u00aedthe testing within those phases for a program. It defines the process to be followed ineach project. This sets the standards for the processes, documents, activities etc. thatshould be followed for each project.For exa\u00aemple, if a product is given for testing, you should decide if it is better to useblack-box testing or white-box testing and if you decide to use both, when will youapply each and to which part of the sof\u00aetware? All these details need to be specified inthe Test Strategy.Project Test Plan - a document defining the test phases to be performed and thetesting within those phases for a particular project.A Test Strategy should cover more than one project and should address the followingissues:\u00aeAn approach to testing high risk areas first, Planning for testing, How toimprove the process based on previous testing, Environments/data used, Testmanagement - Configuration management, Problem management, What Metrics arefollowed, Will the tests be automated and if so which tools will be used, What are theTesting Stages and Tes\u00aeting Methods, Post Testing Review process, Templates.Test planning needs to start as soon as the project requirements are known. The firstdocument that needs to be produced then is the Test Strategy/Testing Approach thatsets the high level approach for testing and covers all the other elements mentionedabove.Once the approach is understood, a detailed test plan can be written. Usually, thistest plan can be written in different styles. Test plans can completely differ fromproject to project in the same organization.PurposeTo describe the scope, approach, resources, and schedule of the testing activities. Toidentify the items being tested, the features to be tested, the testing tasks to beperformed, the personnel responsible for each task, and the risks associated with thisplan.OUTLINEA test plan shall have the following structure: Test contingencies Approvals",
  "page114": "Like any other process in software testing, the major tasks in test planning are to -Develop Test Strategy, Critical Success Factors, Define Test Objectives, IdentifyNeeded Te\u00aest Resources, Plan Test Environment, Define Test Procedures, IdentifyFunctions To Be Tested, Identify Interfaces With Other Systems or Components, WriteTest Scripts, Define Test Cases, Design Test Data, Build Test Matri\u00aex, Determine TestSchedules, Assemble Information, Finalize the Plan.A test case is a detailed procedure that fully tests a feature or an aspect of a feature.While the test plan describes what to test, a\u00aetest case describes how to perform aparticular test. You need to develop test cases for each test listed in the test planAs a tester, the best way to determine the compliance of the software torequirements is by designing effective test cases that provide a thorough test of aunit. Various\u00aetest case design techniques enable the testers to develop effective testcases. Besides, implementing the design techniques, every tester needs to keep inmind general guidelines that will aid in test case design:a. The purpose of each test case is to run the test in the simplest way possible.[Suitable techniques - Specification de\u00aerived tests, Equivalence partitioning]b. Concentrate initially on positive testing i.e. the test case should show that thesoftware does what it is intended to do. [Suitable techniques - Specification derivedtests, Equivalence partitioning, State-transition testing]c. Existing test cases should be enhanced and further test cases should be designedto show that the software does not do anything that it is not specified to do i.e.Negative Testing [Suitable techniques - Error guessing, Boundary value analysis,Internal boundary value testing, State-transition testing]d. Where appropriate, test cases should be designed to address issues such asperformance, safety requirements and security requirements [Suitable techniques -Specification derived tests]e. Further test cases can then be added to the unit test specification to achieve categories: Black boxtechniques, White box techniques and other techniques that do not fall under eithercategorySpecification Derived TestsAs the name suggests, test cases are designed by walking through the relevantspecifications.",
  "page115": "For example, if a program accepts integer values only from 1 to 10. The possible testcases for such a program would be the range of all integers. In such a program, allintegers up\u00aeto 0 and above 10 will cause an error. So, it is reasonable to assume that if11 will fail, all values above it will fail and vice versa.If an input condition is a range of values, let one valid equivalence class is the\u00aerange (0or 10 in this example). Let the values below and above the range be two respectiveinvalid equivalence values (i.e. -1 and 11). Therefore, the above three partition valuescan be used as test cases\u00aefor the above example.Boundary Value AnalysisThis is a selection technique where the test data are chosen to lie along theboundaries of the input domain or the output range. This technique is often called asstress testing and incorporates a degree of negative testing in the test design b\u00aeyanticipating that errors will occur at or around the partition boundaries.For example, a field is required to accept amounts of money between $0 and $10. Asa tester, you need to check if it means up to and including $10 and $9.99 and if $10 isacceptable. So, the boundary values are $0, $0.01, $9.99 and $10.Now, the following test\u00aes can be executed. A negative value should be rejected, 0should be accepted (this is on the boundary), $0.01 and $9.99 should be accepted,null and $10 should be rejected. In this way, it uses the same concept of partitions asequivalence partitioning.State Transition TestingAs the name suggests, test cases are designed to test the transition between thestates by creating the events that cause the transition.Branch TestingIn branch testing, test cases are designed to exercise control flow branches ordecision points in a unit. This is usually aimed at achieving a target level of DecisionCoverage. Branch Coverage, need to test both branches of IF and ELSE. All branchesand compound conditions (e.g. loops and array handling) within the branch should beexercised at least once.Condition TestingThe object of set. Data use isanywhere that a data item is read or used. The objective is to create test cases thatwill drive execution through paths between specific definitions and uses.",
  "page116": "Internal Boundary Value TestingIn many cases, partitions and their boundaries can be identified from a functionalspecification for a unit, as described under equivalence partitioni\u00aeng and boundaryvalue analysis above. However, a unit may also have internal boundary values thatcan only be identified from a structural specification.Error GuessingIt is a test case design technique where the testers u\u00aese their experience to guess thepossible errors that might occur and design test cases accordingly to uncover them.Using any or a combination of the above described test case design techniques; youcan de\u00aevelop effective test cases.A use case describes the system's behavior under various conditions as it responds toa request from one of the users. The user initiates an interaction with the system toaccomplish some goal. Different sequences of behavior, or scenarios, can unfold,dependi\u00aeng on the particular requests made and conditions surrounding the requests.The use case collects together those different scenarios.Use cases are popular largely because they tell coherent stories about how thesystem will behave in use. The users of the system get to see just what this newsystem will be and get to react early.As d\u00aeiscussed earlier, defect is the variance from a desired product attribute (it can bea wrong, missing or extra data). It can be of two types - Defect from the product or avariance from customer/user expectations. It is a flaw in the software system and hasno impact until it affects the user/customer and operational system.With the knowledge of testing so far gained, you can now be able to categorize thedefects you have found. Defects can be categorized into different types basing on thecore issues they address. Some defects address security or database issues whileothers may refer to functionality or UI issues.Security Defects: Application security defects generally involve improper handling ofdata sent from the user to the application. These defects are the most severe andgiven highest priority for a fix.Examples:Authentication: affect the functionality of the application.Examples: All Javascript errors Buttons like Save, Delete, Cancel not performing their intended functionsA missing functionality (or) a feature not functioning the way it is intended toContinuous execution of loops",
  "page117": "User Interface Defects: As the name suggests, the bugs deal with problems related toUI are usually considered less severe.Examples:Improper error/warning/UI messages Spelling mista\u00aekes Alignment problemsOnce the test cases are developed using the appropriate techniques, they areexecuted which is when the bugs occur. It is very important that these bugs bereported as soon as possible because, the e\u00aearlier you report a bug, the more timeremains in the schedule to get it fixed.Simple example is that you report a wrong functionality documented in the Help file afew months before the product release, t\u00aehe chances that it will be fixed are very high.If you report the same bug few hours before the release, the odds are that it won't befixed. The bug is still the same though you report it few months or few hours beforethe release, but what matters is the time.It is not just enough to\u00aefind the bugs; these should also be reported/communicatedclearly and efficiently, not to mention the number of people who will be reading thedefect.Defect tracking tools (also known as bug tracking tools, issue tracking tools orproblem trackers) greatly aid the testers in reporting and tracking the bugsfound in software applicatio\u00aens. They provide a means of consolidating a keyelement of project information in one place. Project managers can then seewhich bugs have been fixed, which are outstanding and how long it is taking tofix defects. Senior management can use reports to understand the state of thedevelopment process.You should provide enough detail while reporting the bug keeping in mind the peoplewho will use it - test lead, developer, project manager, other testers, new testersassigned etc. This means that the report you will write should be concise, straight andclear. Following are the details your report should contain:Bug Title Bug identifier (number, ID, etc.) The application name or identifier and version The function, module, feature, object, screen, etc. where the bug occurredEnvironment (OS, Browser and its version)-Bug Type Comments",
  "page118": "Once the reported defect is fixed, the tester needs to re-test to confirm the fix. This isusually done by executing the possible scenarios where the bug can occur. Onceretesting is\u00aecompleted, the fix can be confirmed and the bug can be closed. Thismarks the end of the bug life cycle.The documents outlined in the IEEE Standard of Software Test Documentation coverstest planning, test specification,\u00aeand test reporting.Test reporting covers four document types: A Test Item Transmittal Report identifies the test items being transmitted fortesting from the development to the testing group in the event\u00aethat a formalbeginning of test execution is desiredDetails to be included in the report - Purpose, Outline, Transmittal-Report Identifier,Transmitted Items, Location, Status, and Approvals.. A Test Log is used by the test team to record what occurred during test executionDetails to be in\u00aecluded in the report - Purpose, Outline, Test-Log Identifier,Description, Activity and Event Entries, Execution Description, Procedure Results,Environmental Information, Anomalous Events, Incident-Report Identifiers A Test Incident report describes any event that occurs during the test executionthat requires further investigationD\u00aeetails to be included in the report - Purpose, Outline, Test-Incident-Report Identifier,Summary, Impact A test summary report summarizes the testing activities associated with one ormore test-design specificationsDetails to be included in the report - Purpose, Outline, Test-Summary-ReportIdentifier, Summary, Variances, Comprehensiveness Assessment, Summary of Results,Summary of Activities, and Approvals.Automating testing is no different from a programmer using a coding language towrite programs to automate any manual process. One of the problems with testinglarge systems is that it can go beyond the scope of small test teams. Because only asmall number of testers are available the coverage and depth of testing provided areinadequate for the task at hand.Expanding the test team beyond a certain size also b to make changes in the current ways you perform testing.Involve people who will be using the tool to help design the automated testingprocess.Create a set of evaluation criteria for functions that you will want to considerwhen using the automated test tool.",
  "page119": "Fully manual testing has the benefit of being relatively cheap and effective. But asquality of the product improves the additional cost for finding further bugs becomesmore expensi\u00aeve. Large scale manual testing also implies large scale testing teams withthe related costs of space overhead and infrastructure. Manual testing is also farmore responsive and flexible than automated testing but is pron\u00aee to tester errorthrough fatigue.Fully automated testing is very consistent and allows the repetitions of similar testsat very little marginal cost. The setup and purchase costs of such automation are ve\u00aeryhigh however and maintenance can be equally expensive. Automation is alsorelatively inflexible and requires rework in order to adapt to changing requirements.Partial Automation incorporates automation only where the most benefits can beachieved. The advantage is that it targets specific\u00aeally the tasks for automation andthus achieves the most benefit from them. It also retains a large component ofmanual testing which maintains the test team's flexibility and offers redundancy bybacking up automation with manual testing. The disadvantage is that it obviouslydoes not provide as extensive benefits as either extr\u00aeeme solution.Take time to define the tool requirements in terms of technology, process,applications, people skills, and organization.During tool evaluation, prioritize which test types are the most critical to yoursuccess and judge the candidate tools on those criteria. Understand the tools and their trade-offs. You may need to use a multi-tool solutionto get higher levels of test-type coverage. For example, you will need to combinethe capture/play-back tool with a load-test tool to cover your performance testcases. Involve potential users in the definition of tool requirements and evaluation criteria. Build an evaluation scorecard to compare each tool's performance against acommon set of criteria. Rank the criteria in terms of relative importance to theorganization. Buying the Wrong Tool Inadequate Test Team Organization Lack of Management Support Incomplete Coverage of Test Types by the selected tool Inadequate Tool Training Difficulty using the tool Lack of a Basic Test Process or Understanding of What to Test Lack of Configuration Management Processes Lack of Tool Compatibility and InteroperabilityLack of Tool Availability",
  "page120": " oversight, Software subcontractmanagement, Software quality assurance, Software configuration managementLevel 3 - Defined: Key practice areas - Organization process focus, Or\u00aeganizationprocess definition, Training program, integrated software management, Softwareproduct engineering, intergroup coordination, Peer reviewsLevel 4 - Manageable: Key practice areas - Quantitative Process Mana\u00aegement,Software Quality ManagementLevel 5 - Optimizing: Key practice areas - Defect prevention, Technology changemanagement, Process change managementSix Sigma is a quality management program to ach\u00aeieve \"six sigma\" levels of quality. Itwas pioneered by Motorola in the mid-1980s and has spread too many othermanufacturing companies, notably General Electric Corporation (GE).Six Sigma is a rigorous and disciplined methodology that uses data and statisticalanalysis to measure and impr\u00aeove a company's operational performance by identifyingand eliminating \"defects\" from manufacturing to transactional and from product toservice. Commonly defined as 3.4 defects per million opportunities, Six Sigma can bedefined and understood at three distinct levels: metric, methodology andphilosophy.Training Sigma processes are\u00aeexecuted by Six Sigma Green Belts and Six Sigma BlackBelts, and are overseen by Six Sigma Master Black BeltsISO - International Organization for Standardization is a network of the nationalstandards institutes of 150 countries, on the basis of one member per country, with aCentral Secretariat in Geneva, Switzerland, that coordinates the system. ISO is a nongovernmental organization. ISO has developed over 13, 000 International Standardson a variety of subjects.",
  "page121": "Capability Maturity git Model - Developed by the software community in 1986 withleadership from the SEI. The CMM describes the principles and practices underlyingsoftware process m\u00aeaturity. It is intended to help software organizations improve thematurity of their software processes in terms of an evolutionary path from ad hoc,chaotic processes to mature, disciplined software processes. The focus\u00aeis onidentifying key process areas and the exemplary practices that may comprise adisciplined software process.What makes up the CMM? The CMM is organized into five maturity levels: Initial Repeatable De\u00aefined ManageableOptimizingExcept for Level 1, each maturity level decomposes into several key process areasthat indicate the areas an organization should focus on to improve its softwareprocess.Level 1 - Initial Level: Disciplined process, Standard, Consistent process, Predictableprocess,\u00aeContinuously Improving processLevel 2 - Repeatable: Key practice areas - Requirements management, Softwareproject planning, Software project tracking ",
  "page122": "Following are some facts that can help you gain a better insight into the realities ofSoftware Engineering.1. The best programmers are up to 28 times better than the worst programm\u00aeers.2. New tools/techniques cause an initial LOSS of productivity/quality.3. The answer to a feasibility study is almost always \"yes\".4. A May 2002 report prepared for the National Institute of Standards andTe\u00aechnologies (NIST)(1) estimate the annual cost of software defects in the UnitedStates as $59.5 billion.5. Reusable components are three times as hard to build6. For every 25% increase in problem complexi\u00aety, there is a 100% increase in solutioncomplexity.7. 80% of software work is intellectual. A fair amount of it is creative. Little of it isclerical.8. Requirements errors are the most expensive to fix during production.9. Missing requirements are the hardest requirement errors to correct\u00ae.10. Error-removal is the most time-consuming phase of the life cycle.11. Software is usually tested at best at the 55-60% (branch) coverage level.12. 100% coverage is still far from enough.13. Rigorous inspections can remove up to 90% of errors before the first test case isrun.15. Maintenance typically consumes 40-80% of software\u00aecosts. It is probably the mostimportant life cycle phase of software.16. Enhancements represent roughly 60% of maintenance costs.17. There is no single best approach to software error removal.",
  "page123": "Testing plays an important role in achieving and assessing the quality of a softwareproduct [25]. On the one hand, we improve the quality of the products as we repeata test-fi\u00aend defects-fix cycle during development. On the other hand, we assess howgood our system is when we perform system-level tests before releasing a product.Thus, as Friedman and Voas [26] have succinctly described, s\u00aeoftware testing is averification process for software quality assessment and improvement. Generallyspeaking, the activities for software quality assessment can be divided into twobroad categories, namely\u00ae, static analysis and dynamic analysis. Static Analysis: As the term \"static\" suggests, it is based on the examination of a number of documents, namely requirements documents, softwaremodels, design documents, and source code. Traditional static analysisincludes code review, ins\u00aepection, walk-through, algorithm analysis, andproof of correctness. It does not involve actual execution of the code underdevelopment. Instead, it examines code and reasons over all possible behaviors that might arise during run time. Compiler optimizations are standardstatic analysis.Dynamic Analysis: Dynamic analysis of a softwa\u00aere system involves actualprogram execution in order to expose possible program failures. The behavioral and performance properties of the program are also observed. Programs are executed with both typical and carefully chosen input values.Often, the input set of a program can be impractically large. However, forpractical considerations, a finite subset of the input set can be selected.Therefore, in testing, we observe some representative program behaviorsand reach a conclusion about the quality of the system. Careful selectionof a finite test set is crucial to reaching a reliable conclusion.",
  "page124": "By performing static and dynamic analyses, practitioners want to identify as manyfaults as possible so that those faults are fixed at an early stage of the softwaredevelopment. Sta\u00aetic analysis and dynamic analysis are complementary in nature,and for better effectiveness, both must be performed repeatedly and alternated.Practitioners and researchers need to remove the boundaries between static and\u00aedynamic analysis and create a hybrid analysis that combines the strengths of bothapproachesTwo similar concepts related to software testing frequently used by practitioners areverification and validation\u00ae. Both concepts are abstract in nature, and each can berealized by a set of concrete, executable activities. The two concepts are explainedas follows: Verification: This kind of activity helps us in evaluating a software systemby determining whether the product of a given development phas\u00aee satisfiesthe requirements established before the start of that phase. One may notethat a product can be an intermediate product, such as requirement specification, design specification, code, user manual, or even the final product.Activities that check the correctness of a development phase are calledverification activities. Val\u00aeidation: Activities of this kind help us in confirming that a productmeets its intended use. Validation activities aim at confirming that a productmeets its customer's expectations. In other words, validation activities focuson the final product, which is extensively tested from the customer point ofview. Validation establishes whether the product meets overall expectationsof the users.Late execution of validation activities is often risky by leading tohigher development cost. Validation activities may be executed at earlystages of the software development cycle [28]. An example of early execution of validation activities can be found in the eXtreme Programming",
  "page125": "(XP) software development methodology. In the XP methodology, the customer closely interacts with the software development group and conductsacceptance tests during each developmen\u00aet iteration [29].The verification process establishes the correspondence of an implementationphase of the software development process with its specification, whereas validationestablishes the correspondence between a s\u00aeystem and users' expectations. One cancompare verification and validation as follows: Verification activities aim at confirming that one is building the product correctly, whereas validation activit\u00aeies aim at confirming that one is buildingthe correct product [30]. Verification activities review interim work products, such as requirementsspecification, design, code, and user manual, during a project life cycle toensure their quality. The quality attributes sought by verification act\u00aeivitiesare consistency, completeness, and correctness at each major stage of system development. On the other hand, validation is performed toward theend of system development to determine if the entire system meets thecustomer's needs and expectations. Verification activities are performed on interim products by applying mos\u00aetlystatic analysis techniques, such as inspection, walkthrough, and reviews,and using standards and checklists. Verification can also include dynamicanalysis, such as actual program execution. On the other hand, validationis performed on the entire system by actually running the system in its realenvironment and using a variety of testsIn the literature on software testing, one can find references to the terms failure,error, fault, and defect. Although their meanings are related, there are importantdistinctions between these four concepts. In the following, we present first threeterms as they are understood in the fault-tolerant computing community",
  "page126": "Failure: A failure is said to occur whenever the external behavior of asystem does not conform to that prescribed in the system specification. Error: An error is a state of t\u00aehe system. In the absence of any correctiveaction by the system, an error state could lead to a failure which wouldnot be attributed to any event subsequent to the error. Fault: A fault is the adjudged cause of an\u00aeerror.A fault may remain undetected for a long time, until some event activates it. Whenan event activates a fault, it first brings the program into an intermediate error state.If computation is allowed\u00aeto proceed from an error state without any correctiveaction, the program eventually causes a failure. As an aside, in fault-tolerant computing, corrective actions can be taken to take a program out of an error state intoa desirable state such that subsequent computation does not eventual\u00aely lead to afailure. The process of failure manifestation can therefore be succinctly representedas a behavior chain [31] as follows: faulterrorfailure. The behavior chaincan iterate for a while, that is, failure of one component can lead to a failure ofanother interacting component.The above definition of failure assu\u00aemes that the given specification is acceptable to the customer. However, if the specification does not meet the expectationsof the customer, then, of course, even a fault-free implementation fails to satisfy thecustomer. It is a difficult task to give a precise definition of fault, error, or failureof software, because of the \"human factor\" involved in the overall acceptance of asystem. In an article titled \"What Is Software Failure\" [32], Ram Chillarege commented that in modern software business software failure means \"the customer'sexpectation has not been met and/or the customer is unable to do useful work withproduct,\" p. 354.Roderick Rees [33] extended Chillarege's comments of software failure bypointing out that \"failure is a matter of function only [and is thus] related to purpose,not to whether an item is physically intact or not\" (p. 163). To substantiate this,Behrooz Parhami [34] provided three interesting examples to show the relevanceof such a view point in wider context. One of the examples is quoted here",
  "page127": "The above definition of failure assumes that the given specification is acceptable to the customer. However, if the specification does not meet the expectationsof the customer, the\u00aen, of course, even a fault-free implementation fails to satisfy thecustomer. It is a difficult task to give a precise definition of fault, error, or failureof software, because of the \"human factor\" involved i\u00aen the overall acceptance of asystem. In an article titled \"What Is Software Failure\" [32], Ram Chillarege commented that in modern software business software failure means \"the customers\u00aeexpectation has not been met and/or the customer is unable to do useful work withproduct.each maturity level decomposes into several key process areasthat indicate the areas an organization should focus on to improve its softwareprocess.Level 1 - Initial Level: Disciplined process, Standard\u00ae, Consistent process, Predictableprocess, Continuously Improving processLevel 2 - Repeatable: Key practice areas - Requirements management, Softwareproject planning, Software project tracking ",
  "page128": " dissatisfactions are errors in the organization's state. The organization's personnel or departmentsprobably begin to malfunction as result of the errors, in turn causin\u00aeg an overall degradation of performance. The end result can be the organization's failure to achieveits goal.There is a fine difference between defects and faults in the above example, thatis, execution of a defect\u00aeive policy may lead to a faulty promotion. In a softwarecontext, a software system may be defective due to design issues; certain systemstates will expose a defect, resulting in the development of faults\u00aedefined as incorrect signal values or decisions within the system. In industry, the term defect iswidely used, whereas among researchers the term fault is more prevalent. For allpractical purpose, the two terms are synonymous. In this book, we use the twoterms interchangeably as required\u00ae. Various test case design techniques enable the testers to develop effective testcases. Besides, implementing the design techniques, every tester needs to keep inmind general guidelines that will aid in test case design:a. The purpose of each test case is to run the test in the simplest way possible.[Suitable techniques - Specifi\u00aecation derived tests, Equivalence partitioning]b. Concentrate initially on positive testing i.e. the test case should show that thesoftware does what it is intended to do. [Suitable techniques - Specification derivedtests, Equivalence partitioning, State-transition testing]c. Existing test cases should be enhanced and further test cases should be designedto show that the software does not do anything that it is not specified to do i.e.Negative Testing [Suitable techniques - Error guessing, Boundary value analysis,Internal boundary value testing, State-transition testing]d. Where appropriate, test cases should be designed to address issues such asperformance, safety requirements and security requirements [Suitable techniques -Specification derived tests]e. Further test cases can then be added to the unit test specification to achieve categories: Black boxtechniques, White box techniques and other techniques that do not fall under eithercategorySpecification Derived TestsAs the name suggests, test cases are designed by walking through the relevantspecifications.",
  "page129": "Roderick Rees [33] extended Chillarege's comments of software failure bypointing out that \"failure is a matter of function only [and is thus] related to purpose,not to wh\u00aeether an item is physically intact or not\" (p. 163). To substantiate this,Behrooz Parhami [34] provided three interesting examples to show the relevanceof such a view point in wider context. One of the examples is\u00aequoted here (p. 451):Consider a small organization. Defects in the organization's staff promotion policies cancause improper promotions, viewed as faults. The resulting ineptitudes A use case descri\u00aebes the system's behavior under various conditions as it responds toa request from one of the users. The user initiates an interaction with the system toaccomplish some goal. Different sequences of behavior, or scenarios, can unfold,depending on the particular requests made and condi\u00aetions surrounding the requests.The use case collects together those different scenarios.Use cases are popular largely because they tell coherent stories about how thesystem will behave in use. The users of the system get to see just what this newsystem will be and get to react early.As discussed earlier, defect is the variance fro\u00aem a desired product attribute (it can bea wrong, missing or extra data). It can be of two types - Defect from the product or avariance from customer/user expectations. It is a flaw in the software system and hasno impact until it affects the user/customer and operational system.With the knowledge of testing so far gained, you can now be able to categorize thedefects you have found. Defects can be categorized into different types basing on thecore issues they address. Some defects address security or database issues whileothers may refer to functionality or UI issues.Security Defects: Application security defects generally involve improper handling ofdata sent from the user to the application. These defects are the most severe andgiven highest priority for a fix.Examples:Authentication: affect the functionality of the application.Examples: All Javascript errors Buttons like Save, Delete, Cancel not performing their intended functionsA missing functionality (or) a feature not functioning the way it is intended toContinuous execution of loops",
  "page130": "No matter how many times we run the test-find faults-fix cycle during softwareNo matter how many times we run the test-find faults-fix cycle during softwaredeve\u00aelopment, some faults are likely to escape our attention, and these will eventually surface at the customer site. Therefore, a quantitative measure that is usefulin assessing the quality of a software is its reliability\u00ae[35]. Software reliability isdefined as the probability of failure-free operation of a software system for a specified time in a specified environment. The level of reliability of a system depends onthos\u00aee inputs that cause failures to be observed by the end users. Software reliabilitycan be estimated via random testing, as suggested by Hamlet [36]. Since the notionof reliability is specific to a \"specified environment,\" test data must be drawn fromthe input distribution to clos\u00aeely resemble the future usage of the system. Capturing the future usage pattern of a system in a general sense is described in a formcalled the operational profileThe stakeholders in a test process are the programmers, the test engineers, theproject managers, and the customers. A stakeholder is a person or an organizationwho influ\u00aeences a system's behaviors or who is impacted by that system [39].Different stakeholders view a test process from different perspectives as explainedbelow",
  "page131": "It does work: While implementing a program unit, the programmer maywant to test whether or not the unit works in normal circumstances. Theprogrammer gets much confidence if the uni\u00aet works to his or her satisfaction. The same idea applies to an entire system as well once a systemhas been integrated, the developers may want to test whether or not thesystem performs the basic functions. Here, for th\u00aee psychological reason,the objective of testing is to show that the system works, rather than itdoes not work.It does not work: Once the programmer (or the development team) issatisfied that a unit (or t\u00aehe system) works to a certain degree, more testsare conducted with the objective of finding faults in the unit (or the system).Here, the idea is to try to make the unit (or the system) fail.Reduce the risk of failure: Most of the complex software systems containfaults, which cause the sys\u00aetem to fail from time to time. This concept of\"failing from time to time\" gives rise to the notion of failure rate. Asfaults are discovered and fixed while performing more and more tests, thefailure rate of a system generally decreases. Thus, a higher level objectiveof performing tests is to bring down the risk of failin\u00aeg to an acceptablelevel.Reduce the cost of testing: The different kinds of costs associated with atest process includethe cost of designing, maintaining, and executing test cases,the cost of analyzing the result of executing each test case,the cost of documenting the test cases, andthe cost of actually executing the system and documenting it.",
  "page132": "Therefore, the less the number of test cases designed, the less will be theassociated cost of testing. However, producing a small number of arbitrarytest cases is not a good way of\u00aesaving cost. The highest level of objectiveof performing tests is to produce low-risk software with fewer numberof test cases. This idea leads us to the concept of effectiveness of testcases. Test engineers must theref\u00aeore judiciously select fewer, effective testcasesIn its most basic form, a test case is a simple pair of < input, expected outcome >.If a program under test is expected to compute the square root of nonn\u00aeegativenumbers, then four examples of test cases are as shown in Figure 1.3.In stateless systems, where the outcome depends solely on the current input,test cases are very simple in structure, as shown in Figure 1.3. A program tocompute the square root of nonnegative numbers is an example\u00aeof a statelesssystem. A compiler for the C programming language is another example of astateless system. A compiler is a stateless system because to compile a program itdoes not need to know about the programs it compiled previously.In state-oriented systems, where the program outcome depends both on thecurrent state of the syste\u00aem and the current input, a test case may consist of asequence of < input, expected outcome > pairs. A telephone switching system andan automated teller machine (ATM) are examples of state-oriented systems. For anATM machine, a test case for testing the withdraw function is shown in Figure 1.4.Here, we assume that the user has already entered validated inputs, such as the cashcard and the personal identification number (PIN).",
  "page133": "In the test case TS1, \"check balance\" and \"withdraw\" in the first, second, andfourth tuples represent the pressing of the appropriate keys on the ATM keypad. It\u00aeisassumed that the user account has $500.00 on it, and the user wants to withdraw anamount of $200.00. The expected outcome \"$200.00\" in the third tuple representsthe cash dispensed by the ATM. After the with\u00aedrawal operation, the user makessure that the remaining balance is $300.00.For state-oriented systems, most of the test cases include some form of decision and timing in providing input to the system. A\u00aetest case may include loopsand timers, which we do not show at this moment.An outcome of program execution is a complex entity that may include thefollowing:Values produced by the program:Outputs for local observation (integer, text, audio, image)Outputs (messages) for remote storage, man\u00aeipulation, or observationState change:State change of the programState change of the database (due to add, delete, and update operations) A sequence or set of values which must be interpreted together for theoutcome to be validAn important concept in test design is the concept of an oracle. An oracleis any entity program, process,\u00aehuman expert, or body of data that tells us theexpected outcome of a particular test or set of tests [40]. A test case is meaningfulonly if it is possible to decide on the acceptability of the result produced by theprogram under test.Ideally, the expected outcome of a test should be computed while designingthe test case. In other words, the test outcome is computed before the program is executed with the selected test input.",
  "page134": "The idea here is that one should be able tocompute the expected outcome from an understanding of the program's requirements. Precomputation of the expected outcome will elimin\u00aeate any implementationbias in case the test case is designed by the developer.In exceptional cases, where it is extremely difficult, impossible, or evenundesirable to compute a single expected outcome, one should identi\u00aefy expectedoutcomes by examining the actual test outcomes, as explained in the following:Execute the program with the selected input.Observe the actual outcome of program execution.Verify that the actual\u00aeoutcome is the expected outcome. Use the verified actual outcome as the expected outcome in subsequentruns of the test case It is not unusual to find people making claims such as \"I have exhaustively testedthe program.\" Complete, or exhaustive, testing means there are no undisc\u00aeoveredfaults at the end of the test phase. All problems must be known at the end ofcomplete testing. For most of the systems, complete testing is near impossiblebecause of the following reasons: The domain of possible inputs of a program is too large to be completelyused in testing a system. There are both valid inputs and invalid\u00aeinputs.The program may have a large number of states. There may be timingconstraints on the inputs, that is, an input may be valid at a certain timeand invalid at other times. An input value which is valid but is not properlytimed is called an inopportune input. The input domain of a system canbe very large to be completely used in testing a program.",
  "page135": "The design issues may be too complex to completely test. The design mayhave included implicit design decisions and assumptions. For example,a programmer may use a global variable o\u00aer a static variable to controlprogram execution. It may not be possible to create all possible execution environments of thesystem. This becomes more significant when the behavior of the softwaresystem depends on the re\u00aeal, outside world, such as weather, temperature,altitude, pressure, and so on.We must realize that though the outcome of complete testing, that is, discovering allfaults, is highly desirable, it is a nea\u00aer-impossible task, and it may not be attempted.The next best thing is to select a subset of the input domain to test a program.Referring to Figure 1.5, let D be the input domain of a program P. Suppose thatwe select a subset D1 of D, that is, D1 \u2282 D, to test program P. It is possible\u00aethatD1 exercises only a part P1, that is, P1 \u2282 P, of the execution behavior of P, inwhich case faults with the other part, P2, will go undetected.By selecting a subset of the input domain D1, the test engineer attemptsto deduce properties of an entire program P by observing the behavior of a partP1 of the entire behavior of\u00aeP on selected inputs D1. Therefore, selection of thesubset of the input domain must be done in a systematic and careful manner sothat the deduction is as accurate and complete as possible. For example, the ideaof coverage is considered while selecting test cases In order to test a program, a test engineer must perform a sequence of testingactivities. Most of these activities have been shown in Figure 1.6 and are explainedin the following. ",
  "page136": "Identify an objective to be tested: The first activity is to identify anobjective to be tested. The objective defines the intention, or purpose, ofdesigning one or more test cases\u00aeto ensure that the program supports theobjective. A clear purpose must be associated with every test caseSelect inputs: The second activity is to select test inputs. Selection of testinputs can be based on the requireme\u00aents specification, the source code,or our expectations. Test inputs are selected by keeping the test objectivein mind.Compute the expected outcome: The third activity is to compute theexpected outcome of\u00aethe program with the selected inputs. In most cases,this can be done from an overall, high-level understanding of the testobjective and the specification of the program under test.Set up the execution environment of the program: The fourth step is toprepare the right execution environmen\u00aet of the program. In this step all theassumptions external to the program must be satisfied. A few examples ofassumptions external to a program are as follows:Initialize the local system, external to the program. This may includemaking a network connection available, making the right databasesystem available, and so on.Initialize\u00aeany remote, external system (e.g., remote partner process in adistributed application.) For example, to test the client code, we mayneed to start the server at a remote siteExecute the program: In the fifth step, the test engineer executes theprogram with the selected inputs and observes the actual outcome of theprogram. To execute a test case, inputs may be provided to the program atdifferent physical locations at different times. The concept of test coordination is used in synchronizing different components of a test case",
  "page137": "Analyze the test result: The final test activity is to analyze the result oftest execution. Here, the main task is to compare the actual outcome ofprogram execution with the expect\u00aeed outcome. The complexity of comparison depends on the complexity of the data to be observed. The observeddata type can be as simple as an integer or a string of characters or ascomplex as an image, a video, or an audi\u00aeo clip. At the end of the analysis step, a test verdict is assigned to the program. There are three majorkinds of test verdicts, namely, pass, fail, and inconclusive, as explainedbelow.If the program pro\u00aeduces the expected outcome and the purpose of thetest case is satisfied, then a pass verdict is assigned.If the program does not produce the expected outcome, then a fail verdictis assigned.However, in some cases it may not be possible to assign a clear passor fail verdict. For example, i\u00aef a timeout occurs while executing atest case on a distributed application, we may not be in a position toassign a clear pass or fail verdict. In those cases, an inconclusive testverdict is assigned. An inconclusive test verdict means that furthertests are needed to be done to refine the inconclusive verdict into aclear pass or fa\u00aeil verdict A test report must be written after analyzing the test result. Themotivation for writing a test report is to get the fault fixed if the test revealeda fault. A test report contains the following items to be informative:Explain how to reproduce the failure.Analyze the failure to be able to describe it.A pointer to the actual outcome and the test case, complete with theinput, the expected outcome, and the execution environment.",
  "page138": "Testing is performed at different levels involving the complete system or parts ofit throughout the life cycle of a software product. A software system goes throughfour stages of t\u00aeesting before it is actually deployed. These four stages are knownas unit, integration, system, and acceptance level testing. The first three levels oftesting are performed by a number of different stakeholders in the d\u00aeevelopmentorganization, where as acceptance testing is performed by the customers. The fourstages of testing have been illustrated in the form of what is called the classical Vmodel In unit testing, prog\u00aerammers test individual program units, such as a procedures, functions, methods, or classes, in isolation. After ensuring that individualunits work to a satisfactory extent, modules are assembled to construct larger subsystems by following integration testing techniques. Integration testi\u00aeng is jointlyperformed by software developers and integration test engineers. The objective ofintegration testing is to construct a reasonably stable system that can withstandthe rigor of system-level testing. System-level testing includes a wide spectrumof testing, such as functionality testing, security testing, robustness testi\u00aeng, loadtesting, stability testing, stress testing, performance testing, and reliability testing.System testing is a critical phase in a software development process because of theneed to meet a tight schedule close to delivery date, to discover most of the faults,and to verify that fixes are working and have not resulted in new faults. Systemtesting comprises a number of distinct activities: creating a test plan, designinga test suite, preparing test environments, executing the tests by following a clearstrategy, and monitoring the process of test execution.",
  "page139": "Regression testing is another level of testing that is performed throughout thelife cycle of a system. Regression testing is performed whenever a component ofthe system is modified\u00ae. The key idea in regression testing is to ascertain that themodification has not introduced any new faults in the portion that was not subjectto modification. To be precise, regression testing is not a distinct level o\u00aef testing.Rather, it is considered as a subphase of unit, integration, and system-level testing,as illustrated in Figure 1.8 [41].In regression testing, new tests are not designed. Instead, tests are sel\u00aeected,prioritized, and executed from the existing pool of test cases to ensure that nothingis broken in the new version of the software. Regression testing is an expensiveprocess and accounts for a predominant portion of testing effort in the industry. Itis desirable to select a subset of\u00aethe test cases from the existing pool to reduce thecost. A key question is how many and which test cases should be selected so thatthe selected test cases are more likely to uncover new faults [42-44].After the completion of system-level testing, the product is delivered to thecustomer. The customer performs their own series\u00aeof tests, commonly known asacceptance testing. The objective of acceptance testing is to measure the qualityof the product, rather than searching for the defects, which is objective of systemtesting. A key notion in acceptance testing is the customer's expectations from thesystem. By the time of acceptance testing, the customer should have developedtheir acceptance criteria based on their own expectations from the system. Thereare two kinds of acceptance testing as explained in the following: User acceptance testing (UAT)Business acceptance testing (BAT)",
  "page140": "User acceptance testing is conducted by the customer to ensure that the systemsatisfies the contractual acceptance criteria before being signed off as meeting userneeds. On the oth\u00aeer hand, BAT is undertaken within the supplier's developmentorganization. The idea in having a BAT is to ensure that the system will eventuallypass the user acceptance test. It is a rehearsal of UAT at the supplier\u00ae's premises.Designing test cases has continued to stay in the foci of the research communityand the practitioners. A software development process generates a large body ofinformation, such as requir\u00aeements specification, design document, and source code.In order to generate effective tests at a lower cost, test designers analyze thefollowing sources of information: Requirements and functional specificationsSource codeinput and output domainsOperational profileFault modelRequirements\u00aeand Functional Specifications The process of software development begins by capturing user needs. The nature and amount of user needsidentified at the beginning of system development will vary depending on thespecific life-cycle model to be followed. Let us consider a few examples. In theWaterfall model [45] of software developmen\u00aet, a requirements engineer tries tocapture most of the requirements. On the other hand, in an agile software development model, such as XP [29] or the Scrum [46-48], only a few requirementsare identified in the beginning. A test engineer considers all the requirementsthe program is expected to meet whichever life-cycle model is chosen to test aprogram",
  "page141": "The requirements might have been specified in an informal manner, such asa combination of plaintext, equations, figures, and flowcharts. Though this form ofrequirements specificati\u00aeon may be ambiguous, it is easily understood by customers.For example, the Bluetooth specification consists of about 1100 pages of descriptions explaining how various subsystems of a Bluetooth interface is expected towo\u00aerk. The specification is written in plaintext form supplemented with mathematical equations, state diagrams, tables, and figures. For some systems, requirementsmay have been captured in the form of use c\u00aeases, entity-relationship diagrams,and class diagrams. Sometimes the requirements of a system may have been specified in a formal language or notation, such as Z, SDL, Estelle, or finite-statemachine. Both the informal and formal specifications are prime sources of testcasesSource Co\u00aede Whereas a requirements specification describes the intendedbehavior of a system, the source code describes the actual behavior of the system.High-level assumptions and constraints take concrete form in an implementation.Though a software designer may produce a detailed design, programmers mayintroduce additional details into th\u00aee system. For example, a step in the detaileddesign can be \"sort array A.\" To sort an array, there are many sorting algorithmswith different characteristics, such as iteration, recursion, and temporarily usinganother array. Therefore, test cases must be designed based on the program [50].Input and Output Domains Some values in the input domain of a programhave special meanings, and hence must be treated separately [5]. To illustrate thispoint, let us consider the factorial function.",
  "page142": "without considering the special case of n 0. The above wrong implementationwill produce the correct result for all positive values of n, but will fail for n = 0.Sometimes even som\u00aee output values have special meanings, and a programmust be tested to ensure that it produces the special values for all possible causes.In the above example, the output value 1 has special significance: (i) it is themi\u00aenimum value computed by the factorial function and (ii) it is the only valueproduced for two different inputs.In the integer domain, the values 0 and 1 exhibit special characteristicsif arithmetic operat\u00aeions are performed. These characteristics are 0 X x = 0 and1 X x = x for all values of x. Therefore, all the special values in the input andoutput domains of a program must be considered while testing the program.Operational Profile As the term suggests, an operational profile i\u00aes a quantitative characterization of how a system will be used. It was created to guide testengineers in selecting test cases (inputs) using samples of system usage. The notionof operational profiles, or usage profiles, was developed by Mills et al.The idea isto infer, from the observed test results, the future reliability of the\u00aesoftware whenit is in actual use. To do this, test inputs are assigned a probability distribution, orprofile, according to their occurrences in actual operation. The ways test engineersassign probability and select test cases to operate a system may significantly differfrom the ways actual users operate a system. However, for accurate estimationof the reliability of a system it is important to test a system by considering theways it will actually be used in the field.",
  "page143": "Fault Model Previously encountered faults are an excellent source of information in designing new test cases. The known faults are classified into differentclasses, such as initial\u00aeization faults, logic faults, and interface faults, and stored ina repository [55, 56]. Test engineers can use these data in designing tests to ensurethat a particular class of faults is not resident in the program.Ther\u00aee are three types of fault-based testing: error guessing, fault seeding,and mutation analysis. In error guessing, a test engineer applies his experienceto (i) assess the situation and guess where and wha\u00aet kinds of faults might exist,and (ii) design tests to specifically expose those kinds of faults. In fault seeding,known faults are injected into a program, and the test suite is executed to assessthe effectiveness of the test suite. Fault seeding makes an assumption that a testsuite that\u00aefinds seeded faults is also likely to find other faults. Mutation analysis issimilar to fault seeding, except that mutations to program statements are made inorder to determine the fault detection capability of the test suite. If the test cases arenot capable of revealing such faults, the test engineer may specify additional test\u00aecases to reveal the faults. Mutation testing is based on the idea of fault simulation,whereas fault seeding is based on the idea of fault injection. In the fault injectionapproach, a fault is inserted into a program, and an oracle is available to assert thatthe inserted fault indeed made the program incorrect. On the other hand, in faultsimulation, a program modification is not guaranteed to lead to a faulty program.In fault simulation, one may modify an incorrect program and turn it into a correctprogram.",
  "page144": "A key idea in Section was that test cases need to be designed by considering information from several sources, such as the specification, source code, andspecial properties of the\u00aeprogram's input and output domains. This is because allthose sources provide complementary information to test designers. Two broad concepts in testing, based on the sources of information for test design, are whi\u00aete-boxand black-box testing. White-box testing techniques are also called structural testing techniques, whereas black-box testing techniques are called functional testingtechniques.In structural testing\u00ae, one primarily examines source code with a focus on control flow and data flow. Control flow refers to flow of control from one instructionto another. Control passes from one instruction to another instruction in a numberof ways, such as one instruction appearing after another, function\u00aecall, messagepassing, and interrupts. Conditional statements alter the normal, sequential flowof control in a program. Data flow refers to the propagation of values from onevariable or constant to another variable. Definitions and uses of variables determinethe data flow aspect in a program.In functional testing, one does not have\u00aeaccess to the internal details of aprogram and the program is treated as a black box. A test engineer is concernedonly with the part that is accessible outside the program, that is, just the inputand the externally visible outcome. A test engineer applies input to a program,observes the externally visible outcome of the program, and determines whetheror not the program outcome is the expected outcome. Inputs are selected fromthe program's requirements specification and properties of the program's input andoutput domains. A test engineer is concerned only with the functionality and thefeatures found in the program's specification",
  "page145": "At this point it is useful to identify a distinction between the scopes ofstructural testing and functional testing. One applies structural testing techniquesto individual units of\u00aea program, whereas functional testing techniques can beapplied to both an entire system and the individual program units. Since individualprogrammers know the details of the source code they write, they themselvesperfo\u00aerm structural testing on the individual program units they write. On the otherhand, functional testing is performed at the external interface level of a system,and it is conducted by a separate software\u00aequality assurance group.Let us consider a program unit U which is a part of a larger program P.A program unit is just a piece of source code with a well-defined objective andwell-defined input and output domains. Now, if a programmer derives test casesfor testing U from a knowledge of the\u00aeinternal details of U , then the programmeris said to be performing structural testing. On the other hand, if the programmerdesigns test cases from the stated objective of the unit U and from his or herknowledge of the special properties of the input and output domains of U , then heor she is said to be performing functional test\u00aeing on the same unit U .a of functional testing.Neither structural testing nor functional testing is by itself good enough todetect most of the faults. Even if one selects all possible inputs, a structural testingtechnique cannot detect all faults if there are missing paths in a program. Intuitively,a path is said to be missing if there is no code to handle a possible condition.",
  "page146": "Similarly, without knowledge of the structural details of a program, many faultswill go undetected. Therefore, a combination of both structural and functionaltesting techniques mus\u00aet be used in program testing.The purpose of system test planning, or simply test planning, is to get ready andorganized for test execution. A test plan provides a framework, scope, details ofresource needed, effort requ\u00aeired, schedule of activities, and a budget. A frameworkis a set of ideas, facts, or circumstances within which the tests will be conducted.The stated scope outlines the domain, or extent, of the test act\u00aeivities. The scopecovers the managerial aspects of testing, rather than the detailed techniques andspecific test cases.Test design is a critical phase of software testing. During the test designphase, the system requirements are critically studied, system features to betested are thorough\u00aely identified, and the objectives of test cases and the detailedbehavior of test cases are defined. Test objectives are identified from differentsources, namely, the requirement specification and the functional specification,and one or more test cases are designed for each test objective. Each test case isdesigned as a combination\u00aeof modular test components called test steps. Thesetest steps can be combined together to create more complex, multistep tests. Atest case is clearly specified so that others can easily borrow, understand, andreuse it ",
  "page147": "Monitoring and measurement are two key principles followed in every scientific andengineering endeavor. The same principles are also applicable to the testing phasesof software dev\u00aeelopment. It is important to monitor certain metrics which trulyrepresent the progress of testing and reveal the quality level of the system. Basedon those metrics, the management can trigger corrective and preventive a\u00aections. Byputting a small but critical set of metrics in place the executive management willbe able to know whether they are on the right track [58]. Test execution metricscan be broadly categorized into\u00aetwo classes as follows: Metrics for monitoring test execution Metrics for monitoring defectsThe first class of metrics concerns the process of executing test cases, whereasthe second class concerns the defects found as a result of test execution. Thesemetrics need to be tracked and analy\u00aezed on a periodic basis, say, daily or weekly.In order to effectively control a test project, it is important to gather valid andaccurate information about the project. One such example is to precisely knowwhen to trigger revert criteria for a test cycle and initiate root cause analysis ofthe problems before more tests can be perf\u00aeormed. By triggering such a revertcriteria, a test manager can effectively utilize the time of test engineers, and possibly money, by suspending a test cycle on a product with too many defects tocarry out a meaningful system test. A management team must identify and monitor metrics while testing is in progress so that important decisions can be made",
  "page148": " It is important to analyze and understand the test metrics, rather than justcollect data and make decisions based on those raw data. Metrics are meaningful only if they enable the\u00aemanagement to make decisions which result in lowercost of production, reduced delay in delivery, and improved quality of softwaresystems.Quantitative evaluation is important in every scientific and engineering field.Qu\u00aeantitative evaluation is carried out through measurement. Measurement lets oneevaluate parameters of interest in a quantitative manner as follows: Evaluate the effectiveness of a technique used in perfor\u00aeming a task. Onecan evaluate the effectiveness of a test generation technique by countingthe number of defects detected by test cases generated by following thetechnique and those detected by test cases generated by other means.Evaluate the productivity of the development activities. One\u00aecan keep trackof productivity by counting the number of test cases designed per day, thenumber of test cases executed per day, and so on. Evaluate the quality of the product. By monitoring the number of defectsdetected per week of testing, one can observe the quality level of thesystem. Evaluate the product testing. For evaluating\u00aea product testing process, thefollowing two measurements are critical:incises in test design. The need for more testing occursas test engineers get new ideas while executing the planned testcases.Test effort effectiveness metric: It is important to evaluate the effectiveness of the testing effort in the development of a product. After aproduct is deployed at the customer's site, one is interested to knowthe effectiveness of testing that was performed",
  "page149": "A common measureof test effectiveness is the number of defects found by the customersthat were not found by the test engineers prior to the release of theproduct. These defects had\u00aeescaped our test effort.In general, software testing is a highly labor intensive task. This is because test casesare to a great extent manually generated and often manually executed. Moreover,the results of test execut\u00aeions are manually analyzed. The durations of those taskscan be shortened by using appropriate tools. A test engineer can use a variety oftools, such as a static code analyzer, a test data generator, and\u00aea network analyzer,if a network-based application or protocol is under test. Those tools are useful inincreasing the efficiency and effectiveness of testing.Test automation is essential for any testing and quality assurance division ofan organization to move forward to become more efficie\u00aent. The benefits of testautomation are as follows: Increased productivity of the testers Better coverage of regression testing Reduced durations of the testing phases Reduced cost of software maintenance Increased effectiveness of test casesTest automation provides an opportunity to improve the skills of the testengineers by writi\u00aeng programs, and hence their morale. They will be more focusedon developing automated test cases to avoid being a bottleneck in product deliveryto the market. Consequently, software testing becomes less of a tedious job.Test automation improves the coverage of regression testing because of accumulation of automated test cases over time. Automation allows an organization tocreate a rich library of reusable test cases and facilitates the execution of a consistent set of test cases. Here consistency means our ability to produce repeatedresults for the same set of tests",
  "page150": " It may be very difficult to reproduce test results inmanual testing, because exact conditions at the time and point of failure may notbe precisely known. In automated testing it i\u00aes easier to set up the initial conditionsof a system, thereby making it easier to reproduce test results. Test automationsimplifies the debugging work by providing a detailed, unambiguous log of activities and intermedi\u00aeate test steps. This leads to a more organized, structured, andreproducible testing approach.Automated execution of test cases reduces the elapsed time for testing, and,thus, it leads to a shorter time t\u00aeo market. The same automated test cases can beexecuted in an unsupervised manner at night, thereby efficiently utilizing the different platforms, such as hardware and configuration. In short, automation increasestest execution efficiency. However, at the end of test execution, it is impor\u00aetant toanalyze the test results to determine the number of test cases that passed or failed.And, if a test case failed, one analyzes the reasons for its failure.In the long run, test automation is cost-effective. It drastically reduces the software maintenance cost. In the sustaining phase of a software system, the regressiontests\u00aerequired after each change to the system are too many. As a result, regressiontesting becomes too time and labor intensive without automation.A repetitive type of testing is very cumbersome and expensive to performmanually, but it can be automated easily using software tools. A simple repetitivetype of application can reveal memory leaks in a software. However, the applicationhas to be run for a significantly long duration, say, for weeks, to reveal memoryleaks. Therefore, manual testing may not be justified, whereas with automation itis easy to reveal memory leaks.",
  "page151": "For example, stress testing is a prime candidate forautomation. Stress testing requires a worst-case load for an extended period of time,which is very difficult to realize by manua\u00ael means. Scalability testing is anotherarea that can be automated. Instead of creating a large test bed with hundreds ofequipment, one can develop a simulator to verify the scalability of the system.Test automation is v\u00aeery attractive, but it comes with a price tag. Sufficienttime and resources need to be allocated for the development of an automated testsuite. Development of automated test cases need to be managed like\u00aea programmingproject. That is, it should be done in an organized manner; otherwise it is highlylikely to fail. An automated test suite may take longer to develop because the testsuite needs to be debugged before it can be used for testing. Sufficient time andresources need to be allocate\u00aed for maintaining an automated test suite and setting upa test environment. Moreover, every time the system is modified, the modificationmust be reflected in the automated test suite. Therefore, an automated test suiteshould be designed as a modular system, coordinated into reusable libraries, andcross-referenced and traceable bac\u00aek to the feature being tested.It is important to remember that test automation cannot replace manual testing. Human creativity, variability, and observability cannot be mimicked throughautomation. Automation cannot detect some problems that can be easily observedby a human being. Automated testing does not introduce minor variations the waya human can. Certain categories of tests, such as usability, interoperability, robustness, and compatibility, are often not suited for automation. It is too difficult toed.",
  "page152": "The objective of test automation is not to reduce the head counts in thetesting department of an organization, but to improve the productivity, quality, andefficiency of test execu\u00aetion. In fact, test automation requires a larger head count inthe testing department in the first year, because the department needs to automatethe test cases and simultaneously continue the execution of manual tests. E\u00aeven afterthe completion of the development of a test automation framework and test caselibraries, the head count in the testing department does not drop below its originallevel. The test organization nee\u00aeds to retain the original team members in order toimprove the quality by adding more test cases to the automated test case repository.Before a test automation project can proceed, the organization must assessand address a number of considerations. The following list of prerequisites mustb\u00aee considered for an assessment of whether the organization is ready for testautomation:The test cases to be automated are well defined. Test tools and an infrastructure are in placeThe test automation professionals have prior successful experience inautomation. Adequate budget should have been allocated for the procurement of soft\u00aeware tools.Testing is a distributed activity conducted at different levels throughout the lifecycle of a software. These different levels are unit testing, integration testing, system testing, and acceptance testing. It is logical to have different testing groups inan organization for each level of testing. However, it is more logical and is thecase in reality that unit-level tests be developed and executed by the programmersthemselves rather than an independent group of unit test engineers. The programmer who develops a software unit should take the ownership and responsibilityof producing good-quality software to his or her satisfaction.",
  "page153": "System integrationtesting is performed by the system integration test engineers. The integration testengineers involved need to know the software modules very well. This means that\u00aeall development engineers who collectively built all the units being integratedneed to be involved in integration testing. Also, the integration test engineersshould thoroughly know the build mechanism, which is key to\u00aeintegrating largesystems.A team for performing system-level testing is truly separated from the development team, and it usually has a separate head count and a separate budget. Themandate of this group\u00aeis to ensure that the system requirements have been met andthe system is acceptable. Members of the system test group conduct different categories of tests, such as functionality, robustness, stress, load, scalability, reliability,and performance. They also execute business acceptance tes\u00aets identified in the useracceptance test plan to ensure that the system will eventually pass user acceptancetesting at the customer site. However, the real user acceptance testing is executedby the client's special user group. The user group consists of people from different backgrounds, such as software quality assurance eng\u00aeineers, business associates,and customer support engineers. It is a common practice to create a temporaryuser acceptance test group consisting of people with different backgrounds, suchas integration test engineers, system test engineers, customer support engineers,and marketing engineers. Once the user acceptance is completed, the group is dismantled. It is recommended to have at least two test groups in an organization:integration test group and system test group.Hiring and retaining test engineers are challenging tasks. Interview is theprimary mechanism for evaluating applicants. Interviewing is a skill that improveswith practice. It is necessary to have a recruiting process in place in order to beeffective in hiring excellent test engineers. In order to retain test engineers, themanagement must recognize the importance of testing efforts at par with development efforts. The management should treat the test engineers as professionals andas a part of the overall team that delivers quality products",
  "page154": "With the above high-level introduction to quality and software testing, we are nowin a position to outline the remaining chapters. Each chapter in the book coverstechnical, process\u00ae, and/or managerial topics related to software testing. The topicshave been designed and organized to facilitate the reader to become a software testspecialist. In Chapter 2 we provide a self-contained introduction to t\u00aehe theory andlimitations of software testing.Chapters 3-6 treat unit testing techniques one by one, as quantitativelyas possible. These chapters describe both static and dynamic unit testing. Static\u00aeunit testing has been presented within a general framework called code review,rather than individual techniques called inspection and walkthrough. Dynamic unittesting, or execution-based unit testing, focuses on control flow, data flow, anddomain testing. The JUnit framework, which is use\u00aed to create and execute dynamicunit tests, is introduced. We discuss some tools for effectively performing unittesting.Chapter 7 discusses the concept of integration testing. Specifically, five kindsof integration techniques, namely, top down, bottom up, sandwich, big bang, andincremental, are explained. Next, we discuss the integ\u00aeration of hardware and software components to form a complete system. We introduce a framework to developa plan for system integration testing. The chapter is completed with a brief discussion of integration testing of off-the-shelf components.Chapters 8-13 discuss various aspects of system-level testing. These sixchapters introduce the reader to the technical details of system testing that is thepractice in industry. These chapters promote both qualitative and quantitative evaluation of a system testing process. The chapters emphasize the need for having anindependent system testing group. A process for monitoring and controlling system testing is clearly explained. Chapter 14 is devoted to acceptance testing, whichincludes acceptance testing criteria, planning for acceptance testing, and acceptancetest execution.",
  "page155": "execution.Chapter 15 contains the fundamental concepts of software reliability and theirapplication to software testing. We discuss the notion of operation profile and itsapplicati\u00aeon in system testing. We conclude the chapter with the description of anexample and the time of releasing a system by determining the additional lengthof system testing. The additional testing time is calculated by usin\u00aeg the idea ofsoftware reliability.In Chapter 16, we present the structure of test groups and how these groupscan be organized in a software company. Next, we discuss how to hire and retaintest engineers\u00aeby providing training, instituting a reward system, and establishingan attractive career path for them within the testing organization. We conclude thischapter with the description of how to build and manage a test team with a focuson teamwork rather than individual gain.Chapters 17 and 1\u00ae8 explain the concepts of software quality and differentmaturity models. Chapter 17 focuses on quality factors and criteria and describesthe ISO 9126 and ISO 9000:2000 standards. Chapter 18 covers the CMM, whichwas developed by the SEI at Carnegie Mellon University. Two test-related models,namely the TPI model and the TMM, are exp\u00aelained at the end of Chapter 18.We define the key words used in the book in a glossary at the end of the book.The reader will find about 10 practice exercises at the end of each chapter. A listof references is included at the end of each chapter for a reader who would like tofind more detailed discussions of some of the topics. Finally, each chapter, exceptthis one, contains a literature review section that, essentially, provides pointers tomore advanced material related to the topics. The more advanced materials arebased on current research and alternate viewpoints.",
  "page156": "Any approach to testing is based on assumptions about the way program faultsoccur. Faults are due to two main reasons:Faults occur due to our inadequate understanding of all condit\u00aeions withwhich a program must deal.Faults occur due to our failure to realize that certain combinations of conditions require special treatments.Goodenough and Gerhart classify program faults as follows:Logic Fault: Thi\u00aes class of faults means a program produces incorrectresults independent of resources required. That is, the program fails becauseof the faults present in the program and not because of a lack of resource\u00aes.Logic faults can be further split into three categories:Requirements fault: This means our failure to capture the real requirements of the customer.Design fault: This represents our failure to satisfy an understoodrequirement.Construction fault: This represents our failure to satisfy a\u00aedesign. Suppose that a design step says \"Sort array A.\" To sort the array with Nelements, one may choose one of several sorting algorithms. Let {:}be the desired for loop construct to sort the array. If a programmerwrites the for loop in the formfor (i = 0; i <= N; i ){:}then there is a construction error in the impleme\u00aentation.Performance Fault: This class of faults leads to a failure of the programto produce expected results within specified or desired resource limitations.A thorough test must be able to detect faults arising from any of the abovereasons. Test data selection criteria must reflect information derived from each stageof software development. Since each type of fault is manifested as an impropereffect produced by an implementation, it is useful to categorize the sources of faultsin implementation terms as folows: ",
  "page157": "Missing Control Flow Paths: Intuitively, a control flow path, or simply apath, is a feasible sequence of instructions in a program. A path may bemissing from a program if we fail t\u00aeo identify a condition and specify apath to handle that condition. An example of a missing path is our failureto test for a zero divisor before executing a division. If we fail to recognizethat a divisor can take a zero\u00aevalue, then we will not include a piece ofcode to handle the special case. Thus, a certain desirable computation willbe missing from the program.Inappropriate Path Selection: A program executes an inapp\u00aeropriate path ifa condition is expressed incorrectly. we show a desiredbehavior and an implemented behavior. Both the behaviors are identicalexcept in the condition part of the if statement. The if part of the implemented behavior contains an additional condition B. It is easy to see tha\u00aet both the desired part and the implemented part behave in the same wayfor all combinations of values of A and B except when A = 1 and B = 0.Inappropriate or Missing Action: There are three instances of this class offault One may calculate a value using a method that does not necessarilygive the correct result. For example, a desi\u00aered expression is x = x X w,whereas it is wrongly written as x = x  w. These two expressionsproduce identical results for several combinations of x and w, such asx = 1.5 and w = 3, for example.Failing to assign a value to a variable is an example of a missing action. Calling a function with the wrong argument list is a kind of inappropriateaction.",
  "page158": "The main danger due to an inappropriate or missing action is that the action isincorrect only under certain combinations of conditions. Therefore, one must dothe following to find\u00aetest data that reliably reveal errors: Identify all the conditions relevant to the correct operation of a program.Select test data to exercise all possible combinations of these conditions.The above idea of selecting te\u00aest data leads us to define the following terms:Test Data: Test data are actual values from the input domain of a programthat collectively satisfy some test selection criteria.Test Predicate: A test predi\u00aecate is a description of conditions and combinations of conditions relevant to correct operation of the program:Test predicates describe the aspects of a program that are to be tested.Test data cause these aspects to be tested.Test predicates are the motivating force for test data selecti\u00aeon.Components of test predicates arise first and primarily from the specifications for a program.Further conditions and predicates may be added as implementations areconsidered.A set of test predicates must at least satisfy the following conditions to have anychance of being reliable. These conditions are key to meaningful testing\u00ae:Every individual branching condition in a program must be represented byan equivalent condition in C.Every potential termination condition in the program, for example, an overflow, must be represented by a condition in C.Every condition relevant to the correct operation of the program that isimplied by the specification and knowledge of the data structure of theprogram must be represented as a condition in C.",
  "page159": " The concepts of reliability and validity have been defined with respect tothe entire input domain of a program. A criterion is guaranteed to be bothreliable and valid if and only\u00aeif it selects the entire domain as a single test.Since such exhaustive testing is impractical, one will have much difficultyin assessing the reliability and validity of a criterion.The concepts of reliability and validi\u00aety have been defined with respect to aprogram. A test selection criterion that is reliable and valid for one programmay not be so for another program. The goodness of a test set should beindependent of i\u00aendividual programs and the faults therein.Neither validity nor reliability is preserved throughout the debugging process. In practice, as program failures are observed, the program is debuggedto locate the faults, and the faults are generally fixed as soon as they arefound. During this de\u00aebugging phase, as the program changes, so does theidealness of a test set. This is because a fault that was revealed beforedebugging is no more revealed after debugging and fault fixing. Thus,properties of test selection criteria are not even \"monotonic\" in the senseof being either always gained or preserved or always lo\u00aest or preserved.A key problem in the theory of Goodenough and Gerhart is that the reliability andvalidity of a criterion depend upon the presence of faults in a program and theirtypes. Weyuker and Ostrand [18] provide a modified theory in which the validityand reliability of test selection criteria are dependent only on the program specification, rather than a program. They propose the concept of a uniformly ideal test",
  "page160": "An ideal goal in software development is to find out whether or not a program iscorrect, where a correct program is void of faults. Much research results have beenreported in the f\u00aeield of program correctness. However, due to the highly constrainednature of program verification techniques, no developer makes any effort to provethe correctness of even small programs of, say, a few thousand lines, l\u00aeet alonelarge programs with millions of lines of code. Instead, testing is accepted in theindustry as a practical way of finding faults in programs. The flip side of testingis that it cannot be used to s\u00aeettle the question of program correctness, which is theideal goal. Even though testing cannot settle the program correctness issue, thereis a need for a testing theory to enable us to compare the power of different testmethods.To motivate a theoretical discussion of testing, we begin with\u00aean ideal processfor software development, which consists of the following steps:A customer and a development team specify the needs. The development team takes the specification and attempts to write a program to meet the specification. A test engineer takes both the specification and the program and selectsa set of test cases. T\u00aehe test cases are based on the specification and theprogram.The program is executed with the selected test data, and the test outcomeis compared with the expected outcome. The program is said to have faults if some tests fail.One can say the program to be ready for use if it passes all the test cases.We focus on the selection of test cases and the interpretation of their results.We assume that the specification is correct, and the specification is the sole arbiterof the correctness of the program. ",
  "page161": "Program Dependent: In this case, T :M (P), that is, test cases arederived solely based on the source code of a system. This is calledwhite-box testing. Here, a test method has comp\u00aelete knowledge of theinternal details of a program. However, from the viewpoint of practicaltesting, a white-box method is not generally applied to an entire program.One applies such a method to small units of a given l\u00aearge system. A unitrefers to a function, procedure, method, and so on. A white-box methodallows a test engineer to use the details of a program unit. Effective use ofa program unit requires a thorough un\u00aederstanding of the unit. Therefore,white-box test methods are used by programmers to test their own code. Specification Dependent: In this case, T = M (S ), that is, test casesare derived solely based on the specification of a system. This is calledblack-box testing. Here, a test method d\u00aeoes not have access to the internaldetails of a program. Such a method uses information provided in thespecification of a system. It is not unusual to use an entire specificationin the generation of test cases because specifications are much smaller insize than their corresponding implementations. Black-box methods aregenerally us\u00aeed by the development team and an independent system testgroup. Expectation Dependent: In practice, customers may generate test casesbased on their expectations from the product at the time of taking deliveryof the system. These test cases may include continuous-operation tests,usability tests, and so on.",
  "page162": "Ideally, all programs should be correct, that is, there is no fault in a program. Dueto the impractical nature of proving even small programs to be correct, customersand software d\u00aeevelopers rely on the efficacy of testing. In this section, we introducetwo main limitations of testing: Testing means executing a program with a generally small, proper subsetof the input domain of the program. A small\u00ae, proper subset of the inputdomain is chosen because cost may not allow a much larger subset tobe chosen, let alone the full input set. Testing with the full input set isknown as exhaustive testing. Thus\u00ae, the inherent need to test a programwith a small subset of the input domain poses a fundamental limit on theefficacy of testing. The limit is in the form of our inability to extrapolate thecorrectness of results for a proper subset of the input domain to programcorrectness. In other word\u00aes, even if a program passes a test set T, wecannot conclude that the program is correct. Once we have selected a subset of the input domain, we are faced with theproblem of verifying the correctness of the program outputs for individualtest input. That is, a program output is examined to determine if the programperformed correctly\u00aeon the test input. The mechanism which verifies thecorrectness of a program output is known as an oracle. The concept ofan oracle is discussed in detail in Chapter 9. Determining the correctnessof a program output is not a trivial task. If either of the following twoconditions hold, a program is considered non testable [20]:There does not exist an oracle.",
  "page163": "Ideally, all programs should be correct, that is, there is no fault in a program. Dueto the impractical nature of proving even small programs to be correct, customersand software d\u00aeevelopers rely on the efficacy of testing. In this section, we introducetwo main limitations of testing: Testing means executing a program with a generally small, proper subsetof the input domain of the program. A small\u00ae, proper subset of the inputdomain is chosen because cost may not allow a much larger subset tobe chosen, let alone the full input set. Testing with the full input set isknown as exhaustive testing. Thus\u00ae, the inherent need to test a programwith a small subset of the input domain poses a fundamental limit on theefficacy of testing. The limit is in the form of our inability to extrapolate thecorrectness of results for a proper subset of the input domain to programcorrectness. In other word\u00aes, even if a program passes a test set T, wecannot conclude that the program is correct. Once we have selected a subset of the input domain, we are faced with theproblem of verifying the correctness of the program outputs for individualtest input. That is, a program output is examined to determine if the programperformed correctly\u00aeon the test input. The mechanism which verifies thecorrectness of a program output is known as an oracle. The concept ofan oracle is discussed in detail in Chapter 9. Determining the correctnessof a program output is not a trivial task. If either of the following twoconditions hold, a program is considered non testable [20]:There does not exist an oracle.",
  "page164": "In this chapter we consider the first level of testing, that is, unit testing. Unit testingrefers to testing program units in isolation. However, there is no consensus on thedefini\u00aetion of a unit. Some examples of commonly understood units are functions,procedures, or methods. Even a class in an object-oriented programming languagecan be considered as a program unit. Syntactically, a program unit\u00aeis a piece ofcode, such as a function or method of class, that is invoked from outside the unitand that can invoke other program units. Moreover, a program unit is assumed toimplement a well-defined func\u00aetion providing a certain level of abstraction to theimplementation of higher level functions. The function performed by a program unitmay not have a direct association with a system-level function. Thus, a programunit may be viewed as a piece of code implementing a \"low\"-level f\u00aeunction. Inthis chapter, we use the terms unit and module interchangeably.Now, given that a program unit implements a function, it is only natural totest the unit before it is integrated with other units. Thus, a program unit is testedin isolation, that is, in a stand-alone manner. There are two reasons for testing aunit in a stan\u00aed-alone manner. First, errors found during testing can be attributedto a specific unit so that it can be easily fixed. Moreover, unit testing removesdependencies on other program units. Second, during unit testing it is desirableto verify that each distinct execution of a program unit produces the expectedresult. In terms of code details, a distinct execution refers to a distinct path in theunit.",
  "page165": "Second, during unit testing it is desirableto verify that each distinct execution of a program unit produces the expectedresult. In terms of code details, a distinct execution refe\u00aers to a distinct path in theunit. Ideally, all possible or as much as possible distinct executions are to beconsidered during unit testing. This requires careful selection of input data for eachdistinct execution. A pro\u00aegrammer has direct access to the input vector of the unit byexecuting a program unit in isolation. This direct access makes it easier to executeas many distinct paths as desirable or possible. If multipl\u00aee units are put together fortesting, then a programmer needs to generate test input with indirect relationshipwith the input vectors of several units under test. The said indirect relationshipmakes it difficult to control the execution of distinct paths in a chosen unit.Unit testing has a\u00aelimited scope. A programmer will need to verify whetheror not a code works correctly by performing unit-level testing. Intuitively, a programmer needs to test a unit as follows:Execute every line of code. This is desirable because the programmer needsto know what happens when a line of code is executed. In the absence ofsuch basi\u00aec observations, surprises at a later stage can be expensive. Execute every predicate in the unit to evaluate them to true and falseseparately. Observe that the unit performs its intended function and ensure that itcontains no known errors.In spite of the above tests, there e is no guarantee that a satisfactorily tested unitis functionally correct from a systemwide perspective. ",
  "page166": "Even though it is not possible to find all errors in a program unit in isolation, itis still necessary to ensure that a unit performs satisfactorily before it is used byother progr\u00aeam units. It serves no purpose to integrate an erroneous unit with otherunits for the following reasons: (i) many of the subsequent tests will be a wasteof resources and (ii) finding the root causes of failures in an in\u00aetegrated system ismore resource consuming.Unit testing is performed by the programmer who writes the program unitbecause the programmer is intimately familiar with the internal details of the unit.The ob\u00aejective for the programmer is to be satisfied that the unit works as expected.Since a programmer is supposed to construct a unit with no errors in it, a unittest is performed by him or her to their satisfaction in the beginning and to thesatisfaction of other programmers when the unit is\u00aeintegrated with other units. Thismeans that all programmers are accountable for the quality of their own work,which may include both new code and modifications to the existing code. The ideahere is to push the quality concept down to the lowest level of the organization andempower each programmer to be responsible for his or her o\u00aewn quality. Therefore,it is in the best interest of the programmer to take preventive actions to minimizethe number of defects in the code. The defects found during unit testing are internalto the software development group and are not reported up the personnel hierarchyto be counted in quality measurement metrics. The source code of a unit is notused for interfacing by other group members until the programmer completes unit ",
  "page167": "testing and checks in the unit to the version control system.Unit testing is conducted in two complementary phases: Static unit testingDynamic unit testingIn static unit testing, a\u00aeprogrammer does not execute the unit; instead, the code isexamined over all possible behaviors that might arise during run time. Static unittesting is also known as non-execution-based unit testing, whereas dynamic uni\u00aettesting is execution based. In static unit testing, the code of each unit is validatedagainst requirements of the unit by reviewing the code. During the review process,potential issues are identified an\u00aed resolved. For example, in the C programminglanguage the two program-halting instructions are abort() and exit(). While the twoare closely related, they have different effects as explained below:Abort(): This means abnormal program termination. By default, a call toabort() results in a r\u00aeun time diagnostic and program self-destruction. Theprogram destruction may or may not flush and close opened files or removetemporary files, depending on the implementation. Exit(): This means graceful program termination. That is, the exit() callcloses the opened files and returns a status code to the execution environment.Wheth\u00aeer to use abort() or exit() depends on the context that can be easilydetected and resolved during static unit testing. More issues caught earlier lead tofewer errors being identified in the dynamic test phase and result in fewer defectsin shipped products. Moreover, performing static tests is less expensive than performing dynamic tests. Code review is one component of the defect minimizationprocess and can help detect problems that are common to software development.After a round of code review, dynamic unit testing is conducted.",
  "page168": "In dynamic unittesting, a program unit is actually executed and its outcomes are observed. Dynamicunit testing means testing the code by actually running it. It may be noted that s\u00aetaticunit testing is not an alternative to dynamic unit testing. A programmer performsboth kinds of tests. In practice, partial dynamic unit testing is performed concurrently with static unit testing. If the entire dyna\u00aemic unit testing has been performedand a static unit testing identifies significant problems, the dynamic unit testingmust be repeated. As a result of this repetition, the development schedule may beaffe\u00aected. To minimize the probability of such an event, it is required that static unittesting be performed prior to the final dynamic unit testingStatic unit testing is conducted as a part of a larger philosophical belief that asoftware product should undergo a phase of inspection and correc\u00aetion at eachmilestone in its life cycle. At a certain milestone, the product need not be in itsfinal form. For example, completion of coding is a milestone, even though codingof all the units may not make the desired product. After coding, the next milestoneis testing all or a substantial number of units forming the major componen\u00aets of theproduct. Thus, before units are individually tested by actually executing them, thoseare subject to usual review and correction as it is commonly understood. The ideabehind review is to find the defects as close to their points of origin as possible sothat those defects are eliminated with less effort, and the interim product containsfewer defects before the next task is undertaken.",
  "page169": "Inspection: It is a step-by-step peer group review of a work product, witheach step checked against predetermined criteria. Walkthrough: It is a review where the author leads the t\u00aeeam through amanual or simulated execution of the product using predefined scenarios.Regardless of whether a review is called an inspection or a walkthrough, it isa systematic approach to examining source code in detail\u00ae. The goal of such anexercise is to assess the quality of the software in question, not the quality of theprocess used to develop the product [3]. Reviews of this type are characterizedby significant pre\u00aeparation by groups of designers and programmers with varyingdegree of interest in the software development project. Code examination can betime consuming. Moreover, no examination process is perfect. Examiners may takeshortcuts, may not have adequate understanding of the product, and may\u00aeaccept aproduct which should not be accepted. Nonetheless, a well-designed code reviewprocess can find faults that may be missed by execution-based testing. The key tothe success of code review is to divide and conquer, that is, having an examinerinspect small parts of the unit in isolation, while making sure of the following:(i)\u00aenothing is overlooked and (ii) the correctness of all examined parts of themodule implies the correctness of the whole module. The decomposition of thereview into discrete steps must assure that each step is simple enough that it canbe carried out without detailed knowledge of the others.The objective of code review is to review the code, not to evaluate the authorof the code",
  "page170": "review is to review the code, not to evaluate the authorof the code. A clash may occur between the author of the code and the reviewers,and this may make the meetings unproductive.\u00aeTherefore, code review must beplanned and managed in a professional manner. There is a need for mutual respect,openness, trust, and sharing of expertise in the group. The general guidelines forperforming code review co\u00aensists of six steps as outlined in Figure 3.1: readiness,preparation, examination, rework, validation, and exit. The input to the readinessstep is the criteria that must be satisfied before the start of\u00aethe code review process,and the process produces two types of documents, a change request (CR) and areport. These steps and documents are explained in the following. Readiness The author of the unit ensures that the unit under test isready for review. A unit is said to be ready if it sati\u00aesfies the followingcriteria. Completeness: All the code relating to the unit to be reviewed must beavailable. This is because the reviewers are going to read the code andtry to understand it. It is unproductive to review partially written codeor code that is going to be significantly modified by the programmer Minimal Functionalit\u00aey: The code must compile and link. Moreover,the code must have been tested to some extent to make sure that itperforms its basic functionalities.Readability: Since code review involves actual reading of code byother programmers, it is essential that the code is highly readable.Some code characteristics that enhance readability are proper formatting, using meaningful identifier names, straightforward use of programming language constructs, and an appropriate level of abstractionusing function calls. ",
  "page171": "Complexity: There is no need to schedule a group meeting to reviewstraightforward code which can be easily reviewed by the programmer.The code to be reviewed must be of sufficient\u00aecomplexity to warrantgroup review. Here, complexity is a composite term referring to thenumber of conditional statements in the code, the number of input dataelements of the unit, the number of output data elements prod\u00aeuced bythe unit, real-time processing of the code, and the number of other unitswith which the code communicates.Requirements and Design Documents: The latest approved versionof the low-level design spec\u00aeification or other appropriate descriptions Hierarchy of System Document of program requirements (see Table 3.1) should be available. Thesedocuments help the reviewers in verifying whether or not the codeunder review implements the expected functionalities. If the low-leveldesign document\u00aeis available, it helps the reviewers in assessing whetheror not the code appropriately implements the design.All the people involved in the review process are informed of thegroup review meeting schedule two or three days before the meeting.They are also given a copy of the work package for their perusal. Reviewsare conducted in\u00aebursts of 1-2 hours. Longer meetings are less and lessproductive because of the limited attention span of human beings. Therate of code review is restricted to about 125 lines of code (in a high-levellanguage) per hour. Reviewing complex code at a higher rate will resultin just glossing over the code, thereby defeating the fundamental purposeof code review. The composition of the review group involves a numberof people with different roles. These roles are explained as follows",
  "page172": "Moderator: A review meeting is chaired by the moderator. The moderator is a trained individual who guides the pace of the review process.The moderator selects the reviewers and sch\u00aeedules the review meetings.Myers suggests that the moderator be a member of a group from anunrelated project to preserve objectivity Author: This is the person who has written the code to be reviewed.Presenter: A presen\u00aeter is someone other than the author of the code.The presenter reads the code beforehand to understand it. It is thepresenter who presents the author's code in the review meeting forthe following re\u00aeasons: (i) an additional software developer will understand the work within the software organization; (ii) if the originalprogrammer leaves the company with a short notice, at least one otherprogrammer in the company knows what is being done; and (iii) theoriginal programmer will have a\u00aegood feeling about his or her work, ifsomeone else appreciates their work. Usually, the presenter appreciatesthe author's work. Recordkeeper: The recordkeeper documents the problems found during the review process and the follow-up actions suggested. The personshould be different than the author and the moderator.Reviewers: T\u00aehese are experts in the subject area of the code underreview. The group size depends on the content of the material underreview. As a rule of thumb, the group size is between 3 and 7. Usuallythis group does not have manager to whom the author reports. Thisis because it is the author's ongoing work that is under review, andneither a completed work nor the author himself is being reviewed.Observers: These are people who want to learn about the code underreview. These people do not participate in the review process but aresimply passive observers.",
  "page173": " Preparation Before the meeting, each reviewer carefully reviews thework package. It is expected that the reviewers read the code and understand its organization and operation befo\u00aere the review meeting. Eachreviewer develops the following: List of Questions: A reviewer prepares a list of questions to be asked,if needed, of the author to clarify issues arising from his or her reading.A general gui\u00aedeline of what to examine while reading the code isoutlined in Table 3.2. Potential CR: A reviewer may make a formal request to make achange. These are called change requests rather than defect reports.A\u00aet this stage, since the programmer has not yet made the code public, it is more appropriate to make suggestions to the author to makechanges, rather than report a defect. Though CRs focus on defects inthe code, these reports are not included in defect statistics related tothe productSugge\u00aested Improvement Opportunities: The reviewers may suggesthow to fix the problems, if there are any, in the code under review.Since reviewers are experts in the subject area of the code, it is notunusual for them to make suggestions for improvements. Examination The examination process consists of the followingactivities: The autho\u00aer makes a presentation of the procedural logic used in thecode, the paths denoting major computations, and the dependency ofthe unit under review on other units.The presenter reads the code line by line. The reviewers may raisequestions if the code is seen to have defects. However, problems are notresolved in the meeting. The reviewers may make general suggestionson how to fix the defects, but it is up to the author of the code to takecorrective measures after the meeting ends.",
  "page174": " The recordkeeper documents the change requests and the suggestionsfor fixing the problems, if there are any. A CR includes the followingdetails Give a brief description of the iss\u00aeue or action item. Assign a priority level (major or minor) to a CR.Assign a person to follow up the issue. Since a CR documents apotential problem, there is a need for interaction between the author of the code and one\u00aeof the reviewers, possibly the reviewer whomade the CR.Set a deadline for addressing a CR.The moderator ensures that the meeting remains focused on the reviewprocess. The moderator makes sure that the m\u00aeeeting makes progress ata certain rate so that the objective of the meeting is achieved.At the end of the meeting, a decision is taken regarding whether ornot to call another meeting to further review the code. If the reviewprocess leads to extensive rework of the code or critical issues\u00aeareidentified in the process, then another meeting is generally convened.Otherwise, a second meeting is not scheduled, and the author is giventhe responsibility of fixing the CRs Rework At the end of the meeting, the recordkeeper produces a summary of the meeting that includes the following information: A list of all the CRs, the\u00aedates by which those will be fixed, and thenames of the persons responsible for validating the CRs A list of improvement opportunitiesThe minutes of the meeting (optional) ",
  "page175": "A copy of the report is distributed to all the members of the review group.After the meeting, the author works on the CRs to fix the problems. Theauthor documents the improvements\u00aemade to the code in the CRs. Theauthor makes an attempt to address the issues within the agreed-upontime frame using the prevailing coding conventions Validation The CRs are independently validated by the moderatoror an\u00aeother person designated for this purpose. The validation processinvolves checking the modified code as documented in the CRs andensuring that the suggested improvements have been implementedcorrectly. Th\u00aee revised and final version of the outcome of the reviewmeeting is distributed to all the group members.Exit Summarizing the review process, it is said to be complete if all ofthe following actions have been taken: Every line of code in the unit has been inspected. If too many defects are\u00aefound in a module, the module is once againreviewed after corrections are applied by the author. As a rule of thumb,if more than 5% of the total lines of code are thought to be contentious,then a second review is scheduled. The author and the reviewers reach a consensus that when correctionshave been applied the code will be pote\u00aentially free of defects. All the CRs are documented and validated by the moderator or someoneelse. The author's follow-up actions are documented. A summary report of the meeting including the CRs is distributed toall the members of the review group.",
  "page176": "The effectiveness of static testing is limited by the ability of a reviewer tofind defects in code by visual means. However, if occurrences of defects depend onsome actual values o\u00aef variables, then it is a difficult task to identify those defectsby visual means. Therefore, a unit must be executed to observe its behaviors inresponse to a variety of inputs. Finally, whatever may be the effectivenes\u00aes of statictests, one cannot feel confident without actually running the code.Code Review Metrics It is important to collect measurement data pertinent toa review process, so that the review process can\u00aebe evaluated, made visible tothe upper management as a testing strategy, and improved to be more effective.Moreover, collecting metrics during code review facilitates estimation of reviewtime and resources for future projects. Thus, code review is a viable testing strategythat can be effe\u00aectively used to improve the quality of products at an early stage.The following metrics can be collected from a code review:Number of lines of code (LOC) reviewed per hour Number of CRs generated per thousand lines of code (KLOC) Number of CRs generated per hour Total number of CRs generated per project Total number of hours spent\u00aeon code review per projectIt is in the best interest of the programmers in particular and the company in generalto reduce the number of CRs generated during code review. This is because CRs areindications of potential problems in the code, and those problems must be resolvedbefore different program units are integrated. Addressing CRs means spending moreresources and potentially delaying the project. Therefore, it is essential to adoptthe concept of defect prevention during code development.",
  "page177": "Code Review Metrics It is important to collect measurement data pertinent toa review process, so that the review process can be evaluated, made visible tothe upper management as a\u00aetesting strategy, and improved to be more effective.Moreover, collecting metrics during code review facilitates estimation of reviewtime and resources for future projects. Thus, code review is a viable testing strategyt\u00aehat can be effectively used to improve the quality of products at an early stage.The following metrics can be collected from a code review: Number of lines of code (LOC) reviewed per hourNumber of CRs ge\u00aenerated per thousand lines of code (KLOC)Number of CRs generated per hour Total number of CRs generated per project Total number of hours spent on code review per projectIt is in the best interest of the programmers in particular and the company in generalto reduce the number of CRs gener\u00aeated during code review. This is because CRs areindications of potential problems in the code, and those problems must be resolvedbefore different program units are integrated. Addressing CRs means spending moreresources and potentially delaying the project. Therefore, it is essential to adoptthe concept of defect prevention durin\u00aeg code development. In practice, defectsare inadvertently introduced by programmers. Those accidents can be reduced bytaking preventive measures. It is useful to develop a set of guidelines to constructcode for defect minimization as explained in the following. These guidelines focuson incorporating suitable mechanisms into the code: ",
  "page178": "Build internal diagnostic tools, also known as instrumentation code, intothe units. Instrumentation codes are useful in providing information aboutthe internal states of the units.\u00aeThese codes allow programmers to realizebuilt-in tracking and tracing mechanisms. Instrumentation plays a passiverole in dynamic unit testing. The role is passive in the sense of observingand recording the internal beh\u00aeavior without actively testing a unit. Use standard controls to detect possible occurrences of error conditions.Some examples of error detection in the code are divides by zero and arrayindex out of boun\u00aeds. Ensure that code exists for all return values, some of which may be invalid.Appropriate follow-up actions need to be taken to handle invalid returnvalues. Ensure that counter data fields and buffer overflow and underflow areappropriately handled. Provide error messages and help texts\u00aefrom a common source so thatchanges in the text do not cause inconsistency. Good error messagesidentify the root causes of the problems and help users in resolving theproblems .Validate input data, such as the arguments, passed to a function. Use assertions to detect impossible conditions, undefined uses of data, andundesirable pr\u00aeogram behavior. An assertion is a Boolean statement whichshould never be false or can be false only if an error has occurred. In otherwords, an assertion is a check on a condition which is assumed to be true,but it can cause a problem if it not true. Assertion should be routinely usedto perform the following kinds of checks:",
  "page179": "Ensure that preconditions are satisfied before beginning to execute aunit. A precondition is a Boolean function on the states of a unit specifying our expectation of the state prio\u00aer to initiating an activity in thecode.Ensure that the expected postconditions are true while exiting from theunit. A postcondition is a Boolean function on the state of a unit specifying our expectation of the state af\u00aeter an activity has been completed.The postconditions may include an invariance. Ensure that the invariants hold. That is, check invariant states conditions which are expected not to change during the ex\u00aeecution of apiece of code. Leave assertions in the code. You may deactivate them in the releasedversion of code in order to improve the operational performance of thesystem. Fully document the assertions that appear to be unclear. After every major computation, reverse-compute the input(s\u00ae) from theresults in the code itself. Then compare the outcome with the actual inputsfor correctness. For example, suppose that a piece of code computes thesquare root of a positive number. Then square the output value and compare the result with the input. It may be needed to tolerate a margin oferror in the comparison process. I\u00aen systems involving message passing, buffer management is an importantinternal activity. Incoming messages are stored in an already allocatedbuffer. It is useful to generate an event indicating low buffer availabilitybefore the system runs out of buffer. Develop a routine to continuallymonitor the availability of buffer after every use, calculate the remainingspace available in the buffer, and call an error handling routine if theamount of available buffer space is too low",
  "page180": "Develop a timer routine which counts down from a preset time until iteither hits zero or is reset. If the software is caught in an infinite loop, thetimer will expire and an except\u00aeion handler routine can be invoked.Include a loop counter within each loop. If the loop is ever executed lessthan the minimum possible number of times or more than the maximumpossible number of times, then invoke an exc\u00aeeption handler routine.Define a variable to indicate the branch of decision logic that will be taken.Check this value after the decision has been made and the right branch hassupposedly been taken. If th\u00aee value of the variable has not been preset,there is probably a fall-through condition in the logic.Execution-based unit testing is referred to as dynamic unit testing. In this testing,a program unit is actually executed in isolation, as we commonly understand it.However, this execution d\u00aeiffers from ordinary execution in the following way: A unit under test is taken out of its actual execution environment. The actual execution environment is emulated by writing more code(explained later in this section) so that the unit and the emulatedenvironment can be compiled togetherThe above compiled aggregate is executed wi\u00aeth selected inputs. Theoutcome of such an execution is collected in a variety of ways, such asstraightforward observation on a screen, logging on files, and softwareinstrumentation of the code to reveal run time behavior. The resultis compared with the expected outcome. Any difference between theactual and expected outcome implies a failure and the fault is inthe code",
  "page181": "An environment for dynamic unit testing is created by emulating the contextof the unit under test, as shown in Figure 3.2. The context of a unit test consistsof two parts: (i) a ca\u00aeller of the unit and (ii) all the units called by the unit. Theenvironment of a unit is emulated because the unit is to be tested in isolationand the emulating environment must be a simple one so that any fault foundas\u00aea result of running the unit can be solely attributed to the unit under test.The caller unit is known as a test driver, and all the emulations of the unitscalled by the unit under test are called stubs.\u00aeThe test driver and the stubs aretogether called scaffolding. The functions of a test driver and a stub are explained asfollows:Test Driver: A test driver is a program that invokes the unit under test.The unit under test executes with input values received from the driverand, upon termina\u00aetion, returns a value to the driver. The driver comparesthe actual outcome, that is, the actual value returned by the unit under test,with the expected outcome from the unit and reports the ensuing test result.The test driver functions as the main unit in the execution process. Thedriver not only facilitates compilation, but also\u00aeprovides input data to theunit under test in the expected format. Stubs: A stub is a \"dummy subprogram\" that replaces a unit that is calledby the unit under test. Stubs replace the units called by the unit under test.A stub performs two tasks. ",
  "page182": "First, it shows an evidence that the stub was, in fact, called. Such evidence can be shown by merely printing a message.Second, the stub returns a precomputed value to the caller s\u00aeo that the unitunder test can continue its execution.The driver and the stubs are never discarded after the unit test is completed.Instead, those are reused in the future in regression testing of the unit if there issuc\u00aeh a need. For each unit, there should be one dedicated test driver and severalstubs as required. If just one test driver is developed to test multiple units, thedriver will be a complicated one. Any modi\u00aefication to the driver to accommodatechanges in one of the units under test may have side effects in testing the otherunits. Similarly, the test driver should not depend on the external input data filesbut, instead, should have its own segregated set of input data. The separate inputdata\u00aefile approach becomes a very compelling choice for large amounts of testinput data. For example, if hundreds of input test data elements are required to testmore than one unit, then it is better to create a separate input test data file ratherthan to include the same set of input test data in each test driver designed to testthe u\u00aenit.The test driver should have the capability to automatically determine thesuccess or failure of the unit under test for each input test data. If appropriate,the driver should also check for memory leaks and problems in allocation anddeallocation of memory. If the module opens and closes files, the test driver shouldcheck that these files are left in the expected open or closed state after each test.",
  "page183": "Mutation testing has a rich and long history. It can be traced back to the late 1970s[8-10]. Mutation testing was originally proposed by Dick Lipton, and the articleby DeMillo\u00ae, Lipton, and Sayward [9] is generally cited as the seminal reference.Mutation testing is a technique that focuses on measuring the adequacy of test data(or test cases). The original intention behind mutation testing wa\u00aes to expose andlocate weaknesses in test cases. Thus, mutation testing is a way to measure thequality of test cases, and the actual testing of program units is an added benefit.Mutation testing is not a\u00aetesting strategy like control flow or data flow testing. Itshould be used to supplement traditional unit testing techniques.A mutation of a program is a modification of the program created by introducing a single, small, legal syntactic change in the code. A modified program soobtained is\u00aecalled a mutant. The term mutant has been borrowed from biology.Some of these mutants are equivalent to the original program, whereas others arefaulty. A mutant is said to be killed when the execution of a test case causes it tofail and the mutant is considered to be dead.Some mutants are equivalent to the given program, that is,\u00aesuch mutantsalways produce the same output as the original program. In the real world, largeprograms are generally faulty, and test cases too contain faults. ",
  "page184": "The result of executing a mutant may be different from the expected result, but a test suite doesnot detect the failure because it does not have the right test case. In this scenar\u00aeiothe mutant is called killable or stubborn, that is, the existing set of test casesis insufficient to kill it. A mutation score for a set of test cases is the percentage of nonequivalent mutants killed by the test suit\u00aee. The test suite is said to bemutation adequate if its mutation score is 100%. Mutation analysis is a two-stepprocess: The adequacy of an existing test suite is determined to distinguish thegiven progra\u00aem from its mutants. A given test suite may not be adequateto distinguish all the nonequivalent mutants. As explained above, thosenonequivalent mutants that could not be identified by the given test suiteare called stubborn mutants. New test cases are added to the existing test suite to ki\u00aell the stubbornmutants. The test suite enhancement process iterates until the test suitehas reached a desired level of mutation score.If we run the modified programs against the test suite, we will get the followingresults:Mutants 1 and 3: The programs will completely pass the test suite. In otherwords, mutants 1 and 3 are not kil\u00aeled.Mutant 2: The program will fail test case 2.Mutant 4: The program will fail test case 1 and test case 2.If we calculate the mutation score, we see that we created four mutants, andtwo of them were killed. This tells us that the mutation score is 50%, assumingthat mutants 1 and 3 are nonequivalent.The score is found to be low. It is low because we assumed that mutants 1 and3 are nonequivalent to the original program. ",
  "page185": "We have to show that either mutants 1 and 3 are equivalent mutants or those are killable. If those are killable, we needto add new test cases to kill these two mutants. First, let\u00aeus analyze mutant 1in order to derive a \"killer\" test. The difference between P and mutant 1 is thestarting point. Mutant 1 starts with i 1, whereas P starts with i = 2. There isno impact on the result r. The\u00aerefore, we conclude that mutant 1 is an equivalentmutant. Second, we add a fourth test case as follows:Test case 4:Input: 2 2 1Then program P will produce the output \"Value of the rank is 1\" an\u00aed mutant 3will produce the output \"Value of the rank is 2.\" Thus, this test data kills mutant 3,which give us a mutation score of 100%.In order to use the mutation testing technique to build a robust test suite, thetest engineer needs to follow the steps that are outlined below:\u00aeStep 1: Begin with a program P and a set of test cases T known to be correct.Step 2: Run each test case in T against the program P. If a test case fails, thatis, the output is incorrect, program P must be modified and retested. Ifthere are no failures, then continue with step 3.Step 3: Create a set of mutants {Pi}, each differing from P by a simple, syntactically correct modification of P",
  "page186": "Execute each test case in T against each mutant Pi . If the output of themutant Pi differs from the output of the original program P, the mutantPi is considered incorrect and is sa\u00aeid to be killed by the test case. If Piproduces exactly the same results as the original program P for the testsin T, then one of the following is true: P and Pi are equivalent. That is, their behaviors cannot be\u00aedistinguished by any set of test cases. Note that the general problem ofdeciding whether or not a mutant is equivalent to the original programis undecidable. Pi is killable. That is, the test cases\u00aeare insufficient to kill the mutantPi . In this case, new test cases must be created.Step 5: Calculate the mutation score for the set of test cases T. The mutation score is the percentage of nonequivalent mutants killed by the testdata, that is, Mutation score 100 X D/(N \u2212 E),\u00aewhere D is the deadmutants, N the total number of mutants, and E the number of equivalentmutants.Step 6: If the estimated mutation adequacy of T in step 5 is not sufficiently high,then design a new test case that distinguishes Pi from P, add the newtest case to T, and go to step 2. If the computed adequacy of T is morethan an app\u00aeropriate threshold, then accept T as a good measure of thecorrectness of P with respect to the set of mutant programs Pi , and stopdesigning new test cases.",
  "page187": "Competent Programmer Hypothesis: This assumption states that programmers are generally competent, and they do not create \"random\" programs.Therefore, we can assume that f\u00aeor a given problem a programmer will create a correct program except for simple errors. In other words, the mutantsto be considered are the ones falling within a small deviation from theoriginal program. In practice, su\u00aech mutants are obtained by systematicallyand mechanically applying a set of transformations, called mutation operators, to the program under test. These mutation operators are expected tomodel programmin\u00aeg errors made by programmers. In practice, this maybe only partly true. Coupling Effect: This assumption was first hypothesized in 1978 byDeMillo et al. [9]. The assumption can be restated as complex faultsare coupled to simple faults in such a way that a test suite detectingall simple fa\u00aeults in a program will detect most of the complex faults.This assumption has been empirically supported by Offutt [11] andtheoretically demonstrated by Wah [12]. The fundamental premise ofmutation testing as coined by Geist et al. [13] is: If the software containsa fault, there will usually be a set of mutants that can only be kil\u00aeled by atest case that also detect that faul Mutation testing helps the tester to inject, by hypothesis, different types offaults in the code and develop test cases to reveal them. In addition, comprehensivetesting can be performed by proper choice of mutant operations. However, a relatively large number of mutant programs need to be tested against many of the testcases before these mutants can be distinguished from the original program. Running the test cases, analyzing the results, identifying equivalent mutants [14], anddeveloping additional test cases to kill the stubborn mutants are all time consuming",
  "page188": "Robust automated testing tools such as Mothra can be used to expeditethe mutation testing process. Recently, with the availability of massive computing power, there has been a res\u00aeurgence of mutation testing processes within theindustrial community to use as a white-box methodology for unit testing [16, 17].Researchers have shown that with an appropriate choice of mutant programs mutation testing\u00aeis as powerful as path testing, domain testing [18], and data flowtesting The programmer, after a program failure, identifies the corresponding fault andfixes it. The process of determining the cause of\u00aea failure is known as debugging.Debugging occurs as a consequence of a test revealing a failure. Myers proposedthree approaches to debugging in his book The Art of Software Testing [20]:Brute Force: The brute-force approach to debugging is preferred by manyprogrammers. Here, \"let th\u00aee computer find the error\" philosophy is used. Print statements are scattered throughout the source code. These print statements provide a crude trace of the way the source code has executed.The availability of a good debugging tool makes these print statementsredundant . A dynamic debugger allows the software engineer to nav\u00aeigateby stepping through the code, observe which paths have executed, andobserve how values of variables change during the controlled execution.A good tool allows the programmer to assign values to several variablesand navigate step by step through the code",
  "page189": "Instrumentation code can bebuilt into the source code to detect problems and to log intermediate values of variables for problem diagnosis. One may use a memory dumpafter a failure\u00aehas occurred to understand the final state of the code beingdebugged. The log and memory dump are reviewed to understand whathappened and how the failure occurred. Cause Elimination: The cause elimination approach can\u00aebe best describedas a process involving induction and deduction [21]. In the induction part,first, all pertinent data related to the failure are collected , such as whathappened and what the symptoms are\u00ae. Next, the collected data are organized in terms of behavior and symptoms, and their relationship is studiedto find a pattern to isolate the causes. A cause hypothesis is devised, and theabove data are used to prove or disprove the hypothesis. In the deductionpart, a list of all possible\u00aecauses is developed in order of their likelihoods,and tests are conducted to eliminate or substantiate each cause in decreasing order of their likelihoods. If the initial tests indicate that a particularhypothesis shows promise, test data are refined in an attempt to isolate theproblem as needed.Backtracking: In this approach, th\u00aee programmer starts at a point in thecode where a failure was observed and traces back the execution to the pointwhere it occurred. This technique is frequently used by programmers, andthis is useful in small programs. However, the probability of tracing backto the fault decreases as the program size increases, because the numberof potential backward paths may become too large.",
  "page190": "Often, software engineers notice other previously undetected problems whiledebugging and applying a fix. These newly discovered faults should not be fixedalong with the fix in focu\u00aes. This is because the software engineer may not have afull understanding of the part of the code responsible for the new fault. The bestway to deal with such a situation is to file a CR. A new CR gives the programmer a\u00aenopportunity to discuss the matter with other team members and software architectsand to get their approval on a suggestion made by the programmer. Once the CR isapproved, the software engineer must file\u00aea defect in the defect tracking databaseand may proceed with the fix. This process is cumbersome, and it interrupts thedebugging process, but it is useful for very critical projects. However, programmersoften do not follow this because of a lack of a procedure to enforce it.A Debugging H\u00aeeuristic The objective of debugging is to precisely identify thecause of a failure. Once the cause is identified, corrective measures are taken to fix the fault. Debugging is conducted by programmers, preferably by those whowrote the code, because the programmer is the best person to know the source codewell enough to analyze the\u00aecode efficiently and effectively. Debugging is usuallya time consuming and error-prone process, which is generally performed understress. Debugging involves a combination of systematic evaluation, intuition, and,sometimes, a little bit of luck. Given a symptom of a problem, the purpose is toisolate and determine its specific cause. The following heuristic may be followedto isolate and correct it",
  "page191": "Reproduce the symptom(s). Read the troubleshooting guide of the product. This guide may includeconditions and logs, produced by normal code, or diagnostics codespecifically written\u00aefor troubleshooting purpose that can be turned on. Try to reproduce the symptoms with diagnostics code turned on.Gather all the information and conduct causal analysis The goal ofcausal analysis is to identify the root\u00aecause of the problem and initiateactions so that the source of defects is eliminated.Step 2: Formulate some likely hypotheses for the cause of the problem based onthe causal analysis.Step 3: Develop a t\u00aeest scenario for each hypothesis to be proved or disproved.This is done by designing test cases to provide unambiguous resultsrelated to a hypothesis. The test cases may be static (reviewing code anddocumentation) and/or dynamic in nature. Preferably, the test cases arenondestructive, hav\u00aee low cost, and need minimum additional hardwareneeds. A test case is said to be destructive if it destroys the hardwaresetup. For example, cutting a cable during testing is called destructivetesting.Step 4: Prioritize the execution of test cases. Test cases corresponding to thehighly probable hypotheses are executed first. Also,\u00aethe cost factor cannotbe overlooked. Therefore, it is desirable to execute the low-cost test casesfirst followed by the more expensive ones. The programmer needs toconsider both factors.Step 5: Execute the test cases in order to find the cause of a symptom. Afterexecuting a test case, examine the result for new evidence. If the testresult shows that a particular hypothesis is promising, test data are refinedin an attempt to isolate the defect. If necessary, go back to earlier stepsor eliminate a particular hypothesis.",
  "page192": "any side effects (collateral damage) due to the changes effected in themodule. After a possible code review, apply the fix. Retest the unit to confirm that the actual c\u00aeause of failure had beenfound. The unit is properly debugged and fixed if tests show that theobserved failure does not occur any more. If there are no dynamic unit test cases that reveal the problem, thenadd a new\u00aetest case to the dynamic unit testing to detect possiblereoccurrences or other similar problems. For the unit under consideration, identify all the test cases that havepassed. Now, perform a regre\u00aession test on the unit with those testcases to ensure that new errors have not been introduced. That is whyit is so important to have archived all the test cases that have beendesigned for a unit. Thus, even unit-level test cases must be managedin a systematic manner to reduce the cost of\u00aesoftware development.Step 7: Document the changes which have been made. Once a defect is fixed,the following changes are required to be applied: Document the changes in the source code itself to reflect the change. Update the overall system documentation. Changes to the dynamic unit test cases. File a defe\u00aect in the defect tracking database if the problem was foundafter the code was checked in to the version control system.",
  "page193": "A TDD approach to code development is used in the XP methodology [22, 23]. Thekey aspect of the TDD approach is that a programmer writes low-level tests beforewriting production co\u00aede. This is referred to as test first [24] in software development. Writing test-driven units is an important concept in the XP methodology. InXP, a few unit tests are coded first, then a simple, partial system is imple\u00aemented topass the tests. Then, one more new unit test is created, and additional code is written to pass the new test, but not more, until a new unit test is created. The processis continued until nothin\u00aeg is left to test. The process is illustrated in Figure 3.3 andoutlined below:Step 1: Pick a requirement, that is, a story.Step 2: Write a test case that will verify a small part of the story and assign afail verdict to it.Step 3: Write the code that implements a particular part of the st\u00aeory to pass thetest.Step 4: Execute all tests Step 5: Rework the code, and test the code until all tests pass.Step 6: Repeat steps 2-5 until the story is fully implemented.The simple cycle in Figure 3.3 shows that, at the beginning of each cycle,the intention is for all tests to pass except the newly added test case. The new\u00aetestcase is introduced to drive the new code development. At the end of the cycle, theprogrammer executes all the unit tests, ensuring that each one passes and, hence,the planned task of the code still works",
  "page194": "A TDD approach to code development is used in the XP methodology [22, 23]. Thekey aspect of the TDD approach is that a programmer writes low-level tests beforewriting production co\u00aede. This is referred to as test first [24] in software development. Writing test-driven units is an important concept in the XP methodology. InXP, a few unit tests are coded first, then a simple, partial system is imple\u00aemented topass the tests. Then, one more new unit test is created, and additional code is written to pass the new test, but not more, until a new unit test is created. The processis continued until nothin\u00aeg is left to test. The process is illustrated in Figure 3.3 andoutlined below:Step 1: Pick a requirement, that is, a story.Step 2: Write a test case that will verify a small part of the story and assign afail verdict to it.Step 3: Write the code that implements a particular part of the st\u00aeory to pass thetest.Step 4: Execute all tests Step 5: Rework the code, and test the code until all tests pass.Step 6: Repeat steps 2-5 until the story is fully implemented.The simple cycle in Figure 3.3 shows that, at the beginning of each cycle,the intention is for all tests to pass except the newly added test case. The new\u00aetestcase is introduced to drive the new code development. At the end of the cycle, theprogrammer executes all the unit tests, ensuring that each one passes and, hence,the planned task of the code still works",
  "page195": " One may not write production code unless the first failing unit test iswritten. One may not write more of a unit test than is sufficient to fail. One may not write more production\u00aecode than is sufficient to make thefailing unit test pass.These three laws ensure that one must write a portion of a unit test that failsand then write just enough production code to make that unit test pass. The goalo\u00aef these three laws is not to follow them strictly it is to decrease the intervalbetween writing unit tests and production code.Creating unit tests helps a developer focus on what needs to be done. Requir\u00aeements, that is, user stories, are nailed down firmly by unit tests. Unit tests arereleased into the code repository along with the code they test. Code without unittests may not be released. If a unit test is discovered to be missing, it must be created immediately. Creating unit tests i\u00aendependently before coding sets up checksand balances and improves the chances of getting the system right the first time.Unit tests provide a safety net of regression tests and validation tests so that XPprogrammers can refactor and integrate effectively.In XP, the code is being developed by two programmers working together sideb\u00aey side. The concept is called pair programming. The two programmers sit side byside in front of the monitor. One person develops the code tactically and the otherone inspects it methodically by keeping in mind the story they are implementing.It is similar to the two-person inspection strategy proposed by Bisant and Lyle",
  "page196": "The JUnit is a unit testing framework for the Java programming language designedby Kent Beck and Erich Gamma. Experience gained with JUnit has motivated thedevelopment of the TDD [\u00ae22] methodology. The idea in the JUnit framework hasbeen ported to other languages, including C# (NUnit), Python (PyUnit), Fortran(fUnit) and C  (CPPUnit). This family of unit testing frameworks is collectivelyreferred\u00aeto as xUnit. This section will introduce the fundamental concepts of JUnitto the reader.Suppose that we want to test the individual methods of a class called PlanetClass. Let Move() be a method in Plane\u00aetClass such that Move() accepts only oneinput parameter of type integer and returns a value of type integer. One can followthe following steps, illustrated using pseudocode in Figure 3.4, to test Move(): Create an object instance of Planet lass. Let us call the instance Mars.Now we are in\u00aeterested in testing the method Move() by invoking it onobject Mars. Select a value for all the input parameters of Move() this function hasjust one input parameter. Let us represent the input value to Move() by x. Know the expected value to be returned by Move(). Let the expectedreturned value be y Invoke method Move() on o\u00aebject Mars with input value x. Let z denotethe value returned by Move().Now compare y with z. If the two values are identical, then the methodMove() in object Mars passes the test. Otherwise, the test is said to havefailed",
  "page197": "In a nutshell, the five steps of unit testing are as follows:Create an object and select a method to execute. Select values for the input parameters of the method. Compute the expe\u00aected values to be returned by the method. Execute the selected method on the created object using the selected inputvalues. Verify the result of executing the method.Performing unit testing leads to a programmer consumi\u00aeng some resources,especially time. Therefore, it is useful to employ a general programming frameworkto code individual test cases, organize a set of test cases as a test suite, initialize atest environme\u00aent, execute the test suite, clean up the test environment, and recordthe result of execution of individual test cases. In the example shown in Figure 3.4,creating the object Mars is a part of the initialization process. The two print()statements are examples of recording the result of tes\u00aet execution. Alternatively,one can write the result of test execution to a file. The JUnit framework has been developed to make test writing simple. Theframework provides a basic class, called TestCase, to write test cases. Programmersneed to extend the TestCase class to write a set of individual test cases. It may benoted that to\u00aewrite, for example, 10 test cases, one need not write 10 subclasses ofthe class Testcase. Rather, one subclass, say Testcase, of Testcase, can contain10 methods one for each test case. Programmers need to make assertions about the state of objects while extending the Testcase class to write test cases. For example, in each test case it isrequired to compare the actual outcome of a computation with the expected outcome.",
  "page198": " Though an if() statement can be used to compare the equality of two valuesor two objects, it is seen to be more elegant to write an assert statement to achievethe same. The class\u00aeTestcase extends a utility class called Assert in the JUnitframework. Essentially, the Assert class provides methods, as explained in the following, to make assertions about the state of objects created and manipulated\u00aewhiletesting.assert True(Boolean condition): This assertion passes if the condition is true;otherwise, it fails.assert Equals(Object expected, Object actual): This assertion passes if theexpected and the\u00aeactual objects are equal according to the equals() method;otherwise, the assertion fails.assert Equals(int expected, int actual): This assertion passes if expected andactual are equal according to the = operator; otherwise, the assertion fails. For each primitive type int, float, double\u00ae, char, byte, long, short, andBoolean, the assertion has an overloaded version.assert Equals(double expected, double actual, double tolerance): This assertion passes if the absolute value of the difference between expected andactual is less than or equal to the tolerance value; otherwise, the assertionfails. The assertion has an o\u00aeverloaded version for float inputs.assert Same(Object expected, Object actual): This assertion passes if theexpected and actual values refer to the same object in memory; otherwise,the assertion fails assert Null(Object testobject): This assertion passes if testobject is null; otherwise the assertion fails.assert False(Boolean condition): This is the logical opposite of assert True()",
  "page199": "The reader may note that the above list of assertions is not exhaustive. Infact, one can build other assertions while extending the TestCase class. When anassertion fails, a progra\u00aemmer may want to know immediately the nature of thefailure. This can be done by displaying a message when the assertion fails. Eachassertion method listed above accepts an optional first parameter of type String ifthe a\u00aessertion fails, then the String value is displayed. This facilitates the programmerto display a desired message when the assertion fails. As an aside, upon failure,the assert Equals() method displays a c\u00aeustomized message showing the expectedvalue and the actual value At this point it is interesting to note that only failed tests are reported. Failedtests can be reported by various means, such as displaying a message, displaying anidentifier for the test case, and counting the total numbe\u00aer of failed test cases. Essentially, an assertion method throws an exception, called AssertionFailedError, whenthe assertion fails, and JUnit catches the exception. The code shown in Figure 3.5illustrates how the assert True() assertion works: When the JUnit framework catchesan exception, it records the fact that the assertion fai\u00aeled and proceeds to the nexttest case. Having executed all the test cases, JUnit produces a list of all those teststhat have failed. MyTestSuite and invoke the two methods MyTest1() and MyTest2(). Whether ornot the two methods, namely Method1() and Method()2, are to be invoked on twodifferent instances of the class TestMe depends on the individual objectives ofthose two test cases. In other words, it is the programmer who decides whether ornot two instances of the class TestMe are to be created",
  "page200": "Programmers can benefit from using tools in unit testing by reducing testing timewithout sacrificing thoroughness. The well-known tools in everyday life are aneditor, a compiler, a\u00aen operating system, and a debugger. However, in some cases, the real execution environment of a unit may not be available to a programmerwhile the code is being developed. In such cases, an emulator of the environmentis\u00aeuseful in testing and debugging the code. Other kinds of tools that facilitateeffective unit testing are as follows:1. Code Auditor: This tool is used to check the quality of software to ensurethat it m\u00aeeets some minimum coding standards. It detects violations of programming, naming, and style guidelines. It can identify portions of code that cannotbe ported between different operating systems and processors. Moreover, it cansuggest improvements to the structure and style of the source c\u00aeode. In addition, itcounts the number of LOC which can be used to measure productivity, that is, LOCproduced per unit time, and calculate defect density, that is, number of defects perKLOC.2. Bound Checker: This tool can check for accidental writes into the instruction areas of memory or to any other memory location outside the da\u00aeta storage areaof the application. This fills unused memory space with a signature pattern (distinct binary pattern) as a way of determining at a later time whether any of thismemory space has been overwritten. The tool can issue diagnostic messages whenboundary violations on data items occur. It can detect violation of the boundaries of array, for example, when the array index or pointer is outside its allowedrange. ",
  "page201": "Introduction to Threads- Like people, computers can multitask. That is, they can be working on several different tasks at the same time. A computer that has just a single central p\u00aerocessing unit can't literally do two things at the same time, any more than a person can, but it can still switch its attention back and forth among several tasks. Furthermore, it is increasingly common for comput\u00aeers to have more than one processing unit, and such computers can literally work on several tasks simultaneously. It is likely that from now on, most of the increase in computing power will come from add\u00aeing additional processors to computers rather than from increasing the speed of individual processors. To use the full power of these multiprocessing computers, a programmer must do parallel programming, which means writing a program as a set of several tasks that can be executed simultan\u00aeeously. Even on a single-processor computer, parallel programming techniques can be useful, since some problems can be tackled most naturally by breaking the solution into a set of simultaneous tasks that cooperate to solve the problem. In Java, a single task is called a thread. The term \"thread\" refers to a \"thread\u00aeof control\" or \"thread of execution,\" meaning a sequence of instructions that are executed one after another the thread extends through time, connecting each instruction to the next. In a multithreaded program, there can be many threads of control, weaving through time in parallel and forming the complete fabric of the program. (Ok, enough with the metaphor, already!) Every Java program has at least one thread; when the Java virtual machine runs your program, it creates a thread that is responsible for executing the main routine of the program. This main thread can in turn create other threads that can continue even after the main thread has terminated. In a GUI program, there is at least one additional thread, which is responsible for handling events and drawing components on the screen. This GUI thread is created when the first window is opened. So in fact, you have already done parallel programming! When a main routine opens a window, both the main thread and the GUI thread can continue to run in parallel. Of course, parallel programming can be used in much more interesting ways.",
  "page202": "Unfortunately, parallel programming is even more difficult than ordinary, single-threaded programming. When several threads are working together on a problem, a whole new category\u00aeof errors is possible. This just means that techniques for writing correct and robust programs are even more important for parallel programming than they are for normal programming. (That's one excuse for having th\u00aeis section in this chapter another is that we will need threads at several points in future chapters, and I didn't have another place in the book where the topic fits more naturally.) Since threads\u00aeare a difficult topic, you will probably not fully understand everything in this section the first time through the material. Your understanding should improve as you encounter more examples of threads in future sections. Creating and Running Threads- In Java, a thread is represented by a\u00aen object belonging to the class java.lang.Thread (or to a subclass of this class). The purpose of a Thread object is to execute a single method. The method is executed in its own thread of control, which can run in parallel with other threads. When the execution of the method is finished, either because the method terminates norma\u00aelly or because of an uncaught exception, the thread stops running. Once this happens, there is no way to restart the thread or to use the same Thread object to start another thread. Operations on Threads- The Thread class includes several useful methods in addition to the start() method that was discussed above. I will mention just a few of them. If thrd is an object of type Thread, then the boolean-valued function thrd.isAlive() can be used to test whether or not the thread is alive. A thread is \"alive\" between the time it is started and the time when it terminates. After the thread has terminated it is said to be \"dead\". (The rather gruesome metaphor is also used when we refer to \"killing\" or \"aborting\" a thread.)",
  "page203": "The static method Thread.sleep(milliseconds) causes the thread that executes this method to \"sleep\" for the specified number of milliseconds. A sleeping thread is still a\u00aelive, but it is not running. While a thread is sleeping, the computer will work on any other runnable threads (or on other programs). Thread.sleep() can be used to insert a pause in the execution of a thread. The sleep\u00aemethod can throw an exception of type InterruptedException, which is an exception class that requires mandatory exception handling. In practice, this means that the sleep method is usually used in a tryc\u00aeatch statement that catches the potential InterruptedException: try { Thread.sleep(lengthOfPause); } catch (InterruptedException e) { } One thread can interrupt another thread to wake it up when it is sleeping or paused for some other reason. A Thread, thrd, can be interrupted by calling\u00aeits method thrd.interrupt(), but you are not likely to do this until you start writing rather advanced applications, and you are not likely to need to do anything in response to an InterruptedException (except to catch it). It's unfortunate that you have to worry about it at all, but that's the way that mandatory excepti\u00aeon handling works. Mutual Exclusion with \"synchronized\" Programming several threads to carry out independent tasks is easy. The real difficulty arises when threads have to interact in some way. One way that threads interact is by sharing resources. When two threads need access to the same resource, such as a variable or a window on the screen, some care must be taken that they don't try to use the same resource at the same time. Otherwise, the situation could be something like this: Imagine several cooks sharing the use of just one measuring cup, and imagine that Cook A fills the measuring cup with milk, only to have Cook B grab the cup before Cook A has a chance to empty the milk into his bowl. There has to be some way for Cook A to claim exclusive rights to the cup while he performs the two operations: Add-Milk-To-Cup and Empty-Cup-Into-Bowl.",
  "page204": "Wait and Notify- Threads can interact with each other in other ways besides sharing resources. For example, one thread might produce some sort of result that is needed by another t\u00aehread. This imposes some restriction on the order in which the threads can do their computations. If the second thread gets to the point where it needs the result from the first thread, it might have to stop and wait fo\u00aer the result to be produced. Since the second thread can't continue, it might as well go to sleep. But then there has to be some way to notify the second thread when the result is ready, so that it\u00aecan wake up and continue its computation. Java, of course, has a way to do this kind of waiting and notification: It has wait() and notify() methods that are defined as instance methods in class Object and so can be used with any object. The reason why wait() and notify() should be associ\u00aeated with objects is not obvious, so don't worry about it at this point. It does, at least, make it possible to direct different notifications to a different recipients, depending on which object's notify() method is called. Volatile Variables- And a final note on communication among threads: In general, threads communic\u00aeate by sharing variables and accessing those variables in synchronized methods or synchronized statements. However, synchronization is fairly expensive computationally, and excessive use of it should be avoided. So in some cases, it can make sense for threads to refer to shared variables without synchronizing their access to those variables. However, a subtle problem arises when the value of a shared variable is set is one thread and used in another. Because of the way that threads are implemented in Java, the second thread might not see the changed value of the variable immediately.",
  "page205": "It is still possible to use a shared variable outside of synchronized code, but in that case, the variable must be declared to be volatile. The volatile keyword is a modifier that\u00aecan be added to a variable declaration, as in private volatile int count; If a variable is declared to be volatile, no thread will keep a local copy of that variable in its cache. Instead, the thread will always use the\u00aeofficial, main copy of the variable. This means that any change made to the variable will immediately be available to all threads. This makes it safe for threads to refer to volatile shared variables ev\u00aeen outside of synchronized code. (Remember, though, that synchronization is still the only way to prevent race conditions.) When the volatile modifier is applied to an object variable, only the variable itself is declared to be volatile, not the contents of the object that the variable po\u00aeints to. For this reason, volatile is generally only used for variables of simple types such as primitive types and enumerated types.Analysis of Algorithms- This chapter has concentrated mostly on correctness of programs. In practice, another issue is also important: efficiency. When analyzing a program in terms of efficiency, we\u00aewant to look at questions such as, \"How long does it take for the program to run?\" and \"Is there another approach that will get the answer more quickly?\" Efficiency will always be less important than correctness; if you don't care whether a program works correctly, you can make it run very quickly indeed, but no one will think it's much of an achievement! On the other hand, a program that gives a correct answer after ten thousand years isn't very useful either, so efficiency is often an important issue.The term \"efficiency\" can refer to efficient use of almost any resource, including time, computer memory, disk space, or network bandwidth. In this section, however, we will deal exclusively with time efficiency, and the major question that we want to ask about a program is, how long does it take to perform its task?",
  "page206": "it really makes little sense to classify an individual program as being \"efficient\" or \"inefficient.\" It makes more sense to compare two (correct) programs that\u00aeperform the same task and ask which one of the two is \"more efficient,\" that is, which one performs the task more quickly. However, even here there are difficulties. The running time of a program is not well-\u00aedefined. The run time can be different depending on the number and speed of the processors in the computer on which it is run and, in the case of Java, on the design of the Java Virtual Machine which is\u00aeused to interpret the program. It can depend on details of the compiler which is used to translate the program from high-level language to machine language. Furthermore, the run time of a program depends on the size of the problem which the program has to solve. It takes a sorting program\u00aelonger to sort 10000 items than it takes it to sort 100 items. When the run times of two programs are compared, it often happens that Program A solves small problems faster than Program B, while Program B solves large problems faster than Program A, so that it is simply not the case that one program is faster than the other in al\u00ael cases. the efficiency of programs. The field is known as Analysis of Algorithms. The focus is on algorithms, rather than on programs as such, to avoid having to deal with multiple implementations of the same algorithm written in different languages, compiled with different compilers, and running on different computers. Analysis of Algorithms is a mathematical field that abstracts away from these down-and-dirty details. Still, even though it is a theoretical field, every working programmer should be aware of some of its techniques and results. This section is a very brief introduction to some of those techniques and results. Because this is not a mathematics book, the treatment will be rather informal.",
  "page207": "One of the main techniques of analysis of algorithms is asymptotic analysis. The term \"asymptotic\" here means basically \"the tendency in the long run.\" An asymp\u00aetotic analysis of an algorithm's run time looks at the question of how the run time depends on the size of the problem. The analysis is asymptotic because it only considers what happens to the run time as the size\u00aeof the problem increases without limit; it is not concerned with what happens for problems of small size or, in fact, for problems of any fixed finite size. Only what happens in the long run, as the prob\u00aelem size increases without limit, is important. Showing that Algorithm A is asymptotically faster than Algorithm B doesn't necessarily mean that Algorithm A will run faster than Algorithm B for problems of size 10 or size 1000 or even size 1000000 it only means that if you keep incre\u00aeasing the problem size, you will eventually come to a point where Algorithm A is faster than Algorithm B. An asymptotic analysis is only a first approximation, but in practice it often gives important and useful information. Central to asymptotic analysis is Big-Oh notation. Using this notation, we might say, for example, that an\u00aealgorithm has a running time that is O(n2 ) or O(n) or O(log(n)). These notations are read \"Big-Oh of n squared,\" \"Big-Oh of n,\" and \"Big-Oh of log n\" (where log is a logarithm function). More generally, we can refer to O(f(n)) (\"Big-Oh of f of n\"), where f(n) is some function that assigns a positive real number to every positive integer n. The \"n\" in this notation refers to the size of the problem. Before you can even begin an asymptotic analysis, you need some way to measure problem size. Usually, this is not a big issue. For example, if the problem is to sort a list of items, then the problem size can be taken to be the number of items in the list. When the input to an algorithm is an integer, as in the case of algorithm that checks whether a given positive integer is prime, the usual measure of the size of a problem is the number of bits in the input integer rather than the integer itself. More generally, the number of bits in the input to a problem is often a good measure of the size of the problem.",
  "page208": "To say that the running time of an algorithm is O(f(n)) means that for large values of the problem size, n, the running time of the algorithm is no bigger than some constant times\u00aef(n). (More rigorously, there is a number C and a positive integer M such that whenever n is greater than M, the run time is less than or equal to C*f(n).) The constant takes into account details such as the speed of th\u00aee computer on which the algorithm is run; if you use a slower computer, you might have to use a bigger constant in the formula, but changing the constant won't change the basic fact that the run tim\u00aee is O(f(n)). The constant also makes it unnecessary to say whether we are measuring time in seconds, years, CPU cycles, or any other unit of measure; a change from one unit of measure to another is just multiplication by a constant. Note also that O(f(n)) doesn't depend at all on wh\u00aeat happens for small problem sizes, only on what happens in the long run as the problem size increases without limit. To look at a simple example, consider the problem of adding up all the numbers in an array. The problem size, n, is the length of the array. Using A as the name of the array, the algorithm can be expressed in Java\u00aeas: total = 0; for (int i = 0; i < n; i++) total = total + A[i]; This algorithm performs the same operation, total = total + A[i], n times. The total time spent on this operation is a*n, where a is the time it takes to perform the operation once. time spent on this operation is a*n, where a is the time it takes to perform the operation once. Now, this is not the only thing that is done in the algorithm. The value of i is incremented and is compared to n each time through the loop. This adds an additional time of b*n to the run time, for some constant b. Furthermore, i and total both have to be initialized to zero; this adds some constant amount c to the running time. The exact running time would then be (a+b)*n+c, where the constants a, b, and c depend on factors such as how the code is compiled and what computer it is run on.",
  "page209": "Using the fact that c is less than or equal to c*n for any positive integer n, we can say that the run time is less than or equal to (a+b+c)*n. That is, the run time is less than o\u00aer equal to a constant times n. By definition, this means that the run time for this algorithm is O(n). If this explanation is too mathematical for you, we can just note that for large values of n, the c in the formula (\u00aea+b)*n+c is insignificant compared to the other term, (a+b)*n. We say that c is a \"lower order term.\" When doing asymptotic analysis, lower order terms can be discarded. A rough, but correct, a\u00aesymptotic analysis of the algorithm would go something like this: Each iteration of the for loop takes a certain constant amount of time. There are n iterations of the loop, so the total run time is a constant times n, plus lower order terms (to account for the initialization). Disregardi\u00aeng lower order terms, we see that the run time is O(n). Note that to say that an algorithm has run time O(f(n)) is to say that its run time is no bigger than some constant times n (for large values of n). O(f(n)) puts an upper limit on the run time. However, the run time could be smaller, even much smaller. For example, if the run\u00aetime is O(n), it would also be correct to say that the run time is O(n2 ) or even O(n10). If the run time is less than a constant times n, then it is certainly less than the same constant times n 2 or n10 Of course, sometimes it's useful to have a lower limit on the run time. That is, we want to be able to say that the run time is greater than or equal to some constant times f(n) (for large values of n). The notation for this is Ohm(f(n)), read \"Omega of f of n.\" \"Omega\" is the name of a letter in the Greek alphabet, and Ohm is the upper case version of that letter. (To be technical, saying that the run time of an algorithm is Ohm(f(n)) means that there is a positive number C and a positive integer M such that whenever n is greater than M, the run time is greater than or equal to C*f(n).) O(f(n)) tells you something about the maximum amount of time that you might have to wait for an algorithm to finish; Ohm(f(n)) tells you something about the minimum time.",
  "page210": "The algorithm for adding up the numbers in an array has a run time that is Ohm(n) as well as O(n). When an algorithm has a run time that is both Ohm(f(n)) and O(f(n)), its ru\u00aen time is said to be Theta(f(n)), read \"Theta of f of n.\" (Theta is another letter from the Greek alphabet.) To say that the run time of an algorithm is Theta(f(n)) means that for large values of n, the run\u00aetime is between a*f(n) and b*f(n), where a and b are constants (with b greater than a, and both greater than 0).So far, my analysis has ignored an important detail. We have looked at how run time depends\u00aeon the problem size, but in fact the run time usually depends not just on the size of the problem but on the specific data that has to be processed. For example, the run time of a sorting algorithm can depend on the initial order of the items that are to be sorted, and not just on the nu\u00aember of items. To account for this dependency, we can consider either the worst case run time analysis or the average case run time analysis of an algorithm. For a worst case run time analysis, we consider all possible problems of size n and look at the longest possible run time for all such problems. For an average case analysis,\u00aewe consider all possible problems of size n and look at the average of the run times for all such problems. Usually, the average case analysis assumes that all problems of size n are equally likely to be encountered, although this is not always realistic or even possible in the case where there is an infinite number of different problems of a given size. In many cases, the average and the worst case run times are the same to within a constant multiple. This means that as far as asymptotic analysis is concerned, they are the same. That is, if the average case run time is O(f(n)) or Theta(f(n)), then so is the worst case. However, later in the book, we will encounter a few cases where the average and worst case asymptotic analyses differ.",
  "page211": "So, what do you really have to know about analysis of algorithms to read the rest of this book? We will not do any rigorous mathematical analysis, but you should be able to follow\u00aeinformal discussion of simple cases such as the examples that we have looked at in this section. Most important, though, you should have a feeling for exactly what it means to say that the running time of an algorithm i\u00aes O(f(n)) or Theta(f(n)) for some common functions f(n). The main point is that these notations do not tell you anything about the actual numerical value of the running time of the algorithm for any par\u00aeticular case. They do not tell you anything at all about the running time for small values of n. What they do tell you is something about the rate of growth of the running time as the size of the problem increases. Suppose you compare two algorithm that solve the same problem. The run tim\u00aee of one algorithm is Theta(n2), while the run time of the second algorithm is Theta(n3). What does this tell you? If you want to know which algorithm will be faster for some particular problem of size, say, 100, nothing is certain. As far as you can tell just from the asymptotic analysis, either algorithm could be faster for th\u00aeat particular case or in any particular case. But what you can say for sure is that if you look at larger and larger problems, you will come to a point where the Theta(n2) algorithm is faster than the Theta(n3) algorithm. Furthermore, as you continue to increase the problem size, the relative advantage of the Theta(n2) algorithm will continue to grow. There will be values of n for which the Theta(n2) algorithm is a thousand times faster, a million times faster, a billion times faster, and so on. This is because for any positive constants a and b, the function a*n3 grows faster than the function b*n2 as n gets larger. (Mathematically, the limit of the ratio of a*n3 to b*n2 is infinite as n approaches infinity.) This means that for \"large\" problems, a Theta(n2) algorithm will definitely be faster than a Theta(n3) algorithm. You just don't know based on the asymptotic analysis alone exactly how large \"large\" has to be. In practice, in fact, it is likely that the Theta(n2) algorithm will be faster even for fairly small values of n, and absent other information you would generally prefer a Theta(n2) algorithm to a Theta(n3) algorithm.",
  "page212": "Recursion- At one time or another, you've probably been told that you can't define something in terms of itself. Nevertheless, if it's done right, defining something\u00aeat least partially in terms of itself can be a very powerful technique. A recursive definition is one that uses the concept or thing that is being defined as part of the definition. For example: An \"ancestor\"\u00aeis either a parent or an ancestor of a parent. A \"sentence\" can be, among other things, two sentences joined by a conjunction such as \"and.\" A \"directory\" is a part of a di\u00aesk drive that can hold files and directories. In mathematics, a \"set\" is a collection of elements, which can themselves be sets. A \"statement\" in Java can be a while statement, which is made up of the word \"while\", a boolean-valued condition, and a statement.\u00aeRecursive definitions can describe very complex situations with just a few words. A definition of the term \"ancestor\" without using recursion might go something like \"a parent, or \"and so on\" is not very rigorous. (I've often thought that recursion is really just a rigorous way of saying \"and so\u00aeon.\") You run into the same problem if you try to define a \"directory\" as \"a file that is a list of files, where some of the files can be lists of files, where some of those files can be lists of files, and so on.\" Trying to describe what a Java statement can look like, without using recursion in the definition, would be difficult and probably pretty comical. Recursion can be used as a programming technique. A recursive subroutine is one that calls itself, either directly or indirectly. To say that a subroutine calls itself directly means that its definition contains a subroutine call statement that calls the subroutine that is being defined. To say that a subroutine calls itself indirectly means that it calls a second subroutine which in turn calls the first subroutine (either directly or indirectly). A recursive subroutine can define a complex task in just a few lines of code. In the rest of this section, we'll look at a variety of examples, and we'll see other examples in the rest of the book.",
  "page213": "Recursive Binary Search- Binary search is used to find a specified value in a sorted list of items (or, if it does not occur in the list, to determine that fact). The idea is to te\u00aest the element in the middle of the list. If that element is equal to the specified value, you are done. If the specified value is less than the middle element of the list, then you should search for the value in the fi\u00aerst half of the list. Otherwise, you should search for the value in the second half of the list. The method used to search for the value in the first or second half of the list is binary search. That is,\u00aeyou look at the middle element in the half of the list that is still under consideration, and either you've found the value you are looking for, or you have to apply binary search to one half of the remaining elements. And so on! This is a recursive description, and we can write a r\u00aeecursive subroutine to implement it. Before we can do that, though, there are two considerations that we need to take into account. Each of these illustrates an important general fact about recursive subroutines. First of all, the binary search algorithm begins by looking at the \"middle element of the list.\" But what if\u00aethe list is empty? If there are no elements in the list, then it is impossible to look at the middle element. In the terminology of Subsection 8.2.1, having a non-empty list is a \"precondition\" for looking at the middle element, and this is a clue that we have to modify the algorithm to take this precondition into account. What should we do if we find ourselves searching for a specified value in an empty list? The answer is easy: If the list is empty, we can be sure that the value does not occur in the list, so we can give the answer without any further work.",
  "page214": "Linked Data Structures- Every useful object contains instance variables. When the type of an instance variable is given by a class or interface name, the variable can hold a refere\u00aence to another object. Such a reference is also called a pointer, and we say that the variable points to the object. (Of course, any variable that can contain a reference to an object can also contain the special value\u00aenull, which points to nowhere.) When one object contains an instance variable that points to another object, we think of the objects as being \"linked\" by the pointer. Data structures of great c\u00aeomplexity can be constructed by linking objects together. Recursive Linking- Something interesting happens when an object contains an instance variable that can refer to another object of the same type. In that case, the definition of the object's class is recursive. Such recursion a\u00aerises naturally in many cases. For example, consider a class designed to represent employees at a company. Suppose that every employee except the boss has a supervisor, who is another employee of the company. As the while loop is executed, runner points in turn to the original employee, emp, then variable is incremented each time\u00aerunner \"visits\" a new employee. The loop ends when runner.supervisor is null, which indicates that runner has reached the boss. At that point, count has counted the number of steps between emp and the boss. In this example, the supervisor variable is quite natural and useful. In fact, data structures that are built by linking objects together are so useful that they are a major topic of study in computer science. We'll be looking at a few typical examples. In this section and the next, we'll be looking at linked lists. A linked list consists of a chain of objects of the same type, linked together by pointers from one object to the next. This is much like the chain of supervisors between emp and the boss in the above example. It's also possible to have more complex situations, in which one object can contain links to several other objects.",
  "page215": "Linked Lists- For most of the examples in the rest of this section, linked lists will be constructed out of objects belonging to the class Node which is defined as follows: class N\u00aeode { String item; Node next; } The term node is often used to refer to one of the objects in a linked data structure. Objects of type Node can be chained together as shown in the top part of the above picture. Each nod\u00aee holds a String and a pointer to the next node in the list (if any). The last node in such a list can always be identified by the fact that the instance variable next in the last node holds the value nu\u00aell instead of a pointer to another node. The purpose of the chain of nodes is to represent a list of strings. The first string in the list is stored in the first node, the second string is stored in the second node, and so on. The pointers and the node objects are used to build the struct\u00aeure, but the data that we are interested in representing is the list of strings. Of course, we could just as easily represent a list of integers or a list of JButtons or a list of any other type of data by changing the type of the item that is stored in each node. Although the Nodes in this example are very simple, we can use them\u00aeto illustrate the common operations on linked lists. Typical operations include deleting nodes from the list, inserting new nodes into the list, and searching for a specified String among the items in the list. We will look at subroutines to perform all of these operations, among others. For a linked list to be used in a program, that program needs a variable that refers to the first node in the list. It only needs a pointer to the first node since all the other nodes in the list can be accessed by starting at the first node and following links along the list from one node to the next. In my examples, I will always use a variable named head, of type Node, that points to the first node in the linked list. When the list is empty, the value of head is null.",
  "page216": "Stacks, Queues, and ADTs- A linked list is a particular type of data structure, made up of objects linked together by pointers. In the previous section, we used a linked list to st\u00aeore an ordered list of Strings, and we implemented insert, delete, and find operations on that list. However, we could easily have stored the list of Strings in an array or ArrayList, instead of in a linked list. We cou\u00aeld still have implemented the same operations on the list. The implementations of these operations would have been different, but their interfaces and logical behavior would still be the same. The term a\u00aebstract data type, or ADT, refers to a set of possible values and a set of operations on those values, without any specification of how the values are to be represented or how the operations are to be implemented. An \"ordered list of strings\" can be defined as an abstract data t\u00aeype. Any sequence of Strings that is arranged in increasing order is a possible value of this data type. The operations on the data type include inserting a new string, deleting a string, and finding a string in the list. There are often several different ways to implement the same abstract data type. For example, the \"ordere\u00aed list of strings\" ADT can be implemented as a linked list or as an array. A program that only depends on the abstract definition of the ADT can use either implementation, interchangeably. In particular, the implementation of the ADT can be changed without affecting the program as a whole. This can make the program easier to debug and maintain, so ADTs are an important tool in software engineering. In this section, we'll look at two common abstract data types, stacks and queues. Both stacks and queues are often implemented as linked lists, but that is not the only possible implementation. You should think of the rest of this section partly as a discussion of stacks and queues and partly as a case study in ADTs.",
  "page217": "Stacks- A stack consists of a sequence of items, which should be thought of as piled one on top of the other like a physical stack of boxes or cafeteria trays. Only the top item on\u00aethe stack is accessible at any given time. It can be removed from the stack with an operation called pop. An item lower down on the stack can only be removed after all the items on top of it have been popped off the st\u00aeack. A new item can be added to the top of the stack with an operation called push. We can make a stack of any type of items. If, for example, the items are values of type int, then the push and pop oper\u00aeations can be implemented as instance methods void push (int newItem) Add newItem to top of stack. int pop() Remove the top int from the stack and return it. It is an error to try to pop an item from an empty stack, so it is important to be able to tell whether a stack is empty. We need a\u00aenother stack operation to do the test, implemented as an instance method boolean isEmpty() Returns true if the stack is empty. To get a better handle on the difference between stacks and queues, consider the sample program DepthBreadth.java. I suggest that you run the program or try the applet version that can be found in the on-l\u00aeine version of this section. The program shows a grid of squares. Initially, all the squares are white. When you click on a white square, the program will gradually mark all the squares in the grid, starting from the one where you click. To understand how the program does this, think of yourself in the place of the program. When the user clicks a square, you are handed an index card. The location of the square its row and column is written on the card. You put the card in a pile, which then contains just that one card. Then, you repeat the following: If the pile is empty, you are done. Otherwise, take an index card from the pile. The index card specifies a square. Look at each horizontal and vertical neighbor of that square. If the neighbor has not already been encountered, write its location on a new index card and put the card in the pile.",
  "page218": "While a square is in the pile, waiting to be processed, it is colored red; that is, red squares have been encountered but not yet processed. When a square is taken from the pile an\u00aed processed, its color changes to gray. Once a square has been colored gray, its color won't change again. Eventually, all the squares have been processed, and the procedure ends. In the index card analogy, the pil\u00aee of cards has been emptied. The program can use your choice of three methods: Stack, Queue, and Random. In each case, the same general procedure is used. The only difference is how the \"pile of ind\u00aeex cards\" is managed. For a stack, cards are added and removed at the top of the pile. For a queue, cards are added to the bottom of the pile and removed from the top. In the random case, the card to be processed is picked at random from among all the cards in the pile. The order of\u00aeprocessing is very different in these three cases. You should experiment with the program to see how it all works. Try to understand how stacks and queues are being used. Try starting from one of the corner squares. While the process is going on, you can click on other white squares, and they will be added to the pile. When you do\u00aethis with a stack, you should notice that the square you click is processed immediately, and all the red squares that were already waiting for processing have to wait. On the other hand, if you do this with a queue, the square that you click will wait its turn until all the squares that were already in the pile have been processed. Queues seem very natural because they occur so often in real life, but there are times when stacks are appropriate and even essential. For example, consider what happens when a routine calls a subroutine. The first routine is suspended while the subroutine is executed, and it will continue only when the subroutine returns. Now, suppose that the subroutine calls a second subroutine, and the second subroutine calls a third, and so on. Each subroutine is suspended while the subsequent subroutines are executed. The computer has to keep track of all the subroutines that are suspended. It does this with a stack.",
  "page219": "When a subroutine is called, an activation record is created for that subroutine. The activation record contains information relevant to the execution of the subroutine, such as it\u00aes local variables and parameters. The activation record for the subroutine is placed on a stack. It will be removed from the stack and destroyed when the subroutine returns. If the subroutine calls another subroutine, t\u00aehe activation record of the second subroutine is pushed onto the stack, on top of the activation record of the first subroutine. The stack can continue to grow as more subroutines are called, and it shri\u00aenks as those subroutines return. Postfix Expressions As another example, stacks can be used to evaluate postfix expressions. An ordinary mathematical expression such as 2+(15-12)*17 is called an infix expression. In an infix expression, an operator comes in between its two operands, as in\u00ae\"2 + 2\". In a postfix expression, an operator comes after its two operands, as in \"2 2 +\". The infix expression \"2+(15-12)*17\" would be written in postfix form as \"2 15 12 - 17 * +\". The \"-\" operator in this expression applies to the two operands that precede it, namely \"15\u00ae201d and \"12\". The \"*\" operator applies to the two operands that precede it, namely \"15 12 -\" and \"17\". And the \"+\" operator applies to \"2\" and \"15 12 - 17 *\". These are the same computations that are done in the original infix expression. Now, suppose that we want to process the expression \"2 15 12 - 17 * +\", from left to right and find its value. The first item we encounter is the 2, but what can we do with it? At this point, we don't know what operator, if any, will be applied to the 2 or what the other operand might be. We have to remember the 2 for later processing. We do this by pushing it onto a stack. Moving on to the next item, we see a 15, which is pushed onto the stack on top of the 2. Then the 12 is added to the stack. Now, we come to the operator, \"-\". This operation applies to the two operands that preceded it in the expression. We have saved those two operands on the stack. So, to process the \"-\" operator, we pop two numbers from the stack, 12 and 15, and compute 15 - 12 to get the answer 3.",
  "page220": "These numbers are multiplied, and the result, 51 is pushed onto the stack. The next item in the expression is a \"+\" operator, which is processed by popping 51 and 2 from\u00aethe stack, adding them, and pushing the result, 53, onto the stack. Finally, we've come to the end of the expression. The number on the stack is the value of the entire expression, so all we have to do is pop the a\u00aenswer from the stack, and we are done! The value of the expression is 53. Tree Traversal- Consider any node in a binary tree. Look at that node together with all its descendents (that is, its children, t\u00aehe children of its children, and so on). This set of nodes forms a binary tree, which is called a subtree of the original tree. For example, in the picture, nodes 2, 4, and 5 form a subtree. This subtree is called the left subtree of the root. Similarly, nodes 3 and 6 make up the right su\u00aebtree of the root. We can consider any non-empty binary tree to be made up of a root node, a left subtree, and a right subtree. Either or both of the subtrees can be empty. This is a recursive definition, matching the recursive definition of the TreeNode class. So it should not be a surprise that recursive subroutines are often us\u00aeed to process trees. Consider the problem of counting the nodes in a binary tree. (As an exercise, you might try to come up with a non-recursive algorithm to do the counting, but you shouldn't expect to find one.) The heart of problem is keeping track of which nodes remain to be counted. It's not so easy to do this, and in fact it's not even possible without an auxiliary data structure such as a stack or queue. With recursion, however, the algorithm is almost trivial. Either the tree is empty or it consists of a root and two subtrees. If the tree is empty, the number of nodes is zero. (This is the base case of the recursion.) Otherwise, use recursion to count the nodes in each subtree. Add the results from the subtrees together, and add one to count the root.",
  "page221": "Binary Sort Trees- One of the examples in Section 9.2 was a linked list of strings, in which the strings were kept in increasing order. While a linked list works well for a small n\u00aeumber of strings, it becomes inefficient for a large number of items. When inserting an item into the list, searching for that item's position requires looking at, on average, half the items in the list. Finding an\u00aeitem in the list requires a similar amount of time. If the strings are stored in a sorted array instead of in a linked list, then searching becomes more efficient because binary search can be used. Howe\u00aever, inserting a new item into the array is still inefficient since it means moving, on average, half of the items in the array to make a space for the new item. A binary tree can be used to store an ordered list of strings, or other items, in a way that makes both searching and insertion\u00aeefficient. A binary tree used in this way is called a binary sort tree. A binary sort tree is a binary tree with the following property: For every node in the tree, the item in that node is greater than every item in the left subtree of that node, and it is less than or equal to all the items in the right subtree of that node. He\u00aere for example is a binary sort tree containing items of type String. (In this picture, I haven't bothered to draw all the pointer variables. Non-null pointers are shown as arrows.)",
  "page222": "Binary sort trees have this useful property: An inorder traversal of the tree will process the items in increasing order. In fact, this is really just another way of expressing the\u00aedefinition. For example, if an inorder traversal is used to print the items in the tree shown above, then the items will be in alphabetical order. The definition of an inorder traversal guarantees that all the items in\u00aethe left subtree of \"judy\" are printed before \"judy\", and all the items in the right subtree of \"judy\" are printed after \"judy\". But the binary sort tree property\u00aeguarantees that the items in the left subtree of \"judy\" are precisely those that precede \"judy\" in alphabetical order, and all the items in the right subtree follow \"judy\" in alphabetical order. So, we know that \"judy\" is output in its proper alpha\u00aebetical position. But the same argument applies to the subtrees. \"Bill\" will be output after \"alice\" and before \"fred\" and its descendents. \"Fred\" will be output after \"dave\" and before \"jane\" and \"joe\". And so on. Suppose that we want to search for a given item\u00aein a binary search tree. Compare that item to the root item of the tree. If they are equal, we're done. If the item we are looking for is less than the root item, then we need to search the left subtree of the root the right subtree can be eliminated because it only contains items that are greater than or equal to the root. Similarly, if the item we are looking for is greater than the item in the root, then we only need to look in the right subtree. In either case, the same procedure can then be applied to search the subtree. Inserting a new item is similar: Start by searching the tree for the position where the new item belongs. When that position is found, create a new node and attach it to the tree at that position.",
  "page223": "Searching and inserting are efficient operations on a binary search tree, provided that the tree is close to being balanced. A binary tree is balanced if for each node, the left su\u00aebtree of that node contains approximately the same number of nodes as the right subtree. In a perfectly balanced tree, the two numbers differ by at most one. Not all binary trees are balanced, but if the tree is created\u00aeby inserting items in a random order, there is a high probability that the tree is approximately balanced. (If the order of insertion is not random, however, it's quite possible for the tree to be\u00aevery unbalanced.) During a search of any binary sort tree, every comparison eliminates one of two subtrees from further consideration. If the tree is balanced, that means cutting the number of items still under consideration in half. This is exactly the same as the binary search algorithm\u00ae, and the result, is a similarly efficient algorithm. In terms of asymptotic analysis, searching, inserting, and deleting in a binary search tree have average case run time Theta(log(n)). The problem size, n, is the number of items in the tree, and the average is taken over all the different orders in which the items could have b\u00aeeen inserted into the tree. As long the actual insertion order is random, the actual run time can be expected to be close to the average. However, the worst case run time for binary search tree operations is Theta(n), which is much worse than Theta(log(n)). The worst case occurs for certain particular insertion orders. For example, if the items are inserted into the tree in order of increasing size, then every item that is inserted moves always to the right as it moves down the tree. The result is a \"tree\" that looks more like a linked list, since it consists of a linear string of nodes strung together by their right child pointers. Operations on such a tree have the same performance as operations on a linked list. Now, there are data structures that are similar to simple binary sort trees, except that insertion and deletion of nodes are implemented in a way that will always keep the tree balanced, or almost balanced. For these data structures, searching, inserting, and deleting have both average case and worst case run times that are Theta(log(n)).",
  "page224": "Backus-Naur Form- Natural and artificial languages are similar in that they have a structure known as grammar or syntax. Syntax can be expressed by a set of rules that describe wha\u00aet it means to be a legal sentence or program. For programming languages, syntax rules are often expressed in BNF (Backus-Naur Form), a system that was developed by computer scientists John Backus and Peter Naur in the l\u00aeate 1950s. Interestingly, an equivalent system was developed independently at about the same time by linguist Noam Chomsky to describe the grammar of natural language. BNF cannot express all possible syn\u00aetax rules. For example, it can't express the fact that a variable must be defined before it is used. Furthermore, it says nothing about the meaning or semantics of the langauge. The problem of specifying the semantics of a language even of an artificial programming langauge is one th\u00aeat is still far from being completely solved. However, BNF does express the basic structure of the language, and it plays a central role in the design of translation programs. Files- The data and programs in a computer's main memory survive only as long as the power is on. For more permanent storage, computers use files, whic\u00aeh are collections of data stored on a hard disk, on a USB memory stick, on a CD-ROM, or on some other type of storage device. Files are organized into directories (sometimes called folders). A directory can hold other directories, as well as files. Both directories and files have names that are used to identify them. Programs can read data from existing files. They can create new files and can write data to files. In Java, such input and output can be done using streams. Human-readable character data is read from a file using an object belonging to the class FileReader, which is a subclass of Reader. Similarly, data is written to a file in human-readable format through an object of type FileWriter, a subclass of Writer. For files that store data in machine format, the appropriate I/O classes are FileInputStream and FileOutputStream. In this section, I will only discuss character oriented file I/O using the FileReader and FileWriter classes. However, FileInputStream and FileOutputStream are used in an exactly parallel fashion. All these classes are defined in the java.io package.",
  "page225": "Word Counting- The final example in this section also deals with storing information about words. The problem here is to make a list of all the words that occur in a file, along wi\u00aeth the number of times that each word occurs. The file will be selected by the user. The output of the program will consist of two lists. Each list contains all the words from the file, along with the number of times th\u00aeat the word occurred. One list is sorted alphabetically, and the other is sorted according to the number of occurrences, with the most common words at the top and the least common at the bottom. The prob\u00aelem here is a generalization, which asked you to make an alphabetical list of all the words in a file, without counting the number of occurrences. Symbol Tables- We begin with a straightforward but important application of maps. When a compiler reads the source code of a program, it encou\u00aenters definitions of variables, subroutines, and classes. The names of these things can be used later in the program. The compiler has to remember the name is encountered later in the program. This is a natural application for a Map. The name can be used as a key in the map. The value associated to the key is the definition of the\u00aename, encoded somehow as an object. A map that is used in this way is called a symbol table. In a compiler, the values in a symbol table can be quite complicated, since the compiler has to deal with names for various sorts of things, and it needs a different type of information for each different type of name. We will keep things simple by looking at a symbol table in another context. Suppose that we want a program that can evaluate expressions entered by the user, and suppose that the expressions can contain variables, in addition to operators, numbers, and parentheses. For this to make sense, we need some way of assigning values to variables. When a variable is used in an expression, we need to retrieve the variable's value. A symbol table can be used to store the data that we need. The keys for the symbol table are variable names. The value associated with a key is the value of that variable, which is of type double.",
  "page226": "Almost all applications require persistent data. Persistence is one of the fundamental concepts in application development. If an information system didn't preserve data when\u00aeit was powered off, the system would be of little practical use. Object persistence means individual objects can outlive the application process; they can be saved to a data store and be re-created at a later point in t\u00aeime. When we talk about persistence in Java, we're normally talking about mapping and storing object instances in a database using SQL. We start by taking a brief look at the technology and how it uses in java\u00aeArmed with this information, we then continue our discussion of persistence and how it's implemented in object-oriented applications. You, like most other software engineers, have probably worked with SQL and relational databases; many of us handle such systems every day.\u00aeRelational database management systems have SQL-based application programming interfaces; hence, we call today's relational database products SQL database management systems (DBMS) or, when we're talking about particular systems, SQL databases. Relational technology is a known quantity, and this alone is sufficient rea\u00aeson for many organizations to choose it. But to say only this is to pay less respect than is due. Relational databases are entrenched because they're an incredibly flexible and robust approach to data management. Due to the well-researched theoretical foundation of the relational data model, relational databases can guarantee and protect the integrity of the stored data, among other desirable characteristics. You may be familiar with E.F. Codd's four-decades-old introduction of the relational model, A Relational Model of Data for Large Shared Data Banks (Codd, 1970). A more recent compendium worth reading, with a focus on SQL, is C. J. Date's SQL and Relational Theory (Date, 2009).  Relational DBMSs aren't specific to Java, nor is an SQL database specific to a particular application. This important principle is known as data independence. In other words, and we can't stress this important fact enough, data lives longer than any application does.",
  "page227": "Before we go into more detail about the practical aspects of SQL databases, we have to mention an important issue: although marketed as relational, a database system providing only\u00aean SQL data language interface isn't really relational and in many ways isn't even close to the original concept. Naturally, this has led to confusion. SQL practitioners blame the relational data model for sh\u00aeortcomings in the SQL language, and relational data management experts blame the SQL standard for being a weak implementation of the relational model and ideals. Application engineers are stuck somewhere\u00aein the middle, with the burden of delivering something that works. We highlight some important and significant aspects of this issue throughout this book, but generally we focus on the practical aspects. If you're interested in more background material, we highly recommend Practical\u00aeIssues in Database Management: A Reference for the Thinking Practitioner by Fabian Pascal (Pascal, 2000) and An Introduction to Database Systems by Chris Date (Date, 2003) for the theory, concepts, and ideals of (relational) database systems. The latter book is an excellent reference (it's big) for all questions you may poss\u00aeibly have about databases and data management. To use Hibernate effectively, you must start with a solid understanding of the relational model and SQL. You need to understand the relational model and topics such as normalization to guarantee the integrity of your data, and you'll need to use your knowledge of SQL to tune the performance of your Hibernate application. Hibernate automates many repetitive coding tasks, but your knowledge of persistence technology must extend beyond Hibernate itself if you want to take advantage of the full power of modern SQL databases. To dig deeper, consult the bibliography at the end of this book.",
  "page228": "You've probably used SQL for many years and are familiar with the basic operations and statements written in this language. Still, we know from our own experience that SQL is\u00aesometimes hard to remember, and some terms vary in usage.  Let's review some of the SQL terms used in this book. You use SQL as a data definition language (DDL) when creating, altering, and dropping artifacts such\u00aeas tables and constraints in the catalog of the DBMS. When this schema is ready, you use SQL as a data manipulation language (DML) to perform operations on data, including insertions, updates, and delet\u00aeions. You retrieve data by executing queries with restrictions, projections, and Cartesian products. For efficient reporting, you use SQL to join, aggregate, and group data as necessary. You can even nest SQL statements inside each other a technique that uses subselects. When your busines\u00aes requirements change, you'll have to modify the database schema again with DDL statements after data has been stored; this is known as schema evolution.  If you're an SQL veteran and you want to know more about optimization and how SQL is executed, get a copy of the excellent book SQL Tuning, by Dan Tow (Tow, 2003). Fo\u00aer a look at the practical side of SQL through the lens of how not to use SQL, SQL Antipatterns: Avoiding the Pitfalls of Database Programming (Karwin, 2010) is a good resource.  Although the SQL database is one part of ORM, the other part, of course, consists of the data in your Java application that needs to be persisted to and loaded from the database.",
  "page229": "Using SQL in Java- When you work with an SQL database in a Java application, you issue SQL statements to the database via the Java Database Connectivity (JDBC) API. Whether the SQL\u00aewas written by hand and embedded in the Java code or generated on the fly by Java code, you use the JDBC API to bind arguments when preparing query parameters, executing the query, scrolling through the query result\u00aeretrieving values from the result set, and so on. These are low-level data access tasks; as application engineers, we're more interested in the business problem that requires this data access. What we\u00aereally like to write is code that saves and retrieves instances of our classes, relieving us of this lowlevel drudgery. Because these data access tasks are often so tedious, we have to ask, are the relational data model and (especially) SQL the right choices for persistence in objectorien\u00aeted applications? We answer this question unequivocally: yes! There are many reasons why SQL databases dominate the computing industry relational database management systems are the only proven generic data management technology, and they're almost always a requirement in Java projects.  Note that we aren't claiming tha\u00aet relational technology is always the best solution. There are many data management requirements that warrant a completely different approach. For example, internet-scale distributed systems (web search engines, content distribution networks, peer-to-peer sharing, instant messaging) have to deal with exceptional transaction volumes. Many of these systems don't require that after a data update completes, all processes see the same updated data (strong transactional consistency). Users might be happy with weak consistency; after an update, there might be a window of inconsistency before all processes see the updated data. Some scientific applications work with enormous but very specialized datasets. Such systems and their unique challenges typically require equally unique and often custom-made persistence solutions. Generic data management tools such as ACID-compliant transactional SQL databases, JDBC, and Hibernate would play only a minor role.",
  "page230": "ORM and JPA- In a nutshell, object/relational mapping is the automated (and transparent) persistence of objects in a Java application to the tables in an SQL database, using metada\u00aeta that describes the mapping between the classes of the application and the schema of the SQL database. In essence, ORM works by transforming (reversibly) data from one representation to another. Before we move on, you\u00aeneed to understand what Hibernate can't do for you. A supposed advantage of ORM is that it shields developers from messy SQL. This view holds that object-oriented developers can't be expected\u00aeto understand SQL or relational databases well and that they find SQL somehow offensive. On the contrary, we believe that Java developers must have a sufficient level of familiarity with and appreciation of relational modeling and SQL in order to work with Hibernate. ORM is an advanced t\u00aeechnique used by developers who have already done it the hard way. To use Hibernate effectively, you must be able to view and interpret the SQL statements it issues and understand their performance implications Let's look at some of the benefits of Hibernate: Productivity Hibernate eliminates much of the grunt work (more than\u00aeyou'd expect) and lets you concentrate on the business problem. No matter which application-development strategy you prefer top-down, starting with a domain model, or bottom-up, starting with an existing database schema Hibernate, used together with the appropriate tools, will significantly reduce development time. Maintainability Automated ORM with Hibernate reduces lines of code (LOC), making the system more understandable and easier to refactor.",
  "page231": "The Hibernate approach to persistence was well received by Java developers, and the standard Java Persistence API was designed along similar lines. JPA became a key part of the si\u00aemplifications introduced in recent EJB and Java EE specifications. We should be clear up front that neither Java Persistence nor Hibernate are limited to the Java EE environment; they're general-purpose solutions\u00aeto the persistence problem that any type of Java (or Groovy, or Scala) application can use.  The JPA specification defines the following: A facility for specifying mapping metadata how persistent classes\u00aeand their properties relate to the database schema. JPA relies heavily on Java annotations in domain model classes, but you can also write mappings in XML files. APIs for performing basic CRUD operations on instances of persistent classes, most prominently javax.persistence.EntityManager\u00aeto store and load data. A language and APIs for specifying queries that refer to classes and properties of classes. This language is the Java Persistence Query Language (JPQL) and looks similar to SQL. The standardized API allows for programmatic creation of criteria queries without string manipulation. How the persistence engine\u00aeinteracts with transactional instances to perform dirty checking, association fetching, and other optimization functions. The latest JPA specification covers some basic caching strategies. Hibernate implements JPA and supports all the standardized mappings, queries, and programming interfaces.",
  "page232": "With object persistence, individual objects can outlive their application process, be saved to a data store, and be re-created later. The object/relational mismatch comes into play\u00aewhen the data store is an SQL-based relational database management system. For instance, a network of objects can't be saved to a database table; it must be disassembled and persisted to columns of portable SQL\u00aedata types. A good solution for this problem is object/relational mapping (ORM). ORM isn't a silver bullet for all persistence tasks; its job is to relieve the developer of 95% of object persistence w\u00aeork, such as writing complex SQL statements with many table joins and copying values from JDBC result sets to objects or graphs of objects. A full-featured ORM middleware solution may provide database portability, certain optimization techniques like caching, and other viable functions t\u00aehat aren't easy to hand-code in a limited time with SQL and JDBC. Better solutions than ORM might exist someday. We (and many others) may have to rethink everything we know about data management systems and their languages, persistence API standards, and application integration. But the evolution of today's systems into\u00aetrue relational database systems with seamless object-oriented integration remains pure speculation. We can't wait, and there is no sign that any of these issues will improve soon (a multibillion-dollar industry isn't very agile). ORM is the best solution currently available, and it's a timesaver for developers facing the object/relational mismatch every day.",
  "page233": "Hibernate is an ambitious project that aims to provide a complete solution to the problem of managing persistent data in Java. Today, Hibernate is not only an ORM service, but also\u00aea collection of data management tools extending well beyond ORM. The Hibernate project suite includes the following: Hibernate ORM Hibernate ORM consists of a core, a base service for persistence with SQL databases,\u00aeand a native proprietary API. Hibernate ORM is the foundation for several of the other projects and is the oldest Hibernate project. You can use Hibernate ORM on its own, independent of any framework or an\u00aey particular runtime environment with all JDKs. It works in every Java EE/J2EE application server, in Swing applications, in a simple servlet container, and so on. As long as you can configure a data source for Hibernate, it works. Hibernate EntityManager This is Hibernate's implemen\u00aetation of the standard Java Persistence APIs, an optional module you can stack on top of Hibernate ORM. You can fall back to Hibernate when a plain Hibernate interface or even a JDBC Connection is needed. Hibernate's native features are a superset of the JPA persistence features in every respect. Hibernate Validator Hi\u00aebernate provides the reference implementation of the Bean Validation (JSR 303) specification. Independent of other Hibernate projects, it provides declarative validation for your domain model (or any other) classes. Hibernate Envers Envers is dedicated to audit logging and keeping multiple versions of data in your SQL database.",
  "page234": "The \"Hello World\" example in the previous chapter introduced you to Hibernate; certainly, it isn't useful for understanding the requirements of real-world applicatio\u00aens with complex data models. For the rest of the book, we use a much more sophisticated example application CaveatEmptor, an online auction system to demonstrate Hibernate and Java Persistence. (Caveat emptor means\u00aethe buyer beware\".) We'll start our discussion of the application by introducing a layered application architecture. Then, you'll learn how to identify the business entities of a problem domain\u00ae. You'll create a conceptual model of these entities and their attributes, called a domain model, and you'll implement it in Java by creating persistent classes. We'll spend some time exploring exactly what these Java classes should look like and where they fit within a typ\u00aeical layered application architecture. We'll also look at the persistence capabilities of the classes and how this aspect influences the design and implementation. We'll add Bean Validation, which helps to automatically verify the integrity of the domain model data not only for persistent information but all business log\u00aeic.  We'll then explore mapping metadata options the ways you tell Hibernate how your persistent classes and their properties relate to database tables and columns. This can be as simple as adding annotations directly in the Java source code of the classes or writing XML documents that you eventually deploy along with the compiled Java classes that Hibernate accesses at runtime. After reading this chapter, you'll know how to design the persistent parts of your domain model in complex real-world projects, and what mapping metadata option you'll primarily prefer and use. Let's start with the example application.",
  "page235": "The example CaveatEmptor application- The CaveatEmptor example is an online auction application that demonstrates ORM techniques and Hibernate functionality. You can download the s\u00aeource code for the application from www.jpwh.org. We won't pay much attention to the user interface in this book (it could be web based or a rich client); we'll concentrate instead on the data access code. Whe\u00aen a design decision about data access code that has consequences for the user interface has to be made, we'll naturally consider both.  In order to understand the design issues involved in ORM, let\u00ae's pretend the CaveatEmptor application doesn't yet exist and that you're building it from scratch. Let's start by looking at the architecture. A layered architecture- With any nontrivial application, it usually makes sense to organize classes by concern. Persistence i\u00aes one concern; others include presentation, workflow, and business logic. A typical object-oriented architecture includes layers of code that represent the concerns. A layered architecture defines interfaces between code that implements the various concerns, allowing changes to be made to the way one concern is implemented without\u00aesignificant disruption to code in the other layers. Layering determines the kinds of inter-layer dependencies that occur. The rules are as follows: Layers communicate from top to bottom. A layer is dependent only on the interface of the layer directly below it. Each layer is unaware of any other layers except for the layer just below it.",
  "page236": "The CaveatEmptor domain model The CaveatEmptor site auctions many different kinds of items, from electronic equipment to airline tickets. Auctions proceed according to the English\u00aeauction strategy: users continue to place bids on an item until the bid period for that item expires, and the highest bidder wins.  In any store, goods are categorized by type and grouped with similar goods into sectio\u00aens and onto shelves. The auction catalog requires some kind of hierarchy of item categories so that a buyer can browse these categories or arbitrarily search by category and item attributes. Lists of ite\u00aems appear in the category browser and search result screens. Selecting an item from a list takes the buyer to an item-detail view where an item may have images attached to it.  An auction consists of a sequence of bids, and one is the winning bid. User details include name, address, and\u00aebilling information.  The result of this analysis, the high-level overview of the domain model, is shown in figure 3.3. Let's briefly discuss some interesting features of this model.  Each item can be auctioned only once, so you don't need to make Item distinct from any auction entities. Instead, you have a single auct\u00aeion item entity named Item. Thus, Bid is associated directly with Item. You model the Address information of a User as a separate class, a User may have three addresses, for home, billing, and shipping. You do allow the user to have many BillingDetails. Subclasses of an abstract class represent the various billing strategies (allowing future extension).  The application may nest a Category inside another Category, and so on. A recursive association, from the Category entity to itself, expresses this relationship. Note that a single Category may have multiple child categories but at most one parent. Each Item belongs to at least one Category.  This representation isn't the complete domain model but only classes for which you need persistence capabilities. You'd like to store and load instances of Category, Item, User, and so on. We have simplified this high-level overview a little; we may introduce additional classes later or make minor modifications to them when needed for more complex examples.",
  "page237": "Implementing the domain model- You'll start with an issue that any implementation must deal with: the separation of concerns. The domain model implementation is usually a cent\u00aeral, organizing component; it's reused heavily whenever you implement new application functionality. For this reason, you should be prepared to go to some lengths to ensure that concerns other than business aspects\u00aedon't leak into the domain model implementation. When concerns such as persistence, transaction management, or authorization start to appear in the domain model classes, this is an example of leaka\u00aege of concerns. The domain model implementation is such an important piece of code that it shouldn't depend on orthogonal Java APIs. For example, code in the domain model shouldn't perform JNDI lookups or call the database via the JDBC API, not directly and not through an interm\u00aeediate abstraction. This allows you to reuse the domain model classes virtually anywhere: The presentation layer can access instances and attributes of domain model entities when rendering views. The controller components in the business layer can also access the state of domain model entities and call methods of the entities to\u00aeexecute business logic. The persistence layer can load and store instances of domain model entities from and to the database, preserving their state.",
  "page238": "Most important, preventing leakage of concerns makes it easy to unit-test the domain model without the need for a particular runtime environment or container, or the need for mocki\u00aeng any service dependencies. You can write unit tests that verify the correct behavior of your domain model classes without any special test harness. (We aren't talking about testing \"load from the database\u00aeand \"store in the database\" aspects, but \"calculate the shipping cost and tax\" behavior.) The Java EE standard solves the problem of leaky concerns with metadata, as annotations within\u00aeyour code or externalized as XML descriptors. This approach allows the runtime container to implement some predefined cross-cutting concerns security, concurrency, persistence, transactions, and remoteness in a generic way, by intercepting calls to application components.  Hibernate isn\u00aeu2019t a Java EE runtime environment, and it's not an application server. It's an implementation of just one specification under the Java EE umbrella JPA and a solution for just one of these concerns: persistence. JPA defines the entity class as the primary programming artifact. This programming model enables transparen\u00aet persistence, and a JPA provider such as Hibernate also offers automated persistence. Transparent and automated persistence- We use transparent to mean a complete separation of concerns between the persistent classes of the domain model and the persistence layer. The persistent classes are unaware of and have no dependency on the persistence mechanism. We use automatic to refer to a persistence solution (your annotated domain, the layer, and mechanism) that relieves you of handling low-level mechanical details, such as writing most SQL statements and working with the JDBC API.",
  "page239": "The Item class of the CaveatEmptor domain model, for example, shouldn't have any runtime dependency on any Java Persistence or Hibernate API. Furthermore: JPA doesn't re\u00aequire that any special superclasses or interfaces be inherited or implemented by persistent classes. Nor are any special classes used to implement attributes and associations. (Of course, the option to use both techniqu\u00aees is always there.) You can reuse persistent classes outside the context of persistence, in unit tests or in the presentation layer, for example. You can create instances in any runtime environment with\u00aethe regular Java new operator, preserving testability and reusability. In a system with transparent persistence, instances of entities aren't aware of the underlying data store; they need not even be aware that they're being persisted or retrieved. JPA externalizes persistence\u00aeconcerns to a generic persistence manager API. Hence, most of your code, and certainly your complex business logic, doesn't have to concern itself with the current state of a domain model entity instance in a single thread of execution.",
  "page240": "We regard transparency as a requirement because it makes an application easier to build and maintain. Transparent persistence should be one of the primary goals of any ORM solution\u00ae. Clearly, no automated persistence solution is completely transparent: Every automated persistence layer, including JPA and Hibernate, imposes some requirements on the persistent classes. For example, JPA requires that\u00aecollectionvalued attributes be typed to an interface such as java.util.Set or java.util.List and not to an actual implementation such as java.util.HashSet (this is a good practice anyway). Or, a JPA ent\u00aeity class has to have a special attribute, called the database identifier (which is also less of a restriction but usually convenient).  You now know why the persistence mechanism should have minimal impact on how you implement a domain model, and that transparent and automated persistence are required. Our preferred programming model to archive this is POJO.",
  "page241": "Around 10 years ago, many developers started talking about POJO, a back-to-basics approach that essentially revives JavaBeans, a component model for UI development, and reapplies i\u00aet to the other layers of a system. Several revisions of the EJB and JPA specifications brought us new lightweight entities, and it would be appropriate to call them persistence-capable JavaBeans. Java engineers often us\u00aee all these terms as synonyms for the same basic design approach.  You shouldn't be too concerned about what terms we use in this book; the ultimate goal is to apply the persistence aspect as trans\u00aeparently as possible to Java classes. Almost any Java class can be persistence-capable if you follow some simple practices. Let's see how this looks in code.  Writing persistence-capable classes Working with fine-grained and rich domain models is a major Hibernate objective. This is\u00aea reason we work with POJOs. In general, using fine-grained objects means more classes than tables.  A persistence-capable plain-old Java class declares attributes, which represent state, and business methods, which define behavior. Some attributes represent associations to other persistence-capable classes.",
  "page242": "JPA doesn't require that persistent classes implement java.io.Serializable. But when instances are stored in an HttpSession or passed by value using RMI, serialization is nece\u00aessary. Although this might not occur in your application, the class will be serializable without any additional work, and there are no downsides to declaring that. (We aren't going to declare it on every example, a\u00aessuming that you know when it will be necessary.)  The class can be abstract and, if needed, extend a non-persistent class or implement an interface. It must be a top-level class, not nested within anot\u00aeher class. The persistence-capable class and any of its methods can't be final (a requirement of the JPA specification).  Unlike the JavaBeans specification, which requires no specific constructor, Hibernate (and JPA) require a constructor with no arguments for every persistent clas\u00aes. Alternatively, you might not write a constructor at all; Hibernate will then use the Java default constructor. Hibernate calls classes using the Java reflection API on such a no argument constructor to create instances. The constructor may not be public, but it has to be at least package-visible if Hibernate will use runtime-ge\u00aenerated proxies for performance optimization. Also, consider the requirements of other specifications: the EJB standard requires public visibility on session bean constructors, just like the JavaServer Faces (JSF) specification requires for its managed beans. There are other situations when you'd want a public constructor to create an \"empty\" state: for example, query-by-example building",
  "page243": "The properties of the POJO implement the attributes of the business entities for example, the username of User. You usually implement properties as private or protected member fiel\u00aeds, together with public or protected property accessor methods: for each field a method for retrieving its value and a method for setting the value. These methods are known as the getter and setter, respectively. The e\u00aexample POJO in listing 3.1 declares getter and setter methods for the username property.  The JavaBean specification defines the guidelines for naming accessor methods; this allows generic tools like Hi\u00aebernate to easily discover and manipulate property values. A getter method name begins with get, followed by the name of the property (the first letter in uppercase); a setter method name begins with set and similarly is followed by the name of the property. You may begin getter methods f\u00aeor Boolean properties with is instead of get. Hibernate doesn't require accessor methods. You can choose how the state of an instance of your persistent classes should be persisted. Hibernate will either directly access fields or call accessor methods. Your class design isn't disturbed much by these considerations. You\u00aecan make some accessor methods non-public or completely remove them then configure Hibernate to rely on field access for these properties.",
  "page244": "Should property fields and accessor methods be private, protected, or package visible? Typically, you want to discourage direct access to the internal state of your class, so you\u00aedon't make attribute fields public. If you make fields or methods private, you're effectively declaring that nobody should ever access them; only you're allowed to do that (or a service like Hibernate). T\u00aehis is a definitive statement. There are often good reasons for someone to access your \"private\" internals usually to fix one of your bugs and you only make people angry if they have to fall ba\u00aeck to reflection access in an emergency. Instead, you might assume or know that the engineer who comes after you has access to your code and knows what they're doing. The protected visibility then is a more reasonable default. You're forbidding direct public access, indicating\u00aethat this particular member detail is internal, but allowing access by subclasses if need be. You trust the engineer who creates the subclass. Package visibility is rude: you're forcing someone to create code in the same package to access member fields and methods; this is extra work for no good reason. Most important, these\u00aerecommendations for visibility are relevant for environments without security policies and a runtime SecurityManager. If you have to keep your internal code private, make it private.",
  "page245": "Implementing POJO associations- You'll now see how to associate and create different kinds of relationships between objects: one-to-many, many-to-one, and bidirectional relati\u00aeonships. We'll look at the scaffolding code needed to create these associations, how to simplify relationship management, and how to enforce the integrity of these relationships. Shouldn't bids on an item be\u00aestored in a list? The first reaction is often to preserve the order of elements as they're entered by users, because this may also be the order in which you will show them later. Certainly, in an au\u00aection application there has to be some defined order in which the user sees bids for an item for example, highest bid first or newest bid last. You might even work with a java.util.List in your user interface code to sort and display bids of an item. That doesn't mean this display or\u00aeder should be durable; data integrity isn't affected by the order in which bids are displayed. You need to store the amount of each bid, so you can find the highest bid, and you need to store a timestamp for each bid when it's created, so you can find the newest bid. When in doubt, keep your system flexible and sort the\u00aedata when it's retrieved from the datastore (in a query) and/or shown to the user (in Java code), not when it's stored.",
  "page246": "The addBid() method not only reduces the lines of code when dealing with Item and Bid instances, but also enforces the cardinality of the association. You avoid errors that arise f\u00aerom leaving out one of the two required actions. You should always provide this kind of grouping of operations for associations, if possible. If you compare this with the relational model of foreign keys in an SQL datab\u00aease, you can easily see how a network and pointer model complicates a simple operation: instead of a declarative constraint, you need procedural code to guarantee data integrity.  Because you want addBi\u00aed() to be the only externally visible mutator method for the bids of an item (possibly in addition to a removeBid() method), you can make the Item#setBids() method private or drop it and configure Hibernate to directly access fields for persistence. Consider making the Bid#setItem() metho\u00aed package-visible, for the same reason. The Item#getBids() getter method still returns a modifiable collection, so clients can use it to make changes that aren't reflected on the inverse side. Bids added directly to the collection wouldn't have a reference to an item an inconsistent state, according to your database co\u00aenstraints. To prevent this, you can wrap the internal collection before returning it from the getter method, with Collections.unmodifiableCollection(c) and Collections.unmodifiableSet(s). The client then gets an exception if it tries to modify the collection; you therefore force every modification to go through the relationship management method that guarantees integrity. Note that in this case you'll have to configure Hibernate for field access, because the collection",
  "page247": "There are several problems with this approach. First, Hibernate can't call this constructor. You need to add a no-argument constructor for Hibernate, and it needs to be at lea\u00aest package-visible. Furthermore, because there is no setItem() method, Hibernate would have to be configured to access the item field directly. This means the field can't be final, so the class isn't guarantee\u00aed to be immutable.  In the examples in this book, we'll sometimes write scaffolding methods such as the Item#addBid() shown earlier, or we may have additional constructors for required values. It\u00aeup to you how many convenience methods and layers you want to wrap around the persistent association properties and/or fields, but we recommend being consistent and applying the same strategy to all your domain model classes. For the sake of readability, we won't always show convenienc\u00aee methods, special constructors, and other such scaffolding in future code samples and assume you'll add them according to your own taste and requirements.  You now have seen domain model classes, how to represent their attributes, and the relationships between them. Next, we'll increase the level of abstraction, adding\u00aemetadata to the domain model implementation and declaring aspects such as validation and persistence rules.",
  "page248": "Domain model metadata- Metadata is data about data, so domain model metadata is information about your domain model. For example, when you use the Java reflection API to discover t\u00aehe names of classes of your domain model or the names of their attributes, you're accessing domain model metadata. ORM tools also require metadata, to specify the mapping between classes and tables, properties and\u00aecolumns, associations and foreign keys, Java types and SQL types, and so on. This object/relational mapping metadata governs the transformation between the different type systems and relationship repres\u00aeentations in objectoriented and SQL systems. JPA has a metadata API, which you can call to obtain details about the persistence aspects of your domain model, such as the names of persistent entities and attributes. First, it's your job as an engineer to create and maintain this infor\u00aemation. JPA standardizes two metadata options: annotations in Java code and externalized XML descriptor files. Hibernate has some extensions for native functionality, also available as annotations and/or XML descriptors. Usually we prefer either annotations or XML files as the primary source of mapping metadata. After reading thi\u00aes section, you'll have the background information to make an educated decision for your own project.",
  "page249": "We'll also discuss Bean Validation (JSR 303) and how it provides declarative validation for your domain model (or any other) classes. The reference implementation of this spec\u00aeification is the Hibernate Validator project. Most engineers today prefer Java annotations as the primary mechanism for declaring metadata. Applying Bean Validation rules Most applications contain a multitude of data-in\u00aetegrity checks. You've seen what happens when you violate one of the simplest data-integrity constraints: you get a NullPointerException when you expect a value to be available. Other examples are a\u00aestring-valued property that shouldn't be empty (remember, an empty string isn't null), a string that has to match a particular regular expression pattern, and a number or date value that must be within a certain range.  These business rules affect every layer of an application\u00ae: The user interface code has to display detailed and localized error messages. The business and persistence layers must check input values received from the client before passing them to the datastore. The SQL database has to be the final validator, ultimately guaranteeing the integrity of durable data.",
  "page250": "The idea behind Bean Validation is that declaring rules such as \"This property can't be null\" or \"This number has to be in the given range\" is much easier\u00aeand less error-prone than writing if-then-else procedures repeatedly. Furthermore, declaring these rules on the central component of your application, the domain model implementation, enables integrity checks in every l\u00aeayer of the system. The rules are then available to the presentation and persistence layers. And if you consider how dataintegrity constraints affect not only your Java application code but also your SQL\u00aedatabase schema which is a collection of integrity rules you might think of Bean Validation constraints as additional ORM metadata You add two more attributes the name of an item and the auctionEnd date when an auction concludes. Both are typical candidates for additional constraints: yo\u00aeu want to guarantee that the name is always present and human readable (one-character item names don't make much sense), but it shouldn't be too long your SQL database will be most efficient with variable-length strings up to 255 characters, and your user interface also has some constraints on visible label space. The en\u00aeding time of an auction obviously should be in the future. If you don't provide an error message, a default message will be used. Messages can be keys to external properties files, for internationalization.",
  "page251": "The validation engine will access the fields directly if you annotate the fields. If you prefer calls through accessor methods, annotate the getter method with validation constrain\u00aets, not the setter. Then constraints are part of the class's API and included in its Javadoc, making the domain model implementation easier to understand. Note that this is independent from access by the JPA provid\u00aeer; that is, Hibernate Validator may call accessor methods, whereas Hibernate ORM may call fields directly.  Bean Validation isn't limited to the built-in annotations; you can create your own const\u00aeraints and annotations. With a custom constraint, you can even use class-level annotations and validate several attribute values at the same time on an instance of the class. The following test code shows how you can manually check the integrity of an Item instance. We're not going t\u00aeo explain this code in detail but offer it for you to explore. You'll rarely write this kind of validation code; most of the time, this aspect is automatically handled by your user interface and persistence framework. It's therefore important to look for Bean Validation integration when selecting a UI framework. JSF vers\u00aeion 2 and newer automatically integrates with Bean Validation, for example.  Hibernate, as required from any JPA provider, also automatically integrates with Hibernate Validator if the libraries are available on the classpath and offers the following features:",
  "page252": "You don't have to manually validate instances before passing them to Hibernate for storage. Hibernate recognizes constraints on persistent domain model classes and trig\u00aegers validation before database insert or update operations. When validation fails, Hibernate throws a ConstraintViolationException, containing the failure details, to the code calling persistence-management operations.\u00ae The Hibernate toolset for automatic SQL schema generation understands many constraints and generates SQL DDL-equivalent constraints for you. For example, an @NotNull annotation translates into an\u00aeSQL NOT NULL constraint, and an @Size(n) rule defines the number of characters in a VARCHAR(n)-typed column. You can control this behavior of Hibernate with the <validation-mode> element in your persistence.xml configuration file. The default mode is AUTO, so Hibernate will only validate\u00aeif it finds a Bean Validation provider (such as Hibernate Validator) on the classpath of the running application. With mode CALLBACK, validation will always occur, and you'll get a deployment error if you forget to bundle a Bean Validation provider. The NONE mode disables automatic validation by the JPA provider. You'll\u00aesee Bean Validation annotations again later in this book; you'll also find them in the example code bundles. At this point we could write much more about Hibernate Validator, but we'd only repeat what is already available in the project's excellent reference guide. Have a look, and find out more about features such as validation groups and the metadata API for discovery of constraints. The Java Persistence and Bean Validation standards embrace annotations aggressively. The expert groups have been aware of the advantages of XML deployment descriptors in certain situations, especially for configuration metadata that changes with each deployment.",
  "page253": "This part is all about actual ORM, from classes and properties to tables and columns. Chapter 4 starts with regular class and property mappings and explains how you can map fine-gr\u00aeained Java domain models. Next, in chapter 5, you'll see how to map basic properties and embeddable components, and how to control mapping between Java and SQL types. In chapter 6, you'll map inheritance hiera\u00aerchies of entities to the database using four basic inheritance-mapping strategies; you'll also map polymorphic associations. Chapter 7 is all about mapping collections and entity associations: you\u00aemap persistent collections, collections of basic and embeddable types, and simple many-to-one and one-to-many entity associations. Chapter 8 dives deeper with advanced entity association mappings like mapping one-to-one entity associations, one-to-many mapping options, and many-to-many an\u00aed ternary entity relationships. Finally, you'll find chapter 9 most interesting if you need to introduce Hibernate in an existing application, or if you have to work with legacy database schemas and handwritten SQL. We'll also talk about customized SQL DDL for schema generation in this chapter. After reading this part o\u00aef the book, you'll be ready to create even the most complex mappings quickly and with the right strategy. You'll understand how the problem of inheritance mapping can be solved and how to map collections and associations. You'll also be able to tune and customize Hibernate for integration with any existing database schema or application.",
  "page254": "Fine-grained domain models- A major objective of Hibernate is support for fine-grained and rich domain models. It's one reason we work with POJOs. In crude terms, fine-grained\u00aemeans more classes than tables.  For example, a user may have a home address in your domain model. In the database, you may have a single USERS table with the columns HOME_STREET, HOME_CITY, and HOME_ZIPCODE. (Remembe\u00aer the problem of SQL types we discussed in section 1.2.1?)  In the domain model, you could use the same approach, representing the address as three string-valued properties of the User class. But it\u00aemuch better to model this using an Address class, where User has a homeAddress property. This domain model achieves improved cohesion and greater code reuse, and it's more understandable than SQL with inflexible type systems. JPA emphasizes the usefulness of fine-grained classes for imp\u00aelementing type safety and behavior. For example, many people model an email address as a string-valued property of User. A more sophisticated approach is to define an EmailAddress class, which adds higher-level semantics and behavior it may provide a prepareMail() method (it shouldn't have a sendMail() method, because you don\u00ae't want your domain model classes to depend on the mail subsystem).  This granularity problem leads us to a distinction of central importance in ORM. In Java, all classes are of equal standing all instances have their own identity and life cycle. When you introduce persistence, some instances may not have their own identity and life cycle but depend on others. Let's walk through an example",
  "page255": "Distinguishing entities and value types- You may find it helpful to add stereotype (a UML extensibility mechanism) information to your UML class diagrams so you can immediately rec\u00aeognize entities and value types. This practice also forces you to think about this distinction for all your classes, which is a first step to an optimal mapping and well-performing persistence layer. The Item and User\u00aeclasses are obvious entities. They each have their own identity, their instances have references from many other instances (shared references), and they have independent lifespans.  Marking the Address\u00aeas a value type is also easy: a single User instance references a particular Address instance. You know this because the association has been created as a composition, where the User instance has been made fully responsible for the life cycle of the referenced Address instance. Therefore,\u00aeAddress instances can't be referenced by anyone else and don't need their own identity.  The Bid class could be a problem. In object-oriented modeling, this is marked as a composition (the association between Item and Bid with the diamond). Thus, an Item is the owner of its Bid instances and holds a collection of refer\u00aeences. At first, this seems reasonable, because bids in an auction system are useless when the item they were made for is gone.  But what if a future extension of the domain model requires a User#bids collection, containing all bids made by a particular User? Right now, the association between Bid and User is unidirectional; a Bid has a bidder reference. What if this was bidirectional?",
  "page256": "Mapping entities with identity- Mapping entities with identity requires you to understand Java identity and equality before we can walk through an entity class example and its mapp\u00aeing. After that, we'll be able to dig in deeper and select a primary key, configure key generators, and finally go through identifier generator strategies. First, it's vital to understand the difference betwee\u00aen Java object identity and object equality before we discuss terms like database identity and the way JPA manages identity. Understanding Java identity and equality- Java developers understand the diffe\u00aerence between Java object identity and equality. Object identity (=) is a notion defined by the Java virtual machine. Two references are identical if they point to the same memory location. On the other hand, object equality is a notion defined by a class's equals() method, sometime\u00aes also referred to as equivalence. Equivalence means two different (non-identical) instances have the same value the same state. Two different instances of String are equal if they represent the same sequence of characters, even though each has its own location in the memory space of the virtual machine. (If you're a Java gur\u00aeu, we acknowledge that String is a special case. Assume we used a different class to make the same point.) Persistence complicates this picture. With object/relational persistence, a persistent instance is an in-memory representation of a particular row (or rows) of a database table (or tables). Along with Java identity and equality, we define database identity",
  "page257": "Objects are identical if they occupy the same memory location in the JVM. This can be checked with the a = b operator. This concept is known as object identity. Objects are\u00aeequal if they have the same state, as defined by the a.equals(Object b) method. Classes that don't explicitly override this method inherit the implementation defined by java.lang.Object, which compares object ident\u00aeity with ==. This concept is known as object equality. Objects stored in a relational database are identical if they share the same table and primary key value. This concept, mapped into the Java\u00aespace, is known as database identity. We now need to look at how database identity relates to object identity and how to express database identity in the mapping metadata. As an example, you'll map an entity of a domain model. Every entity class has to have an @Id property; it'\u00aes how JPA exposes database identity to the application. We don't show the identifier property in our diagrams; we assume that each entity class has one. In our examples, we always name the identifier property id. This is a good practice for your own project; use the same identifier property name for all your domain model enti\u00aety classes. If you specify nothing else, this property maps to a primary key column named ID of the ITEM table in your database schema.",
  "page258": "Hibernate will use the field to access the identifier property value when loading and storing items, not getter or setter methods. Because @Id is on a field, Hibernate will now ena\u00aeble every field of the class as a persistent property by default. The rule in JPA is this: if @Id is on a field, the JPA provider will access fields of the class directly and consider all fields part of the persistent s\u00aetate by default. You'll see how to override this later in this chapter in our experience, field access is often the best choice, because it gives you more freedom for accessor method design.  Shoul\u00aed you have a (public) getter method for the identifier property? Well, the application often uses database identifiers as a convenient handle to a particular instance, even outside the persistence layer. For example, it's common for web applications to display the results of a search\u00aescreen to the user as a list of summaries. When the user selects a particular element, the application may need to retrieve the selected item, and it's common to use a lookup by identifier for this purpose you've probably already used identifiers this way, even in applications that rely on JDBC. Should you have a setter\u00aemethod? Primary key values never change, so you shouldn't allow modification of the identifier property value. Hibernate won't update a primary key column, and you shouldn't expose a public identifier setter method on an entity.  The Java type of the identifier property, java.lang.Long in the previous example, depends on the primary key column type of the ITEM table and how key values are produced. This brings us to the @GeneratedValue annotation and primary keys in general.",
  "page259": "Selecting a primary key-Must primary keys be immutable? The relational model defines that a candidate key must be unique and irreducible (no subset of the key attributes has the un\u00aeiqueness property). Beyond that, picking a candidate key as the primary key is a matter of taste. But Hibernate expects a candidate key to be immutable when used as the primary key. Hibernate doesn't support updati\u00aeng primary key values with an API; if you try to work around this requirement, you'll run into problems with Hibernate's caching and dirty-checking engine. If your database schema relies on upd\u00aeatable primary keys (and maybe uses ON UPDATE CASCADE foreign key constraints), you must change the schema before it will work with Hibernate. The database identifier of an entity is mapped to some table primary key, so let's first get some background on primary keys without worrying\u00aeabout mappings. Take a step back and think about how you identify entities.  A candidate key is a column or set of columns that you could use to identify a particular row in a table. To become the primary key, a candidate key must satisfy the following requirements: The value of any candidate key column is never null. You can\u00ae2019t identify something with data that is unknown, and there are no nulls in the relational model. Some SQL products allow defining (composite) primary keys with nullable columns, so you must be careful. The value of the candidate key column(s) is a unique value for any row. The value of the candidate key column(s) never changes; it's immutable.",
  "page260": "If a table has only one identifying attribute, it becomes, by definition, the primary key. But several columns or combinations of columns may satisfy these properties for a particu\u00aelar table; you choose between candidate keys to decide the best primary key for the table. You should declare candidate keys not chosen as the primary key as unique keys in the database if their value is indeed unique (\u00aebut maybe not immutable).  Many legacy SQL data models use natural primary keys. A natural key is a key with business meaning: an attribute or combination of attributes that is unique by virtue of its b\u00aeusiness semantics. Examples of natural keys are the US Social Security Number and Australian Tax File Number. Distinguishing natural keys is simple: if a candidate key attribute has meaning outside the database context, it's a natural key, regardless of whether it's automaticall\u00aey generated. Think about the application users: if they refer to a key attribute when talking about and working with the application, it's a natural key: \"Can you send me the pictures of item #123-abc?\" Experience has shown that natural primary keys usually cause problems in the end. A good primary key must be uniq\u00aeue, immutable, and never null. Few entity attributes satisfy these requirements, and some that do can't be efficiently indexed by SQL databases (although this is an implementation detail and shouldn't be the deciding factor for or against a particular key). In addition, you should make certain that a candidate key definition never changes throughout the lifetime of the database. Changing the value (or even definition) of a primary key, and all foreign keys that refer to it, is a frustrating task. Expect your database schema to survive decades, even if your application won't.",
  "page261": "Furthermore, you can often only find natural candidate keys by combining several columns in a composite natural key. These composite keys, although certainly appropriate for some s\u00aechema artifacts (like a link table in a many-to-many relationship), potentially make maintenance, ad hoc queries, and schema evolution much more difficult. We talk about composite keys later in the book, For these reaso\u00aens, we strongly recommend that you add synthetic identifiers, also called surrogate keys. Surrogate keys have no business meaning they have unique values generated by the database or application. Applica\u00aetion users ideally don't see or refer to these key values; they're part of the system internals. Introducing a surrogate key column is also appropriate in the common situation when there are no candidate keys. In other words, (almost) every table in your schema should have a ded\u00aeicated surrogate primary key column with only this purpose. There are a number of well-known approaches to generating surrogate key values. The aforementioned @GeneratedValue annotation is how you configure this. Configuring key generators- The @Id annotation is required to mark the identifier property of an entity class. Withou\u00aet the @GeneratedValue next to it, the JPA provider assumes that you'll take care of creating and assigning an identifier value before you save an instance. We call this an application-assigned identifier. Assigning an entity identifier manually is necessary when you're dealing with a legacy database and/or natural primary keys. We have more to say about this kind of mapping in a dedicated section,",
  "page262": "Usually you want the system to generate a primary key value when you save an entity instance, so you write the GeneratedValue annotation next to @Id. JPA standardizes several value\u00ae-generation strategies with the javax.persistence.GenerationType enum, which you select with @GeneratedValue(strategy): GenerationType.AUTO Hibernate picks an appropriate strategy, asking the SQL dialect of your config\u00aeured database what is best. This is equivalent to @GeneratedValue() without any settings. GenerationType.SEQUENCE Hibernate expects (and creates, if you use the tools) a sequence named HIBERNATE_SEQUENCE\u00aein your database. The sequence will be called separately before every INSERT, producing sequential numeric values. GenerationType.IDENTITY Hibernate expects (and creates in table DDL) a special auto-incremented primary key column that automatically generates a numeric value on INSERT, in\u00aethe database. GenerationType.TABLE Hibernate will use an extra table in your database schema that holds the next numeric primary key value, one row for each entity class. This table will be read and updated accordingly, before INSERTs. The default table name is HIBERNATE_SEQUENCES with columns SEQUENCE_NAME and SEQUENCE_NEXT_HI_V\u00aeALUE. (The internal implementation uses a more complex but efficient hi/lo generation algorithm; more on this later.)",
  "page263": "JPA has two built-in annotations you can use to configure named generators: @javax persistence.SequenceGenerator and @javax.persistence.TableGenerator. With these annotations, you\u00aecan create a named generator with your own sequence and table names. As usual with JPA annotations, you can unfortunately only use them at the top of a (maybe otherwise empty) class, and not in a package-info.java file.\u00ae For this reason, and because the JPA annotations don't give us access to the full Hibernate feature set, we prefer an alternative: the native @org.hibernate.annotations .GenericGenerator ann\u00aeotation. It supports all Hibernate identifier generator strategies and their configuration details. Unlike the rather limited JPA annotations, you can use the Hibernate annotation in a package-info.java file, typically in the same package as your domain model classes. The next listi\u00aeng shows a recommended configuration. This Hibernate-specific generator configuration has the following advantages: The enhanced-sequence B strategy produces sequential numeric values. If your SQL dialect supports sequences, Hibernate will use an actual database sequence. If your DBMS doesn't support native sequences, Hibern\u00aeate will manage and use an extra \"sequence table,\" simulating the behavior of a sequence. This gives you real portability: the generator can always be called before performing an SQL INSERT, unlike, for example, auto-increment identity columns, which produce a value on INSERT that has to be returned to the application afterward.",
  "page264": "You can configure the sequence_name C. Hibernate will either use an existing sequence or create it when you generate the SQL schema automatically. If your DBMS doesn't support\u00aesequences, this will be the special \"sequence table\" name. You can start with an initial_value D that gives you room for test data. For example, when your integration test runs, Hibernate will make any new da\u00aeta insertions from test code with identifier values greater than 1000. Any test data you want to import before the test can use numbers 1 to 999, and you can refer to the stable identifier values in your\u00aetests: \"Load item with id 123 and run some tests on it.\" This is applied when Hibernate generates the SQL schema and sequence; it's a DDL option. You can share the same database sequence among all your domain model classes. There is no harm in specifying @GeneratedValue(ge\u00aenerator \"ID_GENERATOR\") in all your entity classes. It doesn't matter if primary key values aren't contiguous for a particular entity, as long as they're unique within one table. If you're worried about contention, because the sequence has to be called prior to every INSERT, we discuss a variation of this generator configuration later,",
  "page265": "Finally, you use java.lang.Long as the type of the identifier property in the entity class, which maps perfectly to a numeric database sequence generator. You could also use a long\u00aeprimitive. The main difference is what someItem.getId() returns on a new item that hasn't been stored in the database: either null or 0. If you want to test whether an item is new, a null check is probably easier\u00aeto understand for someone else reading your code. You shouldn't use another integral type such as int or short for identifiers. Although they will work for a while (perhaps even years), as your data\u00aebase size grows, you may be limited by their range. An Integer would work for almost two months if you generated a new identifier each millisecond with no gaps, and a Long would last for about 300 million years. Although recommended for most applications, the enhanced-sequence stra\u00aetegy as shown in listing is just one of the strategies built into Hibernate. Identifier generator strategies Following is a list of all available Hibernate identifier generator strategies, their options, and our usage recommendations. If you don't want to read the whole list now, enable GenerationType.AUTO and check what Hibe\u00aernate defaults to for your database dialect. It's most likely sequence or identity a good but maybe not the most efficient or portable choice. If you require consistent portable behavior, and identifier values to be available before INSERTs, use enhanced-sequence, as shown in the previous section. This is a portable, flexible, and modern strategy, also offering various optimizers for large datasets.",
  "page266": "We also show the relationship between each standard JPA strategy and its native Hibernate equivalent. Hibernate has been growing organically, so there are now two sets of mappings\u00aebetween standard and native strategies; we call them Old and New in the list. You can switch this mapping with the hibernate.id.new_generator_mappings setting in your persistence.xml file. The default is true; hence the\u00aeNew mapping. Software doesn't age quite as well as wine: native Automatically selects other strategies, such as sequence or identity, depending on the configured SQL dialect. You have to look\u00aeat the Javadoc (or even the source) of the SQL dialect you configured in persistence.xml. Equivalent to JPA GenerationType.AUTO with the Old mapping. sequence Uses a native database sequence named HIBERNATE_SEQUENCE. The sequence is called before each INSERT of a new row. You can c\u00aeustomize the sequence name and provide additional DDL settings; see the Javadoc for the class org.hibernate.id.SequenceGenerator and its parent. enhanced-sequence Uses a native database sequence when supported; otherwise falls back to an extra database table with a single column and row, emulating a sequence. Defaults to nam\u00aee HIBERNATE_SEQUENCE. Always calls the database \"sequence\" before an INSERT, providing the same behavior independently of whether the DBMS supports real sequences. Supports an org.hibernate .id.enhanced.Optimizer to avoid hitting the database before each INSERT; defaults to no optimization and fetching a new value for each INSERT. You can find more examples in chapter 20. For all parameters, see the Javadoc for the class org.hibernate.id.enhanced.SequenceStyleGenerator. Equivalent to JPA GenerationType.SEQUENCE and GenerationType.AUTO with the New mapping enabled, most likely your best option of the built-in strategies.",
  "page267": "seqhilo Uses a native database sequence named HIBERNATE_SEQUENCE, optimizing calls before INSERT by combining hi/lo values. If the hi value retrieved from the sequence is 1,\u00aethe next 9 insertions will be made with key values 11, 12, 13, \u2026, 19. Then the sequence is called again to obtain the next hi value (2 or higher), and the procedure repeats with 21, 22, 23, and so on. You can confi\u00aegure the maximum lo value (9 is the default) with the max_lo parameter. Unfortunately, due to a quirk in Hibernate's code, you can not configure this strategy in @GenericGenerator. The o\u00aenly way to use it is with JPA GenerationType.SEQUENCE and the Old mapping. You can configure it with the standard JPA @SequenceGenerator annotation on a (maybe otherwise empty) class. See the Javadoc for the class org.hibernate.id.SequenceHiLoGenerator and its parent for more inform\u00aeation. Consider using enhanced-sequence instead, with an optimizer hilo Uses an extra table named HIBERNATE_UNIQUE_KEY with the same algorithm as the seqhilo strategy. The table has a single column and row, holding the next value of the sequence. The default maximum lo value is 32767, so you most likely want to configure it\u00aewith the max_lo parameter. See the Javadoc for the class org.hibernate.id.TableHiLoGenerator for more information. We don't recommend this legacy strategy; use enhanced-sequence instead with an optimizer.",
  "page268": "enhanced-table Uses an extra table named HIBERNATE_SEQUENCES, with one row by default representing the sequence, storing the next value. This value is selected and updated when an\u00aeidentifier value has to be generated. You can configure this generator to use multiple rows instead: one for each generator; see the Javadoc for org.hibernate.id.enhanced.TableGenerator. Equivalent to JPA GenerationType\u00ae.TABLE with the New mapping enabled. Replaces the outdated but similar org.hibernate.id.MultipleHiLoPerTableGenerator, which is the Old mapping for JPA GenerationType.TABLE. identity Supports IDEN\u00aeTITY and auto-increment columns in DB2, MySQL, MS SQL Server, and Sybase. The identifier value for the primary key column will be generated on INSERT of a row. Has no options. Unfortunately, due to a quirk in Hibernate's code, you can not configure this strategy in @GenericGenerator.\u00aeThe only way to use it is with JPA GenerationType.IDENTITY and the Old or New mapping, making it the default for GenerationType.IDENTITY. increment At Hibernate startup, reads the maximum (numeric) primary key column value of each entity's table and increments the value by one each time a new row is inserted. Especial\u00aely efficient if a non-clustered Hibernate application has exclusive access to the database; but don't use it in any other scenario.",
  "page269": "select Hibernate won't generate a key value or include the primary key column in an INSERT statement. Hibernate expects the DBMS to assign a (default in schema or by tri\u00aegger) value to the column on insertion. Hibernate then retrieves the primary key column with a SELECT query after insertion. Required parameter is key, naming the database identifier property (such as id) for the SELECT\u00ae. This strategy isn't very efficient and should only be used with old JDBC drivers that can't return generated keys directly. uuid2 Produces a unique 128-bit UUID in the application layer. Usef\u00aeul when you need globally unique identifiers across databases (say, you merge data from several distinct production databases in batch runs every night into an archive). The UUID can be encoded either as a java.lang.String, a byte[16], or a java .util.UUID property in your entity class. R\u00aeeplaces the legacy uuid and uuid .hex strategies. You configure it with an org.hibernate.id.UUIDGenerationStrategy; see the Javadoc for the class org.hibernate.id.UUIDGenerator for more details. guid Uses a globally unique identifier produced by the database, with an SQL function available on Oracle, Ingres, MS SQL Server, a\u00aend MySQL. Hibernate calls the database function before an INSERT. Maps to a java.lang.String identifier property. If you need full control over identifier generation, configure the strategy of @GenericGenerator with the fully qualified name of a class that implements the org.hibernate.id.IdentityGenerator interface.",
  "page270": "QUOTING SQL IDENTIFIERS- From time to time, especially in legacy databases, you'll encounter identifiers with strange characters or whitespace, or wish to force case sensitivi\u00aety. Or, as in the previous example, the automatic mapping of a class or property would require a table or column name that is a reserved keyword.  Hibernate 5 knows the reserved keywords of your DBMS through the\u00aeconfigured database dialect. Hibernate 5 can automatically put quotes around such strings when generating SQL. You can enable this automatic quoting with hibernate.auto_quote _keyword true in your persis\u00aetence unit configuration. If you're using an older version of Hibernate, or you find that the dialect's information is incomplete, you must still apply quotes on names manually in your mappings if there is a conflict with a keyword.  If you quote a table or column name in your\u00aemapping with backticks, Hibernate always quotes this identifier in the generated SQL. This still works in latest versions of Hibernate, but JPA 2.0 standardized this functionality as delimited identifiers with double quotes. If you have to quote all SQL identifiers, create an orm.xml file and add the setting <delimited-iden\u00aetifiers/> to its <persistence-unit-defaults> section, as shown in listing 3.8. Hibernate then enforces quoted identifiers everywhere.  You should consider renaming tables or columns with reserved keyword names whenever possible. Ad hoc SQL queries are difficult to write in an SQL console if you have to quote and escape everything properly by hand. Next, you'll see how Hibernate can help when you encounter organizations with strict conventions for database table and column names.To summarize, our recommendations on identifier generator strategies are as follows: appings. Entity-mapping options- You've now mapped a persistent class with @Entity, using defaults for all other settings, such as the mapped SQL table name. The following section explores some classlevel options and how you control them: Naming defaults and strategies Dynamic SQL generation Entity mutability These are options; you can skip this section and come back later when you have to deal with a specific problem.",
  "page271": "Entities are the coarser-grained classes of your system. Their instances have an independent life cycle and their own identity, and many other instances can reference them. \u00aeValue types, on the other hand, are dependent on a particular entity class. A value type instance is bound to its owning entity instance, and only one entity instance can reference it it has no individual identity. We\u00aelooked at Java identity, object equality, and database identity, and at what makes good primary keys. You learned which generators for primary key values Hibernate provides out of the box, and how to use\u00aeand extend this identifier system. We discussed some useful class mapping options, such as naming strategies and dynamic SQL generation. After spending the previous chapter almost exclusively on entities and the respective class- and identity-mapping options, we now focus on value\u00aetypes in their various forms. We split value types into two categories: basic value-typed classes that come with the JDK, such as String, Date, primitives, and their wrappers; and developerdefined value-typed classes, such as Address and MonetaryAmount in CaveatEmptor. In this chapter, we first map persistent properties wi\u00aeth JDK types and learn the basic mapping annotations. You see how to work with various aspects of properties: overriding defaults, customizing access, and generated values. You also see how SQL is used with derived properties and transformed column values. We wrap up basic properties with temporal properties and mapping enumerations. We then discuss custom value-typed classes and map them as embeddable components. You learn how classes relate to the database schema and make your classes embeddable, while allowing for overriding embedded attributes. We complete embeddable componentsC level is much more efficient if there are fewer statements. How can Hibernate create an UPDATE statement on startup? After all, the columns to be updated aren't known at this time. The answer is that the generated SQL statement updates all columns, and if the value of a particular column isn't modified, the statement sets it to its old value.",
  "page272": "This configuration by exception approach means you don't have to annotate a property to make it persistent; you only have to configure the mapping in an exceptional case. Sev\u00aeu0002eral annotations are available in JPA to customize and control basic property map. Overriding basic property defaults- You might not want all properties of an entity class to be persistent. For example, although it\u00aemakes sense to have a persistent Item#initialPrice property, an Item#totalPriceIncludingTax property shouldn't be persistent if you only compute and use its value at runtime, and hence should\u00aen't be stored in the database. To exclude a property, mark the field or the getter method of the property with the @javax.persistence.Transient annotation or use the Java transient keyword. The transient keyword usually only excludes fields for Java serialization, but it's also\u00aerecognized by JPA providers.  We'll come back to the placement of the annotation on fields or getter methods in a moment. Let's assume as we have before that Hibernate will access fields directly because @Id has been placed on a field. Therefore, all other JPA and Hibernate mapping annotations are also on fie\u00aelds. We have to admit that this annotation isn't very useful. It only has two parameters: the one shown here, optional, marks the property as not optional at the Java object level. By default, all persistent properties are nullable and optional; an Item may have an unknown initialPrice. Mapping the initialPrice property as non-optional makes sense if you have a NOT NULL constraint on the INITIALPRICE column in your SQL schema. If Hibernate is generating the SQL schema, it will include a NOT NULL constraint automatically for non-optional properties. Marty. Otherwise, if you annotate the class of the property as @Embeddable, or you map the property itself as @Embedded, Hibernate maps the property as an embedded component of the owning class. We discuss embedding of components later in this chapter, with the Address and MonetaryAmount embeddable classes of CaveatEmptor. Otherwise, if the type of the property is java.io.Serializable,",
  "page273": "The Column annotation has a few other parameters, most of which control SQL-level details such as catalog and schema names. They're rarely needed, and we only show them throug\u00aehout this book when necessary.  Property annotations aren't always on fields, and you may not want Hibernate to access fields directly. Customizing property access- The persistence engine accesses the properties\u00aeof a class either directly through fields or indirectly through getter and setter methods. An annotated entity inherits the default from the position of the mandatory @Id annotation. For example, if you\u00aeu2019ve declared @Id on a field, not a getter method, all other mapping annotations for that entity are expected on fields. Annotations are never on the setter methods.  The default access strategy isn't only applicable to a single entity class. Any @Embedded class inherits the defa\u00aeult or explicitly declared access strategy of its owning root entity class. We cover embedded components later in this chapter. Furthermore, Hibernate accesses any @MappedSuperclass properties with the default or explicitly declared access strategy of the mapped entity class. Inheritance is the topic of chapter 6.  The\u00aeJPA specification offers the @Access annotation for overriding the default behavior, with the parameters AccessType.FIELD and AccessType.PROPERTY. If you set @Access on the class/entity level, Hibernate accesses all properties of the class according to the selected strategy. You then set any other mapping annotations, including the @Id, on either fields or getter methods, respectively. Now, when you store an Item and forget to set a value on the initialPrice field, Hibernate will complain with an exception before hitting the database with an SQL statement. Hibernate knows that a value nnotation, the @Column annotation, and earlier with the Bean Validation @NotNull annotation in section 3.3.2. All have the same effect on the JPA provider: Hibernate does a null check when saving and generates a NOT NULL constraint in the database schema. We recommend the Bean Validation @NotNull annotation so you can manually validate an Item instance and/or have your user interface code in the presentation layer execute validation checks automatically. The @Column annotation can also override the mapping of the property name to the database column:",
  "page274": "Hibernate reads your Java domain model classes and mapping metadata and generates schema DDL statements. You can export them into a text file or execute them directly on your datab\u00aease whenever you run integration tests. Because schema languages are mostly vendor-specific, every option you put in your mapping metadata has the potential to bind the metadata to a particular database product keep thi\u00aes in mind when using schema features. Hibernate creates the basic schema for your tables and constraints automatically; it even creates sequences, depending on the identifier generator you select. But t\u00aehere are some schema artifacts Hibernate can't and won't create automatically. These include all kinds of highly vendor-specific performance options and any other artifacts that are relevant only for the physical storage of data (tablespaces, for example). Besides these physical\u00aeconcerns, your DBA will often supply custom additional schema statements to improve the generated schema. DBAs should get involved early and verify the automatically generated schema from Hibernate. Never go into production with an unchecked automatically generated schema. If your development process allows, changes made by the D\u00aeBA can flow back into your Java systems, for you to add to mapping metadata. In many projects, the mapping metadata can contain all the necessary schema changes from a DBA. Then Hibernate can generate the final production schema during the regular build by including all comments, constraints, indexes, and so on. In the following sections, we show you how to customize the generated schema and how to add auxiliary database schema artifacts (we call them objects sometimes; we don't mean Java objects here). We discuss custom data types, additional integrity rules, indexes, and how you s you can start a project top-down. There is no existing database schema and maybe not even any data your application is completely new. Many developers like to let Hibernate automatically generate the scripts for a database schema. You'll probably also let Hibernate deploy the schema on the test database on your development machine or your continuous build systems for integration testing.",
  "page275": "This property defines when the create and drop scripts should be executed. Your custom SQL scripts will contain CREATE DOMAIN statements, which must be executed before the tables u\u00aesing these domains are created. With these settings, the schema generator runs the create script first before reading your ORM metadata (annotations, XML files) and creating the tables. The drop script executes after Hi\u00aebernate drops the tables, giving you a chance to clean up anything you created. Other options are metadata (ignore custom script sources) and script (only use a custom script source; ignore ORM metadata\u00aein annotations and XML files). D This is the location of the custom SQL script for creation of the schema. The path is (a) the location of the script resource on the classpath; (b) the location of the script as a file:// URL; or, if neither (a) nor (b) matches, (c) the absolute or relativ\u00aee file path on the local file system. This example uses (a). E This is the custom SQL script for dropping the schema. F This load script runs after the tables have been created. We've mentioned that DDL is usually highly vendor-specific. If your application has to support several database dialects, you may need several sets o\u00aef create/drop/load scripts to customize the schema for each database dialect. You can solve this with several persistence unit configurations in the persistence.xml file. Alternatively, Hibernate has its own proprietary configuration for schema customization in an hbm.xml mapping file, You can hook the following three types of custom SQL scripts into Hibernates schema-generation process: The create script executes when the schema is generated. A custom create script can run before, after, or instead of Hibernates automatically generated scripts. In other words, you cation process is actually standardized; you configure it with JPA properties in persistence.xml for a persistence unit. By default, Hibernate expects one SQL statement per line in scripts. This switches to the more convenient multiline extractor. SQL statements in scripts are terminated with semicolon. You can write your own org.hibernate.tool.hbm2ddl.ImportSqlCommandExtractor implementation if you want to handle the SQL script in a different way.",
  "page276": "Table constraints An integrity rule that applies to several columns or several rows is a table constraint. A typical declarative table constraint is UNIQUE: all rows are checked fo\u00aer duplicate values (for example, each user must have a distinct email address). A rule affecting only a single row but multiple columns is \"the auction end time has to be after the auction start time.\" Databas\u00aee constraints If a rule applies to more than one table, it has database scope. You should already be familiar with the most common database constraint, the foreign key. This rule guarantees the integrity\u00aeof references between rows, usually in separate tables, but not always (self-referencing foreign key constraints aren't uncommon). Other database constraints involving several tables aren't uncommon: for example, a bid can only be stored if the auction end time of the reference\u00aed item hasn't been reached. Most (if not all) SQL database management systems support these kinds of constraints and the most important options of each. In addition to simple keywords such as NOT NULL and UNIQUE, you can usually also declare more complex rules with the CHECK constraint that applies an arbitrary SQL expression\u00ae. Still, integrity constraints are one of the weak areas in the SQL standard, and solutions from vendors can differ significantly. Furthermore, non-declarative and procedural constraints are possible with database triggers that intercept data-modification operations. A trigger can then implement the constraint procedure directly or call an existing stored procedure. Integrity constraints can be checked immediately when a data-modification statement is executed, or the check can be deferred until the end of a transaction. The violation response in SQL databases i. The advantages of declarative rules are fewer possible errors in code and a chance for the DBMS to optimize data access. In SQL databases, we identify four kinds of rules: Domain constraints A domain is (loosely speaking, and in the database world) a data type in a database. Hence, a domain constraint defines the range of possible values a particular data type can handle. For example, an INTEGER data type is usable for integer values.",
  "page277": "This constraint restricts valid username values to a maximum length of 15 characters, and the string can't begin with admin to avoid confusion. You can call any SQL functions\u00aesupported by your DBMS; the column Definition is always passed through into the exported schema. Note that you have a choice: creating and using a domain or adding a single column constraint has the same effect. Domain\u00aes are usually easier to maintain and avoid duplicating. At the time of writing, Hibernate doesn't support its proprietary annotation @org.hibernate.annotations.Check on individual properties; you u\u00aese it for table level constraints. Hibernate appends table constraints to the generated CREATE TABLE statement, which can contain arbitrary SQL expressions. You can implement multirow table constraints with expressions that are more complex. You may need a sub select in the expression to\u00aedo this, which may not be supported by your DBMS. But there are some common multirow table constraints, like UNIQUE, that you can add directly in the mappings. You've already seen the @Column(unique true | false) option in the previous section. Now all pairs of USERNAME and EMAIL must be unique, for all rows in the USERS tab\u00aele. If you don't provide a name for the constraint here, UNQ_USERNAME_EMAIL an automatically generated and probably ugly name is used. The last kinds of constraints we discuss are database-wide rules that span several tables. Hibernate passes on database constraint violations in error exceptions; check whether the exception in your transaction has a cause, somewhere in the exception chain, of type org.hibernate.exception.ConstraintViolationException. This exception can provide more information about the error, such as the name of the failed database constraint. The SQL standard inccond is a UNIQUE column constraint; users can't have duplicate email addresses. At the time of writing, there was unfortunately no way to customize the name of this single-column unique constraint in Hibernate; it will get an ugly auto-generated name in your schema. Last, the column Definition refers to the domain you've added with your custom create script. This definition is an SQL fragment, exported into your schema directly, so be careful with database-specific SQL",
  "page278": "This statement declares the foreign key constraint for the ITEM_ID column in the BID table, referencing the primary key column of the ITEM table. You can customize the name of the\u00aeconstraint with the foreign Key option in an @JoinColumn mapping A foreign Key attribute is also supported in @PrimaryKeyJoinColumn, @MapKeyJoinColumn, @JoinTable, @CollectionTable, and @AssociationOverride mappings. T\u00aehe @ForeignKey annotation has some rarely needed options we haven't shown: You can write your own foreign Key Definition, an SQL fragment such as FOREIGN KEY ([column]) REFERENCES [table]([column])\u00aeON UPDATE [action]. Hibernate will use this SQL fragment instead of the provider-generated fragment, it can be in the SQL dialect supported by your DBMS. The Constraint Mode setting is useful if you want to disable foreign key generation completely, with the value NO_CONSTRAINT. You can\u00aethen write the foreign key constraint yourself with an ALTER TABLE statement, probably in a load script as we've shown. Naming constraints properly is not only good practice, but also helps significantly when you have to read exception messages. This completes our discussion of database integrity rules. Next, we look at some\u00aeoptimization you might want to include in your schema for performance reasons. Indexes are a key feature when optimizing the performance of a database application. The query optimizer in a DBMS can use indexes to avoid excessive scans of the data tables. Because they're relevant only in the physical implementation of a database, indexes aren't part of the SQL standard, and the DDL and available indexing options are product specific. You can, however, embed the most common schema artifacts for typical indexes in mapping metadata A user can only make bids until an auctiondon't know which table Hibernate will create first. Therefore, put your CHECK constraint into an ALTER TABLE statement that executes after all the tables have been created. A good place is the load script, because it always executes at that time A row in the BID table is now valid if its CREATEDON value is less than or equal to the auction end time of the referenced ITEM row. By far the most common rules that span several tables are referential integrity rules.",
  "page279": "If you encounter a USERS table in a legacy schema, it's likely that USERNAME is the primary key. In this case, you have no surrogate identifier that Hibernate generates automa\u00aetically. Instead, your application has to assign the identifier value when saving an instance of the User class Hibernate calls the protected no-argument constructor when it loads a User from the database and then assig\u00aens the username field value directly. When you instantiate a User, call the public constructor with a username. If you don't declare an identifier generator on the @Id property, Hibernate expects th\u00aee application to take care of the primary key value assignment. Composite (natural) primary keys require a bit more work Suppose the primary key of the USERS table is a composite of the two columns USERNAME and DEPARTMENTNR. You write a separate composite identifier class that declares j\u00aeust the key properties and call this class User Id This class has to be @Embeddable and Serializable any type used as an identifier type in JPA must be Serializable. C You don't have to mark the properties of the composite key as @NotNull; their database columns are automatically NOT NULL when embedded as the primary key of a\u00aen entity. D The JPA specification requires a public no-argument constructor for an embeddable identifier class. Hibernate accepts protected visibility. E The only public constructor should have the key values as arguments. F You have to override the equals() and hashCode() methods with the same semantics the composite key has in your database. In this case, this is a straightforward comparison of the username and department values. Many queries in Caveat Emptor will probably involve the username of a User entity. You can speed up these queries by creating an index for the column of this ing the database schema is often possible only if you're working on a new system with no existing data. If you have to deal with an existing legacy schema, one of the most common issues is working with natural and composite keys. We mentioned in section 4.2.3 that we think natural primary keys can be a bad idea. Natural keys often make it difficult to change the data model when business requirements change. They may even, in extreme cases, impact performance. Unfortunately, many legacy schemas use (natural) composite keys heavily;",
  "page280": "We've already shown the @SecondaryTable annotation in an inheritance mapping in section 6.5. It helped to break out properties of a particular subclass into a separate table.\u00aeThis generic functionality has more uses but be aware that a properly designed system should have, simplified, more classes than tables. Suppose that in a legacy schema, you aren't keeping a user's billing ad\u00aedress information with the other user details in the USERS main entity table, but in a separate table. Figure 9.5 shows this schema. The user's home address is stored in the columns STREET, ZIPCODE,\u00aeand CITY of the USERS table. The user's billing address is stored in the BILLING_ADDRESS table, which has the primary key column USER_ID, which is also a foreign key constraint referencing the ID primary key of the USERS table The User class has two properties of embedded type: hom\u00aee Address and billing Address. The first is a regular embedded mapping, and the Address class is @Embeddable. Just as in section 5.2.3, you can use the @AttributeOverrides annotation to override the mapping of embedded properties. Then, @Column maps the individual properties to the BILLING_ADDRESS table, with its table option. Re\u00aemember that an @AttributeOverride replaces all mapping information for a property: any annotations on the Address fields are ignored if you override. Therefore, you have to specify nullability and length again in the @Column override. We've shown you a secondary table mapping example with an embeddable property. Of course, you could also break out simple basic properties like the username string in a secondary table. Keep in mind that reading and maintaining these mappings can be a problem, though; you should only map legacy unchangeable schemas with secondary tables. A foreign key constmple rule. Sometimes a foreign key constraint references a simple unique column a natural non-primary key. Let's assume that in Caveat Emptor, So far, this is nothing special; you've seen such a simple unique property mapping before. The legacy aspect is the SELLER_CUSTOMERNR column in the ITEM table, with a foreign key constraint referencing the user's CUSTOMERNR instead of the user's ID You specify the referencedColumnName attribute of @JoinColumn to declare this relationship.",
  "page281": "strategies for runtime data management. These strategies are crucial to the performance and correct behavior of your applications. In this chapter, we discuss the life cycle of en\u00aetity instances how an instance becomes persistent, and how it stops being considered persistent and the method calls and management operations that trigger these transitions. The JPA Entity Manager is your primary inter\u00aeface for accessing data. Before we look at the API, let's start with entity instances, their life cycle, and the events that trigger a change of state. Although some of the material may be formal,\u00aea solid understanding of the persistence life cycle is essential. Because JPA is a transparent persistence mechanism classes are unaware of their own persistence capability it's possible to write application logic that's unaware whether the data it operates on represents persist\u00aeent state or temporary state that exists only in memory. The application shouldn't necessarily need to care that an instance is persistent when invoking its methods. You can, for example, invoke the Item #calculateTotalPrice() business method without having to consider persistence at all (for example, in a unit test). Any ap\u00aeplication with persistent state must interact with the persistence service whenever it needs to propagate state held in memory to the database (or vice versa). In other words, you have to call the Java Persistence interfaces to store and load data. When interacting with the persistence mechanism that way, the application must concern itself with the state and life cycle of an entity instance with respect to persistence. We refer to this as the persistence life cycle: the states an entity instance goes through during its life. We also use the term unit of work: a set of (possmost important interface in JPA: the Entity Manager. Next, chapter 11 defines database and system transaction essentials and how to control concurrent access with Hibernate and JPA. You'll also see non transactional data access. In chapter 12, we'll go through lazy and eager loading, fetch plans, strategies, and profiles, and wrap up with optimizing SQL execution. Finally, chapter 13 covers cascading state transitions, listening to and intercepting events, auditingtly.",
  "page282": "A persistent entity instance has a representation in the database. It's stored in the database or it will be stored when the unit of work completes. It's an instance with\u00aea database identity, as defined in section 4.2; its database identifier is set to the primary key value of the database representation. The application may have created instances and then made them persistent by calli\u00aeng Entity Manager #persist(). There may be instances that became persistent when the application created a reference to the object from another persistent instance that the JPA provider already manages.\u00aeA persistent entity instance may be an instance retrieved from the database by execution of a query, by an identifier lookup, or by navigating the object graph starting from another persistent instance. Persistent instances are always associated with a persistence context. You see more\u00aeabout this in a moment You can delete a persistent entity instance from the database in several ways: For example, you can remove it with Entity Manager #remove(). It may also become available for deletion if you remove a reference to it from a mapped collection with orphan removal enabled. An entity instance is then in the remov\u00aeed state: the provider will delete it at the end of a unit of work. You should discard any references you may hold to it in the application after you finish working with it for example, after you've rendered the removal confirmation screen your users see Think of the persistence context as a service that remembers all the modifications and state changes you made to data in a particular unit of work (this is somewhat simplified, but it's a good starting point). We now dissect all these terms: entity states, persistence contexts, and managed scope. You're probably more accus to the Entity Manager (and Query) API that trigger transitions. We discuss this chart in this chapter; refer to it whenever you need an overview. Let's explore the states and transitions in more detail. Instances created with the new Java operator are transient, which means their state is lost and garbage-collected as soon as they're no longer referenced. For example, new Item() creates a transient instance of the Item class, just like new Long() and new Big Decimal(). ",
  "page283": "The persistence context acts as a first-level cache; it remembers all entity instances you've handled in a particular unit of work. For example, if you ask Hibernate to load a\u00aen entity instance using a primary key value (a lookup by identifier), Hibernate can first check the current unit of work in the persistence context. If Hibernate finds the entity instance in the persistence context, no\u00aedatabase hit occurs this is a repeatable read for an application. Consecutive em.find(Item. Class, ITEM_ID) calls with the same persistence context will yield the same result. This cache also affects re\u00aesults of arbitrary queries, executed for example with the javax.persistence.Query API. Hibernate reads the SQL result set of a query and transforms it into entity instances. This process first tries to resolve every entity instance in the persistence context by identifier lookup. Only if\u00aean instance with the same identifier value can't be found in the current persistence context does Hibernate read the rest of the data from the result-set row. Hibernate ignores any potentially newer data in the result set, due to read-committed transaction isolation at the database level, if the entity instance is already pre\u00aesent in the persistence context. The persistence context cache is always on it can't be turned off. It ensures the following The persistence layer isn't vulnerable to stack overflows in the case of circular references in an object graph. There can never be conflicting representations of the same database row at the end of a unit of work. The provider can safely write all changes made to an entity instance to the database. Likewise, changes made in a particular persistence context are always immediately visible to all other code executed inside that unit of work and its persis and merging again later in this chapter, in a dedicated section. You should now have a basic understanding of entity instance states and their transitions. Our next topic is the persistence context: an essential service of any Java Persistence provider The persistence context allows the persistence engine to perform automatic dirty checking, detecting which entity instances the application modified. The provider then synchronizes with the database the state of instances monitored by ae Day. This ensures that queries are aware of changes made earlier during the unit of work.",
  "page284": "Hibernate joins the in-progress system transaction when an Entity Manager is created and waits for the transaction to commit. When Hibernate is notified (by the JTA engine) of the\u00aecommit, it performs dirty checking of the persistence context and synchronizes with the database. You can also force dirty checking synchronization manually by calling EntityManager#flush() at any time during a transact\u00aeion. You decide the scope of the persistence context by choosing when to close() the Entity Manager. You have to close the persistence context at some point, so always place the close() call in a finall\u00aey block. How long should the persistence context be open? Let's assume for the following examples that you're writing a server, and each client request will be processed with one persistence context and system transaction in a multithreaded environment. If you're familiar\u00aewith servlets, imagine the code in listing 10.1 embedded in a servlet's service() method. Within this unit of work, you access the Entity Manager to load and store data. A new transient Item is instantiated as usual. Of course, you may also instantiate it before creating the EntityManager. A call to persist() makes the transi\u00aeent instance of Item persistent. It's now managed by and associated with the current persistence context. To store the Item instance in the database, Hibernate has to execute an SQL INSERT statement. When the transaction of this unit of work commits, Hibernate flushes the persistence context, and the INSERT occurs at that time. Hibernate may even batch the INSERT at the JDBC level with other statements. When you call persist(), only the identifier value of the Item is assigned. Alternatively, if your identifier generator isn't pre-insert, the INSERT statement will ommit() occurs in one transaction. For now, keep in mind that all database operations in transaction scope, such as the SQL statements executed by Hibernate, completely either succeed or fail. Don't worry too much about the transaction code for now; you'll read more about concurrency control in the next chapter. We'll look at the same example again with a focus on the transaction and exception-handling code. Don't write empty catch clauses in your code, tnectn you look up or query data and when it flushes changes detected by the persistence context to the database.",
  "page285": "You can modify the Item instance, and the persistence context will detect these changes and record them in the database automatically. When Hibernate flushes the persistence contex\u00aet during commit, it executes the necessary SQL DML statements to synchronize the changes with the database. Hibernate propagates state changes to the database as late as possible, toward the end of the transaction. DML\u00aestatements usually create locks in the database that are held until the transaction completes, so Hibernate keeps the lock duration in the database as short as possible. Hibernate writes the new Item#na\u00aeme to the database with an SQL UPDATE. By default, Hibernate includes all columns of the mapped ITEM table in the SQL UPDATE statement, updating unchanged columns to their old values. Hence, Hibernate can generate these basic SQL statements at startup, not at runtime. If you want to inclu\u00aede only modified (or non-nullable for INSERT) columns in SQL statements, you can enable dynamic SQL generation as discussed in section 4.3.2. Hibernate detects the changed name by comparing the Item with a snapshot copy it took before, when the Item was loaded from the database. If your Item is different from the snapshot, an UPD\u00aeATE is necessary. This snapshot in the persistence context consumes memory. Dirty checking with snapshots can also be time consuming, because Hibernate has to compare all instances in the persistence context with their snapshot during flushing. It's better (but not required) to fully initialize the Item instance before managing it with a persistence context. The SQL INSERT statement contains the values that were held by the instance at the point when persist() was called. If you don't set the name of the Item before making it persistent, a NOT NULL constraint may bescuss exception handling in the next chapter. Next, you load and modify the stored data. You can retrieve persistent instances from the database with the EntityManager. For the next example, we assume you've kept the identifier value of the Item stored in the previous section somewhere and are now looking up the same instance in a new unit of work by identifier You don't need to cast the returned value of the find() operation; it's a generic method, and its return type 00aend() returns null. The find() operation always hits the database if there was no hit for the",
  "page286": "As soon as you call any method such as Item#getName() on the proxy, a SELECT is executed to fully initialize the placeholder. The exception to this rule is a mapped database identi\u00aefier getter method, such as getId(). A proxy may look like the real thing, but it's only a placeholder carrying the identifier value of the entity instance it represents. If the database record no longer exists whe\u00aen the proxy is initialized, an EntityNotFoundException is thrown. Note that the exception can be thrown when Item#getName() is called. E Hibernate has a convenient static initialize() method that loads t\u00aehe proxy's data. F After the persistence context is closed, item is in detached state. If you don't initialize the proxy while the persistence context is still open, you get a LazyInitializationException if you access the proxy. You can't load data on demand once the persis\u00aetence context is closed. The solution is simple: load the data before you close the persistence context. We'll have much more to say about proxies, lazy loading, and on-demand fetching in chapter 12. If you want to remove the state of an entity instance from the database, you have to make it transient. To make an entity ins\u00aetance transient and delete its database representation, call the remove() method on the EntityManager If you call find(), Hibernate executes a SELECT to load the Item. If you call getReference(), Hibernate attempts to avoid the SELECT and returns a proxy. C Calling remove() queues the entity instance for deletion when the unit of work completes; it's now in removed state. If remove() is called on a proxy, Hibernate executes a SELECT to load the data. You may want to customize how Hibernate detects dirty state, using an extension point. Set the property hibernate.entity_dirtiness_str is returned. Sometimes you need an entity instance but you don't want to hit the database. If you don't want to hit the database when loading an entity instance, because you aren't sure you need a fully initialized instance, you can tell the EntityManager to attempt the retrieval of a hollow placeholder a proxy: If the persistence context already contains an Item with the given identifier, that Item instance is returned by getReference() without hitting the databa. C JPA offers PersistenceUnitUtil helper methods such as isLoaded() to detect whether you're working with an uninitialized proxy.",
  "page287": "Java Persistence also offers bulk operations that translate into direct SQL DELETE statements without life cycle interceptors in the application. We'll discuss these operation\u00aes in section 20.1. Let's say you load an entity instance from the database and work with the data. For some reason, you know that another application or maybe another thread of your application has updated the und\u00aeerlying row in the database. Next, we'll see how to refresh the data held in memory. After you load the entity instance, you realize (how isn't important) that someone else changed the data in\u00aethe database. Calling refresh() causes Hibernate to execute a SELECT to read and marshal a whole result set, overwriting changes you already made to the persistent instance in application memory. If the database row no longer exists (someone deleted it), Hibernate throws an EntityNotFound\u00aeException on refresh(). Most applications don't have to manually refresh in-memory state; concurrent modifications are typically resolved at transaction commit time. The best use case for refreshing is with an extended persistence context, which might span several request/response cycles and/or system transactions. While yo\u00aeu wait for user input with an open persistence context, data gets stale, and selective refreshing may be required depending on the duration of the conversation and the dialogue between the user and the system. Refreshing can be useful to undo changes made in memory during a conversation, if the user cancels the dialogue. We'll have more to say about refreshing in a conversation in section. An entity instance must be fully initialized during life cycle transitions. You may have life cycle callback methods or an entity listener enabled (see section 13.2), and the insalue. Sometimes it's useful to work with the \"deleted\" data further: for example, you might want to save the removed Item again if your user decides to undo. As shown in the example, you can call persist() on a removed instance to cancel the deletion before the persistence context is flushed. Alternatively, if you set the property hibernate.use_ identifier_ rollback to true in persistence.xml, Hibernate will reset the identifier value after removal of an entity instance. In the previous code example, the identifier value is reset to the default",
  "page288": "The persistence context is a cache of persistent instances. Every entity instance in persistent state is associated with the persistence context. Many Hibernate users who ignore t\u00aehis simple fact run into an OutOfMemoryException. This is typically the case when you load thousands of entity instances in a unit of work but never intend to modify them. Hibernate still has to create a snapshot of eac\u00aeh instance in the persistence context cache, which can lead to memory exhaustion. (Obviously, you should execute a bulk data operation if you modify thousands of rows we'll get back to this kind of\u00aeunit of work in section 20.1.) The persistence context cache never shrinks automatically. Keep the size of your persistence context to the necessary minimum. Often, many persistent instances in your context are there by accident for example, because you needed only a few items but querie\u00aed for many. Extremely large graphs can have a serious performance impact and require significant memory for state snapshots. Check that your queries return only data you need, and consider the following ways to control Hibernate's caching behavior. You can call EntityManager#detach(i) to evict a persistent instance manually\u00aefrom the persistence context. You can call EntityManager#clear() to detach all persistent entity instances, leaving you with an empty persistence context. The native Session API has some extra operations you might find useful. You can set the entire persistence context to read-only mode. This disables state snapshots and dirty checking, and Hibernate won't write modifications to the database. Replication is useful, for example, when you need to retrieve data from one database and store it in another. Replication takes detached instances loaded in one persistence context and makes them peame identifier in the database. EXCEPTION Throws an exception if there is an existing database row with the same identifier in the target database. LATEST_VERSION Overwrites the row in the database if its version is older than the version of the given entity instance, or ignores the instance otherwise. Requires enabled optimistic concurrency control with entity versioning You may need replication when you reconcile data entered into different databases. An example case is a product upgrade: if the new version of your application requires a new database checking, guaranteed scope of object identity, and so on.",
  "page289": "Next, we look at the detached entity state. We already mentioned some issues you'll see when entity instances aren't associated with a persistence context anymore, such a\u00aes disabled lazy initialization. Let's explore the detached state with some examples, so you know what to expect when you work with data outside of a persistence context. If a reference leaves the scope of guarante\u00aeed identity, we call it a reference to a detached entity instance. When the persistence context is closed, it no longer provides an identity-mapping service. You'll run into aliasing problems when y\u00aeou work with detached entity instances, so make sure you understand how to handle the identity of detached instances. If you look up data using the same database identifier value in the same persistence context, the result is two references to the same in-memory instance on the JVM heap.\u00aeConsider the two units of work shown next. In the first unit of work at begin() B, you start by creating a persistence context C and loading some entity instances. Because references a and b are obtained from the same persistence context, they have the same Java identity D. They're equal from the same persistence context, they hav\u00aee the same Java identity D. They're equal E because by default equals() relies on Java identity comparison. They obviously have the same database identity F. They reference the same Item instance, in persistent state, managed by the persistence context for that unit of work. The first part of this example finishes by committing the transaction. By default, Hibernate flushes the persistence context of an EntityManager and synchronizes changes with the database whenever the joined transaction is committed. All the previous code examples, except some in the last sectthe query. This is the behavior of FlushModeType.AUTO, the default if you join the EntityManager with a transaction. With FlushModeType.COMMIT, you're disabling flushing before queries, so you may see different data returned by the query than what you have in memory. The synchronization then occurs only when the transaction commits. You can at any time, while a transaction is in progress, force dirty checking and synchronization with the database by calling EntityManager#flush(). This concludes our discussion of the transient, persistent, and removed entication is built with these operations.",
  "page290": "When you begin a journey, it's a good idea to have a mental map of the terrain you'll be passing through. The same is true for an intellectual journey, such as learning t\u00aeo write computer programs. In this case, you'll need to know the basics of what computers are and how they work. You'll want to have some idea of what a computer program is and how one is created. Since you wi\u00aell be writing programs in the Java programming language, you'll want to know something about that language in particular and about the modern, networked computing environment for which Java is desig\u00aened. As you read this chapter, don't worry if you can't understand everything in detail. (In fact, it would be impossible for you to learn all the details from the brief expositions in this chapter.) Concentrate on learning enough about the big ideas to orient yourself, in prepa\u00aeration for the rest of the book. Most of what is covered in this chapter will be covered in much greater detail later in the book. A computer is a complex system consisting of many different components. But at the heart or the brain, if you want of the computer is a single component that does the actual computing. This is the Cent\u00aeral Processing Unit, or CPU. In a modern desktop computer, the CPU is a single \"chip\" on the order of one square inch in size. The job of the CPU is to execute programs. A program is simply a list of unambiguous instructions meant to be followed mechanically by a computer. A computer is built to carry out instructions that are written in a very simple type of language called machine language. Each type of computer has its own machine language, and the computer can directly execute a program only if the program is expressed in that language. (It can execute programs writtetion as a signal to the memory; the memory responds by sending back the data contained in the specifie location. The CPU can also store information in memory by specifying the information to be stored and the address of the location where it is to be stored. On the level of machine language, the operation of the CPU is fairly straightforward (although it is very complicated in detail). The CPU executes a program that is stored as a sequence of machine language instructions in main memory. It does this by repeatedly reading, or fetching, an instruction from by the program that the computer is executing.",
  "page291": "Machine language instructions are expressed as binary numbers. A binary number is made up of just two possible digits, zero and one. So, a machine language instruction is just a se\u00aequence of zeros and ones. Each particular sequence encodes some particular instruction. The data that the computer manipulates is also encoded as binary numbers. A computer can work directly with binary numbers because\u00aeswitches can readily represent such numbers: Turn the switch on to represent a one; turn it off to represent a zero. Machine language instructions are stored in memory as patterns of switches turned on o\u00aer off, When a machine language instruction is loaded into CPU, all that happens is that certain switches are turned on or off in the pattern that encodes that particular instruction. The CPU is built to respond to this pattern by executing the instruction it encodes; it does this simply b\u00aeecause of the way all the other switches in the CPU are wired together. So, you should understand this much about how computers work: Main memory holds machine language programs and data. These are encoded as binary numbers. The CPU fetches machine language instructions from memory one after another and executes them. It does this\u00aemechanically, without thinking about or understanding what it does-and therefore the program it executes must be perfect, complete in all details, and unambiguous because the CPU can do nothing but execute it exactly as written. Here is a schematic view of this first-stage understanding of the computer Asynchronous Events: Polling Loops and Interrupts The CPU spends almost all of its time fetching instructions from memory and executing them. However, the CPU and main memory are only two out of many components in areal computer system, A complete system contains other devices such as: A hard dimages into coded binary numbers that can be stored and manipulated on the computer. The list of devices is entirely open ended, and computer systems are built so that they can easily be expanded by adding new devices. Somehow the CPU has to communicate with and control all these devices. The CPU can only do this by executing machine language instructions (which is all it can do, period). The way this works is that for each device in a system, there is a device driver, which consists of software that the CPU executes when it has to deal with the device. InstallCPU.task it was performing before you pressed the key.",
  "page292": "Again, you should understand that this is a purely mechanical process: A device signals an interrupt simply by turning on a wire. The CPU is built so that when that wire is turned\u00aeon, the CPU saves enough information about what it is currently doing so that it can return to the same state later. This information consists of the contents of important internal registers such as the program counter.\u00aeThen the CPU jumps to some predetermined memory location and begins executing the instructions stored there. Those instructions make up an interrupt handler that does the processing necessary to respond\u00aeto the interrupt. (This interrupt handler is part of the device driver software for the device that signalled the interrupt.) At the end of the interrupt handler is an instruction that tells the CPU to jump back to what it was doing; it does that by restoring its previously saved state.\u00aeInterrupts allow the CPU to deal with asynchronous events. In the regular fetch-and execute cycle, things happen in a predetermined order; everything that happens is \"synchronized\" with everything else. Interrupts make it possible for the CPU to deal efficiently with events that happen \"asynchronously,\" that is\u00ae, at unpredictable times. As another example of how interrupts are used, consider what happens when the CPU need to access data that is stored on the hard disk. The CPU can access data directly only if it is in main memory. Data on the disk has to be copied into memory before it can be accessed. Unfortunately, on the scale of speed at which the CPU operates, the disk drive is extremely slow. When the CPU needs data from the disk, it sends a signal to the disk drive telling it to locate the data and get it ready. (This signal is sent synchronously, under the control of a regular program.)ly switch its attention from one user to another, devoting a fraction of a second to each user in turn. This application of multitasking is called timesharing. But a modern personal computer with just a single user also uses multitasking. For example, the user might be typing a paper while a clock is continuously displaying the time and a file is being downloaded over the network. Each of the individual tasks that the CPU is working on is called a thread. (Or a process; there are technical differences between threads and processes, but they are not importuterbut we will return to threads and events later in the text.",
  "page293": "The Java Virtual Machine Machine language consists of very simple instructions that can be executed directly by the CPU of a computer. Almost all programs, though, are written in h\u00aeigh-level programming languages such as Java, Pascal, or C++. A program written in a high-level language cannot be run directly on any computer. First, it has to be translated into machine language. This translation can\u00aebe done by a program called a compiler. A compiler takes a high-level-language program and translates it into an executable machine-language program. Once the translation is done, the machine-language p\u00aerogram can be run any number of times, but of course it can only be run on one type of computer (since each type of computer has its own individual machine language). If the program is to run on another type of computer it has to be re-translated, using a different compiler, into the appr\u00aeopriate machine language. There is an alternative to compiling a high-level language program. Instead of using a compiler, which translates the program all at once, you can use an interpreter, which translates it instruction-by-instruction, as necessary. An interpreter is a program that acts much like a CPU, with a kind of fetch-a\u00aend-execute cycle. In order to execute a program, the interpreter runs in a loop in which it repeatedly reads one instruction from the program, decides what is necessary to carry out that instruction, and then performs the appropriate machine-language commands to do so. one use of interpreters is to execute high-level language programs. For example, the programming language Lisp is usually executed by an interpreter rather than a compiler. However, interpreters have another purpose: they can let you use a machine-language program meant for one type of computer on a completely different type of compilation and interpretation. Programs written in Java are compiled into machine language, but it is a machine language for a computer that doesn't really exist. This so-called \"virtual\" computer is known as the Java virtual machine. The machine language for the Java virtual machine is called Java bytecode. There is no reason why Java bytecode could not be used as the machine language of a real computer, rather than a virtual computer. However, one of the main selling points of Java is that it can actually be used on any computer.",
  "page294": "Programs must be designed. No one can just sit down at the computer and compose a program of any complexity. The discipline called software engineering is concerned with the constr\u00aeuction of correct, working, well-written programs. The software engineer tends to use accepted and proven methods for analyzing the problem to be solved and for designing a program to solve that problem. During the 1970\u00aes and into the 80s, the primary software engineering methodology was structured programming. The structured programming approach to program design was based on the following advice: To solve a large prob\u00aelem, break the problem into several pieces and work on each piece separately; to solve each piece, treat it as a new problem which can itself be broken down into smaller problems; eventually, you will work your way down to problems that can be solved directly, without further decompositio\u00aen. This approach is called top-down programming. There is nothing wrong with top-down programming. It is a valuable and often-used approach to problem-solving. However, it is incomplete. For one thing, it deals almost entirely with producing the instructions necessary to solve a problem. But as time went on, people realized that\u00aethe design of the data structures for a program was as least as important as the design of subroutines and control structures.Top-down programming doesn't give adequate consideration to the data that the program manipulates. Another problem with strict top-down programming is that it makes it difficult to reuse work done for other projects. By starting with a particular problem and subdividing it into convenient pieces, top-down programming tends to produce a design that is unique to that problem. It is unlikely that you will be able to take a large chunk of programming from another program andnteracts with the rest of the system in a simple, well-defined, straightforward manner. The idea is that a module can be \"plugged into\" a system. The details of what goes on inside the module are not important to the system as a whole, as long as the module fulfills its assigned role correctly. This is called information hiding, and it is one of the most important principles of software engineering. One common format for software modules is to contain some data, along with some subroutines for manipulating that data. For example, a mailingo somessages.",
  "page295": "For example, consider a drawing program that lets the user draw lines, rectangles, ovals, polygons, and curves on the screen. In the program, each visible object on the screen coul\u00aed be represented by a software object in the program. There would be five classes of objects in the program, one for each type of visible object that can be drawn. All the lines would belong to one class, all the rectan\u00aegles to another class, and so on. These classes are obviously related; all of them represent \"drawable objects.\" They would, for example, all presumably be able to respond to a \"draw yours\u00aeelf\" message. Another level of grouping, based on the data to represent each type of object, is less obvious, but would be very useful in a program: We can group polygons and curves together as \"multipoint objects,\" while lines, rectangles, and ovals are \"two-point ob\u00aejects.\" (A line is determined by its endpoints, a rectangle by two of its corners, and an oval by two corners of the rectangle that contains it.) We could diagram these relationships as follows DrawableObject, MultipointObject, and TwoPointObject would be classes in the program. MultipointObject and TwoPointObject would be su\u00aebclasses of DrawableObject. The class Line would be a subclass of TwoPointObject and (indirectly) of DrawableObject. A subclass of a class is said to inherit the properties of that class. The subclass can add to its inheritance and it can even \"override\" part of that inheritance (by defining a different response to some method). Nevertheless, lines, rectangles, and so on are drawable objects, and the class DrawableObject expresses this relationship. Inheritance is a powerful means for organizing a program. It is also related to the problem of reusing software components. A class is tdants who would take your programs and data, feed them to the computer, and return the computer's response some time later. When timesharing where the computer switches its attention rapidly from one person to another was invented in the 1960s, it became possible for several people to interact directly with the computer at the same time. On a timesharing system, users sit at \"terminals\" where they type commands to the computer, and the computer types back its response. Early personal computers also used typed commands and responses.",
  "page296": "The use of the term \"message\" here is deliberate. Messages, as you saw in the previous section, are sent to objects. In fact, Java GUI components are implemented as objec\u00aets. Java includes many predefined classes that represent various types of GUI components. Some of these classes are subclasses of others. Here is a diagram showing some of Swing's GUI classes and their relationship\u00aes. Don't worry about the details for now, but try to get some feel about how object-oriented programming and inheritance are used here. Note that all the GUI classes are subclasses, directly or indi\u00aerectly, of a class called JComponent, which represents general properties that are shared by all Swing components. Two of the direct subclasses of JComponent themselves have subclasses. The classes JTextArea and JTextField, which have certain behaviors in common, are grouped together as s\u00aeubclasses of JTextComponent. Similarly JButton and JToggleButton are subclasses of JAbstractButton, which represents properties common to both buttons and checkboxes. (JComboBox, by the way, is the Swing class that represents pop-up menus.) Just from this brief discussion, perhaps you can see how GUI programming can make effective\u00aeuse of object-oriented design. In fact, GUI's, with their \"visible objects,\" are probably a major factor contributing to the popularity of OOP. The Internet and the World-Wide Web - Computers can be connected together on networks. A computer on a network can communicate with other computers on the same network by exchanging data and files or by sending and receiving messages. Computers on a network can even work together on a large computation. Today, millions of computers throughout the world are connected to a single huge network called the Internet. New computers are b to another, and the Transmission Control Protocol (TCP), which ensures that data sent using IP is received in its entirety and without error. These two protocols, which are referred to collectively as TCP/IP, provide a foundation for communication. Other protocols use TCP/IP to send specific types of information such as web pages, electronic mail, and data files. All communication over the Internet is in the form of packets. A packet consists of some data being sent from one computer to another, along with addressing information that indicates where on by the programs that the person uses to send and receive email messages.",
  "page297": "On a basic level (the level of machine language), a computer can perform only very simple operations. A computer performs complex tasks by stringing together large numbers of such\u00aeoperations. Such tasks must be \"scripted\" in complete and perfect detail by programs. Creating complex programs will never be really easy, but the difficulty can be handled to some extent by giving the program\u00aea clear overall structure. The design of the overall structure of a program is what I call \"programming in the large.\" Programming in the small, which is sometimes called coding, would then re\u00aefer to filling in the details of that design. The details are the explicit, step-by-step instructions for performing fairly small-scale tasks. When you do coding, you are working fairly \"close to the machine,\" with some of the same concepts that you might use in machine language\u00ae: memory locations, arithmetic operations, loops and branches. In a high-level language such as Java, you get to work with these concepts on a level several steps above machine language. However, you still have to worry about getting all the details exactly right. This chapter and the next examine the facilities for programming in\u00aethe small in the Java programming language. Don't be misled by the term \"programming in the small\" into thinking that this material is easy or unimportant. This material is an essential foundation for all types of programming. If you don't understand it, you can't write programs, no matter how good you get at designing their large-scale structure. The Basic Java Application - A program is a sequence of instructions that a computer can execute to perform some task. A simple enough idea, but for the computer to make any use of the instructions, they must be writteu fix the problem). So, to be a successful programmer, you have to develop a detailed knowledge of the syntax of the programming language that you are using. However, syntax is only part of the story. It's not enough to write a program that will run you want a program that will run and produce the correct result! That is, the meaning of the program has to be right. The meaning of a program is referred to as its semantics. A semantically correct program is one that does what you want it to. Furthermore, a program can be syntactically and semantically correche mming environments.",
  "page298": "Here is a Java program to display the message \"Hello World!\". Don't expect to understand what's going on here just yet some of it you won't really understa\u00aend until a few chapters from now: // A program to display the message // \\\"Hello World!\\\" on standard output public class HelloWorld { public static void main(String[] args) { System.out.println(\\\"Hello World!\\\"\u00ae); } } // end of class HelloWorld The command that actually displays the message is: System.out.println(\\\"Hello World!\\\"); This command is an example of a subroutine call statement. It uses a \"b\u00aeuilt-in subroutine\" named System.out.println to do the actual work. Recall that a subroutine consists of the instructions for performing some task, chunked together and given a name. That name can be used to \"call\" the subroutine whenever that task needs to be performed. A\u00aebuilt-in subroutine is one that is already defined as part of the language and therefore automatically available for use in any program. When you run this program, the message \"Hello World!\" (without the quotes) will be displayed on standard output. Unfortunately, I can't say exactly what that means! Java is meant t\u00aeo run on many different platforms, and standard output will mean different things on different platforms. However, you can expect the message to show up in some convenient place. (If you use a command-line interface, like that in Sun Microsystem's Java Development Kit, you type in a command to tell the computer to run the program. The computer will type the output from the program, Hello World!, on the next line.) You must be curious about all the other stuff in the above program. Part of it consists of comments. Comments in a program are entirely ignored by the computer; theybove program (not counting the comments) says that this is a class named HelloWorld. \"HelloWorld,\" the name of the class, also serves as the name of the program. Not every class is a program. In order to define a program, a class must include a subroutine named main, with a definition that takes the form public static void main(String[] args) { (statements) } When you tell the Java interpreter to run the program, the interpreter calls the main() subroutine, and the statements that it contains are executed. These statements make up the script that tellloWo for layout that are followed by most programmers.",
  "page299": "Variables and the Primitive Types - Names are fundamental to programming. In programs, names are used to refer to many different sorts of things. In order to use those things, a pr\u00aeogrammer must understand the rules for giving names to things and the rules for using the names to work with those things. That is, the programmer must understand the syntax and the semantics of names. According to the\u00aesyntax rules of Java, a name is a sequence of one or more characters. It must begin with a letter or underscore and must consist entirely of letters, digits, and underscores. (\"Underscore\" refe\u00aers to the character ' '.) For example, here are some legal names: No spaces are allowed in identifiers; HelloWorld is a legal identifier, but \"Hello World\" is not. Upper case and lower case letters are considered to be different, so that HelloWorld, helloworld, HELLOWO\u00aeRLD, and hElloWorLD are all distinct names. Certain names are reserved for special uses in Java, and cannot be used by the programmer for other purposes. These reserved words include: class, public, static, if, else, while, and several dozen other words. Java is actually pretty liberal about what counts as a letter or a digit. Jav\u00aea uses the Unicode character set, which includes thousands of characters from many different languages and different alphabets, and many of these characters count as letters or digits. However, I will be sticking to what can be typed on a regular English keyboard. The pragmatics of naming includes style guidelines about how to choose names for things. For example, it is customary for names of classes to begin with upper case letters, while names of variables and of subroutines begin with lower case letters; you can avoid a lot of confusion by following the same convention in your own programs.t.println. The idea here is that things in Java can contain other things. A compound name is a kind of path to an item through one or more levels of containment. The name System.out.println indicates that something called \"System\" contains something called \"out\" which in turn contains something called \"println\". Non-compound names are called simple identifiers. I'll use the term identifier to refer to any name simple or compound that can be used to refer to something in Java. (Note that the reserved words are not identifiers, an tion is the number 0.07.",
  "page300": "Types and Literals- A variable in Java is designed to hold only one particular type of data; it can legally hold that type of data and no other. The compiler will consider it to be\u00aea syntax error if you try to violate this rule. We say that Java is a strongly typed language because it enforces this rule. There are eight so-called primitive types built into Java. The primitive types are named byte\u00ae, short, int, long, float, double, char, and boolean. The first four types hold integers (whole numbers such as 17, -38477, and 0). The four integer types are distinguished by the ranges of integers they\u00aecan hold. The float and double types hold real numbers (such as 3.6 and -145.99). Again, the two real types are distinguished by their range and accuracy. A variable of type char holds a single character from the Unicode character set. And a variable of type boolean holds one of the two\u00aelogical values true or false. Any data value stored in the computer's memory must be represented as a binary number, that is as a string of zeros and ones. A single zero or one is called a bit. A string of eight bits is called a byte. Memory is usually measured in terms of bytes. Not surprisingly, the byte data type refers to\u00aea single byte of memory. A variable of type byte holds a string of eight bits, which can represent any of the integers between -128 and 127, inclusive. (There are 256 integers in that range; eight bits can represent 256 two raised to the power eight different values.) As for the other integer types, short corresponds to two bytes (16 bits). Variables of type short have values in the range -32768 to 32767. int corresponds to four bytes (32 bits). Variables of type int have values in the range -2147483648 to 2147483647. long corresponds to eight bytes (64 bits). Variables of type long have values in the power 308, and has about 15 significant digits. Ordinarily, you should stick to the double type for real values. A variable of type char occupies two bytes in memory. The value of a char variable is a single character such as A, *, x, or a space character. The value can also be a special character such a tab or a carriage return or one of the many Unicode characters that come from different languages. When a character is typed into a program, it must be surrounded by single quotes; for example: 'A', '*', or 'x'.",
  "page301": "All software problems can be termed as bugs. A software bug usually occurs when the software does not do what it is intended to do or does something that it is not intended to do.\u00aeFlaws in specifications, design, code or other reasons can cause these bugs. Identifying and fixing bugs in the early stages of the software is very important as the cost of fixing bugs grows over time. So, the goal of\u00aea software tester is to find bugs and find them as early as possible and make sure they are fixed. Testing is context-based and risk-driven. It requires a methodical and disciplined approach to finding b\u00aeugs. A good software tester needs to build credibility and possess the attitude to be explorative, troubleshooting, relentless, creative, diplomatic and persuasive. As against the perception that testing starts only after the completion of coding phase, it actually begins even before the\u00aefirst line of code can be written. In the life cycle of the conventional software product, testing begins at the stage when the specifications are written, i.e. from testing the product specifications or product spec. Finding bugs at this stage can save huge amounts of time and money. Once the specifications are well understood, y\u00aeou are required to design and execute the test cases. Selecting the appropriate technique that reduces the number of tests that cover a feature is one of the most important things that you need to take into consideration while designing these test cases. Test cases need to be designed to cover all aspects of the software, i.e. security, database, functionality (critical and general) and the user interface. Bugs originate when the test cases are executed. As a tester you might have to perform testing under different circumstances, i.e. the application could be in t, keeping pace with the latest developments in the field will augment your career as a software test engineer.",
  "page302": "Software is a series of instructions for the computer that perform a particular task,called a program; the two major categories of software are system software andapplication softw\u00aeare. System software is made up of control programs. Applicationsoftware is any program that processes data for the user (spreadsheet, wordprocessor, payroll, etc.).A software product should only be released after it ha\u00aes gone through a properprocess of development, testing and bug fixing. Testing looks at areas such asperformance, stability and error handling by setting up test scenarios undercontrolled conditions and\u00aeassessing the results. This is why exactly any software hasto be tested. It is important to note that software is mainly tested to see that it meetsthe customers' needs and that it conforms to the standards. It is a usual norm thatsoftware is considered of good quality if it meets th\u00aee user requirements.Quality can briefly be defined as \"a degree of excellence\". High quality softwareusually conforms to the user requirements. A customer's idea of quality may cover abreadth of features - conformance to specifications, good performance onplatform(s)/configurations, completely meets operational requ\u00aeirements (even if notspecified!), compatibility to all the end-user equipment, no negative impact onexisting end-user base at introduction time.Quality software saves good amount of time and money. Because software will havefewer defects, this saves time during testing and maintenance phases. Greaterreliability contributes to an immeasurable increase in customer satisfaction as well aslower maintenance costs. Because maintenance represents a large portion of allsoftware costs, the overall cost of the project will most likely be lower than similarprojects.Following are two cases thagn flaws at an early stage beforefailures occur in production, or in the field Promote continual improvement",
  "page303": "As software engineering is now being considered as a technical engineeringprofession, it is important that the software test engineer's posses certain traits witha relentless\u00aeattitude to make them stand out. Here are a few.Know the technology. Knowledge of the technology in which the application isdeveloped is an added advantage to any tester. It helps design better and powerfultest cases ba\u00aesing on the weakness or flaws of the technology. Good testers knowwhat it supports and what it doesn't, so concentrating on these lines will help thembreak the application quickly. Perfectionist and\u00aea realist. Being a perfectionist will help testers spot the problemand being a realist helps know at the end of the day which problems are reallyimportant problems. You will know which ones require a fix and which ones don't.Tactful, diplomatic and persuasive. Good software testers\u00aeare tactful and knowhow to break the news to the developers. They are diplomatic while convincing thedevelopers of the bugs and persuade them when necessary and have their bug(s)fixed. It is important to be critical of the issue and not let the person who developedthe application be taken aback of the findings. An explorer. A bit\u00aeof creativity and an attitude to take risk helps the testersventure into unknown situations and find bugs that otherwise will be looked over.Troubleshoot. Troubleshooting and figuring out why something doesn't workhelps testers be confident and clear in communicating the defects to the developers. Posses people skills and tenacity. Testers can face a lot of resistance fromprogrammers. Being socially smart and diplomatic doesn't mean being indecisive. Thebest testers are both-socially adept and tenacious where it matters.Organized. Best testers very well realize that they too can makat a later stage. Defects can cause serious problems if not managedproperly. Learning from defects helps - prevention of future problems, trackimprovements, improve prediction and estimation.",
  "page304": "Testing can't show that bugs don't exist. An important reason for testing is toprevent defects. You can perform your tests, find and report bugs, but at no point canyou g\u00aeuarantee that there are no bugs. It is impossible to test a program completely. Unfortunately this is not possibleeven with the simplest program because - the number of inputs is very large, numberof outputs is ver\u00aey large, number of paths through the software is very large, and thespecification is subjective to frequent changes. You can't guarantee quality. As a software tester, you cannot test everything and\u00aeare not responsible for the quality of the product. The main way that a tester can failis to fail to report accurately a defect you have observed. It is important to rememberthat we seldom have little control over quality. Target environment and intended end user. Anticipating and testing\u00aetheapplication in the environment user is expected to use is one of the major factors thatshould be considered. Also, considering if the application is a single user system ormulti user system is important for demonstrating the ability for immediate readinesswhen necessary. The error case of Disney's Lion King illustrates th\u00aeis. Disney Companyreleased its first multimedia CD-ROM game for children, The Lion King AnimatedStorybook. It was highly promoted and the sales were huge. Soon there were reportsthat buyers were unable to get the software to work. It worked on a few systems -likely the ones that the Disney programmers used to create the game - but not on themost common systems that the general public used.No application is 100% bug free. It is more reasonable to recognize there arepriorities, which may leave some less critical problems unsolved or unidentified.Simple case is",
  "page305": " Build your credibility. Credibility is like quality that includes reliability, knowledge,consistency, reputation, trust, attitude and attention to detail. It is not instant butsho\u00aeuld be built over time and gives voice to the testers in the organization. Your keysto build credibility - identify your strengths and weaknesses, build good relations,demonstrate competency, and be willing to admi\u00aet mistakes, re-assess and adjust. Test what you observe. It is very important that you test what you can observeand have access to. Writing creative test cases can help only when you have the opportunity\u00aeto observe the results. So, assume nothing.Not all bugs you find will be fixed. Deciding which bugs will be fixed and whichwon't is a risk-based decision. Several reasons why your bug might not be fixed iswhen there is no enough time, the bug is dismissed for a new feature, fixing i\u00aet mightbe very risky or it may not be worth it because it occurs infrequently or has a workaround where the user can prevent or avoid the bug. Making a wrong decision can bedisastrous.Review competitive products. Gaining a good insight into various products of thesame kind and getting to know their functionality and general behavi\u00aeor will help youdesign different test cases and to understand the strengths and weaknesses of yourapplication. This will also enable you to add value and suggest new features andenhancements to your product.Follow standards and processes. As a tester, your need to conform to thestandards and guidelines set by the organization. These standards pertain toreporting hierarchy, coding, documentation, testing, reporting bugs, using automatedtools etc. The software life cycle typically includes the following: requirements analysis, design,coding, testing, installation and maintenance. In betweeto identify which requirementsshould be allocated to which components.Design and Specifications. The outcome of requirements analysis is therequirements specification. Using this, the overall design for the intended softwareis developed.Activities in this phase - Perform Architectural Design for the software, DesignDatabase (If applicable), Design User Interfaces, Select or Develop Algorithms (IfApplicable), Perform Detailed Design.",
  "page306": "Coding. The development process tends to run iteratively through these phasesrather than linearly; several models (spiral, waterfall etc.) have been proposed todescribe this proces\u00aes.Activities in this phase - Create Test Data, Create Source, Generate Object Code,Create Operating Documentation, Plan Integration, Perform IntegrationTesting. The process of using the developed system with the intent\u00aeto find errors.Defects/flaws/bugs found at this stage will be sent back to the developer for a fixand have to be re-tested. This phase is iterative as long as the bugs are fixed to meetthe requirements.A\u00aectivities in this phase - Plan Verification and Validation, Execute Verification andvalidation Tasks, Collect and Analyze Metric Data, Plan Testing, Develop TestRequirements, Execute TestsInstallation. The so developed and tested software will finally need to be installed atthe client pla\u00aece. Careful planning has to be done to avoid problems to the user afterinstallation is done Activities in this phase - Plan Installation, Distribution of Software, Installation ofSoftware, Accept Software in Operational Environment.Operation and Support. Support activities are usually performed by theorganization that developed th\u00aee software. Both the parties usually decide on theseactivities before the system is developed.Activities in this phase - Operate the System, Provide Technical Assistance andConsulting, Maintain Support Request Log.Maintenance. The process does not stop once it is completely implemented andinstalled at user place; this phase undertakes development of new features,enhancements etc.Activities in this phase - Reapplying Software Life Cycle.The way you approach a particular application for testing greatly depends on the lifecycle model it follows. This is because, each life cycle nning High Level Test plan, QA plan (quality goals), identify - reportingprocedures, problem classification, acceptance criteria, databases for testing,measurement criteria (defect quantities/severity level and defect origin), projectmetrics and finally begin the schedule for project testing. Also, plan to maintain all test cases (manual or automated) in a database.",
  "page307": "Analysis. Involves activities that - develop functional validation based on BusinessRequirements (writing test cases basing on these details), develop test case format(time estimat\u00aees and priority assignments), develop test cycles (matrices andtimelines), identify test cases to be automated (if applicable), define area of stressand performance testing, plan the test cycles required for the project\u00aeand regressiontesting, define procedures for data maintenance (backup, restore, validation), reviewdocumentation.Design. Activities in the design phase - Revise test plan based on changes, revise testcy\u00aecle matrices and timelines, verify that test plan and cases are in a database orrequisite, continue to write test cases and add new ones based on changes, developRisk Assessment Criteria, formalize details for Stress and Performance testing, finalizetest cycles (number of test case per cy\u00aecle based on time estimates per test case andpriority), finalize the Test Plan, (estimate resources to support development in unittesting).Construction (Unit Testing Phase). Complete all plans, complete Test Cycle matricesand timelines, complete all test cases (manual), begin Stress and Performance testing,test the automated testi\u00aeng system and fix bugs, (support development in unittesting), run QA acceptance test suite to certify software is ready to turn over to QA.Test Cycle(s) / Bug Fixes (Re-Testing/System Testing Phase). Run the test cases (frontand back end), bug reporting, verification, and revise/add test cases as required.Final Testing and Implementation (Code Freeze Phase). Execution of all front end testcases - manual and automated, execution of all back end test cases - manual andautomated, execute all Stress and Performance tests, provide on-going defecttracking metrics, provide on-ror that causes an unexpected defect,fault, flaw, or imperfection in a computer program. In other words,",
  "page308": "Constantly changing software requirements cause a lot of confusion and pressureboth on the development and testing teams. Often, a new feature added or existingfeature removed can\u00aebe linked to the other modules or components in the software.Overlooking such issues causes bugs. Also, fixing a bug in one part/component of the software might arise another in adifferent or same component. Lack of for\u00aeesight in anticipating such issues can causeserious problems and increase in bug count. This is one of the major issues because ofwhich bugs occur since developers are very often subject to pressure rela\u00aeted totimelines; frequently changing requirements, increase in the number of bugs etc. Designing and re-designing, UI interfaces, integration of modules, databasemanagement all these add to the complexity of the software and the system as awhole. Fundamental problems with software design\u00aeand architecture can cause problemsin programming. Developed software is prone to error as programmers can makemistakes too. As a tester you can check for, data reference/declaration errors, controlflow errors, parameter errors, input/output errors etc. Rescheduling of resources, re-doing or discarding already completed work,chang\u00aees in hardware/software requirements can affect the software too. Assigning anew developer to the project in midway can cause bugs. This is possible if propercoding standards have not been followed, improper code documentation, ineffectiveknowledge transfer etc. Discarding a portion of the existing code might just leave itstrail behind in other parts of the software; overlooking or not eliminating such codecan cause bugs. Serious bugs can especially occur with larger projects, as it getstougher to identify the problem area. Programmers usually tend to rush as the deadline appreate problemselsewhere. In most of the cases, the life cycle gets very complicated and difficult totrack making it imperative to have a bug/defect tracking system in place.See Chapter 7 - Defect TrackingFollowing are the different phases of a Bug Life Cycle:Open: A bug is in Open state when a tester identifies a problem areaAccepted: The bug is then assigned to a developer for a fix. The developer thenaccepts if valid.",
  "page309": "Not Accepted/Won't fix: If the developer considers the bug as low level or does notaccept it as a bug, thus pushing it into Not Accepted/Won't fix state.Such bugs will be\u00aeassigned to the project manager who will decide if the bug needs afix. If it needs, then assigns it back to the developer, and if it doesn't, then assigns itback to the tester who will have to close the bug.Pendin\u00aeg: A bug accepted by the developer may not be fixed immediately. In suchcases, it can be put under Pending state.Fixed: Programmer will fix the bug and resolves it as Fixed.Close: The fixed bug will be a\u00aessigned to the tester who will put it in the Close state.Re-Open: Fixed bugs can be re-opened by the testers in case the fix producesproblems elsewhere.Costs are logarithmic; they increase in size tenfold as the time increases. A bug foundand fixed during the early stages - requireme\u00aents or product spec stage can be fixed bya brief interaction with the concerned and might cost next to nothing.During coding, a swiftly spotted mistake may take only very less effort to fix. Duringintegration testing, it costs the paperwork of a bug report and a formally documentedfix, as well as the delay and expense of a re-test\u00ae.During system testing it costs even more time and may delay delivery. Finally, duringoperations it may cause anything from a nuisance to a system failure, possibly withcatastrophic consequences in a safety-critical system such as an aircraft or anemergency service.It is difficult to determine when exactly to stop testing. Here are a few commonfactors that help you decide when you can stop or reduce testing: Deadlines (release deadlines, testing deadlines, etc.) Test cases completed with certain percentage passed Test budget depleted Coverage of code/functionality/requirements reaches a specified point Bug rate falls below a certain level Beta or alpha testing period ends There are basically three levels of testing i.e. Unit Testing, Integration Testing andSystem Testing. Various types of testing come under these levels.Unit TestingTo verify a single program or a section of a single programIntegration TestingTo verify interaction between system componentsPrerequisite: unit testing completed on all components that compose a system.System TestingTo verify and validate behaviors of the entire system against the original systemobjectives.",
  "page310": "Software testing is a process that identifies the correctness, completeness, andquality of software.Following is a list of various types of software testing and their definitions i\u00aen arandom order: Formal Testing: Performed by test engineers Informal Testing: Performed by the developers Manual Testing: That part of software testing that requires human input, analysis,or evaluation. Automated Testi\u00aeng: Software testing that utilizes a variety of tools to automatethe testing process. Automated testing still requires a skilled quality assuranceprofessional with knowledge of the automation tools and t\u00aehe software being testedto set up the test cases. Black box Testing: Testing software without any knowledge of the back-end of thesystem, structure or language of the module being tested. Black box test cases arewritten from a definitive source document, such as a specification or require\u00aementsdocument. White box Testing: Testing in which the software tester has knowledge of theback-end, structure and language of the software, or at least its purpose. Unit Testing: Unit testing is the process of testing a particular complied program,i.e., a window, a report, an interface, etc. independently as a stand-alonecomponen\u00aet/program. The types and degrees of unit tests can vary among modifiedand newly created programs. Unit testing is mostly performed by the programmerswho are also responsible for the creation of the necessary unit test data. Incremental Testing: Incremental testing is partial testing of an incompleteproduct. The goal of incremental testing is to provide an early feedback to softwaredevelopers. System Testing: System testing is a form of black box testing. The purpose ofsystem testing is to validate an application's accuracy and completeness inperforming the functions as designed.Integration Testing: Testing two or more modules or functions together with theintent of finding interface defects between the modules/functions. System Integration Testing: Testing of software components that have beendistributed across multiple platforms (e.g., client, web server, application server, anddatabase server) to produce failures caused by system integration defects (i.e. defectsinvolving distribution and back- Functional Testing:ngnetwork communication, or interacting with other hardware, application, or system.",
  "page311": "Acceptance Testing: Testing the system with the intent of confirming readiness ofthe product and customer acceptance. Also known as User Acceptance Testing. Adhoc Testing: Testing\u00aewithout a formal test plan or outside of a test plan. Withsome projects this type of testing is carried out as an addition to formal testing.Sometimes, if testing occurs very late in the development cycle, this will be\u00aethe onlykind of testing that can be performed - usually done by skilled testers. Sometimes adhoc testing is referred to as exploratory testing. Configuration Testing: Testing to determine how well t\u00aehe product works with abroad range of hardware/peripheral equipment configurations as well as on differentoperating systems and software. Load Testing: Testing with the intent of determining how well the product handlescompetition for system resources. The competition may come in the form\u00aeof networktraffic, CPU utilization or memory allocation. Stress Testing: Testing done to evaluate the behavior when the system is pushedbeyond the breaking point. The goal is to expose the weak links and to determine ifthe system manages to recover gracefully. Performance Testing: Testing with the intent of determining how effici\u00aeently aproduct handles a variety of events. Automated test tools geared specifically to testand fine-tune performance are used most often for this type of testing. Usability Testing: Usability testing is testing for 'user-friendliness'. A way toevaluate and measure how users interact with a software product or site. Tasks aregiven to users and observations are made. Installation Testing: Testing with the intent of determining if the product iscompatible with a variety of platforms and how easily it installs. Recovery/Error Testing: Testing how well a system recovers from crashes,hardware failures, or other catastrophic problems. Security Testing: Testing of database and network software in order to keepcompany data and resources secure from mistaken/accidental users, hackers, andother malevolent attackers. Penetration Testing: Penetration testing is testing how well the system isprotected against unauthorized internal or external access, or willful damage. Thistype of testing usually requires sophisticated testing ttion, based on information they learn as they're executingtheir tests. Comparison Testing:",
  "page312": "Following are the most common software errors that aid you in software testing. Thishelps you to identify errors systematically and increases the efficiency andproductivity of soft\u00aeware testing.User Interface Errors: Missing/Wrong Functions Doesn't do what the user expects,Missing information, Misleading, Confusing information, Wrong content in Help text,Inappropriate error messages. Performa\u00aence issues - Poor responsiveness, Can'tredirect output, Inappropriate use of key board Error Handling: Inadequate - protection against corrupted data, tests of userinput, version control; Ignores -\u00aeoverflow, data comparison, Error recovery - abortingerrors, recovery from hardware problems. Boundary related errors: Boundaries in loop, space, time, memory, mishandling ofcases outside boundary. Calculation errors: Bad Logic, Bad Arithmetic, Outdated constants, Calculationerrors, i\u00aencorrect conversion from one data representation to another, Wrongformula, incorrect approximation. Initial and Later states: Failure to - set data item to zero, to initialize a loopcontrol variable, or re-initialize a pointer, to clear a string or flag, Incorrectinitialization. Control flow errors: Wrong returning state assumed,\u00aeException handling basedexits, Stack underflow/overflow, Failure to block or un-block interrupts, Comparisonsometimes yields wrong result, Missing/wrong default, and Data Type errors. Errors in Handling or Interpreting Data: Un-terminated null strings, overwriting afile after an error exit or user abort. Race Conditions: Assumption that one event or task finished before anotherbegins, Resource races, Tasks starts before its prerequisites are met, Messages crosor don't arrive in the order sent.Load Conditions: Required resources are not available, No available large memoryarea, Low priority tasks not put off, doesn't erase old files from mass storage, anddoesn't return unused memory.Hardware: Wrong Device, Device unavailable, Underutilizing device intelligence,Misunderstood status or return code, Wrong operation or instruction codes. Source, Version and ID Control: No Title or version ID, Failure to update multiplecopies of data or program files.Testing Errors: Failure to notice/report a problem",
  "page313": "Test Policy - A document characterizing the organization's philosophy towardssoftware testing.Test Strategy - A high-level document defining the test phases to be performed an\u00aedthe testing within those phases for a program. It defines the process to be followed ineach project. This sets the standards for the processes, documents, activities etc. thatshould be followed for each project.For exa\u00aemple, if a product is given for testing, you should decide if it is better to useblack-box testing or white-box testing and if you decide to use both, when will youapply each and to which part of the sof\u00aetware? All these details need to be specified inthe Test Strategy.Project Test Plan - a document defining the test phases to be performed and thetesting within those phases for a particular project.A Test Strategy should cover more than one project and should address the followingissues:\u00aeAn approach to testing high risk areas first, Planning for testing, How toimprove the process based on previous testing, Environments/data used, Testmanagement - Configuration management, Problem management, What Metrics arefollowed, Will the tests be automated and if so which tools will be used, What are theTesting Stages and Tes\u00aeting Methods, Post Testing Review process, Templates.Test planning needs to start as soon as the project requirements are known. The firstdocument that needs to be produced then is the Test Strategy/Testing Approach thatsets the high level approach for testing and covers all the other elements mentionedabove.Once the approach is understood, a detailed test plan can be written. Usually, thistest plan can be written in different styles. Test plans can completely differ fromproject to project in the same organization.PurposeTo describe the scope, approach, resources, and schedule of the testing activities. Toidentify the items being tested, the features to be tested, the testing tasks to beperformed, the personnel responsible for each task, and the risks associated with thisplan.OUTLINEA test plan shall have the following structure: Test plan identifier. A unique identifier assign to the test plan. Introduction: Summarized the software items and features to be tested andthe need for them to be included. Test items: Identify contingencies Approvals",
  "page314": "Like any other process in software testing, the major tasks in test planning are to -Develop Test Strategy, Critical Success Factors, Define Test Objectives, IdentifyNeeded Te\u00aest Resources, Plan Test Environment, Define Test Procedures, IdentifyFunctions To Be Tested, Identify Interfaces With Other Systems or Components, WriteTest Scripts, Define Test Cases, Design Test Data, Build Test Matri\u00aex, Determine TestSchedules, Assemble Information, Finalize the Plan.A test case is a detailed procedure that fully tests a feature or an aspect of a feature.While the test plan describes what to test, a\u00aetest case describes how to perform aparticular test. You need to develop test cases for each test listed in the test planAs a tester, the best way to determine the compliance of the software torequirements is by designing effective test cases that provide a thorough test of aunit. Various\u00aetest case design techniques enable the testers to develop effective testcases. Besides, implementing the design techniques, every tester needs to keep inmind general guidelines that will aid in test case design:a. The purpose of each test case is to run the test in the simplest way possible.[Suitable techniques - Specification de\u00aerived tests, Equivalence partitioning]b. Concentrate initially on positive testing i.e. the test case should show that thesoftware does what it is intended to do. [Suitable techniques - Specification derivedtests, Equivalence partitioning, State-transition testing]c. Existing test cases should be enhanced and further test cases should be designedto show that the software does not do anything that it is not specified to do i.e.Negative Testing [Suitable techniques - Error guessing, Boundary value analysis,Internal boundary value testing, State-transition testing]d. Where appropriate, test cases should be designed to address issues such asperformance, safety requirements and security requirements [Suitable techniques -Specification derived tests]e. Further test cases can then be added to the unit test specification to achievespecific test coverage objectives. Once coverage tests have been designed, the testprocedure can be developed and the tests executed [Suitable techniques - Branchtesting, Condition testing, categories: Black boxtechniques,",
  "page315": "For example, if a program accepts integer values only from 1 to 10. The possible testcases for such a program would be the range of all integers. In such a program, allintegers up\u00aeto 0 and above 10 will cause an error. So, it is reasonable to assume that if11 will fail, all values above it will fail and vice versa.If an input condition is a range of values, let one valid equivalence class is the\u00aerange (0or 10 in this example). Let the values below and above the range be two respectiveinvalid equivalence values (i.e. -1 and 11). Therefore, the above three partition valuescan be used as test cases\u00aefor the above example.Boundary Value AnalysisThis is a selection technique where the test data are chosen to lie along theboundaries of the input domain or the output range. This technique is often called asstress testing and incorporates a degree of negative testing in the test design b\u00aeyanticipating that errors will occur at or around the partition boundaries.For example, a field is required to accept amounts of money between $0 and $10. Asa tester, you need to check if it means up to and including $10 and $9.99 and if $10 isacceptable. So, the boundary values are $0, $0.01, $9.99 and $10.Now, the following test\u00aes can be executed. A negative value should be rejected, 0should be accepted (this is on the boundary), $0.01 and $9.99 should be accepted,null and $10 should be rejected. In this way, it uses the same concept of partitions asequivalence partitioning.State Transition TestingAs the name suggests, test cases are designed to test the transition between thestates by creating the events that cause the transition.Branch TestingIn branch testing, test cases are designed to exercise control flow branches ordecision points in a unit. This is usually aimed at achieving a target level of DecisionCoverage. Branch Coverage, need to test both branches of IF and ELSE. All branchesand compound conditions (e.g. loops and array handling) within the branch should beexercised at least once.Condition TestingThe object of condition testing is to design test cases to show that the individualcomponents of logical conditions and combinations of the individual components arecorrect. Test cases are designed to tess set. Data use isanywhere that a data item is read or used. The objective is to create test cases thatwill drive execution through paths between specific definitions and uses.",
  "page316": "Internal Boundary Value TestingIn many cases, partitions and their boundaries can be identified from a functionalspecification for a unit, as described under equivalence partitioni\u00aeng and boundaryvalue analysis above. However, a unit may also have internal boundary values thatcan only be identified from a structural specification.Error GuessingIt is a test case design technique where the testers u\u00aese their experience to guess thepossible errors that might occur and design test cases accordingly to uncover them.Using any or a combination of the above described test case design techniques; youcan de\u00aevelop effective test cases.A use case describes the system's behavior under various conditions as it responds toa request from one of the users. The user initiates an interaction with the system toaccomplish some goal. Different sequences of behavior, or scenarios, can unfold,dependi\u00aeng on the particular requests made and conditions surrounding the requests.The use case collects together those different scenarios.Use cases are popular largely because they tell coherent stories about how thesystem will behave in use. The users of the system get to see just what this newsystem will be and get to react early.As d\u00aeiscussed earlier, defect is the variance from a desired product attribute (it can bea wrong, missing or extra data). It can be of two types - Defect from the product or avariance from customer/user expectations. It is a flaw in the software system and hasno impact until it affects the user/customer and operational system.With the knowledge of testing so far gained, you can now be able to categorize thedefects you have found. Defects can be categorized into different types basing on thecore issues they address. Some defects address security or database issues whileothers may refer to functionality or UI issues.Security Defects: Application security defects generally involve improper handling ofdata sent from the user to the application. These defects are the most severe andgiven highest priority for a fix.Examples:Authentication: Accepting an invalid username/password Authorization: Accessibility to pages though permission not givenData Quality/Database Defects: Deals with improper handling of data in thedatabasaffect the functionality of the application.Examples: All Javascript errors Buttons like Save, Delete, Cancel",
  "page317": "User Interface Defects: As the name suggests, the bugs deal with problems related toUI are usually considered less severe.Examples:Improper error/warning/UI messages Spelling mista\u00aekes Alignment problemsOnce the test cases are developed using the appropriate techniques, they areexecuted which is when the bugs occur. It is very important that these bugs bereported as soon as possible because, the e\u00aearlier you report a bug, the more timeremains in the schedule to get it fixed.Simple example is that you report a wrong functionality documented in the Help file afew months before the product release, t\u00aehe chances that it will be fixed are very high.If you report the same bug few hours before the release, the odds are that it won't befixed. The bug is still the same though you report it few months or few hours beforethe release, but what matters is the time.It is not just enough to\u00aefind the bugs; these should also be reported/communicatedclearly and efficiently, not to mention the number of people who will be reading thedefect.Defect tracking tools (also known as bug tracking tools, issue tracking tools orproblem trackers) greatly aid the testers in reporting and tracking the bugsfound in software applicatio\u00aens. They provide a means of consolidating a keyelement of project information in one place. Project managers can then seewhich bugs have been fixed, which are outstanding and how long it is taking tofix defects. Senior management can use reports to understand the state of thedevelopment process.You should provide enough detail while reporting the bug keeping in mind the peoplewho will use it - test lead, developer, project manager, other testers, new testersassigned etc. This means that the report you will write should be concise, straight andclear. Following are the details your report should contain:Bug Title Bug identifier (number, ID, etc.) The application name or identifier and version The function, module, feature, object, screen, etc. where the bug occurredEnvironment (OS, Browser and its version)-Bug Type or Category/Severity/Priorityo Bug Category: Security, Database, Functionality (Critical/General), UIo Bug Severity: Severity with which the bug affects the application Comments",
  "page318": "Once the reported defect is fixed, the tester needs to re-test to confirm the fix. This isusually done by executing the possible scenarios where the bug can occur. Onceretesting is\u00aecompleted, the fix can be confirmed and the bug can be closed. Thismarks the end of the bug life cycle.The documents outlined in the IEEE Standard of Software Test Documentation coverstest planning, test specification,\u00aeand test reporting.Test reporting covers four document types: A Test Item Transmittal Report identifies the test items being transmitted fortesting from the development to the testing group in the event\u00aethat a formalbeginning of test execution is desiredDetails to be included in the report - Purpose, Outline, Transmittal-Report Identifier,Transmitted Items, Location, Status, and Approvals.. A Test Log is used by the test team to record what occurred during test executionDetails to be in\u00aecluded in the report - Purpose, Outline, Test-Log Identifier,Description, Activity and Event Entries, Execution Description, Procedure Results,Environmental Information, Anomalous Events, Incident-Report Identifiers A Test Incident report describes any event that occurs during the test executionthat requires further investigationD\u00aeetails to be included in the report - Purpose, Outline, Test-Incident-Report Identifier,Summary, Impact A test summary report summarizes the testing activities associated with one ormore test-design specificationsDetails to be included in the report - Purpose, Outline, Test-Summary-ReportIdentifier, Summary, Variances, Comprehensiveness Assessment, Summary of Results,Summary of Activities, and Approvals.Automating testing is no different from a programmer using a coding language towrite programs to automate any manual process. One of the problems with testinglarge systems is that it can go beyond the scope of small test teams. Because only asmall number of testers are available the coverage and depth of testing provided areinadequate for the task at hand.Expanding the test team beyond a certain size also becomes problematic withincrease in work over head. Feasible way to avoid this without introducing a loss ofquality is through appropriate use of tools that can expand individual to make changes in the current ways you perform testing.Involve people who will be using the tool to help design the automated testingprocess.",
  "page319": "Fully manual testing has the benefit of being relatively cheap and effective. But asquality of the product improves the additional cost for finding further bugs becomesmore expensi\u00aeve. Large scale manual testing also implies large scale testing teams withthe related costs of space overhead and infrastructure. Manual testing is also farmore responsive and flexible than automated testing but is pron\u00aee to tester errorthrough fatigue.Fully automated testing is very consistent and allows the repetitions of similar testsat very little marginal cost. The setup and purchase costs of such automation are ve\u00aeryhigh however and maintenance can be equally expensive. Automation is alsorelatively inflexible and requires rework in order to adapt to changing requirements.Partial Automation incorporates automation only where the most benefits can beachieved. The advantage is that it targets specific\u00aeally the tasks for automation andthus achieves the most benefit from them. It also retains a large component ofmanual testing which maintains the test team's flexibility and offers redundancy bybacking up automation with manual testing. The disadvantage is that it obviouslydoes not provide as extensive benefits as either extr\u00aeeme solution.Take time to define the tool requirements in terms of technology, process,applications, people skills, and organization.During tool evaluation, prioritize which test types are the most critical to yoursuccess and judge the candidate tools on those criteria. Understand the tools and their trade-offs. You may need to use a multi-tool solutionto get higher levels of test-type coverage. For example, you will need to combinethe capture/play-back tool with a load-test tool to cover your performance testcases. Involve potential users in the definition of tool requirements and evaluation criteria. Build an evaluation scorecard to compare each tool's performance against acommon set of criteria. Rank the criteria in terms of relative importance to theorganization. Buying the Wrong Tool Inadequate Test Team Organization Lack of Management Support Incomplete Coverage of Test Types by the selected tool Inadequate Tool Training Difficulty using the tool Lack of a Basic Test Process or Understanding of What to Test Lack of Configuration Management Processes of Tool Compatibility and InteroperabilityLack of Tool Availability",
  "page320": " oversight, Software subcontractmanagement, Software quality assurance, Software configuration managementLevel 3 - Defined: Key practice areas - Organization process focus, Or\u00aeganizationprocess definition, Training program, integrated software management, Softwareproduct engineering, intergroup coordination, Peer reviewsLevel 4 - Manageable: Key practice areas - Quantitative Process Mana\u00aegement,Software Quality ManagementLevel 5 - Optimizing: Key practice areas - Defect prevention, Technology changemanagement, Process change managementSix Sigma is a quality management program to ach\u00aeieve \"six sigma\" levels of quality. Itwas pioneered by Motorola in the mid-1980s and has spread too many othermanufacturing companies, notably General Electric Corporation (GE).Six Sigma is a rigorous and disciplined methodology that uses data and statisticalanalysis to measure and impr\u00aeove a company's operational performance by identifyingand eliminating \"defects\" from manufacturing to transactional and from product toservice. Commonly defined as 3.4 defects per million opportunities, Six Sigma can bedefined and understood at three distinct levels: metric, methodology andphilosophy.Training Sigma processes are\u00aeexecuted by Six Sigma Green Belts and Six Sigma BlackBelts, and are overseen by Six Sigma Master Black BeltsISO - International Organization for Standardization is a network of the nationalstandards institutes of 150 countries, on the basis of one member per country, with aCentral Secretariat in Geneva, Switzerland, that coordinates the system. ISO is a nongovernmental organization. ISO has developed over 13, 000 International Standardson a variety of subjects.",
  "page321": "Capability Maturity git Model - Developed by the software community in 1986 withleadership from the SEI. The CMM describes the principles and practices underlyingsoftware process m\u00aeaturity. It is intended to help software organizations improve thematurity of their software processes in terms of an evolutionary path from ad hoc,chaotic processes to mature, disciplined software processes. The focus\u00aeis onidentifying key process areas and the exemplary practices that may comprise adisciplined software process.What makes up the CMM? The CMM is organized into five maturity levels: Initial Repeatable De\u00aefined ManageableOptimizingExcept for Level 1, each maturity level decomposes into several key process areasthat indicate the areas an organization should focus on to improve its softwareprocess.Level 1 - Initial Level: Disciplined process, Standard, Consistent process, Predictableprocess,\u00aeContinuously Improving processLevel 2 - Repeatable: Key practice areas - Requirements management, Softwareproject planning, Software project tracking The goal is to expose the weak links and to determine ifthe system manages to recover gracefully. Performance Testing: Testing with the intent of determining how efficiently a\u00aeproduct handles a variety of events. Automated test tools geared specifically to testand fine-tune performance are used most often for this type of testing. Usability Testing: Usability testing is testing for 'user-friendliness'. A way toevaluate and measure how users interact with a software product or site. Tasks aregiven to users and observations are made. Installation Testing: Testing with the intent of determining if the product iscompatible with a variety of platforms and how easily it installs. Recovery/Error Testing: Testing how well a system recovers from crashes,hardware failures, or other catastrophic problems. Security Testing: Testing of database and network software in order to keepcompany data and resources secure from mistaken/accidental users, hackers, andother malevolent attackers. Penetration Testing: Penetration testing is testing how well the system isprotected against unauthorized internal or external access, or willful damage. Thistype of testing usually requires sophisticated testing ttion, based on information they learn as they're executingtheir tests. Comparison Testing:",
  "page322": "Following are some facts that can help you gain a better insight into the realities ofSoftware Engineering.1. The best programmers are up to 28 times better than the worst programm\u00aeers.2. New tools/techniques cause an initial LOSS of productivity/quality.3. The answer to a feasibility study is almost always \"yes\".4. A May 2002 report prepared for the National Institute of Standards andTe\u00aechnologies (NIST)(1) estimate the annual cost of software defects in the UnitedStates as $59.5 billion.5. Reusable components are three times as hard to build6. For every 25% increase in problem complexi\u00aety, there is a 100% increase in solutioncomplexity.7. 80% of software work is intellectual. A fair amount of it is creative. Little of it isclerical.8. Requirements errors are the most expensive to fix during production.9. Missing requirements are the hardest requirement errors to correct\u00ae.10. Error-removal is the most time-consuming phase of the life cycle.11. Software is usually tested at best at the 55-60% (branch) coverage level.12. 100% coverage is still far from enough.13. Rigorous inspections can remove up to 90% of errors before the first test case isrun.15. Maintenance typically consumes 40-80% of software\u00aecosts. It is probably the mostimportant life cycle phase of software.16. Enhancements represent roughly 60% of maintenance costs.17. There is no single best approach to software error removal.",
  "page323": "Testing plays an important role in achieving and assessing the quality of a softwareproduct [25]. On the one hand, we improve the quality of the products as we repeata test-fi\u00aend defects-fix cycle during development. On the other hand, we assess howgood our system is when we perform system-level tests before releasing a product.Thus, as Friedman and Voas [26] have succinctly described, s\u00aeoftware testing is averification process for software quality assessment and improvement. Generallyspeaking, the activities for software quality assessment can be divided into twobroad categories, namely\u00ae, static analysis and dynamic analysis. Static Analysis: As the term \"static\" suggests, it is based on the examination of a number of documents, namely requirements documents, softwaremodels, design documents, and source code. Traditional static analysisincludes code review, ins\u00aepection, walk-through, algorithm analysis, andproof of correctness. It does not involve actual execution of the code underdevelopment. Instead, it examines code and reasons over all possible behaviors that might arise during run time. Compiler optimizations are standardstatic analysis.Dynamic Analysis: Dynamic analysis of a softwa\u00aere system involves actualprogram execution in order to expose possible program failures. The behavioral and performance properties of the program are also observed. Programs are executed with both typical and carefully chosen input values.Often, the input set of a program can be impractically large. However, forpractical considerations, a finite subset of the input set can be selected.Therefore, in testing, we observe some representative program behaviorsand reach a conclusion about the quality of the system. Careful selectionof a finite test set is crucial to reaching a reliable conclusion.",
  "page324": "By performing static and dynamic analyses, practitioners want to identify as manyfaults as possible so that those faults are fixed at an early stage of the softwaredevelopment. Sta\u00aetic analysis and dynamic analysis are complementary in nature,and for better effectiveness, both must be performed repeatedly and alternated.Practitioners and researchers need to remove the boundaries between static and\u00aedynamic analysis and create a hybrid analysis that combines the strengths of bothapproachesTwo similar concepts related to software testing frequently used by practitioners areverification and validation\u00ae. Both concepts are abstract in nature, and each can berealized by a set of concrete, executable activities. The two concepts are explainedas follows: Verification: This kind of activity helps us in evaluating a software systemby determining whether the product of a given development phas\u00aee satisfiesthe requirements established before the start of that phase. One may notethat a product can be an intermediate product, such as requirement specification, design specification, code, user manual, or even the final product.Activities that check the correctness of a development phase are calledverification activities. Val\u00aeidation: Activities of this kind help us in confirming that a productmeets its intended use. Validation activities aim at confirming that a productmeets its customer's expectations. In other words, validation activities focuson the final product, which is extensively tested from the customer point ofview. Validation establishes whether the product meets overall expectationsof the users.Late execution of validation activities is often risky by leading tohigher development cost. Validation activities may be executed at earlystages of the software development cycle [28]. An example of early execution of validation activities can be found in the eXtreme Programming",
  "page325": "(XP) software development methodology. In the XP methodology, the customer closely interacts with the software development group and conductsacceptance tests during each developmen\u00aet iteration [29].The verification process establishes the correspondence of an implementationphase of the software development process with its specification, whereas validationestablishes the correspondence between a s\u00aeystem and users' expectations. One cancompare verification and validation as follows: Verification activities aim at confirming that one is building the product correctly, whereas validation activit\u00aeies aim at confirming that one is buildingthe correct product [30]. Verification activities review interim work products, such as requirementsspecification, design, code, and user manual, during a project life cycle toensure their quality. The quality attributes sought by verification act\u00aeivitiesare consistency, completeness, and correctness at each major stage of system development. On the other hand, validation is performed toward theend of system development to determine if the entire system meets thecustomer's needs and expectations. Verification activities are performed on interim products by applying mos\u00aetlystatic analysis techniques, such as inspection, walkthrough, and reviews,and using standards and checklists. Verification can also include dynamicanalysis, such as actual program execution. On the other hand, validationis performed on the entire system by actually running the system in its realenvironment and using a variety of testsIn the literature on software testing, one can find references to the terms failure,error, fault, and defect. Although their meanings are related, there are importantdistinctions between these four concepts. In the following, we present first threeterms as they are understood in the fault-tolerant computing community",
  "page326": "Failure: A failure is said to occur whenever the external behavior of asystem does not conform to that prescribed in the system specification. Error: An error is a state of t\u00aehe system. In the absence of any correctiveaction by the system, an error state could lead to a failure which wouldnot be attributed to any event subsequent to the error. Fault: A fault is the adjudged cause of an\u00aeerror.A fault may remain undetected for a long time, until some event activates it. Whenan event activates a fault, it first brings the program into an intermediate error state.If computation is allowed\u00aeto proceed from an error state without any correctiveaction, the program eventually causes a failure. As an aside, in fault-tolerant computing, corrective actions can be taken to take a program out of an error state intoa desirable state such that subsequent computation does not eventual\u00aely lead to afailure. The process of failure manifestation can therefore be succinctly representedas a behavior chain [31] as follows: faulterrorfailure. The behavior chaincan iterate for a while, that is, failure of one component can lead to a failure ofanother interacting component.The above definition of failure assu\u00aemes that the given specification is acceptable to the customer. However, if the specification does not meet the expectationsof the customer, then, of course, even a fault-free implementation fails to satisfy thecustomer. It is a difficult task to give a precise definition of fault, error, or failureof software, because of the \"human factor\" involved in the overall acceptance of asystem. In an article titled \"What Is Software Failure\" [32], Ram Chillarege commented that in modern software business software failure means \"the customer'sexpectation has not been met and/or the customer is unable to do useful work withproduct,\" p. 354.Roderick Rees [33] extended Chillarege's comments of software failure bypointing out that \"failure is a matter of function only [and is thus] related to purpose,not to whether an item is physically intact or not\" (p. 163). To substantiate this,Behrooz Parhami [34] provided three interesting examples to show the relevanceof such a view point in wider context. One of the examples is quoted here",
  "page327": "The above definition of failure assumes that the given specification is acceptable to the customer. However, if the specification does not meet the expectationsof the customer, the\u00aen, of course, even a fault-free implementation fails to satisfy thecustomer. It is a difficult task to give a precise definition of fault, error, or failureof software, because of the \"human factor\" involved i\u00aen the overall acceptance of asystem. In an article titled \"What Is Software Failure\" [32], Ram Chillarege commented that in modern software business software failure means \"the customers\u00aeexpectation has not been met and/or the customer is unable to do useful work withproduct. Boundaries in loop, space, time, memory, mishandling ofcases outside boundary. Calculation errors: Bad Logic, Bad Arithmetic, Outdated constants, Calculationerrors, incorrect conversion from one data\u00aerepresentation to another, Wrongformula, incorrect approximation. Initial and Later states: Failure to - set data item to zero, to initialize a loopcontrol variable, or re-initialize a pointer, to clear a string or flag, Incorrectinitialization. Control flow errors: Wrong returning state assumed, Exception handling basedexits, Sta\u00aeck underflow/overflow, Failure to block or un-block interrupts, Comparisonsometimes yields wrong result, Missing/wrong default, and Data Type errors. Errors in Handling or Interpreting Data: Un-terminated null strings, overwriting afile after an error exit or user abort. Race Conditions: Assumption that one event or task finished before anotherbegins, Resource races, Tasks starts before its prerequisites are met, Messages crosor don't arrive in the order sent.Load Conditions: Required resources are not available, No available large memoryarea, Low priority tasks not put off, doesn't erase old files from mass storage, anddoesn't return unused memory.Hardware: Wrong Device, Device unavailable, Underutilizing device intelligence,Misunderstood status or return code, Wrong operation or instruction codes. Source, Version and ID Control: No Title or version ID, Failure to update multiplecopies of data or program files.Testing Errors: Failure to notice/report a problem",
  "page328": " dissatisfactions are errors in the organization's state. The organization's personnel or departmentsprobably begin to malfunction as result of the errors, in turn causin\u00aeg an overall degradation of performance. The end result can be the organization's failure to achieveits goal.There is a fine difference between defects and faults in the above example, thatis, execution of a defect\u00aeive policy may lead to a faulty promotion. In a softwarecontext, a software system may be defective due to design issues; certain systemstates will expose a defect, resulting in the development of faults\u00aedefined as incorrect signal values or decisions within the system. In industry, the term defect iswidely used, whereas among researchers the term fault is more prevalent. For allpractical purpose, the two terms are synonymous. In this book, we use the twoterms interchangeably as required\u00ae. Testing in which the software tester has knowledge of theback-end, structure and language of the software, or at least its purpose. Unit Testing: Unit testing is the process of testing a particular complied program,i.e., a window, a report, an interface, etc. independently as a stand-alonecomponent/program. The types and degree\u00aes of unit tests can vary among modifiedand newly created programs. Unit testing is mostly performed by the programmerswho are also responsible for the creation of the necessary unit test data. Incremental Testing: Incremental testing is partial testing of an incompleteproduct. The goal of incremental testing is to provide an early feedback to softwaredevelopers. System Testing: System testing is a form of black box testing. The purpose ofsystem testing is to validate an application's accuracy and completeness inperforming the functions as designed.Integration Testing: Testing two or more modules or functions together with theintent of finding interface defects between the modules/functions. System Integration Testing: Testing of software components that have beendistributed across multiple platforms (e.g., client, web server, application server, anddatabase server) to produce failures caused by system integration defects (i.e. defectsinvolving distribution and back- Functional Testing:ngnetwork communication, or interacting with other hardware, application, or system.",
  "page329": "Roderick Rees [33] extended Chillarege's comments of software failure bypointing out that \"failure is a matter of function only [and is thus] related to purpose,not to wh\u00aeether an item is physically intact or not\" (p. 163). To substantiate this,Behrooz Parhami [34] provided three interesting examples to show the relevanceof such a view point in wider context. One of the examples is\u00aequoted here (p. 451):Consider a small organization. Defects in the organization's staff promotion policies cancause improper promotions, viewed as faults. The resulting ineptitudes Boundary Value An\u00aealysisThis is a selection technique where the test data are chosen to lie along theboundaries of the input domain or the output range. This technique is often called asstress testing and incorporates a degree of negative testing in the test design byanticipating that errors will occur at\u00aeor around the partition boundaries.For example, a field is required to accept amounts of money between $0 and $10. Asa tester, you need to check if it means up to and including $10 and $9.99 and if $10 isacceptable. So, the boundary values are $0, $0.01, $9.99 and $10.Now, the following tests can be executed. A negative value shou\u00aeld be rejected, 0should be accepted (this is on the boundary), $0.01 and $9.99 should be accepted,null and $10 should be rejected. In this way, it uses the same concept of partitions asequivalence partitioning.State Transition TestingAs the name suggests, test cases are designed to test the transition between thestates by creating the events that cause the transition.Branch TestingIn branch testing, test cases are designed to exercise control flow branches ordecision points in a unit. This is usually aimed at achieving a target level of DecisionCoverage. Branch Coverage, need to test both branches of IF and ELSE. All branchesand compound conditions (e.g. loops and array handling) within the branch should beexercised at least once.Condition TestingThe object of condition testing is to design test cases to show that the individualcomponents of logical conditions and combinations of the individual components arecorrect. Test cases are designed to tess set. Data use isanywhere that a data item is read or used. The objective is to create test cases thatwill drive execution through paths between specific definitions and uses.",
  "page330": "No matter how many times we run the test-find faults-fix cycle during softwareNo matter how many times we run the test-find faults-fix cycle during softwaredeve\u00aelopment, some faults are likely to escape our attention, and these will eventually surface at the customer site. Therefore, a quantitative measure that is usefulin assessing the quality of a software is its reliability\u00ae[35]. Software reliability isdefined as the probability of failure-free operation of a software system for a specified time in a specified environment. The level of reliability of a system depends onthos\u00aee inputs that cause failures to be observed by the end users. Software reliabilitycan be estimated via random testing, as suggested by Hamlet [36]. Since the notionof reliability is specific to a \"specified environment,\" test data must be drawn fromthe input distribution to clos\u00aeely resemble the future usage of the system. Capturing the future usage pattern of a system in a general sense is described in a formcalled the operational profileThe stakeholders in a test process are the programmers, the test engineers, theproject managers, and the customers. A stakeholder is a person or an organizationwho influ\u00aeences a system's behaviors or who is impacted by that system [39].Different stakeholders view a test process from different perspectives as explainedbelow",
  "page331": "It does work: While implementing a program unit, the programmer maywant to test whether or not the unit works in normal circumstances. Theprogrammer gets much confidence if the uni\u00aet works to his or her satisfaction. The same idea applies to an entire system as well once a systemhas been integrated, the developers may want to test whether or not thesystem performs the basic functions. Here, for th\u00aee psychological reason,the objective of testing is to show that the system works, rather than itdoes not work.It does not work: Once the programmer (or the development team) issatisfied that a unit (or t\u00aehe system) works to a certain degree, more testsare conducted with the objective of finding faults in the unit (or the system).Here, the idea is to try to make the unit (or the system) fail.Reduce the risk of failure: Most of the complex software systems containfaults, which cause the sys\u00aetem to fail from time to time. This concept of\"failing from time to time\" gives rise to the notion of failure rate. Asfaults are discovered and fixed while performing more and more tests, thefailure rate of a system generally decreases. Thus, a higher level objectiveof performing tests is to bring down the risk of failin\u00aeg to an acceptablelevel.Reduce the cost of testing: The different kinds of costs associated with atest process includethe cost of designing, maintaining, and executing test cases,the cost of analyzing the result of executing each test case,the cost of documenting the test cases, andthe cost of actually executing the system and documenting it.",
  "page332": "Therefore, the less the number of test cases designed, the less will be theassociated cost of testing. However, producing a small number of arbitrarytest cases is not a good way of\u00aesaving cost. The highest level of objectiveof performing tests is to produce low-risk software with fewer numberof test cases. This idea leads us to the concept of effectiveness of testcases. Test engineers must theref\u00aeore judiciously select fewer, effective testcasesIn its most basic form, a test case is a simple pair of < input, expected outcome >.If a program under test is expected to compute the square root of nonn\u00aeegativenumbers, then four examples of test cases are as shown in Figure 1.3.In stateless systems, where the outcome depends solely on the current input,test cases are very simple in structure, as shown in Figure 1.3. A program tocompute the square root of nonnegative numbers is an example\u00aeof a statelesssystem. A compiler for the C programming language is another example of astateless system. A compiler is a stateless system because to compile a program itdoes not need to know about the programs it compiled previously.In state-oriented systems, where the program outcome depends both on thecurrent state of the syste\u00aem and the current input, a test case may consist of asequence of < input, expected outcome > pairs. A telephone switching system andan automated teller machine (ATM) are examples of state-oriented systems. For anATM machine, a test case for testing the withdraw function is shown in Figure 1.4.Here, we assume that the user has already entered validated inputs, such as the cashcard and the personal identification number (PIN).",
  "page333": "In the test case TS1, \"check balance\" and \"withdraw\" in the first, second, andfourth tuples represent the pressing of the appropriate keys on the ATM keypad. It\u00aeisassumed that the user account has $500.00 on it, and the user wants to withdraw anamount of $200.00. The expected outcome \"$200.00\" in the third tuple representsthe cash dispensed by the ATM. After the with\u00aedrawal operation, the user makessure that the remaining balance is $300.00.For state-oriented systems, most of the test cases include some form of decision and timing in providing input to the system. A\u00aetest case may include loopsand timers, which we do not show at this moment.An outcome of program execution is a complex entity that may include thefollowing:Values produced by the program:Outputs for local observation (integer, text, audio, image)Outputs (messages) for remote storage, man\u00aeipulation, or observationState change:State change of the programState change of the database (due to add, delete, and update operations) A sequence or set of values which must be interpreted together for theoutcome to be validAn important concept in test design is the concept of an oracle. An oracleis any entity program, process,\u00aehuman expert, or body of data that tells us theexpected outcome of a particular test or set of tests [40]. A test case is meaningfulonly if it is possible to decide on the acceptability of the result produced by theprogram under test.Ideally, the expected outcome of a test should be computed while designingthe test case. In other words, the test outcome is computed before the program is executed with the selected test input.",
  "page334": "The idea here is that one should be able tocompute the expected outcome from an understanding of the program's requirements. Precomputation of the expected outcome will elimin\u00aeate any implementationbias in case the test case is designed by the developer.In exceptional cases, where it is extremely difficult, impossible, or evenundesirable to compute a single expected outcome, one should identi\u00aefy expectedoutcomes by examining the actual test outcomes, as explained in the following:Execute the program with the selected input.Observe the actual outcome of program execution.Verify that the actual\u00aeoutcome is the expected outcome. Use the verified actual outcome as the expected outcome in subsequentruns of the test case It is not unusual to find people making claims such as \"I have exhaustively testedthe program.\" Complete, or exhaustive, testing means there are no undisc\u00aeoveredfaults at the end of the test phase. All problems must be known at the end ofcomplete testing. For most of the systems, complete testing is near impossiblebecause of the following reasons: The domain of possible inputs of a program is too large to be completelyused in testing a system. There are both valid inputs and invalid\u00aeinputs.The program may have a large number of states. There may be timingconstraints on the inputs, that is, an input may be valid at a certain timeand invalid at other times. An input value which is valid but is not properlytimed is called an inopportune input. The input domain of a system canbe very large to be completely used in testing a program.",
  "page335": "The design issues may be too complex to completely test. The design mayhave included implicit design decisions and assumptions. For example,a programmer may use a global variable o\u00aer a static variable to controlprogram execution. It may not be possible to create all possible execution environments of thesystem. This becomes more significant when the behavior of the softwaresystem depends on the re\u00aeal, outside world, such as weather, temperature,altitude, pressure, and so on.We must realize that though the outcome of complete testing, that is, discovering allfaults, is highly desirable, it is a nea\u00aer-impossible task, and it may not be attempted.The next best thing is to select a subset of the input domain to test a program.Referring to Figure 1.5, let D be the input domain of a program P. Suppose thatwe select a subset D1 of D, that is, D1 \u2282 D, to test program P. It is possible\u00aethatD1 exercises only a part P1, that is, P1 \u2282 P, of the execution behavior of P, inwhich case faults with the other part, P2, will go undetected.By selecting a subset of the input domain D1, the test engineer attemptsto deduce properties of an entire program P by observing the behavior of a partP1 of the entire behavior of\u00aeP on selected inputs D1. Therefore, selection of thesubset of the input domain must be done in a systematic and careful manner sothat the deduction is as accurate and complete as possible. For example, the ideaof coverage is considered while selecting test cases In order to test a program, a test engineer must perform a sequence of testingactivities. Most of these activities have been shown in Figure 1.6 and are explainedin the following. ",
  "page336": "Identify an objective to be tested: The first activity is to identify anobjective to be tested. The objective defines the intention, or purpose, ofdesigning one or more test cases\u00aeto ensure that the program supports theobjective. A clear purpose must be associated with every test caseSelect inputs: The second activity is to select test inputs. Selection of testinputs can be based on the requireme\u00aents specification, the source code,or our expectations. Test inputs are selected by keeping the test objectivein mind.Compute the expected outcome: The third activity is to compute theexpected outcome of\u00aethe program with the selected inputs. In most cases,this can be done from an overall, high-level understanding of the testobjective and the specification of the program under test.Set up the execution environment of the program: The fourth step is toprepare the right execution environmen\u00aet of the program. In this step all theassumptions external to the program must be satisfied. A few examples ofassumptions external to a program are as follows:Initialize the local system, external to the program. This may includemaking a network connection available, making the right databasesystem available, and so on.Initialize\u00aeany remote, external system (e.g., remote partner process in adistributed application.) For example, to test the client code, we mayneed to start the server at a remote siteExecute the program: In the fifth step, the test engineer executes theprogram with the selected inputs and observes the actual outcome of theprogram. To execute a test case, inputs may be provided to the program atdifferent physical locations at different times. The concept of test coordination is used in synchronizing different components of a test case",
  "page337": "Analyze the test result: The final test activity is to analyze the result oftest execution. Here, the main task is to compare the actual outcome ofprogram execution with the expect\u00aeed outcome. The complexity of comparison depends on the complexity of the data to be observed. The observeddata type can be as simple as an integer or a string of characters or ascomplex as an image, a video, or an audi\u00aeo clip. At the end of the analysis step, a test verdict is assigned to the program. There are three majorkinds of test verdicts, namely, pass, fail, and inconclusive, as explainedbelow.If the program pro\u00aeduces the expected outcome and the purpose of thetest case is satisfied, then a pass verdict is assigned.If the program does not produce the expected outcome, then a fail verdictis assigned.However, in some cases it may not be possible to assign a clear passor fail verdict. For example, i\u00aef a timeout occurs while executing atest case on a distributed application, we may not be in a position toassign a clear pass or fail verdict. In those cases, an inconclusive testverdict is assigned. An inconclusive test verdict means that furthertests are needed to be done to refine the inconclusive verdict into aclear pass or fa\u00aeil verdict A test report must be written after analyzing the test result. Themotivation for writing a test report is to get the fault fixed if the test revealeda fault. A test report contains the following items to be informative:Explain how to reproduce the failure.Analyze the failure to be able to describe it.A pointer to the actual outcome and the test case, complete with theinput, the expected outcome, and the execution environment.",
  "page338": "Testing is performed at different levels involving the complete system or parts ofit throughout the life cycle of a software product. A software system goes throughfour stages of t\u00aeesting before it is actually deployed. These four stages are knownas unit, integration, system, and acceptance level testing. The first three levels oftesting are performed by a number of different stakeholders in the d\u00aeevelopmentorganization, where as acceptance testing is performed by the customers. The fourstages of testing have been illustrated in the form of what is called the classical Vmodel In unit testing, prog\u00aerammers test individual program units, such as a procedures, functions, methods, or classes, in isolation. After ensuring that individualunits work to a satisfactory extent, modules are assembled to construct larger subsystems by following integration testing techniques. Integration testi\u00aeng is jointlyperformed by software developers and integration test engineers. The objective ofintegration testing is to construct a reasonably stable system that can withstandthe rigor of system-level testing. System-level testing includes a wide spectrumof testing, such as functionality testing, security testing, robustness testi\u00aeng, loadtesting, stability testing, stress testing, performance testing, and reliability testing.System testing is a critical phase in a software development process because of theneed to meet a tight schedule close to delivery date, to discover most of the faults,and to verify that fixes are working and have not resulted in new faults. Systemtesting comprises a number of distinct activities: creating a test plan, designinga test suite, preparing test environments, executing the tests by following a clearstrategy, and monitoring the process of test execution.",
  "page339": "Regression testing is another level of testing that is performed throughout thelife cycle of a system. Regression testing is performed whenever a component ofthe system is modified\u00ae. The key idea in regression testing is to ascertain that themodification has not introduced any new faults in the portion that was not subjectto modification. To be precise, regression testing is not a distinct level o\u00aef testing.Rather, it is considered as a subphase of unit, integration, and system-level testing,as illustrated in Figure 1.8 [41].In regression testing, new tests are not designed. Instead, tests are sel\u00aeected,prioritized, and executed from the existing pool of test cases to ensure that nothingis broken in the new version of the software. Regression testing is an expensiveprocess and accounts for a predominant portion of testing effort in the industry. Itis desirable to select a subset of\u00aethe test cases from the existing pool to reduce thecost. A key question is how many and which test cases should be selected so thatthe selected test cases are more likely to uncover new faults [42-44].After the completion of system-level testing, the product is delivered to thecustomer. The customer performs their own series\u00aeof tests, commonly known asacceptance testing. The objective of acceptance testing is to measure the qualityof the product, rather than searching for the defects, which is objective of systemtesting. A key notion in acceptance testing is the customer's expectations from thesystem. By the time of acceptance testing, the customer should have developedtheir acceptance criteria based on their own expectations from the system. Thereare two kinds of acceptance testing as explained in the following: User acceptance testing (UAT)Business acceptance testing (BAT)",
  "page340": "User acceptance testing is conducted by the customer to ensure that the systemsatisfies the contractual acceptance criteria before being signed off as meeting userneeds. On the oth\u00aeer hand, BAT is undertaken within the supplier's developmentorganization. The idea in having a BAT is to ensure that the system will eventuallypass the user acceptance test. It is a rehearsal of UAT at the supplier\u00ae's premises.Designing test cases has continued to stay in the foci of the research communityand the practitioners. A software development process generates a large body ofinformation, such as requir\u00aeements specification, design document, and source code.In order to generate effective tests at a lower cost, test designers analyze thefollowing sources of information: Requirements and functional specificationsSource codeinput and output domainsOperational profileFault modelRequirements\u00aeand Functional Specifications The process of software development begins by capturing user needs. The nature and amount of user needsidentified at the beginning of system development will vary depending on thespecific life-cycle model to be followed. Let us consider a few examples. In theWaterfall model [45] of software developmen\u00aet, a requirements engineer tries tocapture most of the requirements. On the other hand, in an agile software development model, such as XP [29] or the Scrum [46-48], only a few requirementsare identified in the beginning. A test engineer considers all the requirementsthe program is expected to meet whichever life-cycle model is chosen to test aprogram",
  "page341": "The requirements might have been specified in an informal manner, such asa combination of plaintext, equations, figures, and flowcharts. Though this form ofrequirements specificati\u00aeon may be ambiguous, it is easily understood by customers.For example, the Bluetooth specification consists of about 1100 pages of descriptions explaining how various subsystems of a Bluetooth interface is expected towo\u00aerk. The specification is written in plaintext form supplemented with mathematical equations, state diagrams, tables, and figures. For some systems, requirementsmay have been captured in the form of use c\u00aeases, entity-relationship diagrams,and class diagrams. Sometimes the requirements of a system may have been specified in a formal language or notation, such as Z, SDL, Estelle, or finite-statemachine. Both the informal and formal specifications are prime sources of testcasesSource Co\u00aede Whereas a requirements specification describes the intendedbehavior of a system, the source code describes the actual behavior of the system.High-level assumptions and constraints take concrete form in an implementation.Though a software designer may produce a detailed design, programmers mayintroduce additional details into th\u00aee system. For example, a step in the detaileddesign can be \"sort array A.\" To sort an array, there are many sorting algorithmswith different characteristics, such as iteration, recursion, and temporarily usinganother array. Therefore, test cases must be designed based on the program [50].Input and Output Domains Some values in the input domain of a programhave special meanings, and hence must be treated separately [5]. To illustrate thispoint, let us consider the factorial function.",
  "page342": "without considering the special case of n 0. The above wrong implementationwill produce the correct result for all positive values of n, but will fail for n = 0.Sometimes even som\u00aee output values have special meanings, and a programmust be tested to ensure that it produces the special values for all possible causes.In the above example, the output value 1 has special significance: (i) it is themi\u00aenimum value computed by the factorial function and (ii) it is the only valueproduced for two different inputs.In the integer domain, the values 0 and 1 exhibit special characteristicsif arithmetic operat\u00aeions are performed. These characteristics are 0 X x = 0 and1 X x = x for all values of x. Therefore, all the special values in the input andoutput domains of a program must be considered while testing the program.Operational Profile As the term suggests, an operational profile i\u00aes a quantitative characterization of how a system will be used. It was created to guide testengineers in selecting test cases (inputs) using samples of system usage. The notionof operational profiles, or usage profiles, was developed by Mills et al.The idea isto infer, from the observed test results, the future reliability of the\u00aesoftware whenit is in actual use. To do this, test inputs are assigned a probability distribution, orprofile, according to their occurrences in actual operation. The ways test engineersassign probability and select test cases to operate a system may significantly differfrom the ways actual users operate a system. However, for accurate estimationof the reliability of a system it is important to test a system by considering theways it will actually be used in the field.",
  "page343": "Fault Model Previously encountered faults are an excellent source of information in designing new test cases. The known faults are classified into differentclasses, such as initial\u00aeization faults, logic faults, and interface faults, and stored ina repository [55, 56]. Test engineers can use these data in designing tests to ensurethat a particular class of faults is not resident in the program.Ther\u00aee are three types of fault-based testing: error guessing, fault seeding,and mutation analysis. In error guessing, a test engineer applies his experienceto (i) assess the situation and guess where and wha\u00aet kinds of faults might exist,and (ii) design tests to specifically expose those kinds of faults. In fault seeding,known faults are injected into a program, and the test suite is executed to assessthe effectiveness of the test suite. Fault seeding makes an assumption that a testsuite that\u00aefinds seeded faults is also likely to find other faults. Mutation analysis issimilar to fault seeding, except that mutations to program statements are made inorder to determine the fault detection capability of the test suite. If the test cases arenot capable of revealing such faults, the test engineer may specify additional test\u00aecases to reveal the faults. Mutation testing is based on the idea of fault simulation,whereas fault seeding is based on the idea of fault injection. In the fault injectionapproach, a fault is inserted into a program, and an oracle is available to assert thatthe inserted fault indeed made the program incorrect. On the other hand, in faultsimulation, a program modification is not guaranteed to lead to a faulty program.In fault simulation, one may modify an incorrect program and turn it into a correctprogram.",
  "page344": "A key idea in Section was that test cases need to be designed by considering information from several sources, such as the specification, source code, andspecial properties of the\u00aeprogram's input and output domains. This is because allthose sources provide complementary information to test designers. Two broad concepts in testing, based on the sources of information for test design, are whi\u00aete-boxand black-box testing. White-box testing techniques are also called structural testing techniques, whereas black-box testing techniques are called functional testingtechniques.In structural testing\u00ae, one primarily examines source code with a focus on control flow and data flow. Control flow refers to flow of control from one instructionto another. Control passes from one instruction to another instruction in a numberof ways, such as one instruction appearing after another, function\u00aecall, messagepassing, and interrupts. Conditional statements alter the normal, sequential flowof control in a program. Data flow refers to the propagation of values from onevariable or constant to another variable. Definitions and uses of variables determinethe data flow aspect in a program.In functional testing, one does not have\u00aeaccess to the internal details of aprogram and the program is treated as a black box. A test engineer is concernedonly with the part that is accessible outside the program, that is, just the inputand the externally visible outcome. A test engineer applies input to a program,observes the externally visible outcome of the program, and determines whetheror not the program outcome is the expected outcome. Inputs are selected fromthe program's requirements specification and properties of the program's input andoutput domains. A test engineer is concerned only with the functionality and thefeatures found in the program's specification",
  "page345": "At this point it is useful to identify a distinction between the scopes ofstructural testing and functional testing. One applies structural testing techniquesto individual units of\u00aea program, whereas functional testing techniques can beapplied to both an entire system and the individual program units. Since individualprogrammers know the details of the source code they write, they themselvesperfo\u00aerm structural testing on the individual program units they write. On the otherhand, functional testing is performed at the external interface level of a system,and it is conducted by a separate software\u00aequality assurance group.Let us consider a program unit U which is a part of a larger program P.A program unit is just a piece of source code with a well-defined objective andwell-defined input and output domains. Now, if a programmer derives test casesfor testing U from a knowledge of the\u00aeinternal details of U , then the programmeris said to be performing structural testing. On the other hand, if the programmerdesigns test cases from the stated objective of the unit U and from his or herknowledge of the special properties of the input and output domains of U , then heor she is said to be performing functional test\u00aeing on the same unit U .a of functional testing.Neither structural testing nor functional testing is by itself good enough todetect most of the faults. Even if one selects all possible inputs, a structural testingtechnique cannot detect all faults if there are missing paths in a program. Intuitively,a path is said to be missing if there is no code to handle a possible condition.",
  "page346": "Similarly, without knowledge of the structural details of a program, many faultswill go undetected. Therefore, a combination of both structural and functionaltesting techniques mus\u00aet be used in program testing.The purpose of system test planning, or simply test planning, is to get ready andorganized for test execution. A test plan provides a framework, scope, details ofresource needed, effort requ\u00aeired, schedule of activities, and a budget. A frameworkis a set of ideas, facts, or circumstances within which the tests will be conducted.The stated scope outlines the domain, or extent, of the test act\u00aeivities. The scopecovers the managerial aspects of testing, rather than the detailed techniques andspecific test cases.Test design is a critical phase of software testing. During the test designphase, the system requirements are critically studied, system features to betested are thorough\u00aely identified, and the objectives of test cases and the detailedbehavior of test cases are defined. Test objectives are identified from differentsources, namely, the requirement specification and the functional specification,and one or more test cases are designed for each test objective. Each test case isdesigned as a combination\u00aeof modular test components called test steps. Thesetest steps can be combined together to create more complex, multistep tests. Atest case is clearly specified so that others can easily borrow, understand, andreuse it ",
  "page347": "Monitoring and measurement are two key principles followed in every scientific andengineering endeavor. The same principles are also applicable to the testing phasesof software dev\u00aeelopment. It is important to monitor certain metrics which trulyrepresent the progress of testing and reveal the quality level of the system. Basedon those metrics, the management can trigger corrective and preventive a\u00aections. Byputting a small but critical set of metrics in place the executive management willbe able to know whether they are on the right track [58]. Test execution metricscan be broadly categorized into\u00aetwo classes as follows: Metrics for monitoring test execution Metrics for monitoring defectsThe first class of metrics concerns the process of executing test cases, whereasthe second class concerns the defects found as a result of test execution. Thesemetrics need to be tracked and analy\u00aezed on a periodic basis, say, daily or weekly.In order to effectively control a test project, it is important to gather valid andaccurate information about the project. One such example is to precisely knowwhen to trigger revert criteria for a test cycle and initiate root cause analysis ofthe problems before more tests can be perf\u00aeormed. By triggering such a revertcriteria, a test manager can effectively utilize the time of test engineers, and possibly money, by suspending a test cycle on a product with too many defects tocarry out a meaningful system test. A management team must identify and monitor metrics while testing is in progress so that important decisions can be made",
  "page348": " It is important to analyze and understand the test metrics, rather than justcollect data and make decisions based on those raw data. Metrics are meaningful only if they enable the\u00aemanagement to make decisions which result in lowercost of production, reduced delay in delivery, and improved quality of softwaresystems.Quantitative evaluation is important in every scientific and engineering field.Qu\u00aeantitative evaluation is carried out through measurement. Measurement lets oneevaluate parameters of interest in a quantitative manner as follows: Evaluate the effectiveness of a technique used in perfor\u00aeming a task. Onecan evaluate the effectiveness of a test generation technique by countingthe number of defects detected by test cases generated by following thetechnique and those detected by test cases generated by other means.Evaluate the productivity of the development activities. One\u00aecan keep trackof productivity by counting the number of test cases designed per day, thenumber of test cases executed per day, and so on. Evaluate the quality of the product. By monitoring the number of defectsdetected per week of testing, one can observe the quality level of thesystem. Evaluate the product testing. For evaluating\u00aea product testing process, thefollowing two measurements are critical:incises in test design. The need for more testing occursas test engineers get new ideas while executing the planned testcases.Test effort effectiveness metric: It is important to evaluate the effectiveness of the testing effort in the development of a product. After aproduct is deployed at the customer's site, one is interested to knowthe effectiveness of testing that was performed",
  "page349": "A common measureof test effectiveness is the number of defects found by the customersthat were not found by the test engineers prior to the release of theproduct. These defects had\u00aeescaped our test effort.In general, software testing is a highly labor intensive task. This is because test casesare to a great extent manually generated and often manually executed. Moreover,the results of test execut\u00aeions are manually analyzed. The durations of those taskscan be shortened by using appropriate tools. A test engineer can use a variety oftools, such as a static code analyzer, a test data generator, and\u00aea network analyzer,if a network-based application or protocol is under test. Those tools are useful inincreasing the efficiency and effectiveness of testing.Test automation is essential for any testing and quality assurance division ofan organization to move forward to become more efficie\u00aent. The benefits of testautomation are as follows: Increased productivity of the testers Better coverage of regression testing Reduced durations of the testing phases Reduced cost of software maintenance Increased effectiveness of test casesTest automation provides an opportunity to improve the skills of the testengineers by writi\u00aeng programs, and hence their morale. They will be more focusedon developing automated test cases to avoid being a bottleneck in product deliveryto the market. Consequently, software testing becomes less of a tedious job.Test automation improves the coverage of regression testing because of accumulation of automated test cases over time. Automation allows an organization tocreate a rich library of reusable test cases and facilitates the execution of a consistent set of test cases. Here consistency means our ability to produce repeatedresults for the same set of tests",
  "page350": " It may be very difficult to reproduce test results inmanual testing, because exact conditions at the time and point of failure may notbe precisely known. In automated testing it i\u00aes easier to set up the initial conditionsof a system, thereby making it easier to reproduce test results. Test automationsimplifies the debugging work by providing a detailed, unambiguous log of activities and intermedi\u00aeate test steps. This leads to a more organized, structured, andreproducible testing approach.Automated execution of test cases reduces the elapsed time for testing, and,thus, it leads to a shorter time t\u00aeo market. The same automated test cases can beexecuted in an unsupervised manner at night, thereby efficiently utilizing the different platforms, such as hardware and configuration. In short, automation increasestest execution efficiency. However, at the end of test execution, it is impor\u00aetant toanalyze the test results to determine the number of test cases that passed or failed.And, if a test case failed, one analyzes the reasons for its failure.In the long run, test automation is cost-effective. It drastically reduces the software maintenance cost. In the sustaining phase of a software system, the regressiontests\u00aerequired after each change to the system are too many. As a result, regressiontesting becomes too time and labor intensive without automation.A repetitive type of testing is very cumbersome and expensive to performmanually, but it can be automated easily using software tools. A simple repetitivetype of application can reveal memory leaks in a software. However, the applicationhas to be run for a significantly long duration, say, for weeks, to reveal memoryleaks. Therefore, manual testing may not be justified, whereas with automation itis easy to reveal memory leaks.",
  "page351": "For example, stress testing is a prime candidate forautomation. Stress testing requires a worst-case load for an extended period of time,which is very difficult to realize by manua\u00ael means. Scalability testing is anotherarea that can be automated. Instead of creating a large test bed with hundreds ofequipment, one can develop a simulator to verify the scalability of the system.Test automation is v\u00aeery attractive, but it comes with a price tag. Sufficienttime and resources need to be allocated for the development of an automated testsuite. Development of automated test cases need to be managed like\u00aea programmingproject. That is, it should be done in an organized manner; otherwise it is highlylikely to fail. An automated test suite may take longer to develop because the testsuite needs to be debugged before it can be used for testing. Sufficient time andresources need to be allocate\u00aed for maintaining an automated test suite and setting upa test environment. Moreover, every time the system is modified, the modificationmust be reflected in the automated test suite. Therefore, an automated test suiteshould be designed as a modular system, coordinated into reusable libraries, andcross-referenced and traceable bac\u00aek to the feature being tested.It is important to remember that test automation cannot replace manual testing. Human creativity, variability, and observability cannot be mimicked throughautomation. Automation cannot detect some problems that can be easily observedby a human being. Automated testing does not introduce minor variations the waya human can. Certain categories of tests, such as usability, interoperability, robustness, and compatibility, are often not suited for automation. It is too difficult toed.",
  "page352": "The objective of test automation is not to reduce the head counts in thetesting department of an organization, but to improve the productivity, quality, andefficiency of test execu\u00aetion. In fact, test automation requires a larger head count inthe testing department in the first year, because the department needs to automatethe test cases and simultaneously continue the execution of manual tests. E\u00aeven afterthe completion of the development of a test automation framework and test caselibraries, the head count in the testing department does not drop below its originallevel. The test organization nee\u00aeds to retain the original team members in order toimprove the quality by adding more test cases to the automated test case repository.Before a test automation project can proceed, the organization must assessand address a number of considerations. The following list of prerequisites mustb\u00aee considered for an assessment of whether the organization is ready for testautomation:The test cases to be automated are well defined. Test tools and an infrastructure are in placeThe test automation professionals have prior successful experience inautomation. Adequate budget should have been allocated for the procurement of soft\u00aeware tools.Testing is a distributed activity conducted at different levels throughout the lifecycle of a software. These different levels are unit testing, integration testing, system testing, and acceptance testing. It is logical to have different testing groups inan organization for each level of testing. However, it is more logical and is thecase in reality that unit-level tests be developed and executed by the programmersthemselves rather than an independent group of unit test engineers. The programmer who develops a software unit should take the ownership and responsibilityof producing good-quality software to his or her satisfaction.",
  "page353": "System integrationtesting is performed by the system integration test engineers. The integration testengineers involved need to know the software modules very well. This means that\u00aeall development engineers who collectively built all the units being integratedneed to be involved in integration testing. Also, the integration test engineersshould thoroughly know the build mechanism, which is key to\u00aeintegrating largesystems.A team for performing system-level testing is truly separated from the development team, and it usually has a separate head count and a separate budget. Themandate of this group\u00aeis to ensure that the system requirements have been met andthe system is acceptable. Members of the system test group conduct different categories of tests, such as functionality, robustness, stress, load, scalability, reliability,and performance. They also execute business acceptance tes\u00aets identified in the useracceptance test plan to ensure that the system will eventually pass user acceptancetesting at the customer site. However, the real user acceptance testing is executedby the client's special user group. The user group consists of people from different backgrounds, such as software quality assurance eng\u00aeineers, business associates,and customer support engineers. It is a common practice to create a temporaryuser acceptance test group consisting of people with different backgrounds, suchas integration test engineers, system test engineers, customer support engineers,and marketing engineers. Once the user acceptance is completed, the group is dismantled. It is recommended to have at least two test groups in an organization:integration test group and system test group.Hiring and retaining test engineers are challenging tasks. Interview is theprimary mechanism for evaluating applicants. Interviewing is a skill that improveswith practice. It is necessary to have a recruiting process in place in order to beeffective in hiring excellent test engineers. In order to retain test engineers, themanagement must recognize the importance of testing efforts at par with development efforts. The management should treat the test engineers as professionals andas a part of the overall team that delivers quality products",
  "page354": "With the above high-level introduction to quality and software testing, we are nowin a position to outline the remaining chapters. Each chapter in the book coverstechnical, process\u00ae, and/or managerial topics related to software testing. The topicshave been designed and organized to facilitate the reader to become a software testspecialist. In Chapter 2 we provide a self-contained introduction to t\u00aehe theory andlimitations of software testing.Chapters 3-6 treat unit testing techniques one by one, as quantitativelyas possible. These chapters describe both static and dynamic unit testing. Static\u00aeunit testing has been presented within a general framework called code review,rather than individual techniques called inspection and walkthrough. Dynamic unittesting, or execution-based unit testing, focuses on control flow, data flow, anddomain testing. The JUnit framework, which is use\u00aed to create and execute dynamicunit tests, is introduced. We discuss some tools for effectively performing unittesting.Chapter 7 discusses the concept of integration testing. Specifically, five kindsof integration techniques, namely, top down, bottom up, sandwich, big bang, andincremental, are explained. Next, we discuss the integ\u00aeration of hardware and software components to form a complete system. We introduce a framework to developa plan for system integration testing. The chapter is completed with a brief discussion of integration testing of off-the-shelf components.Chapters 8-13 discuss various aspects of system-level testing. These sixchapters introduce the reader to the technical details of system testing that is thepractice in industry. These chapters promote both qualitative and quantitative evaluation of a system testing process. The chapters emphasize the need for having anindependent system testing group. A process for monitoring and controlling system testing is clearly explained. Chapter 14 is devoted to acceptance testing, whichincludes acceptance testing criteria, planning for acceptance testing, and acceptancetest execution.",
  "page355": "execution.Chapter 15 contains the fundamental concepts of software reliability and theirapplication to software testing. We discuss the notion of operation profile and itsapplicati\u00aeon in system testing. We conclude the chapter with the description of anexample and the time of releasing a system by determining the additional lengthof system testing. The additional testing time is calculated by usin\u00aeg the idea ofsoftware reliability.In Chapter 16, we present the structure of test groups and how these groupscan be organized in a software company. Next, we discuss how to hire and retaintest engineers\u00aeby providing training, instituting a reward system, and establishingan attractive career path for them within the testing organization. We conclude thischapter with the description of how to build and manage a test team with a focuson teamwork rather than individual gain.Chapters 17 and 1\u00ae8 explain the concepts of software quality and differentmaturity models. Chapter 17 focuses on quality factors and criteria and describesthe ISO 9126 and ISO 9000:2000 standards. Chapter 18 covers the CMM, whichwas developed by the SEI at Carnegie Mellon University. Two test-related models,namely the TPI model and the TMM, are exp\u00aelained at the end of Chapter 18.We define the key words used in the book in a glossary at the end of the book.The reader will find about 10 practice exercises at the end of each chapter. A listof references is included at the end of each chapter for a reader who would like tofind more detailed discussions of some of the topics. Finally, each chapter, exceptthis one, contains a literature review section that, essentially, provides pointers tomore advanced material related to the topics. The more advanced materials arebased on current research and alternate viewpoints.",
  "page356": "Any approach to testing is based on assumptions about the way program faultsoccur. Faults are due to two main reasons:Faults occur due to our inadequate understanding of all condit\u00aeions withwhich a program must deal.Faults occur due to our failure to realize that certain combinations of conditions require special treatments.Goodenough and Gerhart classify program faults as follows:Logic Fault: Thi\u00aes class of faults means a program produces incorrectresults independent of resources required. That is, the program fails becauseof the faults present in the program and not because of a lack of resource\u00aes.Logic faults can be further split into three categories:Requirements fault: This means our failure to capture the real requirements of the customer.Design fault: This represents our failure to satisfy an understoodrequirement.Construction fault: This represents our failure to satisfy a\u00aedesign. Suppose that a design step says \"Sort array A.\" To sort the array with Nelements, one may choose one of several sorting algorithms. Let {:}be the desired for loop construct to sort the array. If a programmerwrites the for loop in the formfor (i = 0; i <= N; i ){:}then there is a construction error in the impleme\u00aentation.Performance Fault: This class of faults leads to a failure of the programto produce expected results within specified or desired resource limitations.A thorough test must be able to detect faults arising from any of the abovereasons. Test data selection criteria must reflect information derived from each stageof software development. Since each type of fault is manifested as an impropereffect produced by an implementation, it is useful to categorize the sources of faultsin implementation terms as folows: ",
  "page357": "Missing Control Flow Paths: Intuitively, a control flow path, or simply apath, is a feasible sequence of instructions in a program. A path may bemissing from a program if we fail t\u00aeo identify a condition and specify apath to handle that condition. An example of a missing path is our failureto test for a zero divisor before executing a division. If we fail to recognizethat a divisor can take a zero\u00aevalue, then we will not include a piece ofcode to handle the special case. Thus, a certain desirable computation willbe missing from the program.Inappropriate Path Selection: A program executes an inapp\u00aeropriate path ifa condition is expressed incorrectly. we show a desiredbehavior and an implemented behavior. Both the behaviors are identicalexcept in the condition part of the if statement. The if part of the implemented behavior contains an additional condition B. It is easy to see tha\u00aet both the desired part and the implemented part behave in the same wayfor all combinations of values of A and B except when A = 1 and B = 0.Inappropriate or Missing Action: There are three instances of this class offault One may calculate a value using a method that does not necessarilygive the correct result. For example, a desi\u00aered expression is x = x X w,whereas it is wrongly written as x = x  w. These two expressionsproduce identical results for several combinations of x and w, such asx = 1.5 and w = 3, for example.Failing to assign a value to a variable is an example of a missing action. Calling a function with the wrong argument list is a kind of inappropriateaction.",
  "page358": "The main danger due to an inappropriate or missing action is that the action isincorrect only under certain combinations of conditions. Therefore, one must dothe following to find\u00aetest data that reliably reveal errors: Identify all the conditions relevant to the correct operation of a program.Select test data to exercise all possible combinations of these conditions.The above idea of selecting te\u00aest data leads us to define the following terms:Test Data: Test data are actual values from the input domain of a programthat collectively satisfy some test selection criteria.Test Predicate: A test predi\u00aecate is a description of conditions and combinations of conditions relevant to correct operation of the program:Test predicates describe the aspects of a program that are to be tested.Test data cause these aspects to be tested.Test predicates are the motivating force for test data selecti\u00aeon.Components of test predicates arise first and primarily from the specifications for a program.Further conditions and predicates may be added as implementations areconsidered.A set of test predicates must at least satisfy the following conditions to have anychance of being reliable. These conditions are key to meaningful testing\u00ae:Every individual branching condition in a program must be represented byan equivalent condition in C.Every potential termination condition in the program, for example, an overflow, must be represented by a condition in C.Every condition relevant to the correct operation of the program that isimplied by the specification and knowledge of the data structure of theprogram must be represented as a condition in C.",
  "page359": " The concepts of reliability and validity have been defined with respect tothe entire input domain of a program. A criterion is guaranteed to be bothreliable and valid if and only\u00aeif it selects the entire domain as a single test.Since such exhaustive testing is impractical, one will have much difficultyin assessing the reliability and validity of a criterion.The concepts of reliability and validi\u00aety have been defined with respect to aprogram. A test selection criterion that is reliable and valid for one programmay not be so for another program. The goodness of a test set should beindependent of i\u00aendividual programs and the faults therein.Neither validity nor reliability is preserved throughout the debugging process. In practice, as program failures are observed, the program is debuggedto locate the faults, and the faults are generally fixed as soon as they arefound. During this de\u00aebugging phase, as the program changes, so does theidealness of a test set. This is because a fault that was revealed beforedebugging is no more revealed after debugging and fault fixing. Thus,properties of test selection criteria are not even \"monotonic\" in the senseof being either always gained or preserved or always lo\u00aest or preserved.A key problem in the theory of Goodenough and Gerhart is that the reliability andvalidity of a criterion depend upon the presence of faults in a program and theirtypes. Weyuker and Ostrand [18] provide a modified theory in which the validityand reliability of test selection criteria are dependent only on the program specification, rather than a program. They propose the concept of a uniformly ideal test",
  "page360": "An ideal goal in software development is to find out whether or not a program iscorrect, where a correct program is void of faults. Much research results have beenreported in the f\u00aeield of program correctness. However, due to the highly constrainednature of program verification techniques, no developer makes any effort to provethe correctness of even small programs of, say, a few thousand lines, l\u00aeet alonelarge programs with millions of lines of code. Instead, testing is accepted in theindustry as a practical way of finding faults in programs. The flip side of testingis that it cannot be used to s\u00aeettle the question of program correctness, which is theideal goal. Even though testing cannot settle the program correctness issue, thereis a need for a testing theory to enable us to compare the power of different testmethods.To motivate a theoretical discussion of testing, we begin with\u00aean ideal processfor software development, which consists of the following steps:A customer and a development team specify the needs. The development team takes the specification and attempts to write a program to meet the specification. A test engineer takes both the specification and the program and selectsa set of test cases. T\u00aehe test cases are based on the specification and theprogram.The program is executed with the selected test data, and the test outcomeis compared with the expected outcome. The program is said to have faults if some tests fail.One can say the program to be ready for use if it passes all the test cases.We focus on the selection of test cases and the interpretation of their results.We assume that the specification is correct, and the specification is the sole arbiterof the correctness of the program. ",
  "page361": "Program Dependent: In this case, T :M (P), that is, test cases arederived solely based on the source code of a system. This is calledwhite-box testing. Here, a test method has comp\u00aelete knowledge of theinternal details of a program. However, from the viewpoint of practicaltesting, a white-box method is not generally applied to an entire program.One applies such a method to small units of a given l\u00aearge system. A unitrefers to a function, procedure, method, and so on. A white-box methodallows a test engineer to use the details of a program unit. Effective use ofa program unit requires a thorough un\u00aederstanding of the unit. Therefore,white-box test methods are used by programmers to test their own code. Specification Dependent: In this case, T = M (S ), that is, test casesare derived solely based on the specification of a system. This is calledblack-box testing. Here, a test method d\u00aeoes not have access to the internaldetails of a program. Such a method uses information provided in thespecification of a system. It is not unusual to use an entire specificationin the generation of test cases because specifications are much smaller insize than their corresponding implementations. Black-box methods aregenerally us\u00aeed by the development team and an independent system testgroup. Expectation Dependent: In practice, customers may generate test casesbased on their expectations from the product at the time of taking deliveryof the system. These test cases may include continuous-operation tests,usability tests, and so on.",
  "page362": "Ideally, all programs should be correct, that is, there is no fault in a program. Dueto the impractical nature of proving even small programs to be correct, customersand software d\u00aeevelopers rely on the efficacy of testing. In this section, we introducetwo main limitations of testing: Testing means executing a program with a generally small, proper subsetof the input domain of the program. A small\u00ae, proper subset of the inputdomain is chosen because cost may not allow a much larger subset tobe chosen, let alone the full input set. Testing with the full input set isknown as exhaustive testing. Thus\u00ae, the inherent need to test a programwith a small subset of the input domain poses a fundamental limit on theefficacy of testing. The limit is in the form of our inability to extrapolate thecorrectness of results for a proper subset of the input domain to programcorrectness. In other word\u00aes, even if a program passes a test set T, wecannot conclude that the program is correct. Once we have selected a subset of the input domain, we are faced with theproblem of verifying the correctness of the program outputs for individualtest input. That is, a program output is examined to determine if the programperformed correctly\u00aeon the test input. The mechanism which verifies thecorrectness of a program output is known as an oracle. The concept ofan oracle is discussed in detail in Chapter 9. Determining the correctnessof a program output is not a trivial task. If either of the following twoconditions hold, a program is considered non testable [20]:There does not exist an oracle.",
  "page363": "Ideally, all programs should be correct, that is, there is no fault in a program. Dueto the impractical nature of proving even small programs to be correct, customersand software d\u00aeevelopers rely on the efficacy of testing. In this section, we introducetwo main limitations of testing: Testing means executing a program with a generally small, proper subsetof the input domain of the program. A small\u00ae, proper subset of the inputdomain is chosen because cost may not allow a much larger subset tobe chosen, let alone the full input set. Testing with the full input set isknown as exhaustive testing. Thus\u00ae, the inherent need to test a programwith a small subset of the input domain poses a fundamental limit on theefficacy of testing. The limit is in the form of our inability to extrapolate thecorrectness of results for a proper subset of the input domain to programcorrectness. In other word\u00aes, even if a program passes a test set T, wecannot conclude that the program is correct. Once we have selected a subset of the input domain, we are faced with theproblem of verifying the correctness of the program outputs for individualtest input. That is, a program output is examined to determine if the programperformed correctly\u00aeon the test input. The mechanism which verifies thecorrectness of a program output is known as an oracle. The concept ofan oracle is discussed in detail in Chapter 9. Determining the correctnessof a program output is not a trivial task. If either of the following twoconditions hold, a program is considered non testable [20]:There does not exist an oracle.",
  "page364": "In this chapter we consider the first level of testing, that is, unit testing. Unit testingrefers to testing program units in isolation. However, there is no consensus on thedefini\u00aetion of a unit. Some examples of commonly understood units are functions,procedures, or methods. Even a class in an object-oriented programming languagecan be considered as a program unit. Syntactically, a program unit\u00aeis a piece ofcode, such as a function or method of class, that is invoked from outside the unitand that can invoke other program units. Moreover, a program unit is assumed toimplement a well-defined func\u00aetion providing a certain level of abstraction to theimplementation of higher level functions. The function performed by a program unitmay not have a direct association with a system-level function. Thus, a programunit may be viewed as a piece of code implementing a \"low\"-level f\u00aeunction. Inthis chapter, we use the terms unit and module interchangeably.Now, given that a program unit implements a function, it is only natural totest the unit before it is integrated with other units. Thus, a program unit is testedin isolation, that is, in a stand-alone manner. There are two reasons for testing aunit in a stan\u00aed-alone manner. First, errors found during testing can be attributedto a specific unit so that it can be easily fixed. Moreover, unit testing removesdependencies on other program units. Second, during unit testing it is desirableto verify that each distinct execution of a program unit produces the expectedresult. In terms of code details, a distinct execution refers to a distinct path in theunit.",
  "page365": "Second, during unit testing it is desirableto verify that each distinct execution of a program unit produces the expectedresult. In terms of code details, a distinct execution refe\u00aers to a distinct path in theunit. Ideally, all possible or as much as possible distinct executions are to beconsidered during unit testing. This requires careful selection of input data for eachdistinct execution. A pro\u00aegrammer has direct access to the input vector of the unit byexecuting a program unit in isolation. This direct access makes it easier to executeas many distinct paths as desirable or possible. If multipl\u00aee units are put together fortesting, then a programmer needs to generate test input with indirect relationshipwith the input vectors of several units under test. The said indirect relationshipmakes it difficult to control the execution of distinct paths in a chosen unit.Unit testing has a\u00aelimited scope. A programmer will need to verify whetheror not a code works correctly by performing unit-level testing. Intuitively, a programmer needs to test a unit as follows:Execute every line of code. This is desirable because the programmer needsto know what happens when a line of code is executed. In the absence ofsuch basi\u00aec observations, surprises at a later stage can be expensive. Execute every predicate in the unit to evaluate them to true and falseseparately. Observe that the unit performs its intended function and ensure that itcontains no known errors.In spite of the above tests, there e is no guarantee that a satisfactorily tested unitis functionally correct from a systemwide perspective. ",
  "page366": "Even though it is not possible to find all errors in a program unit in isolation, itis still necessary to ensure that a unit performs satisfactorily before it is used byother progr\u00aeam units. It serves no purpose to integrate an erroneous unit with otherunits for the following reasons: (i) many of the subsequent tests will be a wasteof resources and (ii) finding the root causes of failures in an in\u00aetegrated system ismore resource consuming.Unit testing is performed by the programmer who writes the program unitbecause the programmer is intimately familiar with the internal details of the unit.The ob\u00aejective for the programmer is to be satisfied that the unit works as expected.Since a programmer is supposed to construct a unit with no errors in it, a unittest is performed by him or her to their satisfaction in the beginning and to thesatisfaction of other programmers when the unit is\u00aeintegrated with other units. Thismeans that all programmers are accountable for the quality of their own work,which may include both new code and modifications to the existing code. The ideahere is to push the quality concept down to the lowest level of the organization andempower each programmer to be responsible for his or her o\u00aewn quality. Therefore,it is in the best interest of the programmer to take preventive actions to minimizethe number of defects in the code. The defects found during unit testing are internalto the software development group and are not reported up the personnel hierarchyto be counted in quality measurement metrics. The source code of a unit is notused for interfacing by other group members until the programmer completes unit ",
  "page367": "testing and checks in the unit to the version control system.Unit testing is conducted in two complementary phases: Static unit testingDynamic unit testingIn static unit testing, a\u00aeprogrammer does not execute the unit; instead, the code isexamined over all possible behaviors that might arise during run time. Static unittesting is also known as non-execution-based unit testing, whereas dynamic uni\u00aettesting is execution based. In static unit testing, the code of each unit is validatedagainst requirements of the unit by reviewing the code. During the review process,potential issues are identified an\u00aed resolved. For example, in the C programminglanguage the two program-halting instructions are abort() and exit(). While the twoare closely related, they have different effects as explained below:Abort(): This means abnormal program termination. By default, a call toabort() results in a r\u00aeun time diagnostic and program self-destruction. Theprogram destruction may or may not flush and close opened files or removetemporary files, depending on the implementation. Exit(): This means graceful program termination. That is, the exit() callcloses the opened files and returns a status code to the execution environment.Wheth\u00aeer to use abort() or exit() depends on the context that can be easilydetected and resolved during static unit testing. More issues caught earlier lead tofewer errors being identified in the dynamic test phase and result in fewer defectsin shipped products. Moreover, performing static tests is less expensive than performing dynamic tests. Code review is one component of the defect minimizationprocess and can help detect problems that are common to software development.After a round of code review, dynamic unit testing is conducted.",
  "page368": "In dynamic unittesting, a program unit is actually executed and its outcomes are observed. Dynamicunit testing means testing the code by actually running it. It may be noted that s\u00aetaticunit testing is not an alternative to dynamic unit testing. A programmer performsboth kinds of tests. In practice, partial dynamic unit testing is performed concurrently with static unit testing. If the entire dyna\u00aemic unit testing has been performedand a static unit testing identifies significant problems, the dynamic unit testingmust be repeated. As a result of this repetition, the development schedule may beaffe\u00aected. To minimize the probability of such an event, it is required that static unittesting be performed prior to the final dynamic unit testingStatic unit testing is conducted as a part of a larger philosophical belief that asoftware product should undergo a phase of inspection and correc\u00aetion at eachmilestone in its life cycle. At a certain milestone, the product need not be in itsfinal form. For example, completion of coding is a milestone, even though codingof all the units may not make the desired product. After coding, the next milestoneis testing all or a substantial number of units forming the major componen\u00aets of theproduct. Thus, before units are individually tested by actually executing them, thoseare subject to usual review and correction as it is commonly understood. The ideabehind review is to find the defects as close to their points of origin as possible sothat those defects are eliminated with less effort, and the interim product containsfewer defects before the next task is undertaken.",
  "page369": "Inspection: It is a step-by-step peer group review of a work product, witheach step checked against predetermined criteria. Walkthrough: It is a review where the author leads the t\u00aeeam through amanual or simulated execution of the product using predefined scenarios.Regardless of whether a review is called an inspection or a walkthrough, it isa systematic approach to examining source code in detail\u00ae. The goal of such anexercise is to assess the quality of the software in question, not the quality of theprocess used to develop the product [3]. Reviews of this type are characterizedby significant pre\u00aeparation by groups of designers and programmers with varyingdegree of interest in the software development project. Code examination can betime consuming. Moreover, no examination process is perfect. Examiners may takeshortcuts, may not have adequate understanding of the product, and may\u00aeaccept aproduct which should not be accepted. Nonetheless, a well-designed code reviewprocess can find faults that may be missed by execution-based testing. The key tothe success of code review is to divide and conquer, that is, having an examinerinspect small parts of the unit in isolation, while making sure of the following:(i)\u00aenothing is overlooked and (ii) the correctness of all examined parts of themodule implies the correctness of the whole module. The decomposition of thereview into discrete steps must assure that each step is simple enough that it canbe carried out without detailed knowledge of the others.The objective of code review is to review the code, not to evaluate the authorof the code",
  "page370": "review is to review the code, not to evaluate the authorof the code. A clash may occur between the author of the code and the reviewers,and this may make the meetings unproductive.\u00aeTherefore, code review must beplanned and managed in a professional manner. There is a need for mutual respect,openness, trust, and sharing of expertise in the group. The general guidelines forperforming code review co\u00aensists of six steps as outlined in Figure 3.1: readiness,preparation, examination, rework, validation, and exit. The input to the readinessstep is the criteria that must be satisfied before the start of\u00aethe code review process,and the process produces two types of documents, a change request (CR) and areport. These steps and documents are explained in the following. Readiness The author of the unit ensures that the unit under test isready for review. A unit is said to be ready if it sati\u00aesfies the followingcriteria. Completeness: All the code relating to the unit to be reviewed must beavailable. This is because the reviewers are going to read the code andtry to understand it. It is unproductive to review partially written codeor code that is going to be significantly modified by the programmer Minimal Functionalit\u00aey: The code must compile and link. Moreover,the code must have been tested to some extent to make sure that itperforms its basic functionalities.Readability: Since code review involves actual reading of code byother programmers, it is essential that the code is highly readable.Some code characteristics that enhance readability are proper formatting, using meaningful identifier names, straightforward use of programming language constructs, and an appropriate level of abstractionusing function calls. ",
  "page371": "Complexity: There is no need to schedule a group meeting to reviewstraightforward code which can be easily reviewed by the programmer.The code to be reviewed must be of sufficient\u00aecomplexity to warrantgroup review. Here, complexity is a composite term referring to thenumber of conditional statements in the code, the number of input dataelements of the unit, the number of output data elements prod\u00aeuced bythe unit, real-time processing of the code, and the number of other unitswith which the code communicates.Requirements and Design Documents: The latest approved versionof the low-level design spec\u00aeification or other appropriate descriptions Hierarchy of System Document of program requirements (see Table 3.1) should be available. Thesedocuments help the reviewers in verifying whether or not the codeunder review implements the expected functionalities. If the low-leveldesign document\u00aeis available, it helps the reviewers in assessing whetheror not the code appropriately implements the design.All the people involved in the review process are informed of thegroup review meeting schedule two or three days before the meeting.They are also given a copy of the work package for their perusal. Reviewsare conducted in\u00aebursts of 1-2 hours. Longer meetings are less and lessproductive because of the limited attention span of human beings. Therate of code review is restricted to about 125 lines of code (in a high-levellanguage) per hour. Reviewing complex code at a higher rate will resultin just glossing over the code, thereby defeating the fundamental purposeof code review. The composition of the review group involves a numberof people with different roles. These roles are explained as follows",
  "page372": "Moderator: A review meeting is chaired by the moderator. The moderator is a trained individual who guides the pace of the review process.The moderator selects the reviewers and sch\u00aeedules the review meetings.Myers suggests that the moderator be a member of a group from anunrelated project to preserve objectivity Author: This is the person who has written the code to be reviewed.Presenter: A presen\u00aeter is someone other than the author of the code.The presenter reads the code beforehand to understand it. It is thepresenter who presents the author's code in the review meeting forthe following re\u00aeasons: (i) an additional software developer will understand the work within the software organization; (ii) if the originalprogrammer leaves the company with a short notice, at least one otherprogrammer in the company knows what is being done; and (iii) theoriginal programmer will have a\u00aegood feeling about his or her work, ifsomeone else appreciates their work. Usually, the presenter appreciatesthe author's work. Recordkeeper: The recordkeeper documents the problems found during the review process and the follow-up actions suggested. The personshould be different than the author and the moderator.Reviewers: T\u00aehese are experts in the subject area of the code underreview. The group size depends on the content of the material underreview. As a rule of thumb, the group size is between 3 and 7. Usuallythis group does not have manager to whom the author reports. Thisis because it is the author's ongoing work that is under review, andneither a completed work nor the author himself is being reviewed.Observers: These are people who want to learn about the code underreview. These people do not participate in the review process but aresimply passive observers.",
  "page373": " Preparation Before the meeting, each reviewer carefully reviews thework package. It is expected that the reviewers read the code and understand its organization and operation befo\u00aere the review meeting. Eachreviewer develops the following: List of Questions: A reviewer prepares a list of questions to be asked,if needed, of the author to clarify issues arising from his or her reading.A general gui\u00aedeline of what to examine while reading the code isoutlined in Table 3.2. Potential CR: A reviewer may make a formal request to make achange. These are called change requests rather than defect reports.A\u00aet this stage, since the programmer has not yet made the code public, it is more appropriate to make suggestions to the author to makechanges, rather than report a defect. Though CRs focus on defects inthe code, these reports are not included in defect statistics related tothe productSugge\u00aested Improvement Opportunities: The reviewers may suggesthow to fix the problems, if there are any, in the code under review.Since reviewers are experts in the subject area of the code, it is notunusual for them to make suggestions for improvements. Examination The examination process consists of the followingactivities: The autho\u00aer makes a presentation of the procedural logic used in thecode, the paths denoting major computations, and the dependency ofthe unit under review on other units.The presenter reads the code line by line. The reviewers may raisequestions if the code is seen to have defects. However, problems are notresolved in the meeting. The reviewers may make general suggestionson how to fix the defects, but it is up to the author of the code to takecorrective measures after the meeting ends.",
  "page374": " The recordkeeper documents the change requests and the suggestionsfor fixing the problems, if there are any. A CR includes the followingdetails Give a brief description of the iss\u00aeue or action item. Assign a priority level (major or minor) to a CR.Assign a person to follow up the issue. Since a CR documents apotential problem, there is a need for interaction between the author of the code and one\u00aeof the reviewers, possibly the reviewer whomade the CR.Set a deadline for addressing a CR.The moderator ensures that the meeting remains focused on the reviewprocess. The moderator makes sure that the m\u00aeeeting makes progress ata certain rate so that the objective of the meeting is achieved.At the end of the meeting, a decision is taken regarding whether ornot to call another meeting to further review the code. If the reviewprocess leads to extensive rework of the code or critical issues\u00aeareidentified in the process, then another meeting is generally convened.Otherwise, a second meeting is not scheduled, and the author is giventhe responsibility of fixing the CRs Rework At the end of the meeting, the recordkeeper produces a summary of the meeting that includes the following information: A list of all the CRs, the\u00aedates by which those will be fixed, and thenames of the persons responsible for validating the CRs A list of improvement opportunitiesThe minutes of the meeting (optional) ",
  "page375": "A copy of the report is distributed to all the members of the review group.After the meeting, the author works on the CRs to fix the problems. Theauthor documents the improvements\u00aemade to the code in the CRs. Theauthor makes an attempt to address the issues within the agreed-upontime frame using the prevailing coding conventions Validation The CRs are independently validated by the moderatoror an\u00aeother person designated for this purpose. The validation processinvolves checking the modified code as documented in the CRs andensuring that the suggested improvements have been implementedcorrectly. Th\u00aee revised and final version of the outcome of the reviewmeeting is distributed to all the group members.Exit Summarizing the review process, it is said to be complete if all ofthe following actions have been taken: Every line of code in the unit has been inspected. If too many defects are\u00aefound in a module, the module is once againreviewed after corrections are applied by the author. As a rule of thumb,if more than 5% of the total lines of code are thought to be contentious,then a second review is scheduled. The author and the reviewers reach a consensus that when correctionshave been applied the code will be pote\u00aentially free of defects. All the CRs are documented and validated by the moderator or someoneelse. The author's follow-up actions are documented. A summary report of the meeting including the CRs is distributed toall the members of the review group.",
  "page376": "The effectiveness of static testing is limited by the ability of a reviewer tofind defects in code by visual means. However, if occurrences of defects depend onsome actual values o\u00aef variables, then it is a difficult task to identify those defectsby visual means. Therefore, a unit must be executed to observe its behaviors inresponse to a variety of inputs. Finally, whatever may be the effectivenes\u00aes of statictests, one cannot feel confident without actually running the code.Code Review Metrics It is important to collect measurement data pertinent toa review process, so that the review process can\u00aebe evaluated, made visible tothe upper management as a testing strategy, and improved to be more effective.Moreover, collecting metrics during code review facilitates estimation of reviewtime and resources for future projects. Thus, code review is a viable testing strategythat can be effe\u00aectively used to improve the quality of products at an early stage.The following metrics can be collected from a code review:Number of lines of code (LOC) reviewed per hour Number of CRs generated per thousand lines of code (KLOC) Number of CRs generated per hour Total number of CRs generated per project Total number of hours spent\u00aeon code review per projectIt is in the best interest of the programmers in particular and the company in generalto reduce the number of CRs generated during code review. This is because CRs areindications of potential problems in the code, and those problems must be resolvedbefore different program units are integrated. Addressing CRs means spending moreresources and potentially delaying the project. Therefore, it is essential to adoptthe concept of defect prevention during code development.",
  "page377": "Code Review Metrics It is important to collect measurement data pertinent toa review process, so that the review process can be evaluated, made visible tothe upper management as a\u00aetesting strategy, and improved to be more effective.Moreover, collecting metrics during code review facilitates estimation of reviewtime and resources for future projects. Thus, code review is a viable testing strategyt\u00aehat can be effectively used to improve the quality of products at an early stage.The following metrics can be collected from a code review: Number of lines of code (LOC) reviewed per hourNumber of CRs ge\u00aenerated per thousand lines of code (KLOC)Number of CRs generated per hour Total number of CRs generated per project Total number of hours spent on code review per projectIt is in the best interest of the programmers in particular and the company in generalto reduce the number of CRs gener\u00aeated during code review. This is because CRs areindications of potential problems in the code, and those problems must be resolvedbefore different program units are integrated. Addressing CRs means spending moreresources and potentially delaying the project. Therefore, it is essential to adoptthe concept of defect prevention durin\u00aeg code development. In practice, defectsare inadvertently introduced by programmers. Those accidents can be reduced bytaking preventive measures. It is useful to develop a set of guidelines to constructcode for defect minimization as explained in the following. These guidelines focuson incorporating suitable mechanisms into the code: ",
  "page378": "Build internal diagnostic tools, also known as instrumentation code, intothe units. Instrumentation codes are useful in providing information aboutthe internal states of the units.\u00aeThese codes allow programmers to realizebuilt-in tracking and tracing mechanisms. Instrumentation plays a passiverole in dynamic unit testing. The role is passive in the sense of observingand recording the internal beh\u00aeavior without actively testing a unit. Use standard controls to detect possible occurrences of error conditions.Some examples of error detection in the code are divides by zero and arrayindex out of boun\u00aeds. Ensure that code exists for all return values, some of which may be invalid.Appropriate follow-up actions need to be taken to handle invalid returnvalues. Ensure that counter data fields and buffer overflow and underflow areappropriately handled. Provide error messages and help texts\u00aefrom a common source so thatchanges in the text do not cause inconsistency. Good error messagesidentify the root causes of the problems and help users in resolving theproblems .Validate input data, such as the arguments, passed to a function. Use assertions to detect impossible conditions, undefined uses of data, andundesirable pr\u00aeogram behavior. An assertion is a Boolean statement whichshould never be false or can be false only if an error has occurred. In otherwords, an assertion is a check on a condition which is assumed to be true,but it can cause a problem if it not true. Assertion should be routinely usedto perform the following kinds of checks:",
  "page379": "Ensure that preconditions are satisfied before beginning to execute aunit. A precondition is a Boolean function on the states of a unit specifying our expectation of the state prio\u00aer to initiating an activity in thecode.Ensure that the expected postconditions are true while exiting from theunit. A postcondition is a Boolean function on the state of a unit specifying our expectation of the state af\u00aeter an activity has been completed.The postconditions may include an invariance. Ensure that the invariants hold. That is, check invariant states conditions which are expected not to change during the ex\u00aeecution of apiece of code. Leave assertions in the code. You may deactivate them in the releasedversion of code in order to improve the operational performance of thesystem. Fully document the assertions that appear to be unclear. After every major computation, reverse-compute the input(s\u00ae) from theresults in the code itself. Then compare the outcome with the actual inputsfor correctness. For example, suppose that a piece of code computes thesquare root of a positive number. Then square the output value and compare the result with the input. It may be needed to tolerate a margin oferror in the comparison process. I\u00aen systems involving message passing, buffer management is an importantinternal activity. Incoming messages are stored in an already allocatedbuffer. It is useful to generate an event indicating low buffer availabilitybefore the system runs out of buffer. Develop a routine to continuallymonitor the availability of buffer after every use, calculate the remainingspace available in the buffer, and call an error handling routine if theamount of available buffer space is too low",
  "page380": "Develop a timer routine which counts down from a preset time until iteither hits zero or is reset. If the software is caught in an infinite loop, thetimer will expire and an except\u00aeion handler routine can be invoked.Include a loop counter within each loop. If the loop is ever executed lessthan the minimum possible number of times or more than the maximumpossible number of times, then invoke an exc\u00aeeption handler routine.Define a variable to indicate the branch of decision logic that will be taken.Check this value after the decision has been made and the right branch hassupposedly been taken. If th\u00aee value of the variable has not been preset,there is probably a fall-through condition in the logic.Execution-based unit testing is referred to as dynamic unit testing. In this testing,a program unit is actually executed in isolation, as we commonly understand it.However, this execution d\u00aeiffers from ordinary execution in the following way: A unit under test is taken out of its actual execution environment. The actual execution environment is emulated by writing more code(explained later in this section) so that the unit and the emulatedenvironment can be compiled togetherThe above compiled aggregate is executed wi\u00aeth selected inputs. Theoutcome of such an execution is collected in a variety of ways, such asstraightforward observation on a screen, logging on files, and softwareinstrumentation of the code to reveal run time behavior. The resultis compared with the expected outcome. Any difference between theactual and expected outcome implies a failure and the fault is inthe code",
  "page381": "An environment for dynamic unit testing is created by emulating the contextof the unit under test, as shown in Figure 3.2. The context of a unit test consistsof two parts: (i) a ca\u00aeller of the unit and (ii) all the units called by the unit. Theenvironment of a unit is emulated because the unit is to be tested in isolationand the emulating environment must be a simple one so that any fault foundas\u00aea result of running the unit can be solely attributed to the unit under test.The caller unit is known as a test driver, and all the emulations of the unitscalled by the unit under test are called stubs.\u00aeThe test driver and the stubs aretogether called scaffolding. The functions of a test driver and a stub are explained asfollows:Test Driver: A test driver is a program that invokes the unit under test.The unit under test executes with input values received from the driverand, upon termina\u00aetion, returns a value to the driver. The driver comparesthe actual outcome, that is, the actual value returned by the unit under test,with the expected outcome from the unit and reports the ensuing test result.The test driver functions as the main unit in the execution process. Thedriver not only facilitates compilation, but also\u00aeprovides input data to theunit under test in the expected format. Stubs: A stub is a \"dummy subprogram\" that replaces a unit that is calledby the unit under test. Stubs replace the units called by the unit under test.A stub performs two tasks. ",
  "page382": "First, it shows an evidence that the stub was, in fact, called. Such evidence can be shown by merely printing a message.Second, the stub returns a precomputed value to the caller s\u00aeo that the unitunder test can continue its execution.The driver and the stubs are never discarded after the unit test is completed.Instead, those are reused in the future in regression testing of the unit if there issuc\u00aeh a need. For each unit, there should be one dedicated test driver and severalstubs as required. If just one test driver is developed to test multiple units, thedriver will be a complicated one. Any modi\u00aefication to the driver to accommodatechanges in one of the units under test may have side effects in testing the otherunits. Similarly, the test driver should not depend on the external input data filesbut, instead, should have its own segregated set of input data. The separate inputdata\u00aefile approach becomes a very compelling choice for large amounts of testinput data. For example, if hundreds of input test data elements are required to testmore than one unit, then it is better to create a separate input test data file ratherthan to include the same set of input test data in each test driver designed to testthe u\u00aenit.The test driver should have the capability to automatically determine thesuccess or failure of the unit under test for each input test data. If appropriate,the driver should also check for memory leaks and problems in allocation anddeallocation of memory. If the module opens and closes files, the test driver shouldcheck that these files are left in the expected open or closed state after each test.",
  "page383": "Mutation testing has a rich and long history. It can be traced back to the late 1970s[8-10]. Mutation testing was originally proposed by Dick Lipton, and the articleby DeMillo\u00ae, Lipton, and Sayward [9] is generally cited as the seminal reference.Mutation testing is a technique that focuses on measuring the adequacy of test data(or test cases). The original intention behind mutation testing wa\u00aes to expose andlocate weaknesses in test cases. Thus, mutation testing is a way to measure thequality of test cases, and the actual testing of program units is an added benefit.Mutation testing is not a\u00aetesting strategy like control flow or data flow testing. Itshould be used to supplement traditional unit testing techniques.A mutation of a program is a modification of the program created by introducing a single, small, legal syntactic change in the code. A modified program soobtained is\u00aecalled a mutant. The term mutant has been borrowed from biology.Some of these mutants are equivalent to the original program, whereas others arefaulty. A mutant is said to be killed when the execution of a test case causes it tofail and the mutant is considered to be dead.Some mutants are equivalent to the given program, that is,\u00aesuch mutantsalways produce the same output as the original program. In the real world, largeprograms are generally faulty, and test cases too contain faults. ",
  "page384": "The result of executing a mutant may be different from the expected result, but a test suite doesnot detect the failure because it does not have the right test case. In this scenar\u00aeiothe mutant is called killable or stubborn, that is, the existing set of test casesis insufficient to kill it. A mutation score for a set of test cases is the percentage of nonequivalent mutants killed by the test suit\u00aee. The test suite is said to bemutation adequate if its mutation score is 100%. Mutation analysis is a two-stepprocess: The adequacy of an existing test suite is determined to distinguish thegiven progra\u00aem from its mutants. A given test suite may not be adequateto distinguish all the nonequivalent mutants. As explained above, thosenonequivalent mutants that could not be identified by the given test suiteare called stubborn mutants. New test cases are added to the existing test suite to ki\u00aell the stubbornmutants. The test suite enhancement process iterates until the test suitehas reached a desired level of mutation score.If we run the modified programs against the test suite, we will get the followingresults:Mutants 1 and 3: The programs will completely pass the test suite. In otherwords, mutants 1 and 3 are not kil\u00aeled.Mutant 2: The program will fail test case 2.Mutant 4: The program will fail test case 1 and test case 2.If we calculate the mutation score, we see that we created four mutants, andtwo of them were killed. This tells us that the mutation score is 50%, assumingthat mutants 1 and 3 are nonequivalent.The score is found to be low. It is low because we assumed that mutants 1 and3 are nonequivalent to the original program. ",
  "page385": "We have to show that either mutants 1 and 3 are equivalent mutants or those are killable. If those are killable, we needto add new test cases to kill these two mutants. First, let\u00aeus analyze mutant 1in order to derive a \"killer\" test. The difference between P and mutant 1 is thestarting point. Mutant 1 starts with i 1, whereas P starts with i = 2. There isno impact on the result r. The\u00aerefore, we conclude that mutant 1 is an equivalentmutant. Second, we add a fourth test case as follows:Test case 4:Input: 2 2 1Then program P will produce the output \"Value of the rank is 1\" an\u00aed mutant 3will produce the output \"Value of the rank is 2.\" Thus, this test data kills mutant 3,which give us a mutation score of 100%.In order to use the mutation testing technique to build a robust test suite, thetest engineer needs to follow the steps that are outlined below:\u00aeStep 1: Begin with a program P and a set of test cases T known to be correct.Step 2: Run each test case in T against the program P. If a test case fails, thatis, the output is incorrect, program P must be modified and retested. Ifthere are no failures, then continue with step 3.Step 3: Create a set of mutants {Pi}, each differing\u00aefrom P by a simple, syntactically correct modification of P",
  "page386": "Execute each test case in T against each mutant Pi . If the output of themutant Pi differs from the output of the original program P, the mutantPi is considered incorrect and is sa\u00aeid to be killed by the test case. If Piproduces exactly the same results as the original program P for the testsin T, then one of the following is true: P and Pi are equivalent. That is, their behaviors cannot be\u00aedistinguished by any set of test cases. Note that the general problem ofdeciding whether or not a mutant is equivalent to the original programis undecidable. Pi is killable. That is, the test cases\u00aeare insufficient to kill the mutantPi . In this case, new test cases must be created.Step 5: Calculate the mutation score for the set of test cases T. The mutation score is the percentage of nonequivalent mutants killed by the testdata, that is, Mutation score 100 X D/(N \u2212 E),\u00aewhere D is the deadmutants, N the total number of mutants, and E the number of equivalentmutants.Step 6: If the estimated mutation adequacy of T in step 5 is not sufficiently high,then design a new test case that distinguishes Pi from P, add the newtest case to T, and go to step 2. If the computed adequacy of T is morethan an app\u00aeropriate threshold, then accept T as a good measure of thecorrectness of P with respect to the set of mutant programs Pi , and stopdesigning new test cases.",
  "page387": "Competent Programmer Hypothesis: This assumption states that programmers are generally competent, and they do not create \"random\" programs.Therefore, we can assume that f\u00aeor a given problem a programmer will create a correct program except for simple errors. In other words, the mutantsto be considered are the ones falling within a small deviation from theoriginal program. In practice, su\u00aech mutants are obtained by systematicallyand mechanically applying a set of transformations, called mutation operators, to the program under test. These mutation operators are expected tomodel programmin\u00aeg errors made by programmers. In practice, this maybe only partly true. Coupling Effect: This assumption was first hypothesized in 1978 byDeMillo et al. [9]. The assumption can be restated as complex faultsare coupled to simple faults in such a way that a test suite detectingall simple fa\u00aeults in a program will detect most of the complex faults.This assumption has been empirically supported by Offutt [11] andtheoretically demonstrated by Wah [12]. The fundamental premise ofmutation testing as coined by Geist et al. [13] is: If the software containsa fault, there will usually be a set of mutants that can only be kil\u00aeled by atest case that also detect that faul Mutation testing helps the tester to inject, by hypothesis, different types offaults in the code and develop test cases to reveal them. In addition, comprehensivetesting can be performed by proper choice of mutant operations. However, a relatively large number of mutant programs need to be tested against many of the testcases before these mutants can be distinguished from the original program. Running the test cases, analyzing the results, identifying equivalent mutants [14], anddeveloping additional test cases to kill the stubborn mutants are all time consuming",
  "page388": "Robust automated testing tools such as Mothra can be used to expeditethe mutation testing process. Recently, with the availability of massive computing power, there has been a res\u00aeurgence of mutation testing processes within theindustrial community to use as a white-box methodology for unit testing [16, 17].Researchers have shown that with an appropriate choice of mutant programs mutation testing\u00aeis as powerful as path testing, domain testing [18], and data flowtesting The programmer, after a program failure, identifies the corresponding fault andfixes it. The process of determining the cause of\u00aea failure is known as debugging.Debugging occurs as a consequence of a test revealing a failure. Myers proposedthree approaches to debugging in his book The Art of Software Testing [20]:Brute Force: The brute-force approach to debugging is preferred by manyprogrammers. Here, \"let th\u00aee computer find the error\" philosophy is used. Print statements are scattered throughout the source code. These print statements provide a crude trace of the way the source code has executed.The availability of a good debugging tool makes these print statementsredundant . A dynamic debugger allows the software engineer to nav\u00aeigateby stepping through the code, observe which paths have executed, andobserve how values of variables change during the controlled execution.A good tool allows the programmer to assign values to several variablesand navigate step by step through the code",
  "page389": "Instrumentation code can bebuilt into the source code to detect problems and to log intermediate values of variables for problem diagnosis. One may use a memory dumpafter a failure\u00aehas occurred to understand the final state of the code beingdebugged. The log and memory dump are reviewed to understand whathappened and how the failure occurred. Cause Elimination: The cause elimination approach can\u00aebe best describedas a process involving induction and deduction [21]. In the induction part,first, all pertinent data related to the failure are collected , such as whathappened and what the symptoms are\u00ae. Next, the collected data are organized in terms of behavior and symptoms, and their relationship is studiedto find a pattern to isolate the causes. A cause hypothesis is devised, and theabove data are used to prove or disprove the hypothesis. In the deductionpart, a list of all possible\u00aecauses is developed in order of their likelihoods,and tests are conducted to eliminate or substantiate each cause in decreasing order of their likelihoods. If the initial tests indicate that a particularhypothesis shows promise, test data are refined in an attempt to isolate theproblem as needed.Backtracking: In this approach, th\u00aee programmer starts at a point in thecode where a failure was observed and traces back the execution to the pointwhere it occurred. This technique is frequently used by programmers, andthis is useful in small programs. However, the probability of tracing backto the fault decreases as the program size increases, because the numberof potential backward paths may become too large.",
  "page390": "Often, software engineers notice other previously undetected problems whiledebugging and applying a fix. These newly discovered faults should not be fixedalong with the fix in focu\u00aes. This is because the software engineer may not have afull understanding of the part of the code responsible for the new fault. The bestway to deal with such a situation is to file a CR. A new CR gives the programmer a\u00aenopportunity to discuss the matter with other team members and software architectsand to get their approval on a suggestion made by the programmer. Once the CR isapproved, the software engineer must file\u00aea defect in the defect tracking databaseand may proceed with the fix. This process is cumbersome, and it interrupts thedebugging process, but it is useful for very critical projects. However, programmersoften do not follow this because of a lack of a procedure to enforce it.A Debugging H\u00aeeuristic The objective of debugging is to precisely identify thecause of a failure. Once the cause is identified, corrective measures are taken to fix the fault. Debugging is conducted by programmers, preferably by those whowrote the code, because the programmer is the best person to know the source codewell enough to analyze the\u00aecode efficiently and effectively. Debugging is usuallya time consuming and error-prone process, which is generally performed understress. Debugging involves a combination of systematic evaluation, intuition, and,sometimes, a little bit of luck. Given a symptom of a problem, the purpose is toisolate and determine its specific cause. The following heuristic may be followedto isolate and correct it",
  "page391": "Reproduce the symptom(s). Read the troubleshooting guide of the product. This guide may includeconditions and logs, produced by normal code, or diagnostics codespecifically written\u00aefor troubleshooting purpose that can be turned on. Try to reproduce the symptoms with diagnostics code turned on.Gather all the information and conduct causal analysis The goal ofcausal analysis is to identify the root\u00aecause of the problem and initiateactions so that the source of defects is eliminated.Step 2: Formulate some likely hypotheses for the cause of the problem based onthe causal analysis.Step 3: Develop a t\u00aeest scenario for each hypothesis to be proved or disproved.This is done by designing test cases to provide unambiguous resultsrelated to a hypothesis. The test cases may be static (reviewing code anddocumentation) and/or dynamic in nature. Preferably, the test cases arenondestructive, hav\u00aee low cost, and need minimum additional hardwareneeds. A test case is said to be destructive if it destroys the hardwaresetup. For example, cutting a cable during testing is called destructivetesting.Step 4: Prioritize the execution of test cases. Test cases corresponding to thehighly probable hypotheses are executed first. Also,\u00aethe cost factor cannotbe overlooked. Therefore, it is desirable to execute the low-cost test casesfirst followed by the more expensive ones. The programmer needs toconsider both factors.Step 5: Execute the test cases in order to find the cause of a symptom. Afterexecuting a test case, examine the result for new evidence. If the testresult shows that a particular hypothesis is promising, test data are refinedin an attempt to isolate the defect. If necessary, go back to earlier stepsor eliminate a particular hypothesis.",
  "page392": "any side effects (collateral damage) due to the changes effected in themodule. After a possible code review, apply the fix. Retest the unit to confirm that the actual c\u00aeause of failure had beenfound. The unit is properly debugged and fixed if tests show that theobserved failure does not occur any more. If there are no dynamic unit test cases that reveal the problem, thenadd a new\u00aetest case to the dynamic unit testing to detect possiblereoccurrences or other similar problems. For the unit under consideration, identify all the test cases that havepassed. Now, perform a regre\u00aession test on the unit with those testcases to ensure that new errors have not been introduced. That is whyit is so important to have archived all the test cases that have beendesigned for a unit. Thus, even unit-level test cases must be managedin a systematic manner to reduce the cost of\u00aesoftware development.Step 7: Document the changes which have been made. Once a defect is fixed,the following changes are required to be applied: Document the changes in the source code itself to reflect the change. Update the overall system documentation. Changes to the dynamic unit test cases. File a defe\u00aect in the defect tracking database if the problem was foundafter the code was checked in to the version control system.",
  "page393": "A TDD approach to code development is used in the XP methodology [22, 23]. Thekey aspect of the TDD approach is that a programmer writes low-level tests beforewriting production co\u00aede. This is referred to as test first [24] in software development. Writing test-driven units is an important concept in the XP methodology. InXP, a few unit tests are coded first, then a simple, partial system is imple\u00aemented topass the tests. Then, one more new unit test is created, and additional code is written to pass the new test, but not more, until a new unit test is created. The processis continued until nothin\u00aeg is left to test. The process is illustrated in Figure 3.3 andoutlined below:Step 1: Pick a requirement, that is, a story.Step 2: Write a test case that will verify a small part of the story and assign afail verdict to it.Step 3: Write the code that implements a particular part of the st\u00aeory to pass thetest.Step 4: Execute all tests Step 5: Rework the code, and test the code until all tests pass.Step 6: Repeat steps 2-5 until the story is fully implemented.The simple cycle in Figure 3.3 shows that, at the beginning of each cycle,the intention is for all tests to pass except the newly added test case. The new\u00aetestcase is introduced to drive the new code development. At the end of the cycle, theprogrammer executes all the unit tests, ensuring that each one passes and, hence,the planned task of the code still works",
  "page394": "A TDD approach to code development is used in the XP methodology [22, 23]. Thekey aspect of the TDD approach is that a programmer writes low-level tests beforewriting production co\u00aede. This is referred to as test first [24] in software development. Writing test-driven units is an important concept in the XP methodology. InXP, a few unit tests are coded first, then a simple, partial system is imple\u00aemented topass the tests. Then, one more new unit test is created, and additional code is written to pass the new test, but not more, until a new unit test is created. The processis continued until nothin\u00aeg is left to test. The process is illustrated in Figure 3.3 andoutlined below:Step 1: Pick a requirement, that is, a story.Step 2: Write a test case that will verify a small part of the story and assign afail verdict to it.Step 3: Write the code that implements a particular part of the st\u00aeory to pass thetest.Step 4: Execute all tests Step 5: Rework the code, and test the code until all tests pass.Step 6: Repeat steps 2-5 until the story is fully implemented.The simple cycle in Figure 3.3 shows that, at the beginning of each cycle,the intention is for all tests to pass except the newly added test case. The new\u00aetestcase is introduced to drive the new code development. At the end of the cycle, theprogrammer executes all the unit tests, ensuring that each one passes and, hence,the planned task of the code still works",
  "page395": " One may not write production code unless the first failing unit test iswritten. One may not write more of a unit test than is sufficient to fail. One may not write more production\u00aecode than is sufficient to make thefailing unit test pass.These three laws ensure that one must write a portion of a unit test that failsand then write just enough production code to make that unit test pass. The goalo\u00aef these three laws is not to follow them strictly it is to decrease the intervalbetween writing unit tests and production code.Creating unit tests helps a developer focus on what needs to be done. Requir\u00aeements, that is, user stories, are nailed down firmly by unit tests. Unit tests arereleased into the code repository along with the code they test. Code without unittests may not be released. If a unit test is discovered to be missing, it must be created immediately. Creating unit tests i\u00aendependently before coding sets up checksand balances and improves the chances of getting the system right the first time.Unit tests provide a safety net of regression tests and validation tests so that XPprogrammers can refactor and integrate effectively.In XP, the code is being developed by two programmers working together sideb\u00aey side. The concept is called pair programming. The two programmers sit side byside in front of the monitor. One person develops the code tactically and the otherone inspects it methodically by keeping in mind the story they are implementing.It is similar to the two-person inspection strategy proposed by Bisant and Lyle",
  "page396": "The JUnit is a unit testing framework for the Java programming language designedby Kent Beck and Erich Gamma. Experience gained with JUnit has motivated thedevelopment of the TDD [\u00ae22] methodology. The idea in the JUnit framework hasbeen ported to other languages, including C# (NUnit), Python (PyUnit), Fortran(fUnit) and C  (CPPUnit). This family of unit testing frameworks is collectivelyreferred\u00aeto as xUnit. This section will introduce the fundamental concepts of JUnitto the reader.Suppose that we want to test the individual methods of a class called PlanetClass. Let Move() be a method in Plane\u00aetClass such that Move() accepts only oneinput parameter of type integer and returns a value of type integer. One can followthe following steps, illustrated using pseudocode in Figure 3.4, to test Move(): Create an object instance of Planet lass. Let us call the instance Mars.Now we are in\u00aeterested in testing the method Move() by invoking it onobject Mars. Select a value for all the input parameters of Move() this function hasjust one input parameter. Let us represent the input value to Move() by x. Know the expected value to be returned by Move(). Let the expectedreturned value be y Invoke method Move() on o\u00aebject Mars with input value x. Let z denotethe value returned by Move().Now compare y with z. If the two values are identical, then the methodMove() in object Mars passes the test. Otherwise, the test is said to havefailed",
  "page397": "In a nutshell, the five steps of unit testing are as follows:Create an object and select a method to execute. Select values for the input parameters of the method. Compute the expe\u00aected values to be returned by the method. Execute the selected method on the created object using the selected inputvalues. Verify the result of executing the method.Performing unit testing leads to a programmer consumi\u00aeng some resources,especially time. Therefore, it is useful to employ a general programming frameworkto code individual test cases, organize a set of test cases as a test suite, initialize atest environme\u00aent, execute the test suite, clean up the test environment, and recordthe result of execution of individual test cases. In the example shown in Figure 3.4,creating the object Mars is a part of the initialization process. The two print()statements are examples of recording the result of tes\u00aet execution. Alternatively,one can write the result of test execution to a file. The JUnit framework has been developed to make test writing simple. Theframework provides a basic class, called TestCase, to write test cases. Programmersneed to extend the TestCase class to write a set of individual test cases. It may benoted that to\u00aewrite, for example, 10 test cases, one need not write 10 subclasses ofthe class Testcase. Rather, one subclass, say Testcase, of Testcase, can contain10 methods one for each test case. Programmers need to make assertions about the state of objects while extending the Testcase class to write test cases. For example, in each test case it isrequired to compare the actual outcome of a computation with the expected outcome.",
  "page398": " Though an if() statement can be used to compare the equality of two valuesor two objects, it is seen to be more elegant to write an assert statement to achievethe same. The class\u00aeTestcase extends a utility class called Assert in the JUnitframework. Essentially, the Assert class provides methods, as explained in the following, to make assertions about the state of objects created and manipulated\u00aewhiletesting.assert True(Boolean condition): This assertion passes if the condition is true;otherwise, it fails.assert Equals(Object expected, Object actual): This assertion passes if theexpected and the\u00aeactual objects are equal according to the equals() method;otherwise, the assertion fails.assert Equals(int expected, int actual): This assertion passes if expected andactual are equal according to the = operator; otherwise, the assertion fails. For each primitive type int, float, double\u00ae, char, byte, long, short, andBoolean, the assertion has an overloaded version.assert Equals(double expected, double actual, double tolerance): This assertion passes if the absolute value of the difference between expected andactual is less than or equal to the tolerance value; otherwise, the assertionfails. The assertion has an o\u00aeverloaded version for float inputs.assert Same(Object expected, Object actual): This assertion passes if theexpected and actual values refer to the same object in memory; otherwise,the assertion fails assert Null(Object testobject): This assertion passes if testobject is null; otherwise the assertion fails.assert False(Boolean condition): This is the logical opposite of assert True()",
  "page399": "The reader may note that the above list of assertions is not exhaustive. Infact, one can build other assertions while extending the TestCase class. When anassertion fails, a progra\u00aemmer may want to know immediately the nature of thefailure. This can be done by displaying a message when the assertion fails. Eachassertion method listed above accepts an optional first parameter of type String ifthe a\u00aessertion fails, then the String value is displayed. This facilitates the programmerto display a desired message when the assertion fails. As an aside, upon failure,the assert Equals() method displays a c\u00aeustomized message showing the expectedvalue and the actual value At this point it is interesting to note that only failed tests are reported. Failedtests can be reported by various means, such as displaying a message, displaying anidentifier for the test case, and counting the total numbe\u00aer of failed test cases. Essentially, an assertion method throws an exception, called AssertionFailedError, whenthe assertion fails, and JUnit catches the exception. The code shown in Figure 3.5illustrates how the assert True() assertion works: When the JUnit framework catchesan exception, it records the fact that the assertion fai\u00aeled and proceeds to the nexttest case. Having executed all the test cases, JUnit produces a list of all those teststhat have failed. MyTestSuite and invoke the two methods MyTest1() and MyTest2(). Whether ornot the two methods, namely Method1() and Method()2, are to be invoked on twodifferent instances of the class TestMe depends on the individual objectives ofthose two test cases. In other words, it is the programmer who decides whether ornot two instances of the class TestMe are to be created",
  "page400": "Programmers can benefit from using tools in unit testing by reducing testing timewithout sacrificing thoroughness. The well-known tools in everyday life are aneditor, a compiler, a\u00aen operating system, and a debugger. However, in some cases, the real execution environment of a unit may not be available to a programmerwhile the code is being developed. In such cases, an emulator of the environmentis\u00aeuseful in testing and debugging the code. Other kinds of tools that facilitateeffective unit testing are as follows:1. Code Auditor: This tool is used to check the quality of software to ensurethat it m\u00aeeets some minimum coding standards. It detects violations of programming, naming, and style guidelines. It can identify portions of code that cannotbe ported between different operating systems and processors. Moreover, it cansuggest improvements to the structure and style of the source c\u00aeode. In addition, itcounts the number of LOC which can be used to measure productivity, that is, LOCproduced per unit time, and calculate defect density, that is, number of defects perKLOC.2. Bound Checker: This tool can check for accidental writes into the instruction areas of memory or to any other memory location outside the da\u00aeta storage areaof the application. This fills unused memory space with a signature pattern (distinct binary pattern) as a way of determining at a later time whether any of thismemory space has been overwritten. The tool can issue diagnostic messages whenboundary violations on data items occur. It can detect violation of the boundaries of array, for example, when the array index or pointer is outside its allowedrange. ",
  "page401": "Documenters: These tools read source code and automatically generatedescriptions and caller/called tree diagram or data model from the source code. Interactive Debuggers: These too\u00aels assist software developers in implementing different debugging approaches discussed in this chapter. These toolsshould have the trace-back and breakpoint capabilities to enable the programmers tounderstand the dynami\u00aecs of program execution and to identify problem areas in thecode. Breakpoint debuggers are based on deductive logic. Breakpoints are placedaccording to a heuristic analysis of code [32]. Another popular\u00aekind of debuggeris known as omniscient debugger (ODB), in which there is no deduction. It simplyfollows the trail of \"bad\" values back to their source no \"guessing\" where to putthe breakpoints. An ODB is like \"the snake in the grass,\" that is, if you see a sn\u00aeakein the grass and you pull its tail, sooner or later you get to its head. In contrast,breakpoint debuggers suffer from the \"lizard in the grass\" problem, that is, whenyou see the lizard and grab its tail, the lizard breaks off its tail and gets away [33].n-Circuit Emulators: An in-circuit emulator, commonly known as IC\u00aeE,is an invaluable software development tool in embedded system design. It providesa high-speed Ethernet connection between a host debugger and a target microprocessor, enabling developers to perform common source-level debugging activities,such as watching memory and controlling large numbers of registers, in a matterof seconds. It is vital for board bring-up, solving complex problems, and manufacturing or testing of products. Many emulators have advanced features, such as ",
  "page402": "performance analysis, coverage analysis, buffering of traces, and advance triggerand breakpoint possibilities.6. Memory Leak Detectors: These tools test the allocation of memory to\u00aeanapplication which requests for memory, but fails to deallocate. These detect thefollowing overflow problems in application programs:Illegal read, that is, accesses to memory which is not allocated to theapplication o\u00aer which the application is not authorized to access. Reads memory which has not been initialized. Dynamic memory overwrites to a memory location that has not been allocated to the application. Reading fr\u00aeom a memory location not allocated, or not initialized, prior tothe read operation.The tools watch the heap, keep track of heap allocations to applications, anddetect memory leaks. The tools also build profiles of memory use, for example,which line-of-code source instruction accesses a pa\u00aerticular memory address.7. Static Code (Path) Analyzer: These tools identify paths to test, basedon the structure of the code such as McCabe's cyclometric complexity measure(Table 3.3). Such tools are dependent on source language and require the sourcecode to be recompiled with the tool. These tools can be used to improve pro\u00aeductivity, resource management, quality, and predictability by providing complexitymeasurement metrics.8. Software Inspection Support: Tools can help schedule group inspections.These can also provide status of items reviewed and follow-up actions and distributethe reports of problem resolution. They can be integrated with other tools, such asstatic code analyzers Test Coverage Analyzer: These tools measure internal test coverage, oftenexpressed in terms of the control structure of the test object, and report the coverage metric. Coverage analyzers track and report what paths were exercised duringdynamic unit testing. ",
  "page403": "Test coverage analyzers are powerful tools that increase confidence in product quality by assuring that tests cover all of the structural parts ofa unit or a program. An important\u00aeaspect in test coverage analysis is to identifyparts of source code that were never touched by any dynamic unit test. Feedbackfrom the coverage reports to the source code makes it easier to design new unittest cases to\u00aecover the specific untested paths. Test Data Generator: These tools assist programmers in selecting test datathat cause a program to behave in a desired manner. Test data generators can offerseveral capa\u00aebilities beyond the basics of data generation:They have generate a large number of variations of a desired data set basedon a description of the characteristics which has been fed into the tool. They can generate test input data from source code. They can generate equivalence classes and\u00aevalues close to the boundaries. They can calculate the desired extent of boundary value testing. They can estimate the likelihood of the test data being able to reveal faults. They can generate data to assist in mutation analysis.Automatic generation of test inputs is an active area of research. Several tools,such as CUTE [34], DA\u00aeRT [35], and EGT system [36], have been developed byresearchers to improve test coverage. Test Harness: This class of tools supports the execution of dynamic unittests by making it almost painless to (i) install the unit under test in a test environment, (ii) drive the unit under test with input data in the expected input format, (iii)generate stubs to emulate the behavior of subordinate modules, and (iv) capturethe actual outcome as generated by the unit under test and log or display it in ausable form. ",
  "page404": "Advanced tools may compare the expected outcome with the actualoutcome and log a test verdict for each input test data.Performance Monitors: The timing characteristics of software\u00aecomponentscan be monitored and evaluated by these tools. These tools are essential for anyreal-time system in order to evaluate the performance characteristics of the system,such as delay and throughput. For example, in\u00aetelecommunication systems, thesetools can be used to calculate the end-to-end delay of a telephone call. Network Analyzers: Network operating systems such as software that runon routers, switches, and c\u00aelient/server systems are tested by network analyzers.These tools have the ability to analyze the traffic and identify problem areas.Many of these networking tools allow test engineers to monitor performance metrics and diagnose performance problems across the networks. These tools areenha\u00aenced to improve the network security monitoring (NSM) capabilities to detectintrusion Simulators and Emulators: These tools are used to replace the real software and hardware that are currently not available. Both kinds of tools are usedfor training, safety, and economy reasons. Some examples are flight simulators,terminal emulato\u00aers, and emulators for base transceiver stations in cellular mobilenetworks. These tools are bundled with traffic generators and performance analyzersin order to generate a large volume of input data Traffic Generators: Large volumes of data needed to stress the interfacesand the integrated system are generated by traffic generators. These produce streamsof transactions or data packets. For example, in testing routers, one needs a trafficthat simulates streams of varying size Internet Protocol (IP) packets arriving fromdifferent sources. These tools can set parameters for mean packet arrival rate,duration, and packet size. Operational profiles can be used to generate traffic forload and stability testing.",
  "page405": "Version Control: A version control system provides functionalities to storea sequence of revisions of the software and associated information files underdevelopment. A system relea\u00aese is a collection of the associated files from a version control tool perspective. These files may contain source code, compiled code,documentation, and environment information, such as version of the tool used towrite\u00aethe software. The objective of version control is to ensure a systematic andtraceable software development process in which all changes are precisely managed, so that a software system is always in a we\u00aell-defined state. With most of theversion control tools, the repository is a central place that holds the master copyof all the files.The configuration management system (CMS) extends the version control fromsoftware and documentation to control the changes made to hardware, firmware,soft\u00aeware, documentation, test, test fixtures, test documentation, and execution environments throughout the development and operational life of a system. Therefore,configuration management tools are larger, better variations of version control tools.The characteristics of the version control and configuration management tools areas fo\u00aellows: Access Control: The tools monitor and control access to components.One can specify which users can access a component or group of components. One can also restrict access to components currently undergoingmodification or testing.Cross Referencing: The tools can maintain linkages among related components, such as problem reports, components, fixes, and documentations. One can merge files and coordinate multiple updates from different versionsto produce one consolidated file.",
  "page406": "Tracking of Modifications: The tools maintain records of all modifications to components. These also allow merging of files and coordinatemultiple updates from different versions t\u00aeo produce one consolidated file.These can track similarities and differences among versions of code, documentation, and test libraries. They also provide an audit trail or history ofthe changes from version to version.R\u00aeelease Generation: The tools can automatically build new systemreleases and insulate the development, test, and shipped versions of theproduct. System Version Management: The tools allow sharing of commo\u00aen components across system versions and controlled use of system versions. Theysupport coordination of parallel development, maintenance, and integrationof multiple components among several programmers or project teams. Theyalso coordinate geographically dispersed development and test tea\u00aems. Archiving: The tools support automatic archiving of retired componentsand system versions. This chapter began with a description of unit-level testing, which means identifyingfaults in a program unit analyzed and executed in isolation. Two complementarytypes of unit testing were introduced: static unit testing and dynamic unit\u00aetesting. Static unit testing involves visual inspection and analysis of code, whereas aprogram unit is executed in a controlled manner in dynamic unit testing.Next, we described a code review process, which comprises six steps: readiness, preparation, examination, rework, validation, and exit. The goal of codereview is to assess the quality of the software in question, not the quality of theprocess used to develop the product. We discussed a few basic metrics that canbe collected from the code review process. Those metrics facilitate estimation ofreview time and resources required for similar projects. Also, the metrics makecode review visible to the upper management and allow upper management to besatisfied with the viability of code review as a testing tool ",
  "page407": "We explained several preventive measures that can be taken during codedevelopment to reduce the number of faults in a program. The preventive measures were presented in the form of\u00aea set of guidelines that programmers canfollow to construct code. Essentially, the guidelines focus on incorporating suitablemechanisms into the code.Next, we studied dynamic unit testing in detail. In dynamic unit tes\u00aeting, aprogram unit is actually executed, and the outcomes of program execution areobserved. The concepts of test driver and stubs were explained in the contextof a unit under test. A test driver is a ca\u00aeller of the unit under test and all the \"dummy modules\" called by the unit are known as stubs. We described how mutation analysis can be used to locate weaknesses in test data used for unit testing.Mutation analysis should be used in conjunction with traditional unit testing tec\u00aehniques such as domain analysis or data flow analysis. That is, mutation testing isnot an alternative to domain testing or data flow analysis.With the unit test model in place to reveal defects, we examined how programmers can locate faults by debugging a unit. Debugging occurs as a consequenceof a test revealing a defect. We disc\u00aeussed three approaches to debugging: bruteforce, cause elimination, and backtracking. The objective of debugging is to precisely identify the cause of a failure. Given the symptom of a problem, the purposeis to isolate and determine its specific cause. We explained a heuristic to performprogram debugging. Next, we explained dynamic unit testing is an integral part of the XP softwaredevelopment process. In the XP process, unit tests are created prior to coding thisis known as test first.",
  "page408": "The test-first approach sets up checks and balances to improvethe chances of getting things right the first time. We then introduced the JUnitframework, which is used to create and\u00aeexecute dynamic unit tests.We concluded the chapter with a description of several tools that can be useful in improving the effectiveness of unit testing. These tools are of the followingtypes: code auditor, bound chec\u00aeker, documenters, interactive debuggers, in-circuitemulators, memory leak detectors, static code analyzers, tools for software inspection support, test coverage analyzers, test data generators, tools for\u00aecreating testharness, performance monitors, network analyzers, simulators and emulators, trafficgenerators, and tools for version control.The Institute of Electrical and Electronics Engineers (IEEE) standard 1028-1988(IEEE Standard for Software Reviews and Audits: IEEE/ANSI Standard) des\u00aecribesthe detailed examination process for a technical review, an inspection, a softwarewalkthrough, and an audit. For each of the examination processes, it includes anobjective, an abstract, special responsibilities, program input, entry criteria, procedures, exit criteria, output, and auditability.Several improvements on Fagan\u00ae2019s inspection techniques have been proposedby researchers during the past three decades. Those proposals suggest ways toenhance the effectiveness of the review process or to fit specific applicationdomains. A number of excellent articles address various issues related to softwareinspection as follows Biffl, and M. Halling, \"Investigating the Defect Effectiveness and CostBenefit of Nominal Inspection Teams,\" IEEE Transactions on SoftwareEngineering, Vol. 29, No. 5, May 2003, pp. 385-397.A. A. Porter and P. M. Johnson, \"Assessing Software Review Meeting:Results of a Comparative Analysis of Two Experimental Studies,\" IEEE",
  "page409": "Transactions on Software Engineering, Vol. 23, No. 3, March 1997, pp.129-145.A. A. Porter, H. P. Say, C. A. Toman, and L. G. Votta, \"An Experimentto Assess the Cost-Benef\u00aeits of Code Inspection in Large Scale SoftwareDevelopment,\" IEEE Transactions on Software Engineering, Vol. 23, No.6, June 1997, pp. 329-346.A. A. Porter and L. G. Votta, \"What Makes Inspection Work,\u00aeIEEE Software,Vol. 14, No. 5, May 1997, pp. 99-102.C. Sauer, D. Jeffery, L. Land, and P. Yetton, \"The Effectiveness of Software Development Technical Reviews: A Behaviorally Motivated Programof Sear\u00aech,\" IEEE Transactions on Software Engineering, Vol. 26, No. 1,January 2000, pp. 1-14.An alternative non-execution-based technique is formal verification of code.Formal verification consists of mathematical proofs to show that a program iscorrect. The two most prominent methods\u00aefor proving program properties are thoseof Dijkstra and Hoare:E. W. Dijkstra, A Discipline of Programming, Prentice-Hall, EnglewoodCliffs, NJ, 1976.C. A. R. Hoare, \"An Axiomatic Basis of Computer Programming,\" Communications of the ACM , Vol. 12, No. 10, October 1969, pp. 576-580. Hoare presented an axiomatic approa\u00aech in which properties of program fragmentsare described using preconditions and postconditions. An example statement witha precondition and a postcondition is {PRE} P {POST}, where PRE is the precondition, POST is the postcondition, and P is the program fragment. Both PREand POST are expressed in first-order predicate calculus, which means that theycan include the universal quantifier \"for all\" or \"for every\" (\"for all\") and existential quantifier \"there exists\" or \"for some\" (\"thereexists\"). The interpretation of the above statement is that if the program fragmentP starts executing in a state satisfying PRE, then if P terminates, P will do so in astate satisfying POST ",
  "page410": "Hoare's logic led to Dijkstra's closely related \"calculus of programs,\" whichis based on the idea of weakest preconditions. The weakest preconditions R withresp\u00aeect to a program fragment P and a postcondition POST is the set of all statesthat, when subject to P, will terminate and leave the state of computation in POST.The weakest precondition is written as WP(P, POST).While mu\u00aetation testing systematically implants faults in programs by applyingsyntactic transformations, perturbation testing is performed to test a program'srobustness by changing the values of program data\u00aeduring run time, so that thesubsequent execution will either fail or succeed. Program perturbation is based onthree parts of software hypothesis as explained in the following: Execution: A fault must be executed. Infection: The fault must change the data state of the computation directly\u00aeafter the fault location. Propagation: The erroneous data state must propagate to an output variable.In the perturbation technique, the programmer injects faults in the data stateof an executing program and traces the injected faults on the program's output.A fault injection is performed by applying a perturbation function t\u00aehat changesthe program's data state. A perturbation function is a mathematical function thattakes a data state as its input, changes the data state according to some specifiedcriteria, and produces a modified data state as output. For the interested readers,two excellent references on perturbation testing are as follow",
  "page411": "M. A. Friedman and J. M. Voas, Software Assessment Reliability, Safety,Testability, Wiley, New York, 1995.J. M. Voas and G. McGraw, Software Fault Injection Inoculating ProgramsAga\u00aeinst Errors, Wiley, New York, 1998.The paper by Steven J. Zeil (\"Testing for Perturbation of Program Statement,\" IEEE Transactions on Software Engineering, Vol. 9, No. 3, May 1983,pp. 335-346) describes a\u00aemethod for deducing sufficient path coverage to ensurethe absence of prescribed errors in a program. It models the program computationand potential errors as a vector space. This enables the conditions\u00aefor nondetectionof an error to be calculated. The above article is an advanced reading for studentswho are interested in perturbation analysis.Those readers actively involved in software configuration management(SCM) systems or interested in a more sophisticated treatment of the topic mus\u00aetread the article by Jacky Estublier, David Leblang, Andre V. Hoek, Reidar \u00b4Conradi, Geoffrey Clemm, Walter Tichy, and Darcy Wiborg-Weber (\"Impactof Software Engineering Research on the Practice of Software ConfigurationManagement,\" ACM Transactions on Software Engineering and Methodology,Vol. 14, No. 4, October 200\u00ae5, pp. 383-430). The authors discussed the evolutionof software configuration management technology, with a particular emphasis onthe impact that university and industrial research has had along the way. Thisarticle creates a detailed record of the critical value of software configurationmanagement research and illustrates the research results that have shaped thefunctionality of SCM systems",
  "page412": "Two kinds of basic statements in a program unit are assignment statements andconditional statements. An assignment statement is explicitly represented by usingan assignment symbol,\u00aesuch as x = 2*y;, where x and y are variables.Program conditions are at the core of conditional statements, such as if(), for()loop, while() loop, and goto. As an example, in if(x! = y), we are testing for theinequalit\u00aey of x and y. In the absence of conditional statements, program instructionsare executed in the sequence they appear. The idea of successive execution ofinstructions gives rise to the concept of control\u00aeflow in a program unit. Conditionalstatements alter the default, sequential control flow in a program unit. In fact,even a small number of conditional statements can lead to a complex control flowstructure in a program.Function calls are a mechanism to provide abstraction in program desig\u00aen.A call to a program function leads to control entering the called function. Similarly,when the called function executes its return statement, we say that control exitsfrom the function. Though a function can have many return statements, for simplicity, one can restructure the function to have exactly one return. A program unit c\u00aeanbe viewed as having a well-defined entry point and a well-defined exit point. Theexecution of a sequence of instructions from the entry point to the exit point of aprogram unit is called a program path. There can be a large, even infinite, numberof paths in a program unit. Each program path can be characterized by an inputand an expected output. A specific input value causes a specific program path to beexecuted; it is expected that the program path performs the desired computation,thereby producing the expected output value. ",
  "page413": "The overall idea of generating test input data for performing control flow testinghas been depicted in Figure 4.1. The activities performed, the intermediate resultsproduced by tho\u00aese activities, and programmer preferences in the test generationprocess are explained below.Inputs: The source code of a program unit and a set of path selection criteriaare the inputs to a process for generating test d\u00aeata. In the following, twoexamples of path selection criteria are given.Example. Select paths such that every statement is executed at least once.Example. Select paths such that every conditional stateme\u00aent, forexample, an if() statement, evaluates to true and false at least once ondifferent occasions. A conditional statement may evaluate to true in onepath and false in a second path.Generation of a Control Flow Graph: A control flow graph (CFG) is adetailed graphical representation of a\u00aeprogram unit. The idea behind drawing a CFG is to be able to visualize all the paths in a program unit. Theprocess of drawing a CFG from a program unit will be explained in thefollowing section. If the process of test generation is automated, a compilercan be modified to produce a CFG. Selection of Paths: Paths are selected from t\u00aehe CFG to satisfy the path selection criteria, and it is done by considering the structure of the CFG.Generation of Test Input Data: A path can be executed if and only if acertain instance of the inputs to the program unit causes all the conditionalstatements along the path to evaluate to true or false as dictated by thecontrol flow. Such a path is called a feasible path. Otherwise, the path issaid to be infeasible.",
  "page414": "It is essential to identify certain values of the inputsfrom a given path for the path to execute.Feasibility Test of a Path: The idea behind checking the feasibility of aselected\u00aepath is to meet the path selection criteria. If some chosen pathsare found to be infeasible, then new paths are selected to meet the criteria A CFG is a graphical representation of a program unit. Three symbols are used\u00aeto construct a CFG, as shown in Figure 4.2. A rectangle represents a sequential computation. A maximal sequential computation can be represented either by asingle rectangle or by many rectangles, each co\u00aerresponding to one statement in thesource code.We label each computation and decision box with a unique integer. The twobranches of a decision box are labeled with T and F to represent the true and falseevaluations, respectively, of the condition within the box. We will not label a mergen\u00aeode, because one can easily identify the paths in a CFG even without explicitlyconsidering the merge nodes. Moreover, not mentioning the merge nodes in a pathwill make a path description shorter.We consider the open files() function shown in Figure 4.3 to illustrate theprocess of drawing a CFG. The function has three statements: a\u00aen assignment statement int i 0;, a conditional statement if(), and a return(i) statement. The readermay note that irrespective of the evaluation of the if(), the function performs thesame action, namely, null. In Figure 4.4, we show a high-level representation of the control flow in openfiles() with three nodes numbered 1, 2, and 3. The flowgraph shows just two paths in open files().",
  "page415": "A closer examination of the condition part of the if() statement reveals thatthere are not only Boolean and relational operators in the condition part, but alsoassignment statement\u00aes. Some of their examples are given below:Assignment statements: fptr1 fopen(\"file1\", \"r\") and i  Relational operator: fptr1! = NULLBoolean operators: Execution of the assignment statements in the\u00aecondition part of the if statementdepends upon the component conditions. For example, consider the following component condition in the if part:((( fptr1 fopen(\"file1\", \"r\")) != NULL) nThe above con\u00aedition is executed as follows: Execute the assignment statement fptr1 fopen(\"file1\", \"r\"). Execute the relational operation fptr1! = NULL. If the above relational operator evaluates to false, skip the evaluation ofthe subsequent condition components (i ) They can gen\u00aeerate equivalence classes and values close to the boundaries. They can calculate the desired extent of boundary value testing. They can estimate the likelihood of the test data being able to reveal faults. They can generate data to assist in mutation analysis.Automatic generation of test inputs is an active area of research. Sever\u00aeal tools,such as CUTE [34], DART [35], and EGT system [36], have been developed byresearchers to improve test coverage. Test Harness: This class of tools supports the execution of dynamic unittests by making it almost painless to (i) install the unit under test in a test environment, (ii) drive the unit under test with input data in the expected input format, (iii)generate stubs to emulate the behavior of subordinate modules, and (iv) capturethe actual outcome as generated by the unit under test and log or display it in ausable form.",
  "page416": "A CFG, such as the one shown in Figure 4.7, can have a large number of differentpaths. One may be tempted to test the execution of each and every path in a programunit. For a progr\u00aeam unit with a small number of paths, executing all the paths may be desirable and achievable as well. On the other hand, for a program unit with alarge number of paths, executing every distinct path may not be practica\u00ael. Thus,it is more productive for programmers to select a small number of program pathsin an effort to reveal defects in the code. Given the set of all paths, one is facedwith a question \"What paths\u00aedo I select for testing?\" The concept of path selectioncriteria is useful is answering the above question. In the following, we state theadvantages of selecting paths based on defined criteria All program constructs are exercised at least once. The programmer needsto observe the out\u00aecome of executing each program construct, for example,statements, Boolean conditions, and returns. We do not generate test inputs which execute the same path repeatedly.Executing the same path several times is a waste of resources. However,if each execution of a program path potentially updates the state of thesystem, for example,\u00aethe database state, then multiple executions of thesame path may not be identical.We know the program features that have been tested and those not tested.For example, we may execute an if statement only once so that it evaluatesto true. If we do not execute it once again for its false evaluation, we are,at least, aware that we have not observed the outcome of the program witha false evaluation of the if statement.",
  "page417": "Now we explain the following well-known path selection criteria: Select all paths. Select paths to achieve complete statement coverage. Select paths to achieve complete branch cove\u00aerage.Select paths to achieve predicate coverage If all the paths in a CFG are selected, then one can detect all faults, except thosedue to missing path errors. However, a program may contain a large number ofpaths, or e\u00aeven an infinite number of paths. The small, loop-free openfiles() functionshown in Figure 4.3 contains more than 25 paths. One does not know whether ornot a path is feasible at the time of selecting path\u00aes, though only eight of all thosepaths are feasible. If one selects all possible paths in a program, then we say thatthe all-path selection criterion has been satisfied.Let us consider the example of the openfiles() function. This function tries toopen the three files file1, file2, and fi\u00aele3. The function returns an integer representingthe number of files it has successfully opened. A file is said to be successfullyopened with \"read\" access if the file exists. The existence of a file is either \"yes\"or \"no.\" Thus, the input domain of the function consists of eight combinations ofthe ex\u00aeistence of the three files, as shown in Table 4.2.We can trace a path in the CFG of Figure 4.5 for each input, that is, eachrow of Table 4.2. Ideally, we identify test inputs to execute a certain path in a program; this will be explained later in this chapter. We give three examples of thepaths executed by the test inputs (Table 4.3). In this manner, we can identify eightpossible paths .The all-paths selection criterion is desirable since itcan detect faults; however, it is difficult to achieve in practice.",
  "page418": "Statement coverage refers to executing individual program statements and observing the outcome. We say that 100% statement coverage has been achieved if allthe statements have been\u00aeexecuted at least once. Complete statement coverage isthe weakest coverage criterion in program testing. Any test suite that achieves lessthan statement coverage for new software is considered to be unacceptable.All pr\u00aeogram statements are represented in some form in a CFG. Referringto the ReturnAverage() method in Figure 4.6 and its CFG in Figure 4.7, the fourassignment statementsi 0;ti = 0;tv = 0;sum = 0;have been re\u00aepresented by node 2. The while statement has been represented as aloop, where the loop control condition(ti < AS value[i] ! -999) as been represented by nodes 3 and 4. Thus, covering a statement in a programmeans visiting one or more nodes representing the statement, more precisely, selec\u00aeting a feasible entry-exit path that includes the corresponding nodes. Since a singleentry-exit path includes many nodes, we need to select just a few paths to coverall the nodes of a CFG. Therefore, the basic problem is to select a few feasiblepaths to cover all the nodes of a CFG in order to achieve the complete statem\u00aeentcoverage criterion. We follow these rules while selecting paths: Select short paths. Select paths of increasingly longer length. Unfold a loop several times ifthere is a need.Select arbitrarily long, \"complex\" paths.One can select the two paths shown in Figure 4.4 to achieve complete statementcoverage.",
  "page419": "Syntactically, a branch is an outgoing edge from a node. All the rectangle nodeshave at most one outgoing branch (edge). The exit node of a CFG does not have anoutgoing branch. All\u00aethe diamond nodes have two outgoing branches. Covering abranch means selecting a path that includes the branch. Complete branch coveragemeans selecting a number of paths such that every branch is included in at leaston\u00aee path.In a preceding discussion, we showed that one can select two paths, SCPath 1and SCPath 2 in Table 4.4, to achieve complete statement coverage. These twopaths cover all the nodes (statements) and m\u00aeost of the branches of the CFG shownin Figure 4.7. The branches which are not covered by these two paths have beenhighlighted by bold dashed lines in Figure 4.8. These uncovered branches correspond to the three independent conditions evaluating to false. This means that as a programmer w\u00aee have not observed theoutcome of the program execution as a result of the conditions evaluating to false.Thus, complete branch coverage means selecting enough number of paths such thatevery condition evaluates to true at least once and to false at least once.We need to select more paths to cover the branches highlighted by the bo\u00aelddashed lines We refer to the partial CFG of Figure 4.9a to explain the concept of predicatecoverage. OB1, OB2, OB3, and OB are four Boolean variables. The programcomputes the values of the individual variables OB1, OB2, and OB3 details oftheir computation are irrelevant to our discussion and have been omitted. Next, OBis computed as shown in the CFG. The CFG checks the value of OB and executeseither OBlock1 or OBlock2 depending on whether OB evaluates to true or false,respectively.",
  "page420": "We need to design just two test cases to achieve both statement coverageand branch coverage. We select inputs such that the four Boolean conditions inFigure 4.9a evaluate to the va\u00aelues shown in Table 4.6. The reader may note thatwe have shown just one way of forcing OB to true. If we select inputs so that thesetwo cases hold, then we do not observe the effect of the computations taking placein no\u00aedes 2 and 3. There may be faults in the computation parts of nodes 2 and 3such that OB2 and OB3 always evaluate to false . Therefore, there is a need to design test cases such that a path is executedun\u00aeder all possible conditions. The False branch of node 5 (Figure 4.9a) is executedunder exactly one condition, namely, when OB1 False, OB2 = False, and OB3 =False, whereas the true branch executes under seven conditions. If all possiblecombinations of truth values of the conditions affecti\u00aeng a selected path have beenexplored under some tests, then we say that predicate coverage has been achieved.Therefore, the path taking the true branch of node 5 in Figure 4.9a must be executedfor all seven possible combinations of truth values of OB1, OB2, and OB3 whichresult in OB = True.A similar situation holds for the partial\u00aeCFG shown in Figure 4.9b, whereAB1, AB2, AB3, and AB are Boolean variables. ",
  "page421": "In Section 4.5 we explained the concept of path selection criteria to cover certainaspects of a program with a set of paths. The program aspects we consideredwere all statements, t\u00aerue and false evaluations of each condition, and combinationsof conditions affecting execution of a path. Now, having identified a path, thequestion is how to select input values such that when the program is executedwi\u00aeth the selected inputs, the chosen paths get executed. In other words, we needto identify inputs to force the executions of the paths. In the following, we definea few terms and give an example of genera\u00aeting test inputs for a selected path.1. Input Vector: An input vector is a collection of all data entities read bythe routine whose values must be fixed prior to entering the routine. Members ofan input vector of a routine can take different forms as listed below:Input arguments to a rout\u00aeine Global variables and constants Files Contents of registers in assembly language programming Network connections TimersA file is a complex input element. In one case, mere existence of a file can beconsidered as an input, whereas in another case, contents of the file are considered to be inputs. Thus, the idea of an input vecto\u00aer is more general than the concept ofinput arguments of a function.Example. An input vector for openfiles() (Figure 4.3) consists of individual presence or absence of the files file1, file2, and file3.Example. The input vector of the ReturnAverage() method shown in Figure 4.6is < value [], AS, MIN, MAX > .2. Predicate: A predicate is a logical function evaluated at a decision point.Example. The construct ti < AS is the predicate in decision node 3 of Figure 4.7.",
  "page422": "he construct OB is the predicate in decision node 5 of Figure 4.9.3. Path Predicate: A path predicate is the set of predicates associated witha path.The path in Figure 4.10 indicat\u00aees that nodes 3, 4, 6, 7, and 10 are decision nodes. The predicate associated with node 3 appears twice in the path; inthe first instance it evaluates to true and in the second instance it evaluates tofalse. The path pr\u00aeedicate associated with the path under consideration is shown inFigure 4.11.We also specify the intended evaluation of the component predicates as foundin the path specification. For instance, we specify\u00aethat value[i] ! 999 mustevaluate to true in the path predicate shown in Figure 4.11. We keep this additionalinformation for the following two reasons:In the absence of this additional information denoting the intended evaluation of a predicate, we will have no way to distinguish between\u00aethe twoinstances of the predicate ti < AS, namely 3(T) and 3(F), associated withnode 3.We must know whether the individual component predicates of a pathpredicate evaluate to true or false in order to generate path forcing inputs.4. Predicate Interpretation: The path predicate shown in Figure 4.11 is composed of elements of the in\u00aeput vector < value[], AS, MIN, MAX >, a vector oflocal variables < i, ti, tv >, and the constant \u2212999. The local variables are notvisible outside a function but are used to hold intermediate results, point to array elements, and control loop iterations. ",
  "page423": "In other words, they play no roles in selecting inputs that force the paths to execute.Therefore, we can easily substitute all the local variables in a predicate with theelements o\u00aef the input vector by using the idea of symbolic substitution. Let usconsider the method shown in Figure 4.12. The input vector for the method inFigure 4.12 is given by < x1, x2 > . The method defines a local variable y\u00aeand alsouses the constants 7 and 0.The predicatex1  y > 0can be rewritten asx1  x2  7 >= 0by symbolically substituting y with x 2  7. The rewritten predicatex1  x2  7 >= 0has been expressed solely\u00aein terms of the input vector < x1,x2 > and the constantvector < 0,7 > . Thus, predicate interpretation is defined as the process of symbolically substituting operations along a path in order to express the predicates solelyin terms of the input vector and a constant vector.In a CFG, ther\u00aee may be several different paths leading up to a decision pointfrom the initial node, with each path doing different computations. Therefore, apredicate may have different interpretations depending on how control reaches thepredicate under consideration. Path Predicate Expression: An interpreted path predicate is called a pathpred\u00aeicate expression. A path predicate expression has the following properties:It is void of local variables and is solely composed of elements of the inputvector and possibly a vector of constants.It is a set of constraints constructed from the elements of the input vectorand possibly a vector of constants.Path forcing input values can be generated by solving the set of constraintsin a path predicate expression.",
  "page424": "If the set of constraints cannot be solved, there exist no input which cancause the selected path to execute. In other words, the selected path is saidto be infeasible. An infeasib\u00aele path does not imply that one or more components of a pathpredicate expression are unsatisfiable. It simply means that the total combination of all the components in a path predicate expression is unsatisfiable.Infeas\u00aeibility of a path predicate expression suggests that one considers otherpaths in an effort to meet a chosen path selection criterion.Example. Consider the path shown in Figure 4.10 from the CFG of Figure\u00ae4.7.Table 4.7 shows the nodes of the path in column 1, the corresponding descriptionof each node in column 2, and the interpretation of each node in column 3. The intended evaluation of each interpreted predicate can be found in column 1 of thesame row.We show the path predicate expressi\u00aeon of the path under consideration in Figure 4.13 for the sake of clarity. The rows of Figure 4.13 have beenobtained from Table 4.11 by combining each interpreted predicate in column 3 withits intended evaluation in column 1. Now the reader may compare Figures 4.11and 4.13 to note that the predicates in Figure 4.13 are interpretat\u00aeions of the corresponding predicates in Figure 4.11 We show in Figure 4.14 an infeasible path appearing in the CFG ofFigure 4.7. The path predicate and its interpretation are shown in Table 4.8, and thepath predicate expression is shown in Figure 4.15. The path predicate expression isunsolvable because the constraint 0 > 0 = True is unsatisfiable. Therefore, the pathshown in Figure 4.14 is an infeasible path.",
  "page425": "Generating Input Data from Path Predicate Expression: We must solvethe corresponding path predicate expression in order to generate input data whichcan force a program to execute a\u00aeselected path. Let us consider the path predicateexpression shown in Figure 4.13. We observe that constraint 1 is always satisfied.Constraints 1 and 5 must be solved together to obtain AS 1. Similarly, constraints2, 3,\u00aeand 4 must be solved together. We note that MIN < = value[0] < = MAXand value[0]! = \u2212999. Therefore, we have many choices to select values of MIN,MAX, and value[0]. An instance of the solutions of\u00aethe constraints of Figure 4.13is shown in Figure 4.16 We give examples of selected test data to achieve complete statement and branchcoverage. We show four sets of test data in Table 4.9. The first two data sets coverall statements of the CFG in Figure 4.7. However, we need all four sets\u00aeof testdata for complete branch coverage.If we execute the method ReturnAverage shown in Figure 4.6 with the foursets of test input data shown in Figure 4.9, then each statement of the method isexecuted at least once, and every Boolean condition evaluates once to true andonce to false. We have thoroughly tested the method in the\u00aesense of completebranch coverage. However, it is possible to introduce simple faults in the methodwhich can go undetected when the method with the above four sets of test data isexecuted. Two examples of fault insertion are given below. in the method. Here the fault is that the method computes the average of thetotal number of inputs, denoted by ti, rather than the total number of valid inputs,denoted by tv.",
  "page426": "he JPA specification only allows properties/paths in the ORDER BY clause if the SELECT clause projects the same properties/paths. The following queries may be nonportable but work\u00aein Hibernate: select i.name from Item i order by i.buyNowPrice asc select i from Item i order by i.seller.username des Be careful with implicit inner joins in path expressions and ORDER BY: The last query returns only I\u00aetem instances that have a seller. This may be unexpected, as the same query without the ORDER BY clause would retrieve all Item instances. (Ignoring for a moment that in our model the Item always has a s\u00aeeller, this issue is visible with optional references.) You'll find a more detailed discussion of inner joins and path. expressions later in this chapter. You now know how to write the FROM, WHERE, and ORDER BY clauses. You know how. to select the entities, you want to retrieve inst\u00aeances of and the necessary expressions. and operations to restrict and order the result. All you need now is the ability to project the data of this result to what you need in your application. In simple terms, selection and restriction in a query is the process of declaring which. tables and rows you want to query. Projection is\u00aedefining the \"columns\" you want returned to the application: the data you need. The SELECT clause in JPQL performs projections. As promised earlier, this criteria query shows how you can add several Roots by calling the from() method several times. To add several elements to your projection, either call the tuple() method of CriteriaBuilder, or the shortcut multiselect().",
  "page427": " Because this is a product, the result contains every possible combination of Item. and Bid rows found in the two underlying tables. Obviously, this query isn't useful, but yo\u00aeu shouldn't be surprised to receive a collection of Object  as a query result. Hibernate manages all Item and Bid entity instances in persistent state, in the persistence. context. Note how the HashSets filter out\u00aeduplicate Item and Bid instances. Alternatively, with the Tuple API, in criteria queries you get typed access to the result list. Start by calling createTupleQuery() to create a CriteriaQuery<Tuple>. T\u00aehen, refine the query definition by adding aliases for the entity classes The Object  returned by this query contain a Long at index 0, a String at index 1, and an Address at index 2. The first two are scalar values; the third is an embedded class instance. None are managed entity instan\u00aeces! Therefore, these values aren't in any persistent state, like an entity instance would be. They aren't transactional and obviously aren't checked automatically for dirty state. We say that all of these values are transient. This is the kind of query you need to write for a simple reporting screen, showing all us\u00aeer names and their home addresses. You have now seen path expressions several times: using dot-notation, you can reference properties of an entity, such as User#username with u.username. For a nested embedded property, for example, you can write the path u.homeAddress.city.zipcode. These are single-valued path expressions, because they don't terminate in a mapped collection property A more convenient alternative than Object[] or Tuple, especially for report queries, is dynamic instantiation in projections, which is next",
  "page428": "Let's say you have a reporting screen in your application where you need to show. some data in a list. You want to show all auction items and when each auction ends. You\u00aewant to load managed Item entity instances, because no data will be modified: you only read data. First, write a class called ItemSummary with a constructor that takes a Long for the item's identifier, a String for the\u00aeitem's name, and a Date for the item's auction end timestamp: We sometimes call these kinds of classes data transfer objects (DTOs), because their main purpose is to shuttle data around in the\u00aeapplication. The ItemSummary class isn't mapped to the database, and you can add arbitrary methods (getter, setter, printing of values) as needed by your reporting user interface. Hibernate can directly return instances of Item Summary from a query with the new keyword in JPQL and t\u00aehe construct() method in criteria We sometimes call these kinds of classes data transfer objects (DTOs), because their main purpose is to shuttle data around in the application. The ItemSummary class isn't mapped to the database, and you can add arbitrary methods (getter, setter, printing of values) as needed by your reportin\u00aeg user interface. Hibernate can directly return instances of ItemSummary from a query with the new keyword in JPQL and the construct() method in criteria ",
  "page429": "your DTO class doesn't have the right constructor, and you want to populate it from a query result through setter methods or fields, apply a ResultTransformer, as shown in in\u00aesection 16.1.3. Later, we have more examples of aggregation and grouping. Next, we're going to look at an issue with projection that is frequently confusing for many engineers: handling duplicates When you create\u00aea projection in a query, the elements of the result aren't guaranteed. to be unique. For example, item names aren't unique, so the following query may return the same name more than once: Its\u00aedifficult to see how it could be meaningful to have two identical rows in a query result, so if you think duplicates are likely, you normally apply the DISTINCT keyword or distinct() method This eliminates duplicates from the returned list of Item descriptions and translates. directly into\u00aethe SQL DISTINCT operator. The filtering occurs at the database level. Later in this chapter, we show you that this isn't always the case. Earlier, you saw function calls in restrictions, in the WHERE clause. You can also call functions in projections, to modify the returned data within the query If an Item doesn't hav\u00aee a buyNowPrice, a BigDecimal for the value zero is returned instead of null. Similar to coalesce() but more powerful are case/when expressions. The following query returns the username of each User and an additional String with either \"Germany\", \"Switzerland For the built-in standard functions, refer to the tables in the previous section. Unlike function calls in restrictions, Hibernate won't pass on an unknown function call in a projection to the database as a plain direct SQL function call. Any function you'd like to call in a projection must be known to Hibernate and/or invoked with the special function() operation of JPQL.",
  "page430": " This projection returns the name of each auction Item and the number of days between item creation and auction end, calling the SQL datediff() function of the H2 database If inste\u00aead you want to call a function directly, you give Hibernate the function's return type, so it can parse the query. You add functions for invocation in projections by extending your configured org.hibernate.Dialect.\u00aeThe datediff() function is already registered for you in the H2 dialect. Then, you can either call it as shown with function(), which works in other JPA providers when accessing H2, or directly as dated\u00aeiff(), which most likely only works in Hibernate. Check the source code of the dialect for your database; you'll probably find many other proprietary SQL functions. already registered there. Furthermore, you can add SQL functions programmatically on boot to Hibernate by calling the\u00aemethod applySqlFunction() on a Hibernate MetadataBuilder See the Javadoc of SQLFunction and its subclasses for more information.  Next, we look at aggregation functions, which are the most useful functions in reporting queries. Reporting queries take advantage of the database's ability to perform efficient grouping and\u00aeaggregation of data. For example, a typical report query would retrieve the highest initial item price in a given category. This calculation can occur in the database, and you don't have to load many Item entity instances into memory. The aggregation functions standardized in JPA are count(), min(), max(), sum(), and avg(). This query returns a BigDecimal, because the amount property is of type BigDecimal. The sum() function also recognizes the BigInteger property type and returns Long for all other numeric property types",
  "page431": "When you call an aggregation function in the SELECT clause, without specifying any grouping in a GROUP BY clause, you collapse the results down to a single row, containing. the agg\u00aeregated value(s). This means (in the absence of a GROUP BY clause) any SELECT. clause that contains an aggregation function must contain only aggregation function for more advanced statistics and for reporting, you need\u00aeto be able to perform. grouping, which is up next JPA standardizes several features of SQL that are most commonly used for reporting although they're also used for other things. In reporting queri\u00aees, you write the SELECT. clause for projection and the GROUP BY and HAVING clauses for aggregation. Just like in SQL, any property or alias that appears outside of an aggregate function. in the SELECT clause must also appear in the GROUP BY clause in this example, the u.lastname propert\u00aey isn't inside an aggregation function, so projected data has to be \"grouped by\" u.lastname. You also don't need to specify the property you want to count; the count(u)expression is automatically translated into. count(u.id) When grouping, you may run into a Hibernate limitation. The following query is specific\u00aeation compliant but not properly handled in Hibernate The JPA specification allows grouping by an entity path expression, group by i. But Hibernate doesn't automatically expand the properties of Item in the generated SQL GROUP BY clause, which then doesn't match the SELECT clause. You have to expand the The JPA specification allows grouping by an entity path expression, group by i. But Hibernate doesn't automatically expand the properties of Item in the generated SQL GROUP BY clause, which then doesn't match the SELECT clause. You have to expand the fix",
  "page432": "Join operations combine data in two (or more) relations. Joining data in a query also enables you to fetch several associated instances and collections in a single query: for examp\u00aele, to load an Item and all its bids in one round trip to the database. We now show you how basic join operations work and how to use them to write such dynamic fetching strategies. Let's first look at how joins wo\u00aerk in SQL queries, without JPA Let's start with the example we already mentioned: joining the data in the ITEM and BID tables, as shown in figure 15.1. The database contains three items: the first\u00aehas three bids, the second has one bid, and the third has no bids. Note that we don't show. all columns; hence the dotted lines. What most people think of when they hear the word join in the context of SQL databases is an inner join. An inner join is the most important of several typ\u00aees of joins and the easiest to understand. Consider the SQL statement and result in figure 15.2. This SQL statement contains an ANSI-style inner join in the FROM clause. If you join the ITEM and BID tables with an inner join, with the condition that the ID of an ITEM row must match the ITEM_ID value of a BID row, you get items co\u00aembined with their bids in the result. Note that the result of this operation contains only items that have bids",
  "page433": "You can think of a join as working as follows: first you take a product of the two. tables, by taking all possible combinations of ITEM rows with BID rows. Second, you filter these\u00aecombined rows with a join condition: the expression in the ON clause. (Any good database engine has much more sophisticated algorithms to evaluate a join; it usually doesn't build a memory-consuming product and th\u00aeen filter out rows.) The join condition is a Boolean expression that evaluates to true if the combined row is to be included in the result. It's crucial to understand that the join condition can be\u00aeany expression that evaluates to true. You can join data in arbitrary ways; you aren't limited to comparisons of identifier values. For example, the join condition on i.ID b.ITEM_ID and amount > 100 would only include rows from the BID table that also have an AMOUNT greater t\u00aehan 100. that a BID has a reference to an ITEM row. This doesn't mean you can only join by comparing primary and foreign key columns. Key columns are of course the most common operands in a join condition, because you often want to retrieve related information together. If you want all items, not just the ones which have re\u00aelated bids, and NULL instead of bid data when there is no corresponding bid, then you write a (left) outer join",
  "page434": " In case of the left outer join, each row in the (left) ITEM table that never satisfies the join condition is also included in the result, with NULL returned for all columns of BID\u00ae. Right outer joins are rarely used; developers always think from left to right and put the \"driving\" table of a join operation first. In figure 15.4, you can see the same result with BID instead of ITEM as th\u00aee driving table, and a right outer join. In SQL, you usually specify the join condition explicitly. Unfortunately, it isn't possible. to use the name of a foreign key constraint to specify how two t\u00aeables are to be joined: select * from ITEM join BID on FK_BID_ITEM_ID doesn't work. You specify the join condition in the ON clause for an ANSI-style join or in the WHERE. clause for a so-called theta-style join: select * from ITEM, BID b where i.ID b.ITEM_ID. This is an inner join;\u00aehere you see that a product is created first in the FROM clause. We now discuss JPA join options. Remember that Hibernate eventually translates all queries into SQL, so even if the syntax is slightly different, you should always refer to the illustrations shown in this section and verify that you understand what the resulting SQ\u00aeL and result set looks like JPA provides four ways of expressing (inner and outer) joins in queries: An implicit association join with path expressions. An ordinary join in the FROM clause with the join operator A fetch join in the FROM clause with the join operator and the fetch keyword for eager fetching A theta-style join in the WHERE clause. Let's start with implicit association joins. In JPA queries, you don't have to specify a join condition explicitly. Rather, you specify the name of a mapped Java class association. This is the same feature we'd prefer to have in SQL: a join condition expressed with a foreign key constraint name. Because you've mapped most, if not all, foreign key relationships of your database schema, you can use the names of these mapped associations in the query language. This is syntactical sugar, but it's convenient",
  "page435": " For example, the Bid entity class has a mapped many-to-one association named item, with the Item entity class. If you refer to this association in a query, Hibernate has enough in\u00aeformation to deduce the join expression with a key column comparison. This helps make queries less verbose and more readable. Earlier in this chapter, we showed you property path expressions, using dot-notation:\u00aesingle-valued path expressions such as user.homeAddress.zipcode and collectionvalued path expressions such as item.bids. You can create a path expression in an implicit inner join query The path b.\u00aeitem.name creates an implicit join on the many-to-one associations from Bid to Item the name of this association is item. Hibernate knows that you mapped this association with the ITEM_ID foreign key in the BID table and generates the SQL join condition accordingly. Implicit joins are alw\u00aeays directed along many-to-one or one-to-one associations, never through a collection-valued association (you can't write item.bids.amount). This query joins rows from the BID, the ITEM, and the USER tables. We frown on the use of this syntactic sugar for more complex queries. SQL joins are important, and especially when opt\u00aeimizing queries, you need to be able to see at a glance exactly how many of them there are How many joins are required to express such a query in SQL? Even if you get the answer right, it takes more than a few seconds to figure out. The answer is two. The generated SQL looks something like this: Alternatively, instead of joins with such complex path expressions, you can write ordinary joins explicitly in the FROM clause",
  "page436": "JPA differentiates between purposes you may have for joining. Suppose you're querying items; there are two possible reasons you may be interested in joining them with bids. Y\u00aeou may want to limit the items returned by the query based on some criterion to apply to their bids. For example, you may want all items that have a bid of more than 100, which requires an inner join. Here, you arent\u00aeinterested in items that have no bids. On the other hand, you may be primarily interested in the items but may want to execute an outer join just because you want to retrieve all bids for the queried item\u00aes in a single SQL statement, something we called eager join fetching earlier. Remember that you prefer to map all associations lazily by default, so an eager fetch query will override the default fetching strategy at runtime for a particular use case. Let's first write some queries\u00aethat use joins for the purpose of restriction. If you want to retrieve Item instances and restrict the result to items that have bids with a certain amount, you have to assign an alias to a joined association. Then you refer to the alias in a WHERE clause to restrict the data you want This query assigns the alias b to the collecti\u00aeon bids and limits the returned Item instances to those with Bid#amount greater than 100. So far, you've only written inner joins. Outer joins are mostly used for dynamic fetching, which we discuss soon. Sometimes, you want to write a simple query with an outer join without applying a dynamic fetching strategy. For example, the following query and retrieves items that have no bids, and items with bids of a minimum bid amount:",
  "page437": "This query returns ordered pairs of Item and Bid, in a List<Object[]>. The first thing that is new in this query is the LEFT keyword and JoinType.LEFT in the criteria query. Optio\u00aenally you can write LEFT OUTER JOIN and RIGHT OUTER JOIN in JPQL, but we usually prefer the short form. The second change is the additional join condition following the ON keyword. If instead you place the amount > 100\u00aeexpression into the WHERE clause, you restrict the result to Item instances that have bids. This isn't what you want here: you want to retrieve items and bids, and even items that don't have b\u00aeids. If an item has bids, the bid amount must be greater than 100. By adding an additional join condition in the FROM clause, you can restrict the Bid instances and still retrieve all Item instances, whether they have bids or not The SQL query will always contain the implied join conditio\u00aen of the mapped association, i.ID b.ITEM_ID. You can only append additional expressions to the join condition. JPA and Hibernate don't support arbitrary outer joins without a mapped entity association or collection. Hibernate has a proprietary WITH keyword, it's the same as the ON keyword in JPQL. You may see it\u00aein older code examples, because JPA only recently standardized ON. You can write a query returning the same data with a right outer join, switching the driving table This right outer join query is more important than you may think. Earlier in this book, we told you to avoid mapping a persistent collection whenever possible. If you don't have a one-to-many Item#bids collection, you need a right outer join to retrieve all Items and their Bid instances. You drive the query from the \"other\" side: the many-toone Bid#item. ",
  "page438": "All the queries you saw in the previous sections have one thing in common: the returned Item instances have a collection named bids. This @OneToMany collection, if mapped as FetchT\u00aeype.LAZY (the default for collections), isn't initialized, and an additional SQL statement is triggered as soon as you access it. The same is true for all single-valued associations, like the @ManyToOne\u00aeassociation seller of each Item. By default, Hibernate generates a proxy and loads the associated User instance lazily and only on demand. What options do you have to change this behavior? First, you c\u00aean change the fetch. plan in your mapping metadata and declare a collection or single-valued association. as FetchType.EAGER. Hibernate then executes the necessary SQL to guarantee that. the desired network of instances is loaded at all times. This also means a single JPA. query may resul\u00aet in several SQL operations! As an example, the simple query selects I from Item i may trigger additional SQL statements to load the bids of each Item, the seller of each Item, and so on. In chapter 12, we made the case for a lazy global fetch plan in mapping metadata, where you shouldn't have FetchType.EAGER on association\u00aeand collection mappings. Then, for a particular use case in your application, you dynamically override the lazy fetch plan and write a query that fetches the data you need as efficiently as possible. For example, there is no reason you need several SQL statements to fetch all Item instances and to initialize their bids collections, or to retrieve the seller for each Item. You can do this at the same time, in a single SQL statement, with a join operation. Eager fetching of associated data is possible with the FETCH keyword in JPQL and the fetch() method in the criteria query API",
  "page439": "You've already seen the SQL query this produces and the result set in figure 15.3. This query returns a List<Item>; each Item instance has its bids collection fully initializ\u00aeed. This is different than the ordered pairs returned by the queries in the previous section! Be careful you may not expect the duplicate results from the previous query: Make sure you understand why these duplicates a\u00aeppear in the result List. Verify the number of Item \"rows\" in the result set, as shown in figure 15.3. Hibernate preserves the rows as list elements; you may need the correct row count to make\u00aerendering a report table in the user interface easier. You can filter out duplicate Item instances by passing the result List through a LinkedHashSet, which doesn't allow duplicate elements but preserves the order of elements. Alternatively, Hibernate can remove the duplicate elemen\u00aets with the DISTINCT operation and distinct() criteria method: Understand that in this case the DISTINCT operation does not execute in the database. There will be no DISTINCT keyword in the SQL statement. Conceptually, you can't remove the duplicate rows at the SQL ResultSet level. Hibernate performs deduplication in memory,\u00aejust as you would manually with a LinkedHashSet This query returns a List<Item>, and each Item has its bids collection initialized. The seller of each Item is loaded as well. Finally, the bidder of each Bid instance is loaded. You can do this in one SQL query by joining rows of the ITEM, BID, and USERS tables. If you write JOIN FETCH without LEFT, you get eager loading with an inner join (also if you use INNER JOIN FETCH) ",
  "page440": "An eager inner join fetch makes sense if there must be a fetched value: an Item must have a seller, and a Bid must have a bidder. There are limits to how many associations you sho\u00aeuld eagerly load in one query and how much data you should fetch in one round trip. Consider the following query, which initializes the Item bids and Item images collections: This is a bad query, because it creates a Ca\u00aertesian product of bids and images, with a potentially extremely large result set. We covered this issue in section 12.2.2. To summarize, eager dynamic fetching in queries has the following caveats: Ne\u00aever assign an alias to any fetch-joined association or collection for further restriction or projection. The query left join fetch i.bids b where b.amount ... is invalid. You can't say, \"Load the Item instances and initialize their bids collections, but only with Bid insta\u00aences that have a certain amount.\" You can assign an alias to a fetch-joined association for further fetching: for example, retrieving the bidder of each Bid: left join fetch i.bids b join fetch b.bidder. You shouldn't fetch more than one collection; otherwise, you create a Cartesian product. You can fetch as many single-\u00aevalued associations as you like without creating a product Queries ignore any fetching strategy you've defined in mapping metadata with @org.hibernate.annotations.Fetch. For example, mapping the bids collection with org.hibernate.annotations.FetchMode.JOIN has no effect on the queries you write. The dynamic fetching strategy of your query ignores the global fetching strategy. On the other hand, Hibernate doesn't ignore the mapped fetch plan: Hibernate always considers a FetchType.EAGER, and you may see several additional SQL statements when you execute your query.",
  "page441": "If you eager-fetch a collection, the List returned by Hibernate preserves the number of rows in the SQL result as duplicate references. You can filter out the duplicates in-memory\u00aeeither manually with a LinkedHashSet or with the special DISTINCT operation in the query. There is one more issue to be aware of, and it deserves some special attention. You can't paginate a result set at the datab\u00aease level if you eagerly fetch a collection. For example, for the query select i from Item i fetch i.bids, how should Query#setFirstResult(21) and Query#setMaxResults(10) be handled? Clearly, you\u00aeexpect to get only 10 items, starting with item 21. But you also want to load all bids of each Item eagerly. Therefore, the database can't do the paging operation; you can't limit the SQL result to 10 arbitrary rows. Hibernate will execute paging in-memory if a collection\u00aeis eagerly fetched in a query. This means all Item instances will be loaded into memory, each with the bids collection fully initialized. Hibernate then gives you the requested page of items: for example, only items 21 to 30. Not all items might fit into memory, and you probably expected the paging to occur in the database before\u00aeit transmitted the result to the application! Therefore, Hibernate will log a warning message if your query contains fetch [collectionPath] and you call setFirstResult() or setMaxResults(). We don't recommend the use of fetch [collectionPath] with setMaxResults() or setFirstResult() options. Usually there is an easier query you can write to get the data you want to render and we don't expect that you load data page by page to modify it. For example, if you want to show several pages of items and for each item the number of bids, write a report query ",
  "page442": "In traditional SQL, a theta-style join is a Cartesian product together with a join condition in the WHERE clause, which is applied on the product to restrict the result. In JP quer\u00aeies, the theta-style syntax is useful when your join condition isn't a foreign key relationship mapped to a class association. For example, suppose you store the User's name in log records instead of mapping\u00aean association from LogRecord to User. The classes don't know anything about each other, because they aren't associated. You can then find all the Users and their Log Records with the following\u00aetheta-style join The join condition here is a comparison of username, present as an attribute in both. classes. If both rows have the same username, they're joined (with an inner join) in the result. The query result consists of ordered pairs You probably won't need to use the\u00aetheta-style joins often. Note that it's currently not. possible in JPA to outer join two tables that don't have a mapped association thetastyle joins are inner joins. Another more common case for theta-style joins is comparisons of primary key or foreign key values to either query parameters or other primary or fo\u00aereign key values in the WHERE clause: This query returns pairs of Item and Bid instances, where the bidder is also the seller. This is an important query in CaveatEmptor because it lets you detect people who bid on their own items. You probably should translate this query into a database constraint and not allow such a Bid instance to be stored.  The previous query also has an interesting comparison expression: i.seller b.bidder. This is an identifier comparison, our next topic",
  "page443": "In this query, i.seller refers to the SELLER_ID foreign key column of the ITEM table, referencing the USERS table. The alias u refers to the primary key of the USERS table. (on the\u00aeID column). Hence, this query has a theta-style join and is equivalent to the easier, readable alternative A path expression ending with id is special in Hibernate: the id name always refers to the identifier property\u00aeof an entity. It doesn't matter what the actual name of the property annotated with @Id is you can always reach it with entityAlias.id. That's why we recommend you always name the identifier pr\u00aeoperty of your entity classes id, to avoid confusion in queries. Note that this isn't a requirement or standardized in JPA; only Hibernate treats an id path element specially. You may also want to compare a key value to a query parameter, perhaps to find all Items for a given seller\u00ae(a User) The first query pair uses an implicit table join; the second has no joins at all!  This completes our discussion of queries that involve joins. Our final topic is nesting selects within selects: subselect Sub selects are an important and powerful feature of SQL. A subselect is a select query embedded in another query, u\u00aesually in the SELECT, FROM, or WHERE clause. JPA supports subqueries in the WHERE clause. Subselects in the FROM clause aren't supported because the query languages doesn't have transitive closure. The result of a query may not be usable for further selection in a FROM clause. The query language also doesn't support subselects in the SELECT clause, but you map can subselects to derived properties with @org.hibernate.annotations.Formula, as shown in section 5.1.3. Subselects can be either correlated with the rest of the query or uncorrelated.",
  "page444": "The result of a subquery may contain either a single row or multiple rows. Typically, subqueries that return single rows perform aggregation. The following subquery returns the tot\u00aeal number of items sold by a user; the outer query returns all users who have sold more than one item: The subquery in this example returns the maximum bid amount in the entire system; the outer query returns all bids w\u00aehose amount is within one (U.S. dollar, Euro, and so on) of that amount. Note that in both cases, parentheses enclose the subquery in JPQL. This is always required Uncorrelated subqueries are harmless, a\u00aend there is no reason not to use them when convenient. You can always rewrite them as two queries, because they don't. reference each other. You should think more carefully about the performance impact. of correlated subqueries. On a mature database, the performance cost of a simple\u00aecorrelated subquery is similar to the cost of a join. But it isn't necessarily possible to rewrite a correlated subquery using several separate queries.  If a subquery returns multiple rows, you combine it with quantification The following quantifiers are standardized: ALL The expression evaluates to true if the comparison i\u00aes true for all values in the result of the subquery. It evaluates to false if a single value of the subquery result fails the comparison test. ANY The expression evaluates to true if the comparison is true for some (any) value in the result of the subquery. If the subquery result is empty or no value satisfies the comparison, it evaluates to false. The keyword SOME is a synonym for ANY. EXISTS Evaluates to true if the result of the subquery consists of one or more values",
  "page445": "This chapter explains query options that you may consider optional or advanced: transforming query results, filtering collections, and the Hibernate criteria query API. First, we d\u00aeiscuss Hibernate's ResultTransformer API, with which you can apply a result transformer to a query result to filter or marshal the result with your own code instead of Hibernate's default behavior. In previou\u00aes chapters, we always advised you to be careful when mapping collections, because it's rarely worth the effort. In this chapter, we introduce collection filters, a native Hibernate feature that make\u00aes persistent collections more valuable. Finally, we look at another proprietary Hibernate feature, the org.hibernate.Criteria query API, and some situations when you might prefer it to the standard JPA query-by-criteria. Let's start with the transformation of query results. T\u00aeransforming query results You can apply a result transformer to a query result so that you can filter or marshal the result with your own procedure instead of the Hibernate default behavior. Hibernates default behavior provides a set of default transformers that you can replace and/or customize. The result you're going to tr\u00aeansform is that of a simple query, but you need to access the native Hibernate API org.hibernate.Query through the Session, as shown in the following listing Each object array is a \"row\" of the query result. Each element of that tuple can be accessed by index: here index 0 is a Long, index 1 a String, and index 2 a Date. The first result transformer we introduce instead returns a List of Lists",
  "page446": "In section 15.3.2, we showed how a query can return instances of a JavaBean dynamically by calling the ItemSummary constructor. In JPQL, you achieve this with the new operator. For\u00aecriteria queries, you use the construct() method. The ItemSummary class must have a constructor that matches the projected query result. Alternatively, if your JavaBean doesn't have the right constructor, you can\u00aestill instantiate and populate its values through setters and/or fields with the AliasToBeanResultTransformer.  You create the transformer with the JavaBean class you want to instantiate, here It\u00aeemSummary. Hibernate requires that this class either has no constructor or a public nonargument constructor. When transforming the query result, Hibernate looks for setter methods and fields with the same names as the aliases in the query. The ItemSummary class must either have the\u00aefields itemId, name, unauctioned, or the setter methods setItemId(), setName(), and setAuctionEnd(). The fields or setter method parameters must be of the right type. If you have fields that map to some query aliases and setter methods for the rest, that's fine too.  You should also know how to write your own ResultTransfor\u00aemer when none of the built-in ones suits you The built-in transformers in Hibernate aren't sophisticated; there isn't much difference between result tuples represented as lists, maps, or object arrays.  Next, we show you how to implement a ResultTransformer. Let's assume that you want a List<ItemSummary> returned from the query shown in listing 16.1, but you can't let Hibernate create an instance of ItemSummary through reflection on a constructor. Maybe your ItemSummary class is predefined and doesn't have the right constructor, fields, and setter methods. Instead, you have an ItemSummaryFactory to produce instances of ItemSummary",
  "page447": "For each result \"row,\" an Object[] tuple must be transformed into the desired result value for that row. Here you access each projection element by index in the tuple arr\u00aeay and then call the ItemSummaryFactory to produce the query result value. Hibernate passes the method the aliases found in the query, for each tuple element. You don't need the aliases in this transformer, though.\u00aeC You can wrap or modify the result list after transforming the tuples. Here you make. the returned List unmodifiable: ideal for a reporting screen where nothing should change the data. As you can see i\u00aen the example, you transform query results in two steps: first you customize how to convert each \"row\" or tuple of the query result to whatever value you desire. Then you work on the entire List of these values, wrapping or converting again. Next, we discuss another convenient\u00aeHibernate feature (where JPA doesn't have an equivalent): collection filters. In chapter 7, you saw reasons you should (or rather, shouldn't) map a collection in your Java domain model. The main benefit of a collection mapping is easier access to data: you can call item.getImages() or item.getBids() to access all images\u00aeand bids associated with an Item. You don't have to write a JPQL or criteria query; Hibernate will execute the query for you when you start iterating through the collection elements. The most obvious problem with this automatic data access is that Hibernate will always write the same query, retrieving all images or bids for an Item. You can customize the order of collection elements, but even that is a static mapping. What would you do to render two lists of bids for an Item, in ascending and descending order by creation date? ",
  "page448": " Instead, you can use a Hibernate proprietary feature, collection filters, that makes writing these queries easier, using the mapped collection. Let's say you have a persisten\u00aet Item instance in memory, probably loaded with the EntityManager API. You want to list all bids made for this Item but further restrict the result to bids made by a particular User. You also want the list sorted in des\u00aecending order by Bid#amount.The session.createFilter() method accepts a persistent collection and a JPQL query fragment. This query fragment doesn't require a select or from clause; here it only has\u00aea restriction with the where clause and an order by clause. The alias this always refers to elements of the collection, here Bid instances. The filter created is an ordinary org.hibernate.Query, prepared with a bound parameter and executed with list(), as usual. Hibernate doesn't e\u00aexecute collection filters in memory. The Item bids collection may be uninitialized when you call the filter and, and if so, remains uninitialized. Furthermore, filters don't apply to transient collections or query results. You may only apply them to a mapped persistent collection currently referenced by an entity instance man\u00aeaged by the persistence context. The term filter is somewhat misleading, because the result of filtering is a completely new and different collection; the original collection isn't touched. To the great surprise of everyone, including the designer of this feature, even trivial filters turn out to be useful. For example, you can use an empty query to paginate collection elements:",
  "page449": "Here, Hibernate executes the query, loading the collection elements and limiting the returned rows to two, starting with row zero of the result. Usually, you'd use an order by\u00aewith paginated queries. You don't need a from clause in a collection filter, but you can have one if that's your style. A collection filter doesn't even need to return elements of the collection being f\u00aeiltered. This next filter returns any Item sold by any of the bidders All this is a lot of fun, but the most important reason for the existence of collection filters is to allow your application to retr\u00aeieve collection elements without initializing the entire collection. For large collections, this is important to achieve acceptable performance. The following query retrieves all bids made for the Item with an amount greater or equal to 100: Again, this doesn't initialize the I\u00aetem#bids collection but returns a new collection. Before JPA 2, query-by-criteria was only available as a proprietary Hibernate API. Today, the standardized JPA interfaces are equally as powerful as the old org.hibernate.Criteria API, so you'll rarely need it. But several features are still only available in the Hibern\u00aeate API, such as query-by-example and embedding of arbitrary SQL fragments. In the following section, you find a short overview of the org.hibernate .Criteria API and some of its unique options Using the org.hibernate.Criteria and org.hibernate.Example interfaces, you can build queries programmatically by creating and combining org.hibernate.criterion.* instances. You see how to use these APIs and how to express selection, restriction, joins, and projection. We assume that you've read the previous chapter and that you know how these operations are translated into SQL.",
  "page450": ". All query examples shown here have an equivalent JPQL or JPA criteria example in the previous chapter, so you can easily flip back and forth if you need to compare all three APIs\u00ae. Let's start with some basic selection examples. When you're ready to execute the query, \"attach\" it to a Session with getExecutableCriteria(). Note that this is a unique feature of the Hibe\u00aernate criteria API. With JPA, you always need at least an EntityManagerFactory to get a CriteriaBuilder. You can declare the order of the results, equivalent to an order by clause in JPQL. The following\u00aequery loads all User instances sorted in ascending order by first and last name: In this example, the code is written in the fluent style (using method chaining); method's such as add Order() return the original org.hibernate.Criteria.  Next, we look at restricting the selected records\u00aeThe Restrictions interface is the factory for individual Criterion you can add to the Criteria. Attributes are addressed with simple strings, here Item#name with \"name\". You can also match substrings, similar to the like operator in JPQL. The following query loads all User instances with username starting with \"j\" or\u00ae\"J\" A unique feature of the Hibernate Criteria API is the ability to write plain SQL fragments in restrictions. This query loads all User instances with a username shorter than eight characters Hibernate sends the SQL fragment to the database as is. You need the {alias} placeholder to prefix any table alias in the final SQL; it always refers to the table the root entity is mapped to (USERS, in this case). You also apply a position parameter (named parameters aren't supported by this API) and specify its type as StandardBasicTypes.INTEGER",
  "page451": "The result of this query is a List of Object[], one array for each tuple. Each array contains a Long (or whatever the type of the user's identifier is), a String, and an Addre\u00aess. Just as with restrictions, you can add arbitrary SQL expressions and function calls to projections This query returns a List of Strings, where strings have the form \"[Item name]:[Auction end date]\".\u00aeThe second parameter for the projection is the name of the alias(es) you used in the query: Hibernate needs this to read the value of the ResultSet. The type of each projected element/alias is also need\u00aeed: here, StandardBasicTypes.STRING. Hibernate supports grouping and aggregation. This query counts users' last names This query returns all Bid instances of any Item sold by User \"johndoe\" that doesn't have a buyNowPrice. The first inner join of the Bid#item as\u00aesociation is made with createCriteria(\"item\") on the root Criteria of the Bid. This nested Criteria now represents the association path, on which another inner join is made with createCriteria(\"seller\"). Further restrictions are placed on each join Criteria; they will be combined with logical and in the where clause of t\u00aehe final SQL query. This query returns all Item instances, loads the Item#bids collection with an outer join, and loads Bid#bidder with an inner join. The Item#seller is also loaded: because it can't be null, it doesn't matter whether an inner or outer join is used. As always, don't fetch several collections in one query, or you'll create a Cartesian products (see section 15.4.5).  Next, you see that subqueries with criteria also work with nested Criteria instances.",
  "page452": "The DetachedCriteria is a query that returns the number of items sold restricted by a given User. The restriction relies on the alias u, so this is a correlated subquery. The\u00aecouter\" query then embeds the DetachedCriteria and provides the alias u. Note that the subquery is the right operand of the lt() (less than) operation, which translates into 1 < ([Result of count query]) in SQL. Again,\u00aethe position of the operands dictates that the comparison is based on geAll() (greater or equal than all) to find the bids with \"less or equal than 10\" amount. So far, there are a few good rea\u00aesons to use the old org.hibernate.Criteria API. You really should use the standardized JPA query languages in new applications, though. The most interesting features of the old proprietary API we've shown are embedded SQL expressions in restrictions and projections. Another Hibernate\u00ae-only feature you may find interesting is query-by-example The idea behind example queries is that you provide an example entity instance, and Hibernate loads all entity instances that \"look like the example.\" This can be convenient if you have a complex search screen in your user interface, because you don't have t\u00aeo write extra classes to hold the entered search terms. Let's say you have a search form in your application where you can search for User instances by last name. You can bind the form field for \"last name\" directly to the User#lastname property and then tell Hibernate to load \"similar\" User instances",
  "page453": "Create an \"empty\" instance of User as a template for your search, and set the property values you're looking for: people with the last name \"Doe\" Create an\u00aeinstance of Example with the template. This API allows you to fine-tune the search. You want the case of the last name to be ignored, and a substring search, so \"Doe\", \"Dex\", or \"Doe Y\" wi\u00aell match. D The User class has a Boolean property called activated. As a primitive, it can't be null, and its default value is false, so Hibernate would include it in the search and only return use\u00aers that aren't activated. You want all users, so tell Hibernate to ignore that property. E The Example is added to a Criteria as a restriction. Because you've written the User entity class following JavaBean rules, binding it to a UI form should be trivial. It has regular getter\u00aeand setter methods, and you can create an \"empty\" instance with the public no-argument constructor (remember our discussion of constructor design in section 3.2.3.) One obvious disadvantage of the Example API is that any string-matching options, such as ignoreCase() and enableLike(), apply to all string-valued pr\u00aeoperties of the template. If you searched for both last name and first name, both would be case insensitive substring matches. nsensitive substring matches. By default, all non-null valued properties of the given entity template are added to the restriction of the example query. As shown in the last code snippet, you can manually exclude properties of the entity template by name with excludeProperty",
  "page454": "Other exclusion options are exclusion of zero-valued properties (such as int or long) with excludeZeroes() and disabling exclusion altogether with excludeNone(). If no properties a\u00aere excluded, any null property of the template is added to the restriction in the SQL query with an is null check. If you need more control over exclusion and inclusion of properties, you can extend Example and write y\u00aeour own PropertySelector: After adding an Example restriction to a Criteria, you can add further restrictions to the query. Alternatively, you can add multiple example restrictions to a single query. Th\u00aee following query returns all Item instances with names starting with \"B\" or \"b\" and a seller matching a User example: You used the ResultTransformer API to write custom code to process a query result, returning a list of lists and a list of maps, and mapping aliases t\u00aeo bean properties. We covered Hibernate's collection-filtering interfaces as well as making better use of mapped persistent collections. You explored the older Hibernate Criteria query facility and when you might use it instead of the standardized criteria queries in JPA. We covered all the relational and Hibernate goo\u00aedies using this API: selection and ordering, restriction, projection and aggregation, joins, subselects, and example queries.",
  "page455": "In this chapter, we cover customizing and embedding SQL in a Hibernate application. SQL was created in the 1970s, but ANSI didn't standardized it until 1986. Although each upd\u00aeate of the SQL standard has seen new (and many controversial) features, every DBMS product that supports SQL does so in its own unique way. The burden of portability is again on the database application developers. This\u00aeis where Hibernate helps: its built-in query languages produce SQL that depends on the configured database dialect. Dialects also help produce all other automatically generated SQL (for example, when Hi\u00aebernate has to retrieve a collection on demand). With a simple dialect switch, you can run your application on a different DBMS. Hibernate generates all SQL statements for you, for all create, read, update, and delete (CRUD) operations. Sometimes, though, you need more control than Hiber\u00aenate and the Java Persistence API provide: you need to work at a lower level of abstraction. With Hibernate, you can write your own SQL statements: Fall back to the JDBC API, and work directly with the Connection, PreparedStatement, and ResultSet interfaces. Hibernate provides the Connection, so you don't have to mainta\u00aein a separate connection pool, and your SQL statements execute within the same (current) transaction. Write plain SQL SELECT statements, and either embed them within your Java code or externalize them (in XML files or annotations) as named queries. You execute these SQL queries with the Java Persistence API, just like a regular JPQL query. Hibernate can then transform the query result according to your mapping. This also works with stored procedure calls.",
  "page456": "Replace SQL statements generated by Hibernate with your own hand-written SQL. For example, when Hibernate loads an entity instance with em.find() or loads a collection on-demand, y\u00aeour own SQL query can perform the load. You can also write your own Data Manipulation Language (DML) statements, such as UPDATE, INSERT, and DELETE. You might even call a stored procedure to preform a CRUD operation. Yo\u00aeu can replace all SQL statements automatically generated by Hibernate with custom statements. We start with JDBC fallback usage and then discuss Hibernate's automatic result-mapping capabiliti\u00aees. Then, we show you how to override queries and DML statements in Hibernate. Last, we discuss integration with stored database procedures. Sometimes you want Hibernate to get out of the way and directly access the database through the JDBC API. To do so, you need a java.sql.Connection i\u00aenterface to write and execute your own PreparedStatement and direct access to your statement ResultSet. Because Hibernate already knows how to obtain and close database connections, it can provide your application with a Connection and release it when you're done. This functionality is available with the org.hibernate.\u00aejdbc.Work API, a callbackstyle interface. You encapsulate your JDBC \"work\" by implementing this interface; Hibernate calls your implementation providing a Connection. The following example executes an SQL SELECT and iterates through the ResultSet For this \"work,\" an item identifier is needed, enforced with the final field and the constructor paramet",
  "page457": "The execute() method is called by Hibernate with a JDBC Connection. You don't have to close the connection when you're done. D You have to close and release other resourc\u00aees you've obtained, though, such as the PreparedStatement and ResultSet In this case, Hibernate has already enlisted the JDBC Connection it provides with the current system transaction. Your statements are committe\u00aed when the system transaction is committed, and all operations, whether executed with the EntityManager or Session API, are part of the same unit of work. Alternatively, if you want to return a value fro\u00aem your JDBC \"work\" to the application, implement the interface org.hibernate.jdbc.ReturningWork. There are no limits on the JDBC operations you can perform in a Work implementation. Instead of a PreparedStatement, you may use a CallableStatement and execute a stored proce\u00aedure in the database; you have full access to the JDBC API. For simple queries and working with a ResultSet, such as the one in the previous example, a more convenient alternative is available. When you execute an SQL SELECT query with the JDBC API or execute a stored procedures that returns a ResultSet, you iterate through each\u00aerow of this result set and retrieve the data you need. This is a labor-intensive task, and you end up duplicating the same lines of code repeatedly. When you execute an SQL SELECT query with the JDBC API or execute a stored procedure that returns a ResultSet, you iterate through each row of this result set and retrieve the data you need. This is a labor-intensive task, and you end up duplicating the same lines of code repeatedly.",
  "page458": "The returned Item instances are in persistent state, managed by the current persistence context. The result is therefore the same as with the JPQL query select i from Item i. For\u00aethis transformation, Hibernate reads the result set of the SQL query and tries to discover the column names and types as defined in your entity mapping metadata. If the column AUCTIONEND is returned, and it's mappe\u00aed to the Item#auctionEnd property, Hibernate knows how to populate that property and returns fully loaded entity instances. Note that Hibernate expects the query to return all columns required to create\u00aean instance of Item, including all properties, embedded components, and foreign key columns. If Hibernate can't find a mapped column (by name) in the result set, an exception is thrown. You may have to use aliases in SQL to return the same column names as defined in your entity mapp\u00aeing metadata. The interfaces javax.persistence.Query and org.hibernate.SQLQuery both support parameter binding. The following query returns only a single Item entity instance Although available in Hibernate for both APIs, the JPA specification doesn't consider named parameter binding for native queries portable. Therefore, s\u00aeome JPA providers may not support named parameters for native queries.  If your SQL query doesn't return the columns as mapped in your Java entity class, and you can't rewrite the query with aliases to rename columns in the result, you must create a result-set mapping",
  "page459": "The following query returns a List of managed Item entity instances. All columns of the ITEM table are included in the SQL projection, as required for the construction of an Item i\u00aenstance. But the query renames the NAME column to EXTENDED_NAME with an alias in the projection Hibernate can no longer automatically match the result set fields to Item properties: the NAME column is missing from the r\u00aeesult set. You therefore specify a \"result mapping\" with You map all fields of the result set to properties of the entity class. Even if only one field/column doesn't match the alrea\u00aedy mapped column name (here EXTENDED_NAME), all other columns and properties have to be mapped as well SQL result mappings in annotations are difficult to read and as usual with JPA annotations, they only work when declared on a class, not in a package-info.java metadata file. We p\u00aerefer externalizing such mappings into XML files. The following provides the same mapping: If both result-set mappings have the same name, the mapping declared in XML overrides the one defined with annotations. You can also externalize the actual SQL query with @NamedNativeQuery or <namednative-query>, as shown in section 1\u00ae4.4. In all following examples, we keep the SQL statement embedded in the Java code, because this will make it easier for you to understand what the code does. But most of the time, you'll see result-set mappings in the more succinct XML syntax.",
  "page460": "With the Hibernate API, you can perform the result-set mapping directly within the query through alias placeholders. When calling addEntity(), you provide an alias, here i. In the\u00aeSQL string, you then let Hibernate generate the actual aliases in the projection with placeholders such as {i.name} and {i.auctionEnd}, which refer to properties of the Item entity. No additional result-set mapping decl\u00aearation is necessary; Hibernate generates the aliases in the SQL string and knows how to read the property values from the query ResultSet. This is much more convenient than the JPA result-set mapping op\u00aetion. Or, if you can't or don't want to modify the SQL statement, use add Root() and add Property() on the org.hibernate.SQLQuery to perform the mapping This is effectively an eager fetch of the association Item#seller. Hibernate knows that each row contains the fields for an I\u00aetem and a User entity instance, linked by the SELLER_ID. The duplicate columns in the result set would be i.ID and u.ID, which both have the same name. You've renamed them with an alias to ITEM_ID and USER_ID, so you have to map how the result set is to be transformed As before, you have to map all fields of each entity resu\u00aelt to column names, even if only two have different names as the original entity mapping. This query is much easier to map with the Hibernate API:",
  "page461": "Hibernate will add auto-generated unique aliases to the SQL statement for the {i.*} and {u.*} placeholders, so the query won't return duplicate column names.  You may have no\u00aeticed the dot syntax in the previous JPA result mapping for the home Address embedded component in a User. Let's look at this special case again We've shown this dot syntax several times before when discussing\u00aeembedded components: you reference the street property of home Address with homeAddress.street. For nested embedded components, you can write homeAddress.city.name if City isn't just a string but a\u00aenother embeddable class. Hibernate's SQL query API also supports the dot syntax in alias placeholders for component properties. Here are the same query and result-set mapping: The query (outer) joins the ITEM and BID tables. The projection returns all columns required to construct I\u00aetem and Bid instances. The query renames duplicate columns such as ID with aliases, so field names are unique in the result. C Because of the renamed fields, you have to map each column to its respective entity property. D Add a Fetch Return for the bids collection with the alias of the owning entity and map the key and element s\u00aepecial properties to the foreign key column BID_ITEM_ID and the identifier of the Bid. Then the code maps each property of Bid to a field of the result set. Some fields are mapped twice, as required by Hibernate for construction of the collection The number of rows in the result set is a product: one item has three bids, one item has one bid, and the last item has no bids, for a total of five rows in the result. F The first element of the result tuple is the Item instance; Hibernate initialized the bids collection",
  "page462": "The second element of the result tuple is each Bid. Alternatively, if you don't have to manually map the result because the field names returned by your SQL query match the al\u00aeready-mapped columns of the entities, you can let Hibernate insert aliases into your SQL statement with placeholders: Eager fetching of collections with dynamic SQL result mappings is only available with the Hibernate A\u00aePI; it's not standardized in JPA So far, you've seen SQL queries returning managed entity instances. You can also return transient instances of any class with the right constructor. The return\u00aeed column types have to match the constructor parameter types; Hibernate would default to BigInteger for the ID column, so you map it to a Long with the class attribute. The Hibernate API gives you a choice. You can either use an existing result mapping for the query by name, or ap\u00aeply a result transformer, as you saw for JPQL queries in section 16.1: You can use an existing result mapping. C Alternatively, you can map the fields returned by the query as scalar values. Without a result transformer, you'd get an Object[] for each result row. D Apply a built-in result transformer to turn the Object[] into\u00aeinstances of ItemSummary. As explained in section 15.3.2, Hibernate can use any class constructor with such a mapping. Instead of ItemSummary, you can construct Item instances. They will be in either transient or detached state, depending on whether you return and map an identifier value in your query.  You can also mix different kinds of result mappings or return scalar values directly.",
  "page463": "Hibernate usually generates CRUD SQL statements at startup. It caches the SQL statements internally for future use, thus avoiding any runtime costs of SQL generation forthe most co\u00aemmon operations. You've seen how you can override the R in CRUD, sonow, let's do the same for CUD. For each entity, you can define custom CUD SQL statements with the Hibernate annotations @SQLInsert, @SQLUpdat\u00aee, and @SQLDelete,respectivelyYou need to be careful with the binding of arguments for SQL statements and parameter placeholders (the ? in the statements). For CUD customization, Hibernate onlysupports p\u00aeositional parameters. What is the right order for the parameters? There's an internal order to howHibernate binds arguments to SQL parameters for CUD statements. The easiest way tofigure out the right SQL statement and parameter order is to let Hibernate generatean example for you. W\u00aeithout any custom SQL statements, enable DEBUG logging forthe org.hibernate.persister.entity category, and search the Hibernate startupoutput for lines like the following:These automatically generated SQL statements show the right parameter order, andHibernate always binds the values in that order. Copy the SQL statements you want\u00aetocustomize into annotations, and make the necessary changes. A special case is properties of an entity class mapped to another table with@SecondaryTable. The CUD customization statements we've shown so far only refer tothe columns of the main entity table. Hibernate still executes automatically generatedSQL statements for insertion, updates, and deletion of rows in the secondary table(s).",
  "page464": "statements in an XML file. This also simplifies ad hoc testing, because you can copyand paste SQL statements between an XML file and your SQL database console. You've probably\u00aenoticed that all the SQL examples in the previous sections weretrivial. In fact, none of the examples required a query written in SQL we could haveused JPQL in each case. To make the next example moreinteresting, we wr\u00aeite a query that can't be expressed inJPQL, only in SQL. This is the mapping of the association a regular@ManyToOne of the PARENT_ID foreign key column:Categories form a tree. The root of the tree i\u00aes a Category node without a parent. Thedatabase data for the example tree is in figure 17.2. You can also represent this data as a tree diagram, as shown in figure 17.3. Alternatively, you can use a sequence of paths and the level of each node:Now, consider how your application loads Cate\u00aegory instances. You may want to findthe root Category of the tree. This is a trivial JPQL query:You can easily query for the categories in a particular level of the tree, such as all children of the root:This query will only return direct children of the root: here, categories Two and Three. How can you load the entire tree (or a\u00aesubtree) in one query? This isn't possible withJPQL, because it would require recursion: \"Load categories at this level, then all the children on the next level, then all the children of those, and so on.\" In SQL, you can writesuch a query, using a common table expression (CTE), a feature also known as subquery factoring.",
  "page465": "It's a complex query, and we won't spend too much time on it here. To understand it,read the last SELECT, querying the CATEGORY_LINK view. Each row in that view represent\u00aes a node in the tree. The view is declared here in the WITH() AS operation. TheCATEGORY_LINK view is a combined (union) result of two other SELECTs. You add additional information to the view during recursion, such as t\u00aehe PATH and the LEVEL ofeach node.The XML maps the ID, CAT_NAME, and PARENT_ID fields to properties of the Categoryentity. The mapping returns the PATH and LEVEL as additional scalar values. To execute t\u00aehe named SQL query and access the result, write the following:Each tuple contains a managed, persistent Category instance; its path in the tree as astring (such as /One, /One/Two, and so on); and the tree level of the node. Alternatively, you can declare and map an SQL query in a Hibernat\u00aee XML metadata file:We left out the SQL query in this snippet; it's the same as the SQL statement shownearlier in the JPA example. As mentioned in section 14.4, with regard to the execution in Java code, it doesn'tmatter which syntax you declare your named queries in: XML file or annotations. Eventhe language doesn'\u00aet matter it can be JPQL or SQL. Both Hibernate and JPA queryinterfaces have methods to \"get a named query\" and execute it independently fromhow you defined it. This concludes our discussion of SQL result mapping for queries. The next subjectis customization of SQL statements for CRUD operations, replacing the SQL automatically generated by Hibernate for creating, reading, updating, and deleting data in thedatabase.",
  "page466": "The first custom SQL you write loads an entity instance of the User class. All the following code examples show the same SQL that Hibernate executes automatically bydefault, withou\u00aet much customization this helps you understand the mapping technique more quickly. You can customize retrieval of an entity instance with a loaderHibernate has two requirements when you override an SQL query to load an\u00aeentityinstance: Write a named query that retrieves the entity instance. We show an example inSQL, but as always, you can also write named queries in JPQL. For an SQL query,you may need a custom result ma\u00aepping, as shown earlier in this chapter.Activate the query on an entity class with @org.hibernate.annotationsLoader. This enables the query as the replacement for the Hibernate-generated query.Let's override how Hibernate loads an instance of the User entity, as shown in the followin\u00aeg listing.Annotations declare the query to load an instance of User; you can also declare it inan XML file (JPA or Hibernate metadata). You can call this named query directly inyour data-access code when needed. The query must have exactly one parameter placeholder, which Hibernate sets as theidentifier value of the instance to lo\u00aead. Here it's a positional parameter, but a namedparameter would also work. For this trivial query, you don't need a custom result-set mapping. The User class mapsall fields returned by the query. Hibernate can automatically transform the result.",
  "page467": "Setting the loader for an entity class to a named query enables the query for all operations that retrieve an instance of User from the database. There's no indication of theq\u00aeuery language or where you declared it; this is independent of the loader declaration.In a named loader query for an entity, you have to SELECT (that is, perform a projection for) the following properties of the entity\u00aeclass: The value of the identifier property or properties, if a composite primary key isused. All scalar properties of basic type. All properties of embedded components. An entity identifier value for ea\u00aech @JoinColumn of each mapped entity association such as @ManyToOne owned by the loaded entity class. All scalar properties, embedded component properties, and association joinreferences that are inside a @SecondaryTable annotation. If you enable lazy loading for some properties, through\u00aeinterception and bytecodeinstrumentation, you don't need to load the lazy properties (see section 12.1.3).Hibernate always calls the enabled loader query when a User has to be retrieved fromthe database by identifier. When you call em.find(User.class, USER_ID), your custom query will execute. When you call someItem.getSeller(\u00ae).getUsername(), andthe Item#seller proxy has to be initialized, your custom query will load the data. You may also want to customize how Hibernate creates, updates, and deletes aninstance of User in the database.",
  "page468": "Hibernate usually generates CRUD SQL statements at startup. It caches the SQL statements internally for future use, thus avoiding any runtime costs of SQL generation forthe most co\u00aemmon operations. You've seen how you can override the R in CRUD, sonow, let's do the same for CUD. For each entity, you can define custom CUD SQL statements with the Hibernate annotations @SQLInsert, @SQLUpdat\u00aee, and @SQLDelete,respectivelyYou need to be careful with the binding of arguments for SQL statements and parameter placeholders (the ? in the statements). For CUD customization, Hibernate onlysupports p\u00aeositional parameters. What is the right order for the parameters? There's an internal order to howHibernate binds arguments to SQL parameters for CUD statements. The easiest way tofigure out the right SQL statement and parameter order is to let Hibernate generatean example for you. W\u00aeithout any custom SQL statements, enable DEBUG logging forthe org.hibernate.persister.entity category, and search the Hibernate startupoutput for lines like the following:These automatically generated SQL statements show the right parameter order, andHibernate always binds the values in that order. Copy the SQL statements you want\u00aetocustomize into annotations, and make the necessary changes. A special case is properties of an entity class mapped to another table with@SecondaryTable. The CUD customization statements we've shown so far only refer tothe columns of the main entity table. Hibernate still executes automatically generatedSQL statements for insertion, updates, and deletion of rows in the secondary table(s).",
  "page469": "You can customize this SQL by adding the @org.hibernate.annotations.Table annotation to your entity class and setting its sqlInsert, sqlUpdate, and sqlDelete attributes. If you pre\u00aefer to have your CUD SQL statements in XML, your only choice is to mapthe entire entity in a Hibernate XML metadata file. The elements in this proprietarymapping format for custom CUD statements are <sql-insert>, <sql-u\u00aepdate>, and<sql-delete>. Fortunately, CUD statements are usually much more trivial than queries, so annotations are fine in most applications. You've now added custom SQL statements for CRUD operati\u00aeons of an entityinstance. Next, we show how to override SQL statements loading and modifying acollection.Let's override the SQL stafortement Hibernate uses when loading the Item#images collection. This is a collection of embeddable components mapped with @ElementCollection; the proc\u00aeedure is the same for collections of basic types or many-valuedentity associations (@OneToMany or @ManyToMany)As before, you declare that a named query will load the collection. This time, however,you must declare and map the result of the query in a Hibernate XML metadata file,which is the only facility that supports mapping of q\u00aeuery results to collection properties:The query has to have one (positional or named) parameter. Hibernate sets its valueto the entity identifier that owns the collection. Whenever Hibernate need to initializethe Item#images collection, Hibernate now executes your custom SQL query.",
  "page470": "QUOTING SQL IDENTIFIERS- From time to time, especially in legacy databases, you'll encounter identifiers with strange characters or whitespace, or wish to force case sensitivi\u00aety. Or, as in the previous example, the automatic mapping of a class or property would require a table or column name that is a reserved keyword.  Hibernate 5 knows the reserved keywords of your DBMS through the\u00aeconfigured database dialect. Hibernate 5 can automatically put quotes around such strings when generating SQL. You can enable this automatic quoting with hibernate.auto_quote _keyword true in your persis\u00aetence unit configuration. If you're using an older version of Hibernate, or you find that the dialect's information is incomplete, you must still apply quotes on names manually in your mappings if there is a conflict with a keyword.  If you quote a table or column name in your\u00aemapping with backticks, Hibernate always quotes this identifier in the generated SQL. This still works in latest versions of Hibernate, but JPA 2.0 standardized this functionality as delimited identifiers with double quotes. If you have to quote all SQL identifiers, create an orm.xml file and add the setting <delimited-iden\u00aetifiers/> to its <persistence-unit-defaults> section, as shown in listing 3.8. Hibernate then enforces quoted identifiers everywhere.  You should consider renaming tables or columns with reserved keyword names whenever possible. Ad hoc SQL queries are difficult to write in an SQL console if you have to quote and escape everything properly by hand.ng names- Let's first talk about the naming of entity classes and tables. If you only specify @Entity on the persistence-capable class, the default mapped table name is the same as the class name. Note that we write SQL artifact names in UPPERCASE to make them easier to distinguish SQL is actually case insensitive. So the Java entity class Item maps to the ITEM table. You can override the table name with the JPA @Table annotation, as shown next.",
  "page471": "Entities are the coarser-grained classes of your system. Their instances have an independent life cycle and their own identity, and many other instances can reference them. \u00aeValue types, on the other hand, are dependent on a particular entity class. A value type instance is bound to its owning entity instance, and only one entity instance can reference it it has no individual identity. We\u00aelooked at Java identity, object equality, and database identity, and at what makes good primary keys. You learned which generators for primary key values Hibernate provides out of the box, and how to use\u00aeand extend this identifier system. We discussed some useful class mapping options, such as naming strategies and dynamic SQL generation. After spending the previous chapter almost exclusively on entities and the respective class- and identity-mapping options, we now focus on value\u00aetypes in their various forms. We split value types into two categories: basic value-typed classes that come with the JDK, such as String, Date, primitives, and their wrappers; and developerdefined value-typed classes, such as Address and MonetaryAmount in CaveatEmptor. In this chapter, we first map persistent properties wit\u00aeh JDK types and learn the basic mapping annotations. You see how to work with various aspects of properties: overriding defaults, customizing access, and generated values. You also see how SQL is used with derived properties and transformed column values. We wrap up basic properties with temporal properties and mapping enumerations. We then discuss custQL statements will be large for even the simplest operations (say, only one column needs updating), you should disable this startup SQL generation and switch to dynamic statements generated at runtime. An extremely large number of entities can also impact startup time, because Hibernate has to generate all SQL statements for CRUD up front. Memory consumption for this query statement cache will also be high if a dozen statements must be cached for thousands of entities. This can be an issue in virtual environments with memory limitations, or on low-power devices.  To disable generation of INSERT and UPDATE SQL statements on startup, you need native Hibernate annotations:",
  "page472": "This configuration by exception approach means you don't have to annotate a property to make it persistent; you only have to configure the mapping in an exceptional case. Sev\u00aeu0002eral annotations are available in JPA to customize and control basic property map. Overriding basic property defaults- You might not want all properties of an entity class to be persistent. For example, although it\u00aemakes sense to have a persistent Item#initialPrice property, an Item#totalPriceIncludingTax property shouldn't be persistent if you only compute and use its value at runtime, and hence should\u00aen't be stored in the database. To exclude a property, mark the field or the getter method of the property with the @javax.persistence.Transient annotation or use the Java transient keyword. The transient keyword usually only excludes fields for Java serialization, but it's also\u00aerecognized by JPA providers.  We'll come back to the placement of the annotation on fields or getter methods in a moment. Let's assume as we have before that Hibernate will access fields directly because @Id has been placed on a field. Therefore, all other JPA and Hibernate mapping annotations are also on fie\u00aelds. We have to admit that this annotation isn't very useful. It only has two parameters: the one shown here, optional, marks the property as not optional at the Java object level. By default, all persistent properties are nullable and optional; an Item may have an unknown initialPrice. Mapping the initialPrice property as non-opy isn't what you want, and you should always map Java classes instead of storing a heap of bytes in the database. Imagine maintaining a database with this binary information when the application is gone in a few years. Otherwise, Hibernate will throw an exception on startup, complaining that it doesn't understand the type of the property",
  "page473": "The Column annotation has a few other parameters, most of which control SQL-level details such as catalog and schema names. They're rarely needed, and we only show them throug\u00aehout this book when necessary.  Property annotations aren't always on fields, and you may not want Hibernate to access fields directly. Customizing property access- The persistence engine accesses the properties\u00aeof a class either directly through fields or indirectly through getter and setter methods. An annotated entity inherits the default from the position of the mandatory @Id annotation. For example, if you\u00aeu2019ve declared @Id on a field, not a getter method, all other mapping annotations for that entity are expected on fields. Annotations are never on the setter methods.  The default access strategy isn't only applicable to a single entity class. Any @Embedded class inherits the defa\u00aeult or explicitly declared access strategy of its owning root entity class. We cover embedded components later in this chapter. Furthermore, Hibernate accesses any @MappedSuperclass properties with the default or explicitly declared access strategy of the mapped entity class. Inheritance is the topic of chapter 6.  The\u00aeJPA specification offers the @Access annotation for overriding the default behavior, with the parameters AccessType.FIELD and AccessType.PROPERTY. If you set @Access on the class/entity level, Hibernate accesses all properties of the class according to the selected strategy. You then set any other mapping annotations, including the @Id, on either fields or gautomatically. The @Column annotation can also override the mapping of the property name to the database column:",
  "page474": "Hibernate reads your Java domain model classes and mapping metadata and generates schema DDL statements. You can export them into a text file or execute them directly on your datab\u00aease whenever you run integration tests. Because schema languages are mostly vendor-specific, every option you put in your mapping metadata has the potential to bind the metadata to a particular database product keep thi\u00aes in mind when using schema features. Hibernate creates the basic schema for your tables and constraints automatically; it even creates sequences, depending on the identifier generator you select. But t\u00aehere are some schema artifacts Hibernate can't and won't create automatically. These include all kinds of highly vendor-specific performance options and any other artifacts that are relevant only for the physical storage of data (tablespaces, for example). Besides these physical\u00aeconcerns, your DBA will often supply custom additional schema statements to improve the generated schema. DBAs should get involved early and verify the automatically generated schema from Hibernate. Never go into production with an unchecked automatically generated schema. If your development process allows, changes made by the D\u00aeBA can flow back into your Java systems, for you to add to mapping metadata. In many projects, the mapping metadata can contain all the necessary schema changes from a DBA. Then Hibernate can generate the final production schema during the regular build by including all comments, constraints, indexes, and so on. In the following sections, we show you how to customied scripts and write an improved and final schema for production deployment. The first part of this chapter shows you how to improve the schema from within JPA and Hibernate, to make your DBA happy. At the other end of the spectrum are systems with existing, possibly complex schemas, with years' worth of data. Your new application is just a small gear in a big machine, and your DBA won't allow any (sometimes even non-disruptive) changes to the database. You need a flexible object/relational mapping so you don't have to twist and bend the Java classes too much when things don't fit right away. This will be the subject of the second half of this chapter, including a discussion of composite primary and foreign keys. Let's start with a clean-room implementation and Hibernate-generated schemas.",
  "page475": "This property defines when the create and drop scripts should be executed. Your custom SQL scripts will contain CREATE DOMAIN statements, which must be executed before the tables u\u00aesing these domains are created. With these settings, the schema generator runs the create script first before reading your ORM metadata (annotations, XML files) and creating the tables. The drop script executes after Hi\u00aebernate drops the tables, giving you a chance to clean up anything you created. Other options are metadata (ignore custom script sources) and script (only use a custom script source; ignore ORM metadata\u00aein annotations and XML files). D This is the location of the custom SQL script for creation of the schema. The path is (a) the location of the script resource on the classpath; (b) the location of the script as a file:// URL; or, if neither (a) nor (b) matches, (c) the absolute or relativ\u00aee file path on the local file system. This example uses (a). E This is the custom SQL script for dropping the schema. F This load script runs after the tables have been created. We've mentioned that DDL is usually highly vendor-specific. If your application has to support several database dialects, you may need several sets o\u00aef create/drop/load scripts to customize the schema for each database dialect. You can solve this with several persistence unit configurations in the persistence.xml file. Alternatively, Hibernate has its own proprietary configuration for schema customization in an hbm.xml mapping file, You can hook the following three types of custom SQL scripts",
  "page476": "Table constraints An integrity rule that applies to several columns or several rows is a table constraint. A typical declarative table constraint is UNIQUE: all rows are checked fo\u00aer duplicate values (for example, each user must have a distinct email address). A rule affecting only a single row but multiple columns is \"the auction end time has to be after the auction start time.\" Databas\u00aee constraints If a rule applies to more than one table, it has database scope. You should already be familiar with the most common database constraint, the foreign key. This rule guarantees the integrity\u00aeof references between rows, usually in separate tables, but not always (self-referencing foreign key constraints aren't uncommon). Other database constraints involving several tables aren't uncommon: for example, a bid can only be stored if the auction end time of the reference\u00aed item hasn't been reached. Most (if not all) SQL database management systems support these kinds of constraints and the most important options of each. In addition to simple keywords such as NOT NULL and UNIQUE, you can usually also declare more complex rules with the CHECK constraint that applies an arbitrary SQL expression\u00ae. Still, integrity constraints are one of the weak areas in the SQL standard, and solutions from vendors can differ significantly. Furthermore, non-declarative and procedural constraints are possible with database triggers that intercept data-modification operations. A trigger can then implement the constraint procedure directly or calCHAR data type can hold character strings: for example, all characters defined in ASCII or some other encoding. Because we mostly use data types built-in the DBMS, we rely on the domain constraints as defined by the vendor. If supported by your SQL database, you can use the (often limited) support for custom domains to add additional constraints for particular existing data types, or create user-defined data types (UDT). Column constraints Restricting a column to hold values of a particular domain and type creates a column constraint. For example, you declare in the schema that the EMAIL column holds values of VARCHAR type. Alternatively, you could create a new domain called EMAIL_ADDRESS with further constraints and apply it to a column instead of VARCHAR. A special column constraint in an SQL database is NOT NULL.",
  "page477": "This constraint restricts valid username values to a maximum length of 15 characters, and the string can't begin with admin to avoid confusion. You can call any SQL functions\u00aesupported by your DBMS; the column Definition is always passed through into the exported schema. Note that you have a choice: creating and using a domain or adding a single column constraint has the same effect. Domain\u00aes are usually easier to maintain and avoid duplicating. At the time of writing, Hibernate doesn't support its proprietary annotation @org.hibernate.annotations.Check on individual properties; you u\u00aese it for table level constraints. Hibernate appends table constraints to the generated CREATE TABLE statement, which can contain arbitrary SQL expressions. You can implement multirow table constraints with expressions that are more complex. You may need a sub select in the expression to\u00aedo this, which may not be supported by your DBMS. But there are some common multirow table constraints, like UNIQUE, that you can add directly in the mappings. You've already seen the @Column(unique true | false) option in the previous section. Now all pairs of USERNAME and EMAIL must be unique, for all rows in the USERS tab\u00aele. If you don't provide a name for the constraint here, UNQ_USERNAME_EMAIL an automatically generated and probably ugly name is used. The last kinds of constraints we discuss are database-wide rules that span several tables. Hibernate passes on database constraint violations in error exceptions; check whether the exception in your transaction has a caustly, so be careful with database-specific SQL",
  "page478": "This statement declares the foreign key constraint for the ITEM_ID column in the BID table, referencing the primary key column of the ITEM table. You can customize the name of the\u00aeconstraint with the foreign Key option in an @JoinColumn mapping A foreign Key attribute is also supported in @PrimaryKeyJoinColumn, @MapKeyJoinColumn, @JoinTable, @CollectionTable, and @AssociationOverride mappings. T\u00aehe @ForeignKey annotation has some rarely needed options we haven't shown: You can write your own foreign Key Definition, an SQL fragment such as FOREIGN KEY ([column]) REFERENCES [table]([column])\u00aeON UPDATE [action]. Hibernate will use this SQL fragment instead of the provider-generated fragment, it can be in the SQL dialect supported by your DBMS. The Constraint Mode setting is useful if you want to disable foreign key generation completely, with the value NO_CONSTRAINT. You can\u00aethen write the foreign key constraint yourself with an ALTER TABLE statement, probably in a load script as we've shown. Naming constraints properly is not only good practice, but also helps significantly when you have to read exception messages. This completes our discussion of database integrity rules. Next, we look at some\u00aeoptimization you might want to include in your schema for performance reasons. Indexes are a key feature when optimizing the performance of a database application. The query optimizer in a DBMS can use indexes to avoid excessive scans of the data tables. Because they're relevant only in the physical implementation of a database, indexes aren'ntial integrity rules. They're widely known as foreign keys, which are a combination of two things: a key value copy from a related row and a constraint that guarantees that the referenced value exists. Hibernate creates foreign key constraints automatically for all foreign key columns in association mappings. If you check the schema produced by Hibernate, you'll notice that these constraints also have automatically generated database identifiers names that aren't easy to read and that make debugging more difficult. You see this kind of statement in the generated schema",
  "page479": "If you encounter a USERS table in a legacy schema, it's likely that USERNAME is the primary key. In this case, you have no surrogate identifier that Hibernate generates automa\u00aetically. Instead, your application has to assign the identifier value when saving an instance of the User class Hibernate calls the protected no-argument constructor when it loads a User from the database and then assig\u00aens the username field value directly. When you instantiate a User, call the public constructor with a username. If you don't declare an identifier generator on the @Id property, Hibernate expects th\u00aee application to take care of the primary key value assignment. Composite (natural) primary keys require a bit more work Suppose the primary key of the USERS table is a composite of the two columns USERNAME and DEPARTMENTNR. You write a separate composite identifier class that declares j\u00aeust the key properties and call this class User Id This class has to be @Embeddable and Serializable any type used as an identifier type in JPA must be Serializable. C You don't have to mark the properties of the composite key as @NotNull; their database columns are automatically NOT NULL when embedded as the primary key of a\u00aen entity. D The JPA specification requires a public no-argument constructor for an embeddable identifier class. Hibernate accepts protected visibility. E The only public constructor should have the key values as arguments. F You have to override the equals() and hashCode() methods with the same semantics the composite key has in your database. In this case, thmpact performance. Unfortunately, many legacy schemas use (natural) composite keys heavily; and for the reason we discourage the use of composite keys, it may be difficult to change the legacy schema to use non-composite natural or surrogate keys. Therefore, JPA supports natural and composite primary and foreign keys.",
  "page480": "We've already shown the @SecondaryTable annotation in an inheritance mapping in section 6.5. It helped to break out properties of a particular subclass into a separate table.\u00aeThis generic functionality has more uses but be aware that a properly designed system should have, simplified, more classes than tables. Suppose that in a legacy schema, you aren't keeping a user's billing ad\u00aedress information with the other user details in the USERS main entity table, but in a separate table. Figure 9.5 shows this schema. The user's home address is stored in the columns STREET, ZIPCODE,\u00aeand CITY of the USERS table. The user's billing address is stored in the BILLING_ADDRESS table, which has the primary key column USER_ID, which is also a foreign key constraint referencing the ID primary key of the USERS table The User class has two properties of embedded type: hom\u00aee Address and billing Address. The first is a regular embedded mapping, and the Address class is @Embeddable. Just as in section 5.2.3, you can use the @AttributeOverrides annotation to override the mapping of embedded properties. Then, @Column maps the individual properties to the BILLING_ADDRESS table, with its table option. Re\u00aemember that an @AttributeOverride replaces all mapping information for a property: any annotations on the Address fields are ignored if you override. Therefore, you have to specify nullability and length again in the @Column override. We've shown you a secondary table mapping example with an embeddable property. Of course, you could also break out simple basicName attribute of @JoinColumn to declare this relationship. Hibernate now knows that the referenced target column is a natural key, and not the primary key, and manages the foreign key relationship accordingly. If the target natural key is a composite key, use @JoinColumns instead as in the previous section. Fortunately, it's often straightforward to clean up such a schema by refactoring foreign keys to reference primary keys if you can make changes to the database that don't disturb other applications sharing the data. This completes our discussion of natural, composite, and foreign key-related problems you may have to deal with when you try to map a legacy schema. Let's move on to another interesting special strategy",
  "page481": "strategies for runtime data management. These strategies are crucial to the performance and correct behavior of your applications. In this chapter, we discuss the life cycle of en\u00aetity instances how an instance becomes persistent, and how it stops being considered persistent and the method calls and management operations that trigger these transitions. The JPA Entity Manager is your primary inter\u00aeface for accessing data. Before we look at the API, let's start with entity instances, their life cycle, and the events that trigger a change of state. Although some of the material may be formal,\u00aea solid understanding of the persistence life cycle is essential. Because JPA is a transparent persistence mechanism classes are unaware of their own persistence capability it's possible to write application logic that's unaware whether the data it operates on represents persist\u00aeent state or temporary state that exists only in memory. The application shouldn't necessarily need to care that an instance is persistent when invoking its methods. You can, for example, invoke the Item #calculateTotalPrice() business method without having to consider persistence at all (for example, in a unit test). Any ap\u00aeplication with persistent state must interact with the persistence service whenever it needs to propagate state held in memory to the database (or vice versa). In other words, you have to call the Java Persistence interfaces to store and load data. When interacting with the persistence mechanism that way, the application must concern itself with tning to and intercepting events, auditing and versioning with Hibernate Envers, and filtering data",
  "page482": "A persistent entity instance has a representation in the database. It's stored in the database or it will be stored when the unit of work completes. It's an instance with\u00aea database identity, as defined in section 4.2; its database identifier is set to the primary key value of the database representation. The application may have created instances and then made them persistent by calli\u00aeng Entity Manager #persist(). There may be instances that became persistent when the application created a reference to the object from another persistent instance that the JPA provider already manages.\u00aeA persistent entity instance may be an instance retrieved from the database by execution of a query, by an identifier lookup, or by navigating the object graph starting from another persistent instance. Persistent instances are always associated with a persistence context. You see more\u00aeabout this in a moment You can delete a persistent entity instance from the database in several ways: For example, you can remove it with Entity Manager #remove(). It may also become available for deletion if you remove a reference to it from a mapped collection with orphan removal enabled. An entity instance is then in the remov\u00aeed state: the provider will delete it at the end of a unit of work. You should discard any references you may hold to it in the application after you finish working with it for example, after you've rendered the removal confirmation screen your users see Think of the persistence context as a service that remembers all the modifications and state changes yst like new Long() and new Big Decimal(). Hibernate doesn't provide any rollback functionality for transient instances; if you modify the price of a transient Item, you can't automatically undo the change. For an entity instance to transition from transient to persistent state, to become managed, requires either a call to the Entity Manager #persist() method or the creation of a reference from an already-persistent instance and enabled cascading of ",
  "page483": "The persistence context acts as a first-level cache; it remembers all entity instances you've handled in a particular unit of work. For example, if you ask Hibernate to load a\u00aen entity instance using a primary key value (a lookup by identifier), Hibernate can first check the current unit of work in the persistence context. If Hibernate finds the entity instance in the persistence context, no\u00aedatabase hit occurs this is a repeatable read for an application. Consecutive em.find(Item. Class, ITEM_ID) calls with the same persistence context will yield the same result. This cache also affects re\u00aesults of arbitrary queries, executed for example with the javax.persistence.Query API. Hibernate reads the SQL result set of a query and transforms it into entity instances. This process first tries to resolve every entity instance in the persistence context by identifier lookup. Only if\u00aean instance with the same identifier value can't be found in the current persistence context does Hibernate read the rest of the data from the result-set row. Hibernate ignores any potentially newer data in the result set, due to read-committed transaction isolation at the database level, if the entity instance is already pre\u00aesent in the persistence context. The persistence context cache is always on it can't be turned off. It ensures the following The persistence layer isn't vulnerable to stack overflows in the case of circular references in an object graph. There can never be conflicting representations of the same database row at the end of a unit of work. The provider can ase the state of instances monitored by a persistence context, either automatically or on demand. Typically, when a unit of work completes, the provider propagates state held in memory to the database through the execution of SQL INSERT, UPDATE, and DELETE statements (all part of the Data Modification Language [DML]) This flushing procedure may also occur at other times. For example, Hibernate may synchronize with the database before execution of a query. This ensures that queries are aware of changes made earlier during the unit of work.",
  "page484": "Hibernate joins the in-progress system transaction when an Entity Manager is created and waits for the transaction to commit. When Hibernate is notified (by the JTA engine) of the\u00aecommit, it performs dirty checking of the persistence context and synchronizes with the database. You can also force dirty checking synchronization manually by calling EntityManager#flush() at any time during a transact\u00aeion. You decide the scope of the persistence context by choosing when to close() the Entity Manager. You have to close the persistence context at some point, so always place the close() call in a finall\u00aey block. How long should the persistence context be open? Let's assume for the following examples that you're writing a server, and each client request will be processed with one persistence context and system transaction in a multithreaded environment. If you're familiar\u00aewith servlets, imagine the code in listing 10.1 embedded in a servlet's service() method. Within this unit of work, you access the Entity Manager to load and store data. A new transient Item is instantiated as usual. Of course, you may also instantiate it before creating the EntityManager. A call to persist() makes the transi\u00aeent instance of Item persistent. It's now managed by and associated with the current persistence context. To store the Item instance in the database, Hibernate has to execute an SQL INSERT statement. When the transaction of this unit of work commits, Hibernate flushes the persistence context, and the INSERT occurs at that time. Hibernate may write empty catch clauses in your code, though you'll have to roll back the transaction and handle exceptions. Creating an Entity Manager starts its persistence context. Hibernate won't access the database until necessary; the Entity Manager doesn't obtain a JDBC Connection from the pool until SQL statements have to be executed. You can create and close an Entity Manager without hitting the database. Hibernate executes SQL statements when you look up or query data and when it flushes changes detected by the persistence context to the database. Hibernate joins the in-progress system transaction when an Entity Manager is created and waits for the transaction to commit. When Hibernate is notified (by the JTA engine) of the commit, it performs dirty checking of the persistence context and synchronizes with the database.",
  "page485": "You can modify the Item instance, and the persistence context will detect these changes and record them in the database automatically. When Hibernate flushes the persistence contex\u00aet during commit, it executes the necessary SQL DML statements to synchronize the changes with the database. Hibernate propagates state changes to the database as late as possible, toward the end of the transaction. DML\u00aestatements usually create locks in the database that are held until the transaction completes, so Hibernate keeps the lock duration in the database as short as possible. Hibernate writes the new Item#na\u00aeme to the database with an SQL UPDATE. By default, Hibernate includes all columns of the mapped ITEM table in the SQL UPDATE statement, updating unchanged columns to their old values. Hence, Hibernate can generate these basic SQL statements at startup, not at runtime. If you want to inclu\u00aede only modified (or non-nullable for INSERT) columns in SQL statements, you can enable dynamic SQL generation as discussed in section 4.3.2. Hibernate detects the changed name by comparing the Item with a snapshot copy it took before, when the Item was loaded from the database. If your Item is different from the snapshot, an UPD\u00aeATE is necessary. This snapshot in the persistence context consumes memory. Dirty checking with snapshots can also be time consuming, because Hibernate has to compare all instances in the persistence context with their snapshot during flushing. It's better (but not required) to fully initialize the Item instance before managing it with a persi9s a generic method, and its return type is set as a side effect of the first parameter. The retrieved entity instance is in persistent state, and you can now modify it inside the unit of work. If no persistent instance with the given identifier value can be found, find() returns null. The find() operation always hits the database if there was no hit for the",
  "page486": "As soon as you call any method such as Item#getName() on the proxy, a SELECT is executed to fully initialize the placeholder. The exception to this rule is a mapped database identi\u00aefier getter method, such as getId(). A proxy may look like the real thing, but it's only a placeholder carrying the identifier value of the entity instance it represents. If the database record no longer exists whe\u00aen the proxy is initialized, an EntityNotFoundException is thrown. Note that the exception can be thrown when Item#getName() is called. E Hibernate has a convenient static initialize() method that loads t\u00aehe proxy's data. F After the persistence context is closed, item is in detached state. If you don't initialize the proxy while the persistence context is still open, you get a LazyInitializationException if you access the proxy. You can't load data on demand once the persis\u00aetence context is closed. The solution is simple: load the data before you close the persistence context. We'll have much more to say about proxies, lazy loading, and on-demand fetching in chapter 12. If you want to remove the state of an entity instance from the database, you have to make it transient. To make an entity ins\u00aetance transient and delete its database representation, call the remove() method on the EntityManager If you call find(), Hibernate executes a SELECT to load the Item. If you call getReference(), Hibernate attempts to avoid the SELECT and returns a proxy. C Calling remove() queues the entity instance for deletion when the unit of work completes; it's now ference() without hitting the database. Furthermore, if no persistent instance with that identifier is currently managed, Hibernate produces a hollow placeholder: a proxy. This means getReference() won't access the database, and it doesn't return null, unlike find(). C JPA offers PersistenceUnitUtil helper methods such as isLoaded() to detect whether you're working with an uninitialized proxy.",
  "page487": "Java Persistence also offers bulk operations that translate into direct SQL DELETE statements without life cycle interceptors in the application. We'll discuss these operation\u00aes in section 20.1. Let's say you load an entity instance from the database and work with the data. For some reason, you know that another application or maybe another thread of your application has updated the und\u00aeerlying row in the database. Next, we'll see how to refresh the data held in memory. After you load the entity instance, you realize (how isn't important) that someone else changed the data in\u00aethe database. Calling refresh() causes Hibernate to execute a SELECT to read and marshal a whole result set, overwriting changes you already made to the persistent instance in application memory. If the database row no longer exists (someone deleted it), Hibernate throws an EntityNotFound\u00aeException on refresh(). Most applications don't have to manually refresh in-memory state; concurrent modifications are typically resolved at transaction commit time. The best use case for refreshing is with an extended persistence context, which might span several request/response cycles and/or system transactions. While yo\u00aeu wait for user input with an open persistence context, data gets stale, and selective refreshing may be required depending on the duration of the conversation and the dialogue between the user and the system. Refreshing can be useful to undo changes made in memory during a conversation, if the user cancels the dialogue. We'll have more lue after removal of an entity instance. In the previous code example, the identifier value is reset to the default value of null (it's a Long). The Item is now the same as in transient state, and you can save it again in a new persistence context.",
  "page488": "The persistence context is a cache of persistent instances. Every entity instance in persistent state is associated with the persistence context. Many Hibernate users who ignore t\u00aehis simple fact run into an OutOfMemoryException. This is typically the case when you load thousands of entity instances in a unit of work but never intend to modify them. Hibernate still has to create a snapshot of eac\u00aeh instance in the persistence context cache, which can lead to memory exhaustion. (Obviously, you should execute a bulk data operation if you modify thousands of rows we'll get back to this kind of\u00aeunit of work in section 20.1.) The persistence context cache never shrinks automatically. Keep the size of your persistence context to the necessary minimum. Often, many persistent instances in your context are there by accident for example, because you needed only a few items but querie\u00aed for many. Extremely large graphs can have a serious performance impact and require significant memory for state snapshots. Check that your queries return only data you need, and consider the following ways to control Hibernate's caching behavior. You can call EntityManager#detach(i) to evict a persistent instance manually\u00aefrom the persistence context. You can call EntityManager#clear() to detach all persistent entity instances, leaving you with an empty persistence context. The native Session API has some extra operations you might find useful. You can set the entire persistence context to read-only mode. This disables state snapshots and dirty checking, and Hibernate won't write modnt databases. An example case is a product upgrade: if the new version of your application requires a new database (schema), you may want to migrate and replicate the existing data once. The persistence context does many things for you: automatic dirty checking, guaranteed scope of object identity, and so on. It's equally important that you know some of the details of its management, and that you sometimes influence what goes on behind the scenes. ",
  "page489": "Next, we look at the detached entity state. We already mentioned some issues you'll see when entity instances aren't associated with a persistence context anymore, such a\u00aes disabled lazy initialization. Let's explore the detached state with some examples, so you know what to expect when you work with data outside of a persistence context. If a reference leaves the scope of guarante\u00aeed identity, we call it a reference to a detached entity instance. When the persistence context is closed, it no longer provides an identity-mapping service. You'll run into aliasing problems when y\u00aeou work with detached entity instances, so make sure you understand how to handle the identity of detached instances. If you look up data using the same database identifier value in the same persistence context, the result is two references to the same in-memory instance on the JVM heap.\u00aeConsider the two units of work shown next. In the first unit of work at begin() B, you start by creating a persistence context C and loading some entity instances. Because references a and b are obtained from the same persistence context, they have the same Java identity D. They're equal from the same persistence context, they hav\u00aee the same Java identity D. They're equal E because by default equals() relies on Java identity comparison. They obviously have the same database identity F. They reference the same Item instance, in persistent state, managed by the persistence context for that unit of work. The first part of this example finishes by committing the transactiotabase by calling EntityManager#flush(). This concludes our discussion of the transient, persistent, and removed entity states, and the basic usage of the EntityManager API. Mastering these state transitions and API methods is essential; every JPA application is built with these operations.",
  "page490": "When you begin a journey, it's a good idea to have a mental map of the terrain you'll be passing through. The same is true for an intellectual journey, such as learning t\u00aeo write computer programs. In this case, you'll need to know the basics of what computers are and how they work. You'll want to have some idea of what a computer program is and how one is created. Since you wi\u00aell be writing programs in the Java programming language, you'll want to know something about that language in particular and about the modern, networked computing environment for which Java is desig\u00aened. As you read this chapter, don't worry if you can't understand everything in detail. (In fact, it would be impossible for you to learn all the details from the brief expositions in this chapter.) Concentrate on learning enough about the big ideas to orient yourself, in prepa\u00aeration for the rest of the book. Most of what is covered in this chapter will be covered in much greater detail later in the book. A computer is a complex system consisting of many different components. But at the heart or the brain, if you want of the computer is a single component that does the actual computing. This is the Cent\u00aeral Processing Unit, or CPU. In a modern desktop computer, the CPU is a single \"chip\" on the order of one square inch in size. The job of the CPU is to execute programs. A program is simply a list of unambiguous instructions meant to be followed mechanically by a computer. A computer is built to carry out instructions that are written in a very simplchine language instructions in main memory. It does this by repeatedly reading, or fetching, an instruction from memory and then carrying out, or executing, that instruction is to be executed in the next cycle. A computer executes machine language programs mechanically that is without understanding them or thinking about them simply because of the way it is physically put together. This is not an easy concept. A computer is a machine built of millions of tiny switches called transistors, which have the property that they can be wired together in such a way that an output from one switch can turn another switch on or off. As a computer computes, these switches turn each other on or off in a pattern determined both by the way they are wired together and by the program that the computer is executing.",
  "page491": "Machine language instructions are expressed as binary numbers. A binary number is made up of just two possible digits, zero and one. So, a machine language instruction is just a se\u00aequence of zeros and ones. Each particular sequence encodes some particular instruction. The data that the computer manipulates is also encoded as binary numbers. A computer can work directly with binary numbers because\u00aeswitches can readily represent such numbers: Turn the switch on to represent a one; turn it off to represent a zero. Machine language instructions are stored in memory as patterns of switches turned on o\u00aer off, When a machine language instruction is loaded into CPU, all that happens is that certain switches are turned on or off in the pattern that encodes that particular instruction. The CPU is built to respond to this pattern by executing the instruction it encodes; it does this simply\u00aebecause of the way all the other switches in the CPU are wired together. So, you should understand this much about how computers work: Main memory holds machine language programs and data. These are encoded as binary numbers. The CPU fetches machine language instructions from memory one after another and executes them. It does this\u00aemechanically, without thinking about or understanding what it does-and therefore the program it executes must be perfect, complete in all details, and unambiguous because the CPU can do nothing but execute it exactly as written. Here is a schematic view of this first-stage understanding of the computer Asynchronous Events: Polling Loops and Interrupts The CPU spendere is a device driver, which consists of software that the CPU executes when it has to deal with the device. Installing a new device on a system generally has two steps: plugging waiting for input. To avoid this inefficiency, interrupts are often used instead of polling. An interrupt is a signal sent by another device to the CPU. The CPU responds to an interrupt signal by putting aside whatever it is doing in order to respond to the interrupt. Once it has handled the interrupt, it returns to what it was doing before the interrupts occurred. For example, when you press a key on your computer keyboard, a keyboard interrupt is sent to the CPU. The CPU responds to this signal by interrupting what it is doing, reading the key that you pressed, processing it, and returning to the task it was performing before you pressed the key.",
  "page492": "Again, you should understand that this is a purely mechanical process: A device signals an interrupt simply by turning on a wire. The CPU is built so that when that wire is turned\u00aeon, the CPU saves enough information about what it is currently doing so that it can return to the same state later. This information consists of the contents of important internal registers such as the program counter.\u00aeThen the CPU jumps to some predetermined memory location and begins executing the instructions stored there. Those instructions make up an interrupt handler that does the processing necessary to respond\u00aeto the interrupt. (This interrupt handler is part of the device driver software for the device that signalled the interrupt.) At the end of the interrupt handler is an instruction that tells the CPU to jump back to what it was doing; it does that by restoring its previously saved state.\u00aeInterrupts allow the CPU to deal with asynchronous events. In the regular fetch-and execute cycle, things happen in a predetermined order; everything that happens is \"synchronized\" with everything else. Interrupts make it possible for the CPU to deal efficiently with events that happen \"asynchronously,\" that is\u00ae, at unpredictable times. As another example of how interrupts are used, consider what happens when the CPU need to access data that is stored on the hard disk. The CPU can access data directly only if it is in main memory. Data on the disk has to be copied into memory before it can be accessed. Unfortunately, on the scale of speed at which the CPU operates, the disalled a thread. (Or a process; there are technical differences between threads and processes, but they are not important here.) At any given time, only one thread can actually be",
  "page493": "The Java Virtual Machine Machine language consists of very simple instructions that can be executed directly by the CPU of a computer. Almost all programs, though, are written in h\u00aeigh-level programming languages such as Java, Pascal, or C++. A program written in a high-level language cannot be run directly on any computer. First, it has to be translated into machine language. This translation can\u00aebe done by a program called a compiler. A compiler takes a high-level-language program and translates it into an executable machine-language program. Once the translation is done, the machine-language p\u00aerogram can be run any number of times, but of course it can only be run on one type of computer (since each type of computer has its own individual machine language). If the program is to run on another type of computer it has to be re-translated, using a different compiler, into the appr\u00aeopriate machine language. There is an alternative to compiling a high-level language program. Instead of using a compiler, which translates the program all at once, you can use an interpreter, which translates it instruction-by-instruction, as necessary. An interpreter is a program that acts much like a CPU, with a kind of fetch-a\u00aend-execute cycle. In order to execute a program, the interpreter runs in a loop in which it repeatedly reads one instruction from the program, decides what is necessary to carry out that instruction, and then performs the appropriate machine-language commands to do so. one use of interpreters is to execute high-level language programs. For example, the programming lputer. However, one of the main selling points of Java is that it can actually be used on any computer. All that the computer needs is an interpreter for Java bytecode. Such an in that there is no necessary connection between Java and Java bytecode. A program written in Java could certainly be compiled into the machine language of a real computer. And programs written in other languages could be compiled into Java bytecode. However, it is the combination of Java and Java bytecode that is platform-independent, secure, and network compatible while allowing you to program in a modern high-level object-oriented language. I should also note that the really hard part of platform-independence is providing a \"Graphical User Interface\" with windows, buttons, etc. that will work on all the platforms that support Java.",
  "page494": "Programs must be designed. No one can just sit down at the computer and compose a program of any complexity. The discipline called software engineering is concerned with the constr\u00aeuction of correct, working, well-written programs. The software engineer tends to use accepted and proven methods for analyzing the problem to be solved and for designing a program to solve that problem. During the 1970\u00aes and into the 80s, the primary software engineering methodology was structured programming. The structured programming approach to program design was based on the following advice: To solve a large prob\u00aelem, break the problem into several pieces and work on each piece separately; to solve each piece, treat it as a new problem which can itself be broken down into smaller problems; eventually, you will work your way down to problems that can be solved directly, without further decompositio\u00aen. This approach is called top-down programming. There is nothing wrong with top-down programming. It is a valuable and often-used approach to problem-solving. However, it is incomplete. For one thing, it deals almost entirely with producing the instructions necessary to solve a problem. But as time went on, people realized that t\u00aehe design of the data structures for a program was as least as important as the design of subroutines and control structures. Top-down programming doesn't give adequate consideration to the data that the program manipulates. Another problem with strict top-down programming is that it makes it difficult to reuse work done for other projects. By starting with a particutware modules is to contain some data, along with some subroutines for manipulating that data. For example, a mailing-list module might contain a list of names and addresses alonghat those objects should respond to.",
  "page495": "For example, consider a drawing program that lets the user draw lines, rectangles, ovals, polygons, and curves on the screen. In the program, each visible object on the screen coul\u00aed be represented by a software object in the program. There would be five classes of objects in the program, one for each type of visible object that can be drawn. All the lines would belong to one class, all the rectan\u00aegles to another class, and so on. These classes are obviously related; all of them represent \"drawable objects.\" They would, for example, all presumably be able to respond to a \"draw yours\u00aeelf\" message. Another level of grouping, based on the data to represent each type of object, is less obvious, but would be very useful in a program: We can group polygons and curves together as \"multipoint objects,\" while lines, rectangles, and ovals are \"two-point ob\u00aejects.\" (A line is determined by its endpoints, a rectangle by two of its corners, and an oval by two corners of the rectangle that contains it.) We could diagram these relationships as follows DrawableObject, MultipointObject, and TwoPointObject would be classes in the program. MultipointObject and TwoPointObject would be su\u00aebclasses of DrawableObject. The class Line would be a subclass of TwoPointObject and (indirectly) of DrawableObject. A subclass of a class is said to inherit the properties of that class. The subclass can add to its inheritance and it can even \"override\" part of that inheritance (by defining a different response to some method). Nevertheless, lines, rectangles, the computer types back its response. Early personal computers also used typed commands and responses, except that there was only one person involved at a time. This type ot area when the number of lines of text becomes larger than will fit in the text area. And in fact, in Java terminology, the whole applet is itself considered to be a component.",
  "page496": "The use of the term \"message\" here is deliberate. Messages, as you saw in the previous section, are sent to objects. In fact, Java GUI components are implemented as objec\u00aets. Java includes many predefined classes that represent various types of GUI components. Some of these classes are subclasses of others. Here is a diagram showing some of Swing's GUI classes and their relationship\u00aes. Don't worry about the details for now, but try to get some feel about how object-oriented programming and inheritance are used here. Note that all the GUI classes are subclasses, directly or indi\u00aerectly, of a class called JComponent, which represents general properties that are shared by all Swing components. Two of the direct subclasses of JComponent themselves have subclasses. The classes JTextArea and JTextField, which have certain behaviors in common, are grouped together as s\u00aeubclasses of JTextComponent. Similarly JButton and JToggleButton are subclasses of JAbstractButton, which represents properties common to both buttons and checkboxes. (JComboBox, by the way, is the Swing class that represents pop-up menus.) Just from this brief discussion, perhaps you can see how GUI programming can make effective\u00aeuse of object-oriented design. In fact, GUI's, with their \"visible objects,\" are probably a major factor contributing to the popularity of OOP. The Internet and the World-Wide Web - Computers can be connected together on networks. A computer on a network can communicate with other computers on the same network by exchanging data and files or by sendi some data being sent from one computer to another, along with addressing information that indicates where on the Internet that data is supposed to go. Think of a packet as an envd-Wide Web. Each service has its own protocols, which are used to control transmission of data over the network. Each service also has some sort of user interface, which allows the user to view, send, and receive data through the service.",
  "page497": "On a basic level (the level of machine language), a computer can perform only very simple operations. A computer performs complex tasks by stringing together large numbers of such\u00aeoperations. Such tasks must be \"scripted\" in complete and perfect detail by programs. Creating complex programs will never be really easy, but the difficulty can be handled to some extent by giving the program\u00aea clear overall structure. The design of the overall structure of a program is what I call \"programming in the large.\" Programming in the small, which is sometimes called coding, would then re\u00aefer to filling in the details of that design. The details are the explicit, step-by-step instructions for performing fairly small-scale tasks. When you do coding, you are working fairly \"close to the machine,\" with some of the same concepts that you might use in machine language\u00ae: memory locations, arithmetic operations, loops and branches. In a high-level language such as Java, you get to work with these concepts on a level several steps above machine language. However, you still have to worry about getting all the details exactly right. This chapter and the next examine the facilities for programming in\u00aethe small in the Java programming language. Don't be misled by the term \"programming in the small\" into thinking that this material is easy or unimportant. This material is an essential foundation for all types of programming. If you don't understand it, you can't write programs, no matter how good you get at designing their large-scaect program is one that does what you want it to. Furthermore, a program can be syntactically and semantically correct but still be a pretty bad program. Using the language correcter to do this is really a big first step in learning a new programming language (especially if it's your first programming language). It means that you understand the basic process of: 1. getting the program text into the computer,",
  "page498": "Here is a Java program to display the message \"Hello World!\". Don't expect to understand what's going on here just yet some of it you won't really understa\u00aend until a few chapters from now: // A program to display the message // \\\"Hello World!\\\" on standard output public class HelloWorld { public static void main(String[] args) { System.out.println(\\\"Hello World!\\\"\u00ae); } } // end of class HelloWorld The command that actually displays the message is: System.out.println(\\\"Hello World!\\\"); This command is an example of a subroutine call statement. It uses a \"b\u00aeuilt-in subroutine\" named System.out.println to do the actual work. Recall that a subroutine consists of the instructions for performing some task, chunked together and given a name. That name can be used to \"call\" the subroutine whenever that task needs to be performed. A\u00aebuilt-in subroutine is one that is already defined as part of the language and therefore automatically available for use in any program. When you run this program, the message \"Hello World!\" (without the quotes) will be displayed on standard output. Unfortunately, I can't say exactly what that means! Java is meant t\u00aeo run on many different platforms, and standard output will mean different things on different platforms. However, you can expect the message to show up in some convenient place. (If you use a command-line interface, like that in Sun Microsystem's Java Development Kit, you type in a command to tell the computer to run the program. The computer will he main() subroutine, and the statements that it contains are executed. These statements make up the script that tells the computer exactly what to do when the program is executedother file named HelloWorld.class will be produced. This class file, HelloWorld.class, contains the Java bytecode that is executed by a Java interpreter. HelloWorld.java is called the source code for the program. To execute the part of the syntax or semantics of the language. The computer doesn't care about layout you could run the entire program together on one line as far as it is concerned. However, layout is important to human readers, and there are certain style guidelines for layout that are followed by most programmers. These style guidelines are part of the pragmatics of the Java programming language.",
  "page499": "Variables and the Primitive Types - Names are fundamental to programming. In programs, names are used to refer to many different sorts of things. In order to use those things, a pr\u00aeogrammer must understand the rules for giving names to things and the rules for using the names to work with those things. That is, the programmer must understand the syntax and the semantics of names. According to the\u00aesyntax rules of Java, a name is a sequence of one or more characters. It must begin with a letter or underscore and must consist entirely of letters, digits, and underscores. (\"Underscore\" refe\u00aers to the character ' '.) For example, here are some legal names: No spaces are allowed in identifiers; HelloWorld is a legal identifier, but \"Hello World\" is not. Upper case and lower case letters are considered to be different, so that HelloWorld, helloworld, HELLOWO\u00aeRLD, and hElloWorLD are all distinct names. Certain names are reserved for special uses in Java, and cannot be used by the programmer for other purposes. These reserved words include: class, public, static, if, else, while, and several dozen other words. Java is actually pretty liberal about what counts as a letter or a digit. Jav\u00aea uses the Unicode character set, which includes thousands of characters from many different languages and different alphabets, and many of these characters count as letters or digits. However, I will be sticking to what can be typed on a regular English keyboard. The pragmatics of naming includes style guidelines about how to choose names for things. For example, it is cimple or compound that can be used to refer to something in Java. (Note that the reserved words are not identifiers, since they can't be used as names for things.) Variables mean that she wants to fill the office, not that she wants to be George Bush.) In Java, the only way to get data into a variable that is, into the box that the variable names is with an assignment statement.",
  "page500": "Types and Literals- A variable in Java is designed to hold only one particular type of data; it can legally hold that type of data and no other. The compiler will consider it to be\u00aea syntax error if you try to violate this rule. We say that Java is a strongly typed language because it enforces this rule. There are eight so-called primitive types built into Java. The primitive types are named byte\u00ae, short, int, long, float, double, char, and boolean. The first four types hold integers (whole numbers such as 17, -38477, and 0). The four integer types are distinguished by the ranges of integers they\u00aecan hold. The float and double types hold real numbers (such as 3.6 and -145.99). Again, the two real types are distinguished by their range and accuracy. A variable of type char holds a single character from the Unicode character set. And a variable of type boolean holds one of the two\u00aelogical values true or false. Any data value stored in the computer's memory must be represented as a binary number, that is as a string of zeros and ones. A single zero or one is called a bit. A string of eight bits is called a byte. Memory is usually measured in terms of bytes. Not surprisingly, the byte data type refers to\u00aea single byte of memory. A variable of type byte holds a string of eight bits, which can represent any of the integers between -128 and 127, inclusive. (There are 256 integers in that range; eight bits can represent 256 two raised to the power eight different values.) As for the other integer types, short corresponds to two bytes (16 bits). Variables of type short have vrogram, it must be surrounded by single quotes; for example: 'A', '*', or 'x'. Without the quotes, A would be an identifier and * would be a multiplied as a value of type float. (Occasionally, you need to know this because the rules of Java say that you can't assign a value of type double to a variable of type float, so you might be confronted with a ridiculous-seeming errowhy I advise sticking to type double for real numbers.) Even for integer literals, there are some complications."
}

const userData = JSON.parse(atob(sessionStorage.getItem("userData")));